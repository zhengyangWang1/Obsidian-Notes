---
title: 图神经网络
categories:
  - Notes
  - 课程
  - 大三（上）
  - 神经网络与深度学习
tags:
  - 神经网络
date:
---
图是一种描述样本间关系的通用语言
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026095632.png)

## 图神经网络
图神经网络是一种基于图结构数据的深度学习方法，学习图结构数据中的节点特征、边特征、图级表示
- 图是图神经网络研究的基本对象;𝐺=(𝑉,𝐸)
- 是描述复杂事务的数据表示形式，由节点和边组成；
- 可描述不规则数据（非欧式数据），充分利用数据间关系信息
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100249.png)

使用神经网络变换、聚合来自目标节点其邻居的信息，迭代生成节点嵌入表示
1. 对于单个节点，求取邻居节点的表示的均值获得邻居表示
2. 通过一个全连接层对邻居表示和该节点的自身表示进行线性加权
3. 通过非线性激活函数得到该节点的聚合表示
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100526.png)

- 节点在每一层都有一个表示
- 模型可以设计任意多层
- 输入特征可以视为第0层的节点表示
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026101101.png)

### 图卷积神经网络
#### 基于谱域的方法
𝑮=𝑽,𝑬,𝑨,𝑿, 𝑽为节点集合, 𝑬为边集合, 𝐀为邻接矩阵, 𝑿是节点特征矩阵
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102249.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102316.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102345.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102358.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102419.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102735.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102806.png)

#### 基于空间结构的方法
- 对于每个节点，选择固定数量的节点作为其邻近节点
- 按照一定的邻近度量根据邻近度进行排序
- 参数共享
GraphSAGE
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103613.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103647.png)

### 图注意力神经网络
人脑通过注意力来解决信息超载问题
#### 注意力机制
Attention 机制选择一些关键的信息输入进行处理，来提高神经网络的效率。具体通过在模型训练和推断过程中动态调整模型对数据/特征不同部分的权重提升模型的学习效果
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104614.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104623.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104643.png)

#### 自注意力机制
当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，但是只建模了输入信息的局部依赖关系
自注意力模型：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105602.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105742.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105815.png)

#### 多头自注意力模型
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105855.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105910.png)

#### 图注意力网络
GAT 在图神经网络中引入了Attention，关注邻居节点集合中比较重要的部分。GAT 采用masked attention的方式——仅将注意力分配到节点i的邻居节点集𝑁i上，即j ∈𝑁i
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026110938.png)
对于单个节点，将其表示作为查询𝑞，邻居节点的表示作为键和值<𝑘,𝑣>，执行注意力操作的结果即为该节点的聚合表示：
1. 获得第𝑖个节点的查询，获得和其相邻的所有节点的键和值
2. 计算其和所有邻居节点的相关性
- 将查询和键拼接起来，然后通过激活函数为LeakyReLU的全连接神经网络进行变换，最终得到一个代表相关分数的实数
3. 计算归一化注意力得分
4. 以注意力得分为权重，对值进行加权求和，计算节点的聚合表示图注意力网络
5. 最终的聚合结果可以由多个注意力头的结果拼接得到

## 图汇聚
图汇聚（Graph Pooling）：获取一个完整图级别的表示

平面图汇聚：
- 直接从节点表示生成图级别表示
- 最大汇聚、平均汇聚、求和汇聚、注意力汇聚
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111724.png)


层次图汇聚：逐步粗化图得到图级别表示
- 基于降采样的层次图汇聚
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111802.png)

- 基于超节点的层次图汇聚
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111833.png)

## 异构图神经网络
异构信息网络的核心：
1. 元路径（Meta-path）：具有语义含义的路径
2. 语义空间（Semantic Space）

同构图：节点类型和边的类型只有一种的图

异构图：相比于以往的同构图𝐺= (𝑉, 𝐸) ，异构图多了两个属性R、T ，其中R表示边的类型，T表示节点的类型，可以表示为：𝐺= (𝑉, 𝐸, 𝑅, 𝑇)。异构图神经网络的关键在于如何设计合适的聚合函数以捕获邻域所包含的语义。
聚合公式：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112122.png)
异构图神经网络的特点：
1. 不同meta-path 对应不同语义空间
2. 每个语义空间内节点的邻居和信息均不同
3. 节点的最终表征与每个语义空间均有关
异构图注意力网络：
1. 使用meta-path 异构信息网络投影到多个同构图
2. 在每个同构图内使用注意力网络整合邻居信息
3. 对不同meta-path 对应的多个同构图使用全局注意力机制
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112241.png)

## 图神经网络的应用
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112424.png)

### 文本分类
文本分类是自然语言处理（NLP）中的一项基础任务，它的目标是将输入的文本分配给一个或多个类别。常见的文本分类应用包括情感分析、主题分类、垃圾邮件检测等。
面向文本数据的图构建：文本数据（词、文档）不直接包含关系信息，但可以基于统计进行关系构建
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113021.png)

### 知识图谱嵌入
定义：知识图谱嵌入(Knowledge Graph Embeddings, KGEs)，也称为知识图谱表示学习(Knowledge Graph Representing Learning)，是将包含实体和关系的知识图谱的组成部分嵌入到低维度、连续的向量空间中，从而**在保持知识图谱固有结构的同时简化操作**。
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113151.png)

模型概述：
1. 多模态实体嵌入模块：编码器通过多层感知器网络学习不同模态（图片、文本）数据的实体嵌入。
2. 局部对比多关系特征聚合模块：将实体的局部邻域的关系类型区分为多关系子图，通过注意力机制进行关系内、关系间特征聚合。
3. 高阶对比编码模块：显式对比来自实体及其高阶邻居的编码来丰富实体特征。

## 总结
图是一个非常强大的工具，几乎所有的数据都能够表示成图
- 计算机视觉、生物化学、推荐系统、交通、程序验证、程序推理......
然而：
- 在图上进行优化非常困难，因为图是稀疏架构，每个节点的边数高度可变，空间效率低
- 图神经网络对超参数非常敏感