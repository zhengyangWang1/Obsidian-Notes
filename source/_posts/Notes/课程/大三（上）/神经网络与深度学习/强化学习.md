---
title: 强化学习
categories:
  - Notes
  - 课程
  - 大三（上）
  - 神经网络与深度学习
tags:
  - 深度学习
date:
---
### 强化学习
一种试错型学习范式
随即环境，智能体的动作引起环境的变化
评价：包含噪声的延迟奖励
目标：最大化长期累计回报
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102158.png)

#### 状态
状态是用于决定下一步发生什么的信息
形式上，状态是一个关于历史信息的表示
历史是一个状态、动作和奖励组成的序列

#### 环境状态
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102437.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102701.png)


完全可观测性：智能体能够直接观测到环境状态$O_t = s_t$
部分可观测性：智能体间接观察环境$O_t != s_t$

#### 目标
智能体的目标：最大化其收到的奖励总和
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103205.png)


### 马尔可夫决策过程
理想情况下，一个状态应该总结过去的“经历”，以便保留所有必要的信息，也就是说，它应该具有马尔可夫性：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103615.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104447.png)

#### 策略
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104526.png)

![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105033.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105039.png)

#### 贝尔曼等式
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105248.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105313.png)

#### 策略改进
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111336.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111525.png)
