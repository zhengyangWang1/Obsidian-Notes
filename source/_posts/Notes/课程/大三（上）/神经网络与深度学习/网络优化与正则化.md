---
categories:
  - Notes
  - 课程
  - 大三（上）
  - 神经网络与深度学习
title: 网络优化与正则化
tags:
  - 神经网络
date:
---
### 激活函数


#### 梯度截断：防止梯度爆炸



#### 学习率衰减
- 余弦衰减：不引入任何参数
- 分段衰减
- 逆时衰减
- 指数衰减
- 自然指数衰减

#### 批量大小
批量的大小不影响随机梯度的期望，但是会影响随机梯度的方差
- 批量越大，随机梯度的方差越小，训练稳定，可以设置较大的学习率
- 批量越小，设置小的学习率

### 优化方法
#### 动量梯度下降法

#### RMSProp

#### Adam

### 参数初始化
#### 权重全零初始化

#### 使用较小的随机值初始化权重
从均值等于0，方差等于0.01的高斯分布中采样
```
W = 0.01 * np.random.randn(Din, Dout)
```
适合层数较少的神经网络

- Sigmoid或Tanh函数使用Xavier初始化
- relu使用kaiming初始化

### 损失函数
#### 交叉熵

### 归一化

归一化（Normalization）方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到[0, 1]或[−1, 1]区间内，或者映射为服从均值为0、方差为1 的标准正态分布

#### 逐层归一化
##### 批量归一化

### 网络正则化

