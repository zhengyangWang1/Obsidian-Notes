---
title: 深度学习
categories:
  - Notes
  - 课程
  - 智能计算系统
tags:
  - 深度学习
date:
---
### 卷积神经网络（CNN）
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019081247.png)
- 局部连接：一个像素与它周围的几个像素有很强的联系，但是与离它很远的像素联系可能很弱
- 权重共享：卷积神经网络中，权重又称为卷积模板，用于表达一种图像的特征。在图像的不同位置找特征，可以使用一样的卷积模板

### CNN组成：
- 卷积层
- 池化层
- 全连接层
- Softmax
#### 卷积层
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019082709.png)

边界扩充（padding）
- 扩大图像的尺寸并填充像素
- 防止深度网络中图像被动持续减小
- 强化图像边缘信息
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084544.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084557.png)

#### 池化层
Max Pooling / Avg Pooling / L2 Pooling
提取卷积核中的最大值/平均值/L2范数

- 主动减小图片尺寸，从而减少参数的数量和计算量，控制过拟合
- 不引入额外参数

#### 全连接层
- 卷积层和池化层构成特征提取器，全连接层则为分类器
- 将特征提取得到的高维特征图映射成一维特征向量，该特征向量包含 所有特征信息，可转化为各个类别的概率。

#### softmax
- 通常作为网络的最后一层，对输出进行归一化，输出分类概率
- 凸显其中最大的值并抑制远低于最大值的其他分量
- Softmax层输入、输出数据规模相同

### 序列模型：循环神经网络
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031164605.png)

- 时序sequence：RNN能建模序列数据，序列指的是前、后输入数据( 𝒙 (𝑡) ， 𝒙 (𝑡+1) )不独立，相互影响； 
- 循环recurrent：对每个输入的操作都是一样的，循环往复地重复这些相 同操作，每时刻有相同参数W和U（参数共享）；
- 记忆memory： 隐藏层𝒉 (𝑡)中捕捉了所有时刻t之前的信息，理论上𝒉 (𝑡)记 忆的内容可以无限长，然而实际上记忆还是有限的；

#### 正向计算过程
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165458.png)

#### 反向传播
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165539.png)

#### RNN的梯度爆炸和梯度消失
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165725.png)
由于梯度爆炸或消失的存在，循环神经网络实际上只能学习到短期的依赖关系，无法处理长期依赖关系

改进梯度爆炸：梯度截断
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170209.png)

### 长短期记忆模型（LSTM）
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170726.png)

- 隐藏状态：作为神经网络的记忆，保存着网络先前观察到的数据信息。
- 单元状态：类似信息传送带，它贯穿整个链条，只有一些小的线性相互作用；这很容 易让信息以不变的方式向下流动；LSTM有能力向单元状态中移除或添加信息，这种管理结构称为门限

![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170856.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170929.png)

#### 变体
##### 窥视孔连接
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171128.png)

##### 耦合输入门和遗忘门
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171150.png)

##### GRU
在LSTM的基础上，将单元状态和隐藏状态合并，将遗忘门和输入门合并为更新门，无输出门。更新门决定历史信息和当前信息如何相加；重置门决定保留多少历史信息进入当前信息
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171221.png)

##### LSTM与GRU
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171330.png)
GRU参数量更少，训练速度快；在训练数据足够的情况下，LSTM的表征能力更强。

### 生成对抗网络（GAN）
损失函数通过学习得到，网络学习到的是数据的分布
