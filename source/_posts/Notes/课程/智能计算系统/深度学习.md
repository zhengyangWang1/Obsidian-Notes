---
title: 深度学习
categories:
  - Notes
  - 课程
  - 智能计算系统
tags:
  - 深度学习
date:
---
### 卷积神经网络（CNN）
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019081247.png)
- 局部连接：一个像素与它周围的几个像素有很强的联系，但是与离它很远的像素联系可能很弱
- 权重共享：卷积神经网络中，权重又称为卷积模板，用于表达一种图像的特征。在图像的不同位置找特征，可以使用一样的卷积模板

### CNN组成：
- 卷积层
- 池化层
- 全连接层
- Softmax
#### 卷积层
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019082709.png)

边界扩充（padding）
- 扩大图像的尺寸并填充像素
- 防止深度网络中图像被动持续减小
- 强化图像边缘信息
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084544.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084557.png)

#### 池化层
Max Pooling / Avg Pooling / L2 Pooling
提取卷积核中的最大值/平均值/L2范数

- 主动减小图片尺寸，从而减少参数的数量和计算量，控制过拟合
- 不引入额外参数

#### 全连接层
- 卷积层和池化层构成特征提取器，全连接层则为分类器
- 将特征提取得到的高维特征图映射成一维特征向量，该特征向量包含 所有特征信息，可转化为各个类别的概率。

#### softmax
- 通常作为网络的最后一层，对输出进行归一化，输出分类概率
- 凸显其中最大的值并抑制远低于最大值的其他分量
- Softmax层输入、输出数据规模相同

### 序列模型：循环神经网络
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031164605.png)

- 时序sequence：RNN能建模序列数据，序列指的是前、后输入数据( 𝒙 (𝑡) ， 𝒙 (𝑡+1) )不独立，相互影响； 
- 循环recurrent：对每个输入的操作都是一样的，循环往复地重复这些相 同操作，每时刻有相同参数W和U（参数共享）；
- 记忆memory： 隐藏层𝒉 (𝑡)中捕捉了所有时刻t之前的信息，理论上𝒉 (𝑡)记 忆的内容可以无限长，然而实际上记忆还是有限的；

#### 正向计算过程
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165458.png)

#### 反向传播
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165539.png)

#### RNN的梯度爆炸和梯度消失
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165725.png)
由于梯度爆炸或消失的存在，循环神经网络实际上只能学习到短期的依赖关系，无法处理长期依赖关系

改进梯度爆炸：梯度截断
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170209.png)

### 长短期记忆模型（LSTM）
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170726.png)

- 隐藏状态：作为神经网络的记忆，保存着网络先前观察到的数据信息。
- 单元状态：类似信息传送带，它贯穿整个链条，只有一些小的线性相互作用；这很容 易让信息以不变的方式向下流动；LSTM有能力向单元状态中移除或添加信息，这种管理结构称为门限

![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170856.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170929.png)

#### 变体
##### 窥视孔连接
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171128.png)

##### 耦合输入门和遗忘门
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171150.png)

##### GRU
在LSTM的基础上，将单元状态和隐藏状态合并，将遗忘门和输入门合并为更新门，无输出门。更新门决定历史信息和当前信息如何相加；重置门决定保留多少历史信息进入当前信息
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171221.png)

##### LSTM与GRU
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171330.png)
GRU参数量更少，训练速度快；在训练数据足够的情况下，LSTM的表征能力更强。

### 生成对抗网络（GAN）
损失函数通过学习得到，网络学习到的是数据的分布
模型由两部分组成：
- 生成器（伪装者）：找出观测数据内部的统计规律，尽可能生成 能够以假乱真的样本，使判别网络输出接近0.5，难以区分真假。
- 判别器（警察）：判断输入数据是来自真实样本集还是生成样本 集。如果输入是真样本，输出接近1；如果输入是生成样本，输出接近0。

#### 训练过程
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031173437.png)

更新k次判别器后更新一次生成器
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174016.png)

解决梯度消失问题：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174326.png)

#### 模式崩溃（model collapse）
产生原因： GAN的损失函数使判别器假样本的惩罚是巨大的，一旦生成的某一类 假样本成功骗过判别器，生成器就趋向于生成相似的样本，导致生成样本缺乏多样性
应对方法（WGAN）：采用更加平滑的损失函数，参见Wasserstein GAN

#### GAN结构变种
卷积GAN
- DCGAN：将GAN中全连接神经网络扩展到卷积神经网络
- ResGAN：图像恢复，ResNet
- SRGAN：超分辨率，ResNet
- CycleGAN：图像转换
条件GAN
- CGAN
- InfoGAN
集成推断模型的GAN
- BiGAN
对抗自编码器
- VAE-GAN

### 图像风格迁移
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109080759.png)
给定一张风格图像 a 和一张内容图像 p:
- 风格图像 a 经过 CNN 生成的 feature maps 组成风格特征集A
- 内容图像 p 通过 CNN 生成的 feature maps 组成内容特征集P
- 输入一张随机噪声图像 x，随机噪声图像 x 通过 CNN 生成的 feature maps 构 成内容特征和风格特征集合 F和 G ，目标损失函数由 A, P, F ,G 计算得到
- 优化函数是希望调整图像 x，使其最后看起来既保持内容图像 p的内容, 又有风 格图像 a 的风格。
- CNN网络为在imageNet上训练好的VGG19， 去除了最后的全连接层和softmax
- 损失函数：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081123.png)![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081133.png)

