---
categories:
  - Notes
  - 课程
  - 神经网络与深度学习
title: 前馈神经网络
date: 
tags:
---
### 神经网络

#### 激活函数
**激活函数：最关键部分**
- 激活函数：连续并可导的非线性函数
- 激活函数及其导函数要尽可能简单
- 激活函数的导函数要在一个合适的区间内
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921102752.png)

##### Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数
>![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105310.png)

- Logistic函数：Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到 (0, 1)
- Tanh函数：Tanh函数可以看作放大并平移的Logistic函数，其值域是(−1, 1)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105110.png)
Tanh 函数的输出是零中心化的（Zero-Centered），而 Logistic 函数的输出恒大于 0．非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下 降的收敛速度变慢．
- Hard-Logistic函数和Hard-Tanh函数：分段函数来近似Logistic和Tanh

##### ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）
ReLU为左饱和函数，在 𝑥 > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。
>死亡 ReLU 问题：ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当更新后，第一个隐藏层中的某个 ReLU 神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活。

为避免ReLU的问题，有几种ReLU的变种
- 带泄露的ReLU（Leaky ReLU）：在输入 𝑥 < 0时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活
- 带参数的ReLU（Parametric ReLU，PReLU）：引入一个可学习的参数
- ELU（Exponential Linear Unit，指数线性单元）
- Softplus 函数

神经网络的要素：
- 拓扑结构：前馈网络，反馈网络，图网络。
前馈神经网络（FNN），循环神经网络（RNN）

- 网络的表示
- 学习算法

**前馈神经网络**：
一层的输出作用于下一层的输入
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921104850.png)![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921104913.png)
向量表示：
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921105212.png)

**输出层**： 根据任务确定输出层的激活函数
- 回归任务：根据输出的值域选择激活函数
- 分类任务：softmax函数
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110000.png)

**反向传播算法：**
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110918.png)
J1为结果的误差，根据结果误差更新参数
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921111754.png)
需要会推导

均方误差会使深层的反向传播训练缓慢

神经网络具有通用近似性，只要神经元数量足够，可以拟合任何函数

非凸优化问题
梯度消失问题