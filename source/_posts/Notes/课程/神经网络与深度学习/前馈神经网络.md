---
categories:
  - Notes
  - 课程
  - 神经网络与深度学习
title: 前馈神经网络
date: 
tags:
---
### 神经网络

#### 激活函数
**激活函数：最关键部分**
- 激活函数：连续并可导的非线性函数
- 激活函数及其导函数要尽可能简单
- 激活函数的导函数要在一个合适的区间内
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921102752.png)

**Sigmoid型函数**：指一类S型曲线函数，为两端饱和函数
>![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105310.png)

- Logistic函数：Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到 (0, 1)
- Tanh函数：Tanh函数可以看作放大并平移的Logistic函数，其值域是(−1, 1)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105110.png)
Tanh 函数的输出是零中心化的（Zero-Centered），而 Logistic 函数的输出恒大于 0．非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下 降的收敛速度变慢．



神经网络的要素：
- 拓扑结构：前馈网络，反馈网络，图网络。
前馈神经网络（FNN），循环神经网络（RNN）

- 网络的表示
- 学习算法

**前馈神经网络**：
一层的输出作用于下一层的输入
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921104850.png)![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921104913.png)
向量表示：
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921105212.png)

**输出层**： 根据任务确定输出层的激活函数
- 回归任务：根据输出的值域选择激活函数
- 分类任务：softmax函数
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110000.png)

**反向传播算法：**
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110918.png)
J1为结果的误差，根据结果误差更新参数
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921111754.png)
需要会推导

均方误差会使深层的反向传播训练缓慢

神经网络具有通用近似性，只要神经元数量足够，可以拟合任何函数

非凸优化问题
梯度消失问题