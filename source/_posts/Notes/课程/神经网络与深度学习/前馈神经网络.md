---
categories:
  - Notes
  - 课程
  - 神经网络与深度学习
title: 前馈神经网络
date: 
tags:
---

### 激活函数
**激活函数：最关键部分**
- 激活函数：连续并可导的非线性函数
- 激活函数及其导函数要尽可能简单
- 激活函数的导函数要在一个合适的区间内
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921102752.png)

#### Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数
>![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105310.png)

- Logistic函数：Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到 (0, 1)
- Tanh函数：Tanh函数可以看作放大并平移的Logistic函数，其值域是(−1, 1)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105110.png)
Tanh 函数的输出是零中心化的（Zero-Centered），而 Logistic 函数的输出恒大于 0．非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下 降的收敛速度变慢．
- Hard-Logistic函数和Hard-Tanh函数：分段函数来近似Logistic和Tanh

#### ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）
ReLU为左饱和函数，在 𝑥 > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。
>死亡 ReLU 问题：ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当更新后，第一个隐藏层中的某个 ReLU 神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活。

为避免ReLU的问题，有几种ReLU的变种
- 带泄露的ReLU（Leaky ReLU）：在输入 𝑥 < 0时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活
- 带参数的ReLU（Parametric ReLU，PReLU）：引入一个可学习的参数
- ELU（Exponential Linear Unit，指数线性单元）
- Softplus 函数
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924110331.png)

### 网络结构

- **前馈网络**：整个网络中的信息是朝一个方向传播，没有反向的信息传播。包括全连接前馈网络、卷积神经网络等。
- **反馈网络（记忆网络）**：和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递。
- **图网络**：图网络是定义在图结构数据上的神经网络，图中每个节点都由 一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个 节点可以收到来自相邻节点或自身的信息


### 前馈神经网络
每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层．第0层称为输入层，最后一层称为输出层，其他中间层称为隐藏层．整个网络中无反馈，信号从输入层向输出层单向传播。
传播公式：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112107.png)

通用近似定理：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112220.png)
根据通用近似定理，前馈神经网络可以以任意精度拟合任意di

**输出层**： 根据任务确定输出层的激活函数
- 回归任务：根据输出的值域选择激活函数
- 分类任务：softmax函数
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110000.png)

**反向传播算法：**
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110918.png)
J1为结果的误差，根据结果误差更新参数
![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921111754.png)
需要会推导

均方误差会使深层的反向传播训练缓慢

神经网络具有通用近似性，只要神经元数量足够，可以拟合任何函数

非凸优化问题
梯度消失问题