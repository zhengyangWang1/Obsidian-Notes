---
title: A6分析与简答
categories:
  - Notes
  - 课程
  - 神经网络与深度学习
tags:
  - 深度学习
  - 神经网络
date:
---
### A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用。
[一文读懂卷积神经网络中的1x1卷积核 - 知乎](https://zhuanlan.zhihu.com/p/40050371)

1. 改变通道数
	当卷积核的个数大于或小于输入通道数时，可以改变输出的通道数
2. 增加非线性 
	1x1卷积核可以在不改变特征维度的情况下添加非线性激活
### A6.2 (5分) 计算函数$𝑦 = max(𝑥_1,⋯,𝑥_𝐷)$和函数$𝑦 = argmax(𝑥_1,⋯,𝑥_𝐷)$的梯度。
$𝑦 = max(𝑥_1,⋯,𝑥_𝐷)$：如果对于输入向量中的某个元素$𝑥_𝑖$，它是最大值，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为
$$
\frac{\partial y}{\partial x_{i}}=\left\{\begin{array}{ll}
1, & \text { if } x_{i}=max(𝑥_1,⋯,𝑥_𝐷) \\
0, & \text { otherwise }
\end{array}\right.
$$
argmax(𝑥_1,⋯,𝑥_𝐷)输出的是使得函数取得最大值的元素的索引位置.如果对于输入向量中的某个元素𝑥𝑖，它的索引位置与最大值所在的位置相同，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为
$$
\frac{\partial y}{\partial x_{i}}=\left\{\begin{array}{ll}
1, & \text { if } i=argmax(𝑥_1,⋯,𝑥_𝐷) \\
0, & \text { otherwise }
\end{array}\right.
$$
### A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。
[[循环神经网络#长短时记忆网络（LSTM）]]
[LSTM - 长短期记忆递归神经网络 - 知乎](https://zhuanlan.zhihu.com/p/123857569)
[Fetching Title#6dum](https://zhuanlan.zhihu.com/p/136223550)

遗忘门
$$f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)$$

记忆门
$$\begin{array}{l}
i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right) \\
\widetilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right) \\

\end{array}$$
输出门
$$\begin{array}{l}
o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right.) \\
h_{t}=o_{t} * \tanh \left(C_{t}\right)
\end{array}$$
单元状态更新
$$C_{t}=f_{t} * C_{t-1}+i_{t} * \widetilde{C_{t}}$$

LSTM 通过记忆单元 C 来缓解梯度消失问题。针对 $\frac{\partial C^{(k)}}{\partial C^{(k-1)}}$ 求得，
$$
\begin{aligned}
\frac{\partial C^{(k)}}{\partial C^{(k-1)}}= & \frac{\partial C^{(k)}}{\partial f^{(k)}} \frac{\partial f^{(k)}}{\partial h^{(k-1)}} \frac{\partial h^{(k-1)}}{\partial C^{(k-1)}}+\frac{\partial C^{(k)}}{\partial i^{(k)}} \frac{\partial i^{(k)}}{\partial h^{(k-1)}} \frac{\partial h^{(k-1)}}{\partial C^{(k-1)}} \\
& +\frac{\partial C^{(k)}}{\partial a^{(k)}} \frac{\partial a^{(k)}}{\partial h^{(k-1)}} \frac{\partial h^{(k-1)}}{\partial C^{(k-1)}}+\frac{\partial C^{(k)}}{\partial C^{(k-1)}}
\end{aligned}
$$

具体计算后得到，
$$
\begin{array}{c}
\frac{\partial C^{(k)}}{\partial C^{(k-1)}}=C^{(k-1)} \sigma^{\prime}(\cdot) W_{f} o^{(k-1)} \tanh ^{\prime}\left(C^{(k-1)}\right) \\
+a^{(k)} \sigma^{\prime}(\cdot) W_{i} o^{(k-1)} \tanh ^{\prime}\left(C^{(k-1)}\right) \\
+i^{(k)} \tanh ^{\prime}(\cdot) W_{c} * o^{(k-1)} \tanh ^{\prime}\left(C^{(k-1)}\right) \\
+f^{(t)} \\
\prod_{k=t+1}^{T} \frac{\partial C^{(k)}}{\partial C^{(k-1)}}=\left(f^{(k)} f^{(k+1)} \ldots f^{(T)}\right)+\text { other }
\end{array}
$$

在LSTM迭代过程中，针对  $\prod_{k=t+1}^{T} \frac{\partial C^{(k)}}{\partial C^{(k-1)}}$  而言，每一步  $\frac{\partial C^{(k)}}{\partial C^{(k-1)}}$  可以自主的选择在  $[0,1]$  之间，或者大于1，因为  $f^{(k)}$  是可训练学习的。那么整体  $\prod_{k=t+1}^{T} \frac{\partial C^{(k)}}{\partial C^{(k-1)}}$  也就不会一直减小，远距离梯度不至于完全消失，也就能够解决RNN中存在的梯度消失问题。
### A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。
