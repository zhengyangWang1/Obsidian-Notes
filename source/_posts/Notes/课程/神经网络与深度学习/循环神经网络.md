---
title: 循环神经网络
categories:
  - Notes
  - 课程
  - 神经网络与深度学习
tags:
  - 神经网络
date:
---
### 概述

![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019100538.png)

特点：不同于在样本上做多个独立预测，而是假设样本之间存在关联，进而在样本序列上做预测

采用链式法则表示一个观测序列的联合概率：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101449.png)
考虑一个观测和所有历史观测之间的依赖关系复杂度随着观测个数指数级增长

马尔可夫模型：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101541.png)
马尔可夫模型假设当前观察只和较近的观测有关

考虑两个不同的序列（如输入和输出序列），可以使用隐马尔可夫模型：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101946.png)

联合分布为：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102434.png)

最可能的隐状态为：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102453.png)

循环神经网络（Recurrent neural networks，RNN）：
- HMM是一个生成模型，RNN是一个判别模型

### 循环神经网络（RNN）
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103834.png)
RNN可以被认为是同一网络的多个副本，每个副本都将消息传递给后者
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103930.png)
可以保留序列前面的特征，但无法建立与序列后面的联系

双向RNN（BRNN）除了前向建模，还添加了一层后向建模
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104056.png)

#### 形式化表示
在每个时间步应用递归公式：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104638.png)

T个时间步的递归：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104956.png)

### 随时间反向传播（BPTT）
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111347.png)
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111406.png)
用链式法则：
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111434.png)
一直传播到t=0
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111500.png)

### 梯度弥散和梯度爆炸


### 长短时记忆网络（LSTM）
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019112925.png)
