---
title: 分析与简答
categories:
  - Notes
  - 课程
  - 神经网络与深度学习
tags:
  - 深度学习
date:
---
• A2.1 (5 points) 尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch_size的选择依据和影响。
Epoch代表全部的训练数据在模型中训练的次数
Iteration表示在一个Epoch中参数更新的次数
Batch表示一次正向和反向传播中的一组数据样本
batch_size表示Batch中的数据量，不考虑硬件因素，小的batch_size可能会引入噪声，但也可能使训练过程更稳定


• A2.2 (5 points) 以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。 



• A2.3 (5 points) 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。

• A2.4 (5 points) 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然

• A2.5 (5 points) 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。

• A2.6 (5 points) 分析Batch normalization对参数优化起到什么作用、如何起到这种作用。