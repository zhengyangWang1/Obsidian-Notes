---
title: 扩散模型调研
categories:
  - Notes
  - 论文
tags:
  - 扩散模型
date:
---

12.27 
调研AI发展中的表格类数据研究趋势
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231227125526.png)





看扩散模型[超详细的扩散模型（Diffusion Models）原理+代码 - 知乎](https://zhuanlan.zhihu.com/p/624221952)
看TabDDPM
![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104173325.png)

看Improved DDPM
看Generative models for tabular problems、ddpm diffusion model相关检索。

### Accelerating Diffusion Models via Early Stop of the Diffusion Process
#### 2022 CVPR [2205.12524.pdf](https://arxiv.org/pdf/2205.12524.pdf)
在实际应用中，DDPM往往需要数百甚至数千个去噪步骤才能从高斯噪声中获得高质量的样本，从而导致推理效率极低。
在这项工作中，我们提出了一种针对 DDPM 的原则性加速策略，称为 Early-Stopped DDPM (ES-DDPM)。关键思想是尽早停止扩散过程，其中仅考虑少数初始扩散步骤，并且反向降噪过程从非高斯分布开始。 ES-DDPM中进一步**采用强大的预训练生成模型，如GAN和VAE，通过扩散从预训练生成模型获得的样本**，可以有效地实现对目标非高斯分布的采样。通过这种方式，所需的去噪步骤数量显着减少。

为了最大化数据对数似然 log p(x0)，我们需要最小化两个损失项：LDDPM 和LVAE。我们将证明，这意味着我们可以分别训练 ES-DDPM 和变分自动编码器（VAE），然后将它们组合在一起以获得新的生成模型。
分别训练ES-DDPM和VAE后，我们可以将它们组合起来形成新的生成模型。我们按照方程 5 生成样本，如图 1 所示：首先从标准高斯分布中采样 z，然后使用 VAE 的解码器生成图像 fφ(z)。接下来，从公式 11 中描述的分布中采样 xT '。最后，使用 ES-DDPM 从 pθ(xt−1|xt) 中采样 xt−1，其中 t = T ', T ' − 1, · · · , 1 ，并输出生成的图像x0。注意，在上面的采样过程中，我们只需要使用解码器fφ(z)，而没有使用编码器qψ(z|x0)。这意味着我们不仅可以将 ES-DDPM 与 VAE 结合起来，还可以与任何可以将潜在代码 z 映射到干净图像的生成模型（例如 GAN）结合起来。由于 GAN 训练完成后，它的生成器非常类似于 VAE 的解码器，只是生成器的编码器是未知的，但我们根本不需要编码器从组合模型生成样本。因此，我们也可以将 fφ(z) 视为经过训练的 GAN 的生成器。

### Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality
### 2022 ICLR [Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality | OpenReview](https://openreview.net/forum?id=VFBjuF8HEp)
**通过区分样本质量进行快速采样**
我们引入可微扩散采样器搜索（DDSS）：一种通过区分样本质量分数来优化任何预训练扩散模型的快速采样器的方法。我们还提出了广义高斯扩散模型（GGDM），这是一系列用于扩散模型的灵活非马尔可夫采样器。我们表明，通过梯度下降最大化样本质量分数来优化 GGDM 采样器的自由度可以提高样本质量。我们的优化过程使用重参数化技巧和梯度重物化通过采样过程进行反向传播。

### Learning to Efﬁciently Sample from Diffusion Probabilistic Models
**引入了一种动态规划算法，减少推理步数，进行快速采样**
在这项工作中，我们将推理调度路径的选择视为一个独立的优化问题，其中我们尝试学习最佳调度。我们的方法依赖于动态规划算法，在给定 K 个细化步骤的固定预算和预先训练的 DDPM 的情况下，我们找到最大化优化目标 (ELBO) 的时间步集。作为优化目标，ELBO 具有关键的可分解性属性：总 ELBO 是各个 KL 项的总和，对于任何两个推理路径，如果时间步 (s, t) 连续出现在两者中，则它们共享一个共同的 KL 项，因此承认记忆化。
引入了一种动态规划算法，该算法基于 ELBO 为 K 细化步骤的所有可能计算预算找到最佳推理路径。该算法搜索 T > K 个时间步长，仅需要 O(T ) 神经网络前向传递。它只需要对预训练的DDPM 应用一次，不需要训练或重新训练DDPM，并且适用于时间离散和时间连续的DDPM。


### Deep Equilibrium Approaches to Diffusion Models

从不同的角度（深度）平衡（DEQ）定点模型来研究扩散模型。具体来说，我们扩展了最近的去噪扩散隐式模型（DDIM）[68]，并将整个采样链建模为联合多变量定点系统。该设置提供了扩散和平衡模型的优雅统一，并显示出以下优点：1）单图像采样，因为它用并行采样过程取代了全串行典型采样过程； 2）模型反演，我们可以利用 DEQ 设置中的快速梯度来更快地找到生成给定图像的噪声。该方法也是正交的，因此与用于减少采样时间或改进模型反演的其他方法互补。

### Improved Denoising Diffusion Probabilistic Models
在本文中，我们证明 DDPM 可以实现与其他基于似然的模型竞争的对数似然，即使在 ImageNet 等高多样性数据集上也是如此。为了更紧密地优化变分下界（VLB），我们使用简单的重新参数化和混合学习目标来学习逆向过程方差，该混合学习目标将 VLB 与 Ho 等人的简化目标相结合。我们令人惊讶地发现，通过我们的混合目标，我们的模型比直接优化对数似然获得的模型获得了更好的对数似然，并且发现后一个目标在训练过程中具有更多的梯度噪声。我们证明了一种简单的重要性采样技术可以减少这种噪声，并使我们能够比混合目标获得更好的对数似然。在将学习到的方差合并到我们的模型中后，我们惊讶地发现我们可以用很少的时间从我们的模型中**以更少的步骤进行采样**。