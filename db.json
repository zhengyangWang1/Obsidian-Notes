{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-fluid/source/css/gitalk.css","path":"css/gitalk.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight-dark.styl","path":"css/highlight-dark.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.css","path":"dist/APlayer.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.js","path":"dist/APlayer.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.css.map","path":"dist/APlayer.min.css.map","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.js.map","path":"dist/APlayer.min.js.map","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/dist/music.js","path":"dist/music.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/fluid.png","path":"img/fluid.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/xml/local-search.xml","path":"xml/local-search.xml","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/color-schema.js","path":"js/color-schema.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/boot.js","path":"js/boot.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/events.js","path":"js/events.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/leancloud.js","path":"js/leancloud.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/img-lazyload.js","path":"js/img-lazyload.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/plugins.js","path":"js/plugins.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-fluid/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/about/index.md","hash":"d52099ffc13917b040e988afd8c757bd1891c173","modified":1695827026322},{"_id":"source/_posts/Notes/AI/AI绘画.md","hash":"33b4524ff31558469334abf0901df58b337d64c4","modified":1699170319816},{"_id":"source/_posts/Notes/TODO.md","hash":"6910c9bd713cfb016a97c1029dc29f744f1b2b9c","modified":1698801039865},{"_id":"source/_posts/Notes/Git/Obsidian Git使用.md","hash":"ccf71aff6f4d4c974f4bf911d84659b5e2520a09","modified":1695817988777},{"_id":"source/_posts/Notes/Git/连接Github.md","hash":"732a77da6558aca927302c03766ebb350f34949c","modified":1695817974484},{"_id":"source/_posts/Notes/Git/Git操作.md","hash":"e5ca13143560e4babc7c88d3d641f0ed6c687b5f","modified":1695817969723},{"_id":"source/_posts/Notes/Linux学习/Shell编程.md","hash":"bf10b69bd06a2c4ec87becf6055c2438c1600b99","modified":1695818028737},{"_id":"source/_posts/Notes/Linux学习/基本命令.md","hash":"aa2b7cfd2d7a7eab55ce2384be5234c336b8b1d1","modified":1695818008253},{"_id":"source/_posts/Notes/Linux学习/文件系统.md","hash":"06a7e2a10257c0701058c8ea0a2f050e39dd4946","modified":1695818022344},{"_id":"source/_posts/Notes/Linux学习/文本编辑器.md","hash":"5bb2f3318821632e89bdb4693e60f5bfa1c3d4e2","modified":1695818017657},{"_id":"source/_posts/Notes/Linux学习/程序与进程管理.md","hash":"daa93f0446b4f904a4a89c0960a39f79358f1fe6","modified":1695818001281},{"_id":"source/_posts/Notes/Ob插件/Advanced Tables 表格绘制.md","hash":"4d55e78df3456061b672e8a1396edae258b905a3","modified":1695818075346},{"_id":"source/_posts/Notes/旅游/景点.md","hash":"03dd7208bd64d7c0434cbc1d3ff7777f88ef5b88","modified":1695817898107},{"_id":"source/_posts/Notes/旅游/美食.md","hash":"3e9dc07a80c618e75126049f8857846b2ec35d62","modified":1695817902877},{"_id":"source/_posts/Notes/旅游/西安8.16-20.md","hash":"dc339331e3d91da91b66cf850621b6f2f1f15edf","modified":1695817908879},{"_id":"source/_posts/Notes/花里胡哨/PowerShell美化.md","hash":"e49c17f2496f735f0d6230914df826b892909ba5","modified":1695817654964},{"_id":"source/_posts/Notes/编程/WSL配置.md","hash":"14b40a7f47de79e9e2e77e5c4ce6e9e7582581a3","modified":1695818127361},{"_id":"source/_posts/Notes/花里胡哨/hexo+Obsidian+github博客.md","hash":"c5a8770d67295b899e90eaeb8f01895131375684","modified":1695817821126},{"_id":"source/_posts/Notes/课程/智能计算系统.md","hash":"81723706080ba9fda5cf9ee7452ded0838d8e96e","modified":1695817878862},{"_id":"source/_posts/Notes/论文/扩散模型论文todo（已过期）.md","hash":"2f7c1cecbf69f5919b1527819f826d9e1c03cabd","modified":1695817864414},{"_id":"source/_posts/Notes/编程/Django/First Class.md","hash":"ec65b9ffe0e51780a06a3fe575cbb86d00fd7ebf","modified":1695817840435},{"_id":"source/_posts/Notes/编程/MySQL/安装MySQL.md","hash":"31bdc1d5cfe4799e6f1b6779cd6420abc29bea5a","modified":1695818112198},{"_id":"source/_posts/Notes/编程/MySQL/MySQL基本使用.md","hash":"052741addf9c57c89650c193819612af6ad88d55","modified":1696822540611},{"_id":"source/_posts/Notes/课程/数据库/SQL.md","hash":"30746671d4c4906d188142d55e45e4458177bf42","modified":1696824775419},{"_id":"source/_posts/Notes/课程/数据库/数据库.md","hash":"5a45a372d222690027ec0f030c78b810c2dc117f","modified":1704309285063},{"_id":"source/_posts/Notes/课程/软件工程/作业1 课后阅读.md","hash":"93fc1a604b6f92398b81a8b759c02765fbbec441","modified":1695817710445},{"_id":"source/_posts/Notes/课程/软件工程/大作业 酒店温控计费系统 规划.md","hash":"79b1d5fcce7b502e21e3fed99ab1796d8749c112","modified":1703423064342},{"_id":"source/_posts/Notes/课程/软件工程/软件工程.md","hash":"8ab47dc2252976f85c5d6570ff567d57cafe38a4","modified":1695817695604},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/神经网络与深度学习.md","hash":"6788086276966ba639e3bffdb7f246fe8f1b5a4a","modified":1700103148673},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/前馈神经网络.md","hash":"cdf66288c024401a273d0107af22ca6d1c4887e8","modified":1696944195842},{"_id":"source/_posts/Notes/课程/操作系统/Blitz软件.md","hash":"ef32e0704c90fb7011c51f5a204ceb5921aa1a01","modified":1695817675159},{"_id":"source/_posts/Notes/课程/操作系统/操作系统.md","hash":"782369867ff10b09ed286eaf16d9c622d8776bcb","modified":1698936513948},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/languages/de.yml","hash":"0e7d455d9e004ff15d8924b7a0c35cea25ee5b1d","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/LICENSE","hash":"26f9356fd6e84b5a88df6d9014378f41b65ba209","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/README.md","hash":"49f681a203eecfa7127ac22edc13bd3b49693d0a","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/package.json","hash":"b2c283d4e9aaf9ba49b8abb81adc03117b0e07db","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/_config.yml","hash":"0f016fb33f6d5866ab79e5ccdc7f951026ab1315","modified":1695549270212},{"_id":"node_modules/hexo-theme-fluid/languages/en.yml","hash":"cb11b39f44ea069652c9647179606b6cecc98d50","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/languages/eo.yml","hash":"a556251cc50a5680578c03f1efbf252b1f4ab860","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/languages/es.yml","hash":"7112594259c88c04714be152af7fd377687dad40","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/languages/ja.yml","hash":"3dd6d20f8d26585a7c154a8e59fe8d5d902f4c6a","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/languages/ru.yml","hash":"7dc78f22696649a4c68dc65a9b52d9a992fa82a0","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/languages/zh-CN.yml","hash":"f96a22f989897ecddc69d5867a206e1cf6b8f610","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/languages/zh-HK.yml","hash":"80ed400a7adaa92ea54fc7f5d534c9af795bed00","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/layout/404.ejs","hash":"b84d575c7b7f778b4cb64e89ad3d0aed4a896820","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/layout/about.ejs","hash":"163bee643e6a38912d3ae70923c83c48d57222e7","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/languages/zh-TW.yml","hash":"596d031dff3826ae8e4ffc8931fff28977b73247","modified":1695121494262},{"_id":"node_modules/hexo-theme-fluid/layout/.DS_Store","hash":"e2295dbe42d85b294e6f3aeefaf3623bd31759ed","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/layout/categories.ejs","hash":"13859726c27b6c79b5876ec174176d0f9c1ee164","modified":1695121494178},{"_id":"node_modules/hexo-theme-fluid/layout/archive.ejs","hash":"7c1f44005849791feae4abaa10fae4cb983d3277","modified":1695121494172},{"_id":"node_modules/hexo-theme-fluid/layout/category.ejs","hash":"f099161b738a16a32253f42085b5444f902018ed","modified":1695121494180},{"_id":"node_modules/hexo-theme-fluid/layout/index.ejs","hash":"9b4c154462ce78de4c9ea7dd15dce4ca8e8c1cf8","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/layout.ejs","hash":"7e0023474128fbe4d68c467704c41f1712432415","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/links.ejs","hash":"1cac32ec4579aaf7b9fa39d317497331d4c5e1dd","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/page.ejs","hash":"ed5007a3feb8f14d3d2843271bfb298eb0c56219","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/post.ejs","hash":"75ab6958d929e92566ca580d0b8bd0eeae10649a","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/scripts/.DS_Store","hash":"daec53fd4601c37ca272321ba2eb594d9b0a43ac","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/source/.DS_Store","hash":"e11e97632e6d13d5b9dccadcc514268f3c039508","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/layout/tag.ejs","hash":"9d686364c4d16a1a9219471623af452035c5b966","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/archive-list.ejs","hash":"7520fbf91f762207c2ab06b2c293235cd5b23905","modified":1695121494172},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/category-list.ejs","hash":"f8d2f1907450e61968e6d54443e9be8138196a77","modified":1695121494180},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments.ejs","hash":"d707c47b2638c94e489bc43d4cfd098b7c58447f","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/category-chains.ejs","hash":"18309584aab83bc4deb20723ebad832149dd2e24","modified":1695121494178},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/css.ejs","hash":"85f6e051550907681ab4ed2e268ac8f6e9ebf931","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/head.ejs","hash":"7b7b1d098726e86687a15fe3d520d178577ffcae","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/footer.ejs","hash":"10ccfb8eef4e16182183c9a3e175c90d5b6397d3","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/header.ejs","hash":"0d5e397d30051e5fbabe7b47cfd1f1e6a5820af1","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/tags.ejs","hash":"1d06af34b6cf1d8a20d2eb565e309326ceba309f","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/paginator.ejs","hash":"0f38a2c238169edcb63fc46c23bfc529ff3859b7","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/markdown-plugins.ejs","hash":"fc4bdf7de0cf1a66d0e5e4fba1b31d6f7ed49468","modified":1695121494192},{"_id":"node_modules/hexo-theme-fluid/scripts/events/.DS_Store","hash":"80308812974d7cb7e001cd8f64ff9fced30ff139","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/scripts/events/index.js","hash":"79de5a379b28cad759a49048351c7f6b8915bd7d","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/default-injects.js","hash":"b2013ae8e189cd07ebc8a2ff48a78e153345210f","modified":1695121494210},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/locals.js","hash":"58d0fec976f6b1d35e7ea03edc45414088acf05c","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/filters/post-filter.js","hash":"0047666f996c54017e06668b5242ed8a311ebce0","modified":1695121494222},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/search.ejs","hash":"70e1c929e084ca8a2648cedabf29b372511ea2b8","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/index-generator.js","hash":"9159fc22fa84a7b605dd15fe4104f01fe9c71147","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/scripts.ejs","hash":"da5810785105e5075861593c7ac22c7aa9665a72","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/date.js","hash":"9bda6382f61b40a20c24af466fe10c8366ebb74c","modified":1695121494210},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/local-search.js","hash":"9ac5ddad06e9b0e6015ce531430018182a4bc0fa","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/export-config.js","hash":"8e67b522c47aa250860e3fe2c733f1f958a506c0","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/import.js","hash":"ca53e8dbf7d44cfd372cfa79ac60f35a7d5b0076","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/generators/pages.js","hash":"d3e75f53c59674d171309e50702954671f31f1a4","modified":1695121494222},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/engine.js","hash":"d3a231d106795ce99cb0bc77eb65f9ae44515933","modified":1695121494210},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/page.js","hash":"4607607445233b3029ef20ed5e91de0da0a7f9c5","modified":1695121494222},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/scope.js","hash":"d41d9d658fcb54964b388598e996747aadb85b0f","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/injects.js","hash":"1ad2ae6b11bd8806ee7dd6eb7140d8b54a95d613","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/button.js","hash":"3eb43a8cdea0a64576ad6b31b4df6c2bf5698d4c","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/url.js","hash":"2a6a8288176d0e0f6ec008056bf2745a86e8943e","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/utils.js","hash":"226f99b465ff513de075a8e78b321d6cb62592ca","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/label.js","hash":"f05a6d32cca79535b22907dc03edb9d3fa2d8176","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/checkbox.js","hash":"4938610c3543a921a341bc074626d511cb1a4b45","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/mermaid.js","hash":"75160561e1ef3603b6d2ad2938464ab1cb77fd38","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/helpers/wordcount.js","hash":"4543b8954c5c2ca91191cc0d53cf071b3f26faaa","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/compare-versions.js","hash":"dbbc928c914fc2bd242cd66aa0c45971aec13a5d","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/object.js","hash":"33b57e4decdc5e75c518859f168c8ba80b2c665b","modified":1695121494222},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/group-image.js","hash":"4aeebb797026f1df25646a5d69f7fde79b1bcd26","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/url-join.js","hash":"718aab5e7b2059a06b093ca738de420d9afa44ba","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/scripts/utils/resolve.js","hash":"8c4a8b62aa8608f12f1e9046231dff04859dc3e9","modified":1695121494222},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight-dark.styl","hash":"45695ef75c31a4aa57324dd408b7e2327a337018","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/dist/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1695547017452},{"_id":"node_modules/hexo-theme-fluid/source/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/scripts/tags/note.js","hash":"e3b456a079e5dc0032473b516c865b20f83d2c26","modified":1695121494222},{"_id":"node_modules/hexo-theme-fluid/source/css/highlight.styl","hash":"a9efc52a646a9e585439c768557e3e3c9e3326dc","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1695547017515},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1695547017625},{"_id":"node_modules/hexo-theme-fluid/source/css/main.styl","hash":"855ae5fe229c51afa57f7645f6997a27a705d7e4","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.css.map","hash":"c59d2bc9472922cf6ef9a99e052dbee6cc7e6b36","modified":1695547017556},{"_id":"node_modules/hexo-theme-fluid/source/dist/music.js","hash":"7b3b865f8d06fe896a3a738ec91105dff73ffd74","modified":1695548697248},{"_id":"node_modules/hexo-theme-fluid/source/img/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1695121494162},{"_id":"node_modules/hexo-theme-fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/source/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/source/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/source/js/color-schema.js","hash":"76a198f8721352ebeaf5b2ef2f4db00612da4796","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/source/js/boot.js","hash":"38bd26c6b7acdafda86dda3560e6a3ca488d3c76","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/source/js/events.js","hash":"89e3561488a618ed0caeb9edf18e441978e29c25","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/source/js/leancloud.js","hash":"eff77c7a5c399fcaefda48884980571e15243fc9","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/source/js/local-search.js","hash":"b9945f76f8682f3ec32edfb285b26eb559f7b7e8","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/source/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/changyan.ejs","hash":"c9b2d68ed3d375f1953e7007307d2a3f75ed6249","modified":1695121494180},{"_id":"node_modules/hexo-theme-fluid/source/js/plugins.js","hash":"c34916291e392a774ff3e85c55badb83e8661297","modified":1695121494222},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/discuss.ejs","hash":"98d065b58ce06b7d18bff3c974e96fa0f34ae03a","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/source/js/utils.js","hash":"b82e7c289a66dfd36064470fd41c0e96fc598b43","modified":1695121494226},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/disqus.ejs","hash":"aab4a4d24c55231a37db308ae94414319cecdd9b","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/cusdis.ejs","hash":"5f9dc012be27040bbe874d0c093c0d53958cc987","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/giscus.ejs","hash":"95f8b866b158eff9352c381c243b332a155a5110","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/gitalk.ejs","hash":"843bc141a4545eb20d1c92fb63c85d459b4271ec","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/livere.ejs","hash":"2264758fed57542a7389c7aa9f00f1aefa17eb87","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/twikoo.ejs","hash":"d84bcb5ccd78470a60c067fc914ac0ac67ac8777","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/utterances.ejs","hash":"c7ccf7f28308334a6da6f5425b141a24b5eca0e2","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/valine.ejs","hash":"19ba937553dddd317f827d682661a1066a7b1f30","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/remark42.ejs","hash":"d4e9532feeb02aed61bd15eda536b5b631454dac","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/comments/waline.ejs","hash":"12727da7cf3ac83443270f550be4d1c06135b52b","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/footer/statistics.ejs","hash":"454d8dd4c39f9494ebeb03ca0746f5bc122af76a","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/footer/beian.ejs","hash":"4fb9b5dd3f3e41a586d6af44e5069afe7c81fff2","modified":1695121494172},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/header/navigation.ejs","hash":"870db75e4e403a840c4463dfeed2c9114846e7cc","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/analytics.ejs","hash":"4f68c80bd1395e2f6d11e373116e54de11cb62e8","modified":1695121494172},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/code-widget.ejs","hash":"3a505cba37942badf62a56bbb8b605b72af330aa","modified":1695121494180},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/header/banner.ejs","hash":"e07757b59e7b89eea213d0e595cb5932f812fd32","modified":1695121494172},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/fancybox.ejs","hash":"9d1ea2a46b8c8ad8c168594d578f40764818ef13","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/highlight.ejs","hash":"7529dd215b09d3557804333942377b9e20fa554e","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/math.ejs","hash":"dcbf9a381ee76f2f1f75fcbc22c50a502ec85023","modified":1695121494192},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/encrypt.ejs","hash":"0fff24cf5bf99fbe5c56c292e2eac4a89bf29db4","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/nprogress.ejs","hash":"4c2d39ce816b8a6dcd6b53113c8695f8bd650a23","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/mermaid.ejs","hash":"03ac02762f801970d1c4e73d6ec8d4c503780e50","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/typed.ejs","hash":"f345374885cd6a334f09a11f59c443b5d577c06c","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/category-bar.ejs","hash":"8772bce97ed297e7a88523f4e939ed6436c22f87","modified":1695121494178},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/copyright.ejs","hash":"529f3069742b3d338c769ba2d836e7f3c342a09d","modified":1695121494182},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/meta-top.ejs","hash":"ce6e9f578f4faa45840abddf8f46af3f4b69c177","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/plugins/anchorjs.ejs","hash":"40181442d3a2b8734783a0ad7caf2d2522e3f2ab","modified":1695121494172},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/meta-bottom.ejs","hash":"375974ec017696e294dc12469fb0ae257800dc2d","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/sidebar-right.ejs","hash":"d5fcc9b60e02f869a29a8c17a16a6028ecc1e6d8","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/sidebar-left.ejs","hash":"9992c99b3eb728ad195970e1b84d665f2c8691c4","modified":1695121494194},{"_id":"node_modules/hexo-theme-fluid/layout/_partials/post/toc.ejs","hash":"635a89060fbf72eeda066fc4bd0a97462f069417","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/footnote.js","hash":"b2f61b91fffb17d11ad56811f07d52d23f012741","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/compatible-configs.js","hash":"ef474d1fa5bbafc52619ced0f9dc7eaf2affb363","modified":1695121494202},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/hello.js","hash":"44c5eb97b98813a07c659d6afedd17fad63b1821","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/source/css/_functions/base.styl","hash":"2e46f3f4e2c9fe34c1ff1c598738fc7349ae8188","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/lazyload.js","hash":"9ba0d4bc224e22af8a5a48d6ff13e5a0fcfee2a4","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/highlight.js","hash":"8d3ae1ec6660fbb0e563bc08c2f8deefde1f3bf6","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/merge-configs.js","hash":"7c944c43b2ece5dd84859bd9d1fe955d13427387","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/scripts/events/lib/injects.js","hash":"5ae4b07204683e54b5a1b74e931702bbce2ac23e","modified":1695121494212},{"_id":"node_modules/hexo-theme-fluid/source/css/_mixins/base.styl","hash":"542e306ee9494e8a78e44d6d7d409605d94caeb3","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/css/_variables/base.styl","hash":"4ed5f0ae105ef4c7dd92eaf652ceda176c38e502","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/pages.styl","hash":"b8e887bc7fb3b765a1f8ec9448eff8603a41984f","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_about/about.styl","hash":"97fe42516ea531fdad771489b68aa8b2a7f6ae46","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_archive/archive.styl","hash":"c475e6681546d30350eaed11f23081ecae80c375","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/base.styl","hash":"643284c567665f96915f0b64e59934dda315f74d","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/color-schema.styl","hash":"e413212e5a667d5b8299c4d2a39c4dfa1378d119","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/inline.styl","hash":"411a3fa3f924a87e00ff04d18b5c83283b049a4d","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/print.styl","hash":"166afbc596ea4b552bad7290ec372d25ec34db7b","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/keyframes.styl","hash":"94065ea50f5bef7566d184f2422f6ac20866ba22","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/category-bar.styl","hash":"cc6df43fef6bb3efecbfdd8b9e467424a1dea581","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/category-chain.styl","hash":"0cdf7ef50dfd0669d3b257821384ff31cd81b7c9","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_index/index.styl","hash":"0acbd71633bcc7191672ea4e1b2277bea350d73b","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_links/links.styl","hash":"5c7f2044e3f1da05a3229537c06bd879836f8d6e","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/comment.styl","hash":"780f3788e7357bcd3f3262d781cb91bb53976a93","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_category/category-list.styl","hash":"7edfe1b571ecca7d08f5f4dbcf76f4ffdcfbf0b5","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/highlight.styl","hash":"4df764d298fe556e501db4afc2b05686fe6ebcfb","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/markdown.styl","hash":"1e3d3a82721e7c10bcfcecec6d81cf2979039452","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/post-page.styl","hash":"ecf3488566b374d564ae985c61e08562ba908023","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_post/post-tag.styl","hash":"27f70062415ccf66a9b6f4952db124fc1471fda5","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_tag/tags.styl","hash":"65bfc01c76abc927fa1a23bf2422892b0d566c3f","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/anchorjs.styl","hash":"e0cebda4a6f499aff75e71417d88caa7ceb13b94","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/banner.styl","hash":"7a0bd629bc234fc75e3cc8e3715ffada92f09e73","modified":1695121494232},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/board.styl","hash":"4397037fc3f0033dbe546c33cd9dbdabd8cb1632","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/code-widget.styl","hash":"b66ab013f0f37d724a149b85b3c7432afcf460ad","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/copyright.styl","hash":"26f71a9cd60d96bb0cb5bbdf58150b8e524d9707","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/footer.styl","hash":"2caaca71dd1ff63d583099ed817677dd267b457e","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/header.styl","hash":"c4459248c66ea1326feed021179b847ae91d465f","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/footnote.styl","hash":"ae9289cc89649af2042907f8a003303b987f3404","modified":1695121494242},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/ngrogress.styl","hash":"5d225357b4a58d46118e6616377168336ed44cb2","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/modal.styl","hash":"adf6c1e5c8e1fb41c77ce6e2258001df61245aa2","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/noscript.styl","hash":"0cf2f2bb44f456150d428016675d5876a9d2e2aa","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/pagination.styl","hash":"8bb1b68e5f3552cb48c2ffa31edbc53646a8fb4c","modified":1695121494252},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/qrcode.styl","hash":"78704a94c0436097abfb0e0a57abeb3429c749b7","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/search.styl","hash":"10f7e91a91e681fb9fe46f9df7707b9ef78707c8","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/toc.styl","hash":"9e7452aa2372153f25d7a4675c9d36d281a65d24","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/source/css/_pages/_base/_widget/scroll-btn.styl","hash":"f0e429a27fa8a7658fcbddbb4d4dbe4afa12499a","modified":1695121494258},{"_id":"node_modules/hexo-theme-fluid/source/dist/APlayer.min.js.map","hash":"31a19da0f0cb6b00ec212eafa847f31af86788df","modified":1695547017708},{"_id":"node_modules/hexo-theme-fluid/source/img/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1695121494232},{"_id":"public/local-search.xml","hash":"3c71fa55e3286a8173c75ff655448ee5d7990ca3","modified":1732672711732},{"_id":"public/about/index.html","hash":"3403be7dad3f040cf178116f5f88c893c43c12e6","modified":1698849453676},{"_id":"public/archives/page/3/index.html","hash":"440d695a17b302689087c42ba7364fc75ab77dbe","modified":1732672128096},{"_id":"public/archives/index.html","hash":"c80b24bd222b8715a22be8e83b952402eb3187a8","modified":1732672128096},{"_id":"public/archives/page/4/index.html","hash":"d14f98b13bf5c9156a7f61a272b6fb834f111e3d","modified":1732672128096},{"_id":"public/archives/page/2/index.html","hash":"a13020736b7fb0a0c52a52f146734cc9df79a27a","modified":1732672128096},{"_id":"public/archives/2023/index.html","hash":"539ce5a399a41fff08922eda25aaf2246e7cfc93","modified":1732672128096},{"_id":"public/archives/2023/page/3/index.html","hash":"fbcfe0a33a140b349c160f7690bc5ea053499393","modified":1732672128096},{"_id":"public/archives/2023/09/index.html","hash":"08308a0691564947b6fc648adacc8472c8edcdde","modified":1732672128096},{"_id":"public/archives/2023/page/4/index.html","hash":"e2f0186bb74ee633c753322012eb5ae0f102a69d","modified":1732672128096},{"_id":"public/archives/2023/09/page/2/index.html","hash":"335bb85063c0887e721123323cc1eacaee791383","modified":1732672128096},{"_id":"public/archives/2023/page/2/index.html","hash":"70b4cdae346130b85c6ddcb257d5bdf88293a9ba","modified":1732672128096},{"_id":"public/archives/2023/09/page/3/index.html","hash":"69e0337d203932b457aee667339def65f3dc52e5","modified":1732672128096},{"_id":"public/archives/2023/09/page/4/index.html","hash":"f95a1298ff47d577e11589667c6eb39b734de4b7","modified":1732672128096},{"_id":"public/tags/TODO/index.html","hash":"bd08e067abd491376e29449116505b1bbbbf5194","modified":1695827081113},{"_id":"public/page/4/index.html","hash":"c8f88852b9750095fa804b96c4af138c41117148","modified":1732672128096},{"_id":"public/tags/Git/index.html","hash":"2401097af1222414d501e0593b7e50961b207d57","modified":1695827081113},{"_id":"public/tags/StableDiffusion/index.html","hash":"a145450937ada20366f3419e03a9b86013649fe9","modified":1699199518315},{"_id":"public/tags/Linux/index.html","hash":"23083f86facb33fe037cd637bd65e3f0de078ff1","modified":1695827081113},{"_id":"public/tags/Github/index.html","hash":"b5dae873c9579194f21f26dddd40a925405cbe72","modified":1695827081113},{"_id":"public/tags/西安/index.html","hash":"eccefcbf5a887fdeb0cafff16059285e3128ce4f","modified":1695827081113},{"_id":"public/tags/Obsidian/index.html","hash":"86f448c11ad7e3e90b32d3693eb298c91215cdd5","modified":1695827081113},{"_id":"public/tags/powershell/index.html","hash":"f39c11808b23c0c0e7cb1be7920ed97055b9b5d4","modified":1695827081113},{"_id":"public/tags/智能计算系统/index.html","hash":"134a6ec8410f9e78c4086616891560809a61d83b","modified":1709353181761},{"_id":"public/tags/WSL/index.html","hash":"a1018745fbbb580e99cd1be37c8c06ad21460aac","modified":1695827081113},{"_id":"public/tags/软件工程/index.html","hash":"e6e0bd6b9368e5ae6d4cb789101553d3526be9f2","modified":1709353181761},{"_id":"public/tags/扩散模型/index.html","hash":"2329d43accf96383d87fd32b23e378cfb955d31b","modified":1698852828408},{"_id":"public/tags/Python/index.html","hash":"8d7a680fdbd1fd9ef6be47b95fc8748af2d1c30b","modified":1709353181761},{"_id":"public/tags/MySQL/index.html","hash":"36ed1650a6841dd9be6972c2891ed72aa7a909d8","modified":1696907687758},{"_id":"public/tags/hexo/index.html","hash":"8da5890cbf62775d5bb71e1286b10dbc2d3bfb2b","modified":1695827081113},{"_id":"public/tags/SQL/index.html","hash":"24c7685b65d5349b8da89d055dd52531952d13dd","modified":1709353181761},{"_id":"public/tags/数据库/index.html","hash":"d7281de2f3175420ebd661621f55f23b3bde880e","modified":1709353181761},{"_id":"public/tags/作业/index.html","hash":"a2eeb89bc4734cd3f77c15e9ec9e73b6c5a58563","modified":1709353181761},{"_id":"public/tags/深度学习/index.html","hash":"527168c97163ef2e495b3322732e2316709cce41","modified":1709353181761},{"_id":"public/tags/神经网络/index.html","hash":"1dd9405a3bdba941bdd170cdfccce1be19e8db53","modified":1709353181761},{"_id":"public/categories/Notes/index.html","hash":"e16ad899fba88e9fb77c99e666fde5774a0d7fd2","modified":1732672711732},{"_id":"public/tags/操作系统/index.html","hash":"bf53621e223472c5a7ceb2c4616b084179f305d3","modified":1709353181761},{"_id":"public/categories/Notes/page/2/index.html","hash":"ed2a41808be794f2444891838d88b64d156ba536","modified":1732672711732},{"_id":"public/categories/Notes/AI/index.html","hash":"d330ff9fe538683eac1217a15ee7f58ee1319a79","modified":1699199518315},{"_id":"public/categories/Notes/page/3/index.html","hash":"5c8ea3f3dd3c1078e0a1e96132d49ac53d7275db","modified":1732672711732},{"_id":"public/categories/Notes/Git/index.html","hash":"984fdfdaec54a40c810bf9afa3ffd6d1a19b3b47","modified":1695827081113},{"_id":"public/categories/Notes/page/4/index.html","hash":"3b4314d6fc3e7ac131922ff344abb2b629ccac3e","modified":1732672711732},{"_id":"public/categories/Notes/Linux学习/index.html","hash":"9124c09672a2731ccf2ff36ff1ec243e307404a3","modified":1695827081113},{"_id":"public/categories/Notes/旅游/index.html","hash":"1d9aa48d9ce5cd5249094a55db0d1a40c58a5c97","modified":1695827081113},{"_id":"public/categories/Notes/Ob插件/index.html","hash":"4821061ba970165cda29e3ec3b4f3475f2109ae9","modified":1695827081113},{"_id":"public/categories/Notes/编程/index.html","hash":"3233ad1bd239162017fcea5b02d1b33438af7686","modified":1698994389603},{"_id":"public/categories/Notes/花里胡哨/index.html","hash":"9960251526c52b3838547fd0bc8a6759e94c876f","modified":1709726521535},{"_id":"public/categories/Notes/课程/index.html","hash":"9cd059ef36947dfbb99e46b1406cb7ed55ca1ce5","modified":1724638998994},{"_id":"public/categories/Notes/编程/MySQL/index.html","hash":"f827f2ddceb3ef64c884df6f6456f1682e28f8ad","modified":1696908347765},{"_id":"public/categories/Notes/论文/index.html","hash":"5a9543536f7c6ae900c161b38bf9daf64aa85918","modified":1700045605489},{"_id":"public/categories/Notes/课程/数据库/index.html","hash":"7db213b6a04aae0cd82afa6c8f1f041438240f08","modified":1709353181761},{"_id":"public/categories/Notes/编程/Django/index.html","hash":"f5a826ce416ac649ae57fa837086e2e2ea6ff0ca","modified":1698460826199},{"_id":"public/categories/Notes/课程/软件工程/index.html","hash":"e22d2bbdf71c4d30f511c4145b66be7afaa12558","modified":1709353181761},{"_id":"public/404.html","hash":"cb3b89793fa696fc5a3d7d1822868fb69be52da7","modified":1695827081113},{"_id":"public/tags/index.html","hash":"ac762cf4c57ca9eb9d124dc85bc064509a6b76a4","modified":1704598248222},{"_id":"public/categories/Notes/课程/神经网络与深度学习/index.html","hash":"8bc6f078b42f46782039fdf0229e70e879b29dd3","modified":1709353181761},{"_id":"public/categories/Notes/课程/操作系统/index.html","hash":"24ad92e47fa50531d4ce3a70db3e0efcf06d0114","modified":1709353181761},{"_id":"public/2023/09/27/Notes/编程/MySQL/MySQL基本使用/index.html","hash":"59a2266a72bbd079a1a4e1a7c41dc047fc324e56","modified":1698849453676},{"_id":"public/2023/09/27/Notes/编程/MySQL/安装MySQL/index.html","hash":"088009006be707dad5453482b0dcfdb7d75ab70e","modified":1709353181761},{"_id":"public/2023/09/25/Notes/课程/数据库/SQL/index.html","hash":"e691de4cabef25f7fbd8c55f92bce314962e7c67","modified":1696821937635},{"_id":"public/2023/09/24/Notes/Git/连接Github/index.html","hash":"2e6583a621365a489d54b02ab9774173e3b82558","modified":1709353181761},{"_id":"public/2023/09/24/Notes/课程/神经网络与深度学习/前馈神经网络/index.html","hash":"9b726af1633877be6500e4adf2d8f45ed582b4ab","modified":1698849453676},{"_id":"public/2023/09/23/Notes/编程/Django/First Class/index.html","hash":"6dd20e58a0e59fd24af02f92c78b3816d875800a","modified":1696556336329},{"_id":"public/2023/09/22/Notes/课程/软件工程/软件工程/index.html","hash":"ff7f82a208d9b234596d3a19649afd9f902c3b1c","modified":1695827081113},{"_id":"public/2023/09/22/Notes/课程/软件工程/作业1 课后阅读/index.html","hash":"05ac785994e694deb96742b8fe8ed48b591ef00f","modified":1695827081113},{"_id":"public/2023/09/22/Notes/课程/软件工程/大作业 酒店温控计费系统 规划/index.html","hash":"5b4a9a4ab6add120af963caa21aa5e1175c6a5e6","modified":1703423590531},{"_id":"public/2023/09/22/Notes/课程/数据库/数据库/index.html","hash":"312ef3b1466449500202c8086ed696464fb85c88","modified":1704309817752},{"_id":"public/2023/09/22/Notes/课程/神经网络与深度学习/神经网络与深度学习/index.html","hash":"64036968af31bc0e6a5f0d69a8a90274813947bd","modified":1700103428408},{"_id":"public/2023/09/22/Notes/课程/智能计算系统/index.html","hash":"d54a631a0ab1b0313377f43b6c5163388e4ae398","modified":1695827081113},{"_id":"public/2023/09/22/Notes/论文/扩散模型论文todo（已过期）/index.html","hash":"f43ec0f4215e129ca4a131bc9073ee6df3a44bf8","modified":1695827081113},{"_id":"public/2023/09/22/Notes/课程/操作系统/Blitz软件/index.html","hash":"227462d03ab3f2041c3d2c4b88e50e16030a7dcc","modified":1698849453676},{"_id":"public/2023/09/22/Notes/课程/操作系统/操作系统/index.html","hash":"341cb7101f165d906ff1f2a96fc606b86f2c6a4a","modified":1698936621578},{"_id":"public/2023/09/22/Notes/旅游/景点/index.html","hash":"5f35ff10f3d4e0c2a6d34aaa8b25691e68e22662","modified":1709353181761},{"_id":"public/2023/09/22/Notes/TODO/index.html","hash":"bc95deba4445568ca2d63347e86bee2aa5de3977","modified":1698849453676},{"_id":"public/2023/09/22/Notes/旅游/美食/index.html","hash":"13d135b81092f56a7a32f81d4efb6fe5ce863272","modified":1698849453676},{"_id":"public/2023/09/22/Notes/旅游/西安8.16-20/index.html","hash":"479eb72ae16d22569b0546ca8618d09398b296df","modified":1699319945543},{"_id":"public/2023/09/22/Notes/Ob插件/Advanced Tables 表格绘制/index.html","hash":"e5f90fd4277a513ca9ce8320d617570cb327bdc0","modified":1699199518315},{"_id":"public/2023/09/22/Notes/花里胡哨/hexo+Obsidian+github博客/index.html","hash":"fb1e9ac1ff8c6fd630c67779648d6430a1689f13","modified":1698849453676},{"_id":"public/2023/09/22/Notes/花里胡哨/PowerShell美化/index.html","hash":"6abbdb51f645ce26038a08836270d37bbe2c13c8","modified":1699319945543},{"_id":"public/2023/09/22/Notes/AI/AI绘画/index.html","hash":"a82fd8ba088f437223789ba77c58cf3ffc06437c","modified":1699199518315},{"_id":"public/2023/09/19/Notes/Linux学习/文本编辑器/index.html","hash":"7d22754e4279f1064c528b90646c03898c7a33c5","modified":1698849453676},{"_id":"public/2023/09/19/Notes/Linux学习/程序与进程管理/index.html","hash":"fc1445004c9db1d5c83443f4b35684e5560e0181","modified":1699199518315},{"_id":"public/2023/09/19/Notes/Linux学习/文件系统/index.html","hash":"7c151d431d4e8bb00afdd3714c4e5ece011f88f0","modified":1698849453676},{"_id":"public/2023/09/19/Notes/编程/WSL配置/index.html","hash":"7be721468dc1dbd7ffd4ffa3ece0eae888626d84","modified":1698849453676},{"_id":"public/2023/09/19/Notes/Linux学习/基本命令/index.html","hash":"16c156c823634c72ab6f87fc1ec41855abd5725e","modified":1698849453676},{"_id":"public/2023/09/19/Notes/Git/Obsidian Git使用/index.html","hash":"4ebd39d10e6731c21e3dbbaaae7cc4966932d045","modified":1698849453676},{"_id":"public/2023/09/19/Notes/Linux学习/Shell编程/index.html","hash":"fc1dd5c0b8e16724372540f473d496f0b408d7a5","modified":1698849453676},{"_id":"public/2023/09/19/Notes/Git/Git操作/index.html","hash":"e0978465e3730fe5f3ccbe5a1883b3e9c79826ff","modified":1698849453676},{"_id":"public/index.html","hash":"29064ca886909b1967d07a39ddfead58e8cd67ea","modified":1732672711732},{"_id":"public/page/3/index.html","hash":"4303b1efa7ca59c14891c229f442088c28444381","modified":1732672128096},{"_id":"public/page/2/index.html","hash":"7fb31d1eae1c248744416d42e54907170d3ea4cb","modified":1732672128096},{"_id":"public/categories/index.html","hash":"a4926209f7816670d55a2d8c66a63b7ef65764e7","modified":1732672711732},{"_id":"public/dist/APlayer.min.css.map","hash":"c59d2bc9472922cf6ef9a99e052dbee6cc7e6b36","modified":1695827081113},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1695827081113},{"_id":"public/xml/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1695827081113},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1695827081113},{"_id":"public/assets/js/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1695827081113},{"_id":"public/assets/css/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1695827081113},{"_id":"public/img/fluid.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1695827081113},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1695827081113},{"_id":"public/css/gitalk.css","hash":"a57b3cc8e04a0a4a27aefa07facf5b5e7bca0e76","modified":1695827081113},{"_id":"public/css/highlight-dark.css","hash":"2b0daa6e5343da9dbb26d617d224b8397e48556b","modified":1695827081113},{"_id":"public/css/highlight.css","hash":"0f9a477d33d3b15ebe7e163e756fb7c54c7ded6b","modified":1695827081113},{"_id":"public/dist/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1695827081113},{"_id":"public/dist/music.js","hash":"930c67926ee0f075e4a2a346381bc613dd7c556c","modified":1695827081113},{"_id":"public/js/boot.js","hash":"38bd26c6b7acdafda86dda3560e6a3ca488d3c76","modified":1695827081113},{"_id":"public/js/img-lazyload.js","hash":"cbdeca434ec4da51f488c821d51b4d23c73294af","modified":1695827081113},{"_id":"public/js/leancloud.js","hash":"eff77c7a5c399fcaefda48884980571e15243fc9","modified":1695827081113},{"_id":"public/js/events.js","hash":"89e3561488a618ed0caeb9edf18e441978e29c25","modified":1695827081113},{"_id":"public/js/color-schema.js","hash":"76a198f8721352ebeaf5b2ef2f4db00612da4796","modified":1695827081113},{"_id":"public/js/local-search.js","hash":"b9945f76f8682f3ec32edfb285b26eb559f7b7e8","modified":1695827081113},{"_id":"public/js/utils.js","hash":"b82e7c289a66dfd36064470fd41c0e96fc598b43","modified":1695827081113},{"_id":"public/js/plugins.js","hash":"c34916291e392a774ff3e85c55badb83e8661297","modified":1695827081113},{"_id":"public/css/main.css","hash":"526a4522eb54bb51772f51fb98295dc4fe03429f","modified":1695827081113},{"_id":"public/dist/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1695827081113},{"_id":"public/assets/js/Meting.min.js","hash":"a0585220b918d78649a7893279e1ec4fb5abe835","modified":1695827081113},{"_id":"public/dist/APlayer.min.js.map","hash":"31a19da0f0cb6b00ec212eafa847f31af86788df","modified":1695827081113},{"_id":"public/img/default.png","hash":"167a12978d80371cf578c8a2e45c24a2eb25b6fb","modified":1695827081113},{"_id":"source/_posts/Notes/编程/MySQL/Django操作MySQL.md","hash":"420bf020d46f67b15ed7947ac7b9320d64ae680d","modified":1696139265023},{"_id":"public/2023/09/27/Notes/编程/MySQL/Django操作MySQL/index.html","hash":"625eebb726a4073b37a676d85c9afdc0ae10b2bb","modified":1696139583462},{"_id":"source/_posts/Notes/课程/智能计算系统/神经网络基础.md","hash":"b3689c74815328c2b5e148d3ed34ed76fb8af3a1","modified":1697075453689},{"_id":"source/_posts/Notes/课程/智能计算系统/智能计算系统.md","hash":"6bf0a64e0c2b2a11f636c1dd35dfc41886ef4f17","modified":1695860731337},{"_id":"public/2023/09/28/Notes/课程/智能计算系统/神经网络基础/index.html","hash":"d47525959b256171276053b8aea57a03c7cf24c4","modified":1698849453676},{"_id":"public/2023/09/22/Notes/课程/智能计算系统/智能计算系统/index.html","hash":"276e76ca1a167ca33139227b9b68c38e7270ff45","modified":1698849453676},{"_id":"public/categories/Notes/课程/page/2/index.html","hash":"16935fbe649c5f39e9b2752b60eadc23888eb90c","modified":1724638998994},{"_id":"public/categories/Notes/课程/智能计算系统/index.html","hash":"ed60086b18b84a1b81c002dcc7455f983d0cfd8c","modified":1709353181761},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/未命名.md","hash":"e90b23dfaa82a31aade40083f53c53e3504eca54","modified":1695866819764},{"_id":"public/2023/09/28/Notes/课程/神经网络与深度学习/未命名/index.html","hash":"70e15a9e15dac26c27cbdc4c006a30b6378f1c35","modified":1695867418896},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/梯度放缓问题.md","hash":"1eb756e43d0da5a372be7a192166c219347183e1","modified":1695872876510},{"_id":"public/2023/09/28/Notes/课程/神经网络与深度学习/梯度放缓问题/index.html","hash":"0ea68dba8d74c3ade00d9d48d295c38fd4f18065","modified":1695872912755},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/网络优化与正则化.md","hash":"8bac790e92b79bf0b776a401939545a5c1986b1e","modified":1695873598425},{"_id":"public/2023/09/28/Notes/课程/神经网络与深度学习/网络优化与正则化/index.html","hash":"a9058328bd4b7059316cbfa4f343e2b4a5880a8c","modified":1698849453676},{"_id":"source/_posts/Notes/课程/软件工程/作业.md","hash":"080f2f4c2140d2038606f840367f82c672db6fee","modified":1702790022455},{"_id":"source/_posts/Notes/课程/软件工程/软件生命周期.md","hash":"70d3b88a9430ab91ac9ae13fd778d365eb27baec","modified":1696556825803},{"_id":"public/2023/09/22/Notes/课程/软件工程/软件生命周期/index.html","hash":"6410e1286fac8ae4c98b11200f6aea187514942c","modified":1698849453676},{"_id":"public/2023/09/22/Notes/课程/软件工程/作业/index.html","hash":"be7797afaaf2726e37e39cd2f25d20a113cf916a","modified":1702790017889},{"_id":"source/_posts/Notes/编程/MySQL/通过MySQL Workbench操作.md","hash":"8bc7e3c1b81e8b1ce7bad52fc5c5f190367016b1","modified":1696575693494},{"_id":"public/archives/2023/10/index.html","hash":"b43b1a94cc60e03aa41b44d542b8cb389a9e59f4","modified":1732672128096},{"_id":"public/2023/10/06/Notes/编程/MySQL/通过MySQL Workbench操作/index.html","hash":"05b45d401dd07b0a03a822722c61a2decfd6c0e1","modified":1709353181761},{"_id":"source/_posts/Notes/课程/数据库/SQL基础.md","hash":"08eeda89f7f7edb6cca083c21af08c547c6a3e44","modified":1696822544505},{"_id":"public/2023/09/25/Notes/课程/数据库/SQL基础/index.html","hash":"6393ad738c4d74ac42d9061e15bc2e9e1471944e","modified":1698849453676},{"_id":"public/2023/10/09/Notes/课程/数据库/SQL/index.html","hash":"a9cab03cd61319781ef5b358c1c1b9b813f5a7c6","modified":1696831402074},{"_id":"source/_posts/Notes/课程/操作系统/Threads线程.md","hash":"f2407542e0bf98590f1b350919073cbb08bb0c07","modified":1699151565969},{"_id":"public/2023/10/09/Notes/课程/操作系统/Threads线程/index.html","hash":"316cbe8cdd333e21ebcee7044728af6072464979","modified":1699151805086},{"_id":"source/_posts/Notes/课程/数据库/SQL中级.md","hash":"a2031ede462e4e0f2e3a6f1c023bc64e472c0d06","modified":1697951902821},{"_id":"public/2023/10/09/Notes/课程/数据库/SQL中级/index.html","hash":"1fa34f366ed36769b4bae216f9fa4e247605f427","modified":1698849453676},{"_id":"source/_posts/Notes/编程/Django/Django操作MySQL.md","hash":"86d1801f729b50e629c16ba087075b3d41333063","modified":1698159021245},{"_id":"public/2023/09/27/Notes/编程/Django/Django操作MySQL/index.html","hash":"47b2a3a467fe7775ee32dd2f589665e8af4a18a7","modified":1709353181761},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/实验 用numpy搭建全连接神经网络用于手写数字识别.md","hash":"f73e58bec6840749f6b236d02c29ca009a80656e","modified":1697646865167},{"_id":"public/2023/10/10/Notes/课程/神经网络与深度学习/实验 用numpy搭建全连接神经网络用于手写数字识别/index.html","hash":"3eb890987a11949914e6105709da47392fa4a30d","modified":1698849453676},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/卷积神经网络.md","hash":"337401c1f9af8aaa952b1ebb0ef72e6c06ee5fb7","modified":1697082782049},{"_id":"public/2023/10/12/Notes/课程/神经网络与深度学习/卷积神经网络/index.html","hash":"698da5062a78915255a909a14aa27e4744ad1dbc","modified":1698994389603},{"_id":"source/_posts/Notes/编程/Vue/Vue.md","hash":"9339c63c51f69867524373f7002c7ecc037f928d","modified":1697167289113},{"_id":"public/tags/Vue/index.html","hash":"17f2d22c8ac7c632a17795001878dc347f607618","modified":1697167287764},{"_id":"public/2023/10/13/Notes/编程/Vue/Vue/index.html","hash":"1ce8e2ccfe922ecb4f2c0d8fb76907c0743731f7","modified":1698849453676},{"_id":"public/categories/Notes/编程/Vue/index.html","hash":"77acde99a15a6bdeb9cc95e36c6de0890ae4c430","modified":1697167887759},{"_id":"source/_posts/Notes/课程/操作系统/未命名.md","hash":"2ae62d53f22f9b8750f9e0c2167591842c2f11ef","modified":1697418668140},{"_id":"public/archives/page/5/index.html","hash":"e4d995d9df74761de7b6eafb22a7159c6e8b9558","modified":1732672128096},{"_id":"public/archives/2023/page/5/index.html","hash":"6f75e0823082f39b91d2faf1541185701f77989d","modified":1732672128096},{"_id":"public/page/5/index.html","hash":"010eb5112499a2c85fc2e2a9961e772511f32a56","modified":1732672128096},{"_id":"public/2023/10/16/Notes/课程/操作系统/未命名/index.html","hash":"5d082e69bb5033def9d23051ed9d068b032df6ca","modified":1697419323656},{"_id":"public/categories/Notes/page/5/index.html","hash":"b4ec3946983513a48dff546ee2c58d5dd89c8a91","modified":1732672711732},{"_id":"source/_posts/Notes/课程/操作系统/Semaphores信号量.md","hash":"482e176b25be181013330f52219994380dff9662","modified":1699161658555},{"_id":"public/2023/10/16/Notes/课程/操作系统/Semaphores信号量/index.html","hash":"03603ee1102fab4163e957a52150ef9ab97e5353","modified":1699161711712},{"_id":"source/_posts/Notes/课程/操作系统/Monitor管程.md","hash":"b67fea28a56c684fe0bdf7e620e5fa524f45fc20","modified":1699162278891},{"_id":"public/2023/10/18/Notes/课程/操作系统/Monitor管程/index.html","hash":"95c2a7503960206234dfe48e60a3ae78eb24d21d","modified":1699162311693},{"_id":"source/_posts/Notes/课程/智能计算系统/深度学习.md","hash":"2b7942a126b0aeb038d25a15222974e8f5dd7f83","modified":1699488697758},{"_id":"public/2023/10/19/Notes/课程/智能计算系统/深度学习/index.html","hash":"895b67203bd543199f8704758651d02bf7a2fda9","modified":1699488737014},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/循环神经网络.md","hash":"d4b783ab08ae26863b4ac6ce96f72ec524c387f1","modified":1700040616290},{"_id":"public/2023/10/19/Notes/课程/神经网络与深度学习/循环神经网络/index.html","hash":"ba00931310dacbd58d4bf1123f76395b092b2f15","modified":1700041108405},{"_id":"source/_posts/Notes/课程/软件工程/软件需求分析.md","hash":"44d4be0aabf8708d53c068c45786b601a75b9b16","modified":1697768035827},{"_id":"public/archives/2023/10/page/2/index.html","hash":"7eb6a570f4eb55aa0c8a1c4269572aae4da5848e","modified":1732672128096},{"_id":"public/2023/10/20/Notes/课程/软件工程/软件需求分析/index.html","hash":"4b46f80eab9d41bd5cdc86ea74386a689512b618","modified":1698849453676},{"_id":"public/categories/Notes/课程/page/3/index.html","hash":"2a877c25ffc0e4e555e7193572a93b981c7550f8","modified":1724638998994},{"_id":"source/_posts/Notes/课程/软件工程/UML.md","hash":"f3ee37aa065304be326db9616e222989af00c01e","modified":1697771544437},{"_id":"public/2023/10/20/Notes/课程/软件工程/UML/index.html","hash":"d3e5cbf4cb600641e7129b1951a171fe1217f27d","modified":1698849453676},{"_id":"source/_posts/Notes/课程/软件工程/面向对象分析.md","hash":"ca348cb282e0f53e4fe690e44b260a21e16c3b90","modified":1698378606318},{"_id":"public/2023/10/20/Notes/课程/软件工程/面向对象分析/index.html","hash":"1e4075ac9b8594be03bfc9f73b97869afbe3c45b","modified":1698849453676},{"_id":"source/_posts/Notes/课程/数据库/SQL高级.md","hash":"f84f3b85fc38a9df515f3ac6e2bd0c8b3cdd84d3","modified":1698634105659},{"_id":"public/2023/10/23/Notes/课程/数据库/SQL高级/index.html","hash":"c80da5518627920e6d3cd5ae1b0aebd7021a4b40","modified":1698849453676},{"_id":"source/_posts/Notes/训练计划/三分化.md","hash":"4c1223bb317799fc70127f706ff46792f165a4d3","modified":1698139730237},{"_id":"public/2023/10/24/Notes/训练计划/三分化/index.html","hash":"fcd36bd40d1b9bddeb153a16b5904d737126d46c","modified":1709353181761},{"_id":"public/tags/训练/index.html","hash":"d78112d2d6987df46e78f34ec9a63c5990ea419f","modified":1698139700522},{"_id":"public/tags/三分化/index.html","hash":"4489ac1b7e1d7da79daa714a1c9d698e5b45810e","modified":1698139700522},{"_id":"public/categories/Notes/训练计划/index.html","hash":"ec2c829b850a75a0b34a2f5368d87bfb2681627f","modified":1698140275519},{"_id":"source/_posts/Notes/编程/Django/Django基础.md","hash":"ada5a45a546e96d5c3369f74628844b69b744911","modified":1698159027294},{"_id":"public/2023/09/23/Notes/编程/Django/Django基础/index.html","hash":"20696dcfaadb85613a46ecb8331f8172cd479cd3","modified":1709353181761},{"_id":"source/_posts/Notes/课程/操作系统/Deadlock死锁.md","hash":"a8095ba16d1cd6a56feaead1e6dd4c743e963f49","modified":1699170142950},{"_id":"public/2023/10/25/Notes/课程/操作系统/Deadlock死锁/index.html","hash":"20ec5370fe4e706ac11a60a9b52700816c85edb0","modified":1699170179622},{"_id":"source/_posts/Notes/论文/扩散模型论文todo.md","hash":"2f67382cb770b40a606861e508dc0dbbf1f5de49","modified":1705553955158},{"_id":"public/2023/09/22/Notes/论文/扩散模型论文todo/index.html","hash":"b6f619a8cfc99f9aead522341876d36deac8f2ce","modified":1709353181761},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/图神经网络.md","hash":"240de79ca9a82f02ad8d3f81444eea5b7c759b00","modified":1698291503766},{"_id":"public/archives/page/6/index.html","hash":"bbe1ffa5498540a0fc9f2a191945c65c38dbc768","modified":1732672128096},{"_id":"public/archives/2023/page/6/index.html","hash":"efe88f43ccb9bdbe0be16055f39d16fb64317c96","modified":1732672128096},{"_id":"public/page/6/index.html","hash":"e6117711ea9ecc46e2002de6d809d6ae118ab8c4","modified":1732672128096},{"_id":"public/2023/10/26/Notes/课程/神经网络与深度学习/图神经网络/index.html","hash":"6148001cbae3bad9774ec4056ef97485035e399e","modified":1698849453676},{"_id":"public/categories/Notes/page/6/index.html","hash":"4cd1dcc120272b6196ca83c1b9f52b9fb178a48b","modified":1732672711732},{"_id":"source/_posts/Notes/编程/Django/Django模板.md","hash":"4f51aa9ba4ff8ea733690eabf11f948ea6db0a2b","modified":1698995529437},{"_id":"public/2023/10/27/Notes/编程/Django/Django模板/index.html","hash":"4d6bffe4d718e7381253b92dadaf9ea1b4f5a9d7","modified":1709353181761},{"_id":"source/_posts/Notes/课程/数据库/6.md","hash":"8e69a07e513cf5f911ed34013217ba354c3f3ee2","modified":1698638094560},{"_id":"public/2023/10/30/Notes/课程/数据库/6/index.html","hash":"06a9c3556b726cfa9bb15f1cdab584f098a1f6ea","modified":1698666751719},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/分析与简答.md","hash":"d6b58a9db06c59f874985cd664bb8eb13de0f787","modified":1698847485199},{"_id":"public/2023/10/30/Notes/课程/神经网络与深度学习/分析与简答/index.html","hash":"7dfb2cbce831ae7980e128803e0d2bbe3adaf28d","modified":1698849453676},{"_id":"source/_posts/Notes/课程/个人发展规划大作业.md","hash":"fdbd98241b17f2321a276c7c901bea7aa23f73ce","modified":1698735729492},{"_id":"public/archives/2023/10/page/3/index.html","hash":"19fe1daf85526c7af21da2eeaa65f47c4ae25946","modified":1698929874771},{"_id":"public/2023/10/31/Notes/课程/个人发展规划大作业/index.html","hash":"da205e592564a759c99e20502922663d63892453","modified":1702270798090},{"_id":"source/_posts/Notes/论文/扩散模型调研.md","hash":"ed85c16e857c7e1d8e440ee62b4e642c333db9ee","modified":1703653231442},{"_id":"public/archives/2023/11/index.html","hash":"67032d5e322e4cd5886424fa335292dbdb54332a","modified":1732672128096},{"_id":"public/2023/11/01/Notes/论文/扩散模型调研/index.html","hash":"99fd6726bb1bae2b0af46c291a60551854919b6f","modified":1709353181761},{"_id":"source/_posts/Notes/课程/操作系统/Memory内存.md","hash":"8ef44960eea17d8686b6307dac6a09c9b3db43a5","modified":1698806077819},{"_id":"public/2023/11/01/Notes/课程/操作系统/Memory内存/index.html","hash":"572bfcda2f0227c339bb25997c99b3af35102494","modified":1698891009481},{"_id":"source/_posts/Notes/课程/数据库/使用E-R模型的数据库设计.md","hash":"9729629b2fb08b5d2e0f5ab321874973deb27a58","modified":1699772011218},{"_id":"public/2023/10/30/Notes/课程/数据库/使用E-R模型的数据库设计/index.html","hash":"ebac68de2efd834d151ada7c8f694d0b1096565b","modified":1702270798090},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/注意力机制.md","hash":"adcf19408ba1604470a6fa526bad6537b0e2844a","modified":1698895520446},{"_id":"public/2023/11/02/Notes/课程/神经网络与深度学习/注意力机制/index.html","hash":"f0bc95714339f5c2255c6a6872032a4bd4261e98","modified":1698929874771},{"_id":"public/categories/Notes/课程/page/4/index.html","hash":"1def25be7961f8f751e6b7bb2e62206078807681","modified":1724638998994},{"_id":"source/_posts/Notes/课程/操作系统/Processes进程.md","hash":"52798df8fafa13121e68529958fbb84dfb3c64b9","modified":1698997001431},{"_id":"public/2023/11/02/Notes/课程/操作系统/Processes进程/index.html","hash":"7feb20b49980d31f237c46e204a6d18cd6854de5","modified":1698997990602},{"_id":"source/_posts/Notes/课程/操作系统/Scheduling调度.md","hash":"3cfc1b844b1d761d8160c76f542b3b6b4f778beb","modified":1699201303070},{"_id":"public/2023/11/03/Notes/课程/操作系统/Scheduling调度/index.html","hash":"b8a9d229644fc34b9274d30f1f2b80bd5880a817","modified":1699489925013},{"_id":"source/_posts/Notes/课程/智能计算系统/编程框架使用.md","hash":"a19d7be638b36c5e5680d6e63763a0dc1b32b321","modified":1704092832859},{"_id":"public/tags/Pytorch/index.html","hash":"4a4ad5ef14d67e9e29be675bd5433777fc62c1ef","modified":1709353181761},{"_id":"public/2023/11/09/Notes/课程/智能计算系统/编程框架使用/index.html","hash":"f0d78746b9b5e2c49a05bcde76d45af00da6a808","modified":1704093396304},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/生成模型.md","hash":"7513e1e04e67751505349ca3c543bef246887f07","modified":1699499247704},{"_id":"public/2023/11/09/Notes/课程/神经网络与深度学习/生成模型/index.html","hash":"39111d045b78084f1c8877847511559a406d163e","modified":1699500248342},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/生成对抗网络.md","hash":"4c49a2ba3539b7c7cd801541a266d4906c4c3a37","modified":1699501676697},{"_id":"public/archives/page/7/index.html","hash":"e76ec454fb898b9e8caaf81b6b91bde02ed4176b","modified":1732672128096},{"_id":"public/archives/2023/page/7/index.html","hash":"352a18ce329742b3f1986c6ed079c60865f2c603","modified":1732672128096},{"_id":"public/page/7/index.html","hash":"6fdb62af24509ec457ebb1a9e6a71404107bc81d","modified":1732672128096},{"_id":"public/2023/11/09/Notes/课程/神经网络与深度学习/生成对抗网络/index.html","hash":"fc6fadbc0d9660273c3b2b0692033d7a7077c966","modified":1699879920598},{"_id":"public/categories/Notes/page/7/index.html","hash":"250b89dcb49129a1db5a84572c15b4517960ebe0","modified":1732672711732},{"_id":"public/categories/Notes/课程/神经网络与深度学习/page/2/index.html","hash":"6f80e10b6cf4b66cf0d204b3d21d973566c456ef","modified":1709353181761},{"_id":"source/_posts/Notes/论文/ChatEdit.md","hash":"643c61c444941f38186c093676ad29e9d59ccf43","modified":1699884380761},{"_id":"public/2023/11/13/Notes/论文/ChatEdit/index.html","hash":"f98d3417f430a186e244d4ec13311d490b8e6797","modified":1709353181761},{"_id":"source/_posts/Notes/课程/数据库/关系型数据库设计：模式规范化.md","hash":"54c2f101798f1939279b7f07116173fdc7ea39ba","modified":1701051879814},{"_id":"public/2023/11/15/Notes/课程/数据库/关系型数据库设计：模式规范化/index.html","hash":"6c0f843505fec7f0c50dba9939006f6ca43c19bf","modified":1701051943672},{"_id":"source/_posts/Notes/论文/论文查阅、写作与投稿的综合指南.md","hash":"32cff14681766923fa612a10e31329f2234568ef","modified":1700052976272},{"_id":"public/archives/2023/11/page/2/index.html","hash":"71db0f7d9882779506b0f84eecdd509532fbb578","modified":1732672128096},{"_id":"public/2023/11/15/Notes/论文/论文查阅、写作与投稿的综合指南/index.html","hash":"370ccc385dfc3c44b8e420c1d439a64986aadf50","modified":1709353181761},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/强化学习.md","hash":"7a65ed7f187ca49a8d6411ae1749a25499959462","modified":1700104527366},{"_id":"public/2023/11/16/Notes/课程/神经网络与深度学习/强化学习/index.html","hash":"3217ba8a3f3dc44ed490817e19481483ec9dbaab","modified":1700465425499},{"_id":"source/_posts/Notes/课程/智能计算系统/第三章作业.md","hash":"fef41663dd463b63d34aa495ac99a69fe3a5863d","modified":1700481233427},{"_id":"public/2023/11/20/Notes/课程/智能计算系统/第三章作业/index.html","hash":"7e10228a4bfb71d8e8e49bdb3275a986c5901128","modified":1701139940032},{"_id":"source/_posts/Notes/课程/智能计算系统/编程框架机理.md","hash":"f317ac5ec765a4ff8fd02dd2e709c0312a59d277","modified":1704096294201},{"_id":"public/2023/11/28/Notes/课程/智能计算系统/编程框架机理/index.html","hash":"502731cca0923e95984c751ac976504a03711693","modified":1704096385878},{"_id":"source/_posts/Notes/课程/数据库/数据储存结构.md","hash":"8bbf2c59eb3e13995228f2cc06e50e0a91f72261","modified":1701597699780},{"_id":"public/archives/2023/12/index.html","hash":"086229aba467001073f24b24af090ba416965439","modified":1732672128096},{"_id":"public/2023/12/03/Notes/课程/数据库/数据储存结构/index.html","hash":"4356367eb73bb2f0a0c2b6b02addc6f1342f6b0f","modified":1702270798090},{"_id":"public/categories/Notes/课程/page/5/index.html","hash":"7922657c676cc562c099c57a19ff70ad7fe89c06","modified":1724638998994},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/A2分析与简答.md","hash":"4fde858fee95fb83a415087f4dd246dcb22952e1","modified":1702270235925},{"_id":"source/_posts/Notes/课程/神经网络与深度学习/A6分析与简答.md","hash":"c1c10f4d2b77f26f0288fc693bf9a7b1ad84f438","modified":1702294305268},{"_id":"public/2023/12/11/Notes/课程/神经网络与深度学习/A6分析与简答/index.html","hash":"481c48f5eb40bd00861eeb7cdffee11539d0c3cb","modified":1704598248222},{"_id":"public/2023/10/30/Notes/课程/神经网络与深度学习/A2分析与简答/index.html","hash":"b19f3325fccf523104ccb30372e5c971ebb5b35f","modified":1704029382754},{"_id":"source/_posts/Notes/课程/操作系统/内存.md","hash":"16057a78828e8d53451a34f79f0b15b6457c5787","modified":1704648445367},{"_id":"public/archives/2024/index.html","hash":"27a4f6c5ee8e6395c102fb408bf36c592b4775a1","modified":1732672128096},{"_id":"public/archives/2024/01/index.html","hash":"3dc2d91170f19df21a76d87dbad7fee004e3f55d","modified":1732672128096},{"_id":"public/2024/01/07/Notes/课程/操作系统/内存/index.html","hash":"456f798c5dd8e2b369b5b49f32ca0c79ae1ec59e","modified":1706882870795},{"_id":"source/_posts/Notes/考研/择校.md","hash":"6400259ff88c4faa3938635de173c5bfbf027867","modified":1706883458785},{"_id":"public/archives/2024/02/index.html","hash":"084c1a546b0b895d49dc9be993d4728778479b8c","modified":1732672128096},{"_id":"public/2024/02/02/Notes/考研/择校/index.html","hash":"f891a0059b79e61e9d28abdb16406528868ec557","modified":1709353181761},{"_id":"public/categories/Notes/考研/index.html","hash":"4f055a29342570a5ad038c81c3b4e9366c0ac7bf","modified":1709556989111},{"_id":"source/_posts/Notes/考研/C语言基础.md","hash":"2e91b45cc50de6288371b1531ac40151e2779b21","modified":1708844975036},{"_id":"public/2024/02/25/Notes/考研/C语言基础/index.html","hash":"410c2ba35ca23f2e44529a748061ee4b711b9145","modified":1709353181761},{"_id":"public/archives/page/8/index.html","hash":"e002a6e546a948451a54a799c48a5986d73df078","modified":1732672128096},{"_id":"public/page/8/index.html","hash":"67453f5ea49e2c0242e1391864099e7540389e2a","modified":1732672128096},{"_id":"public/categories/Notes/page/8/index.html","hash":"adcfa08b7e44bc24160e1ef2be0147612af1be52","modified":1732672711732},{"_id":"source/_posts/Notes/课程/自然语言处理/课程介绍.md","hash":"e3c72cc0ef5a5e7c374105d3211b972f9f42cb21","modified":1709353346382},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Blitz软件.md","hash":"f38e5055f6a4124e27c21602b9f93e577738a78d","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Deadlock死锁.md","hash":"75fb986b5829047ac310e6100ac166a0914784c1","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Monitor管程.md","hash":"827f9f9e91be5221c62771b28756f55f5ed5dfbc","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Memory内存.md","hash":"b293c188291f163490c0aa8dd84d33c50cbfc58e","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Scheduling调度.md","hash":"5f80b09ede36013a8256fac58996e8515eb931a5","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/数据库/SQL中级.md","hash":"1699ecee7be5391292a2af96620b92426f9f44a4","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Processes进程.md","hash":"099e07e0d6f986b1512a1a5dbc93e8b97dfc72c7","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/数据库/SQL基础.md","hash":"2e4f69dea3eff499c55b627ce7b2ad1fefb92cb9","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/内存.md","hash":"be5de40b9a6f24e5c616490d33eb6b5f3d59e31a","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/操作系统.md","hash":"c436b1e3aa8cc3e581bf524470820acf9889942b","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/数据库/使用E-R模型的数据库设计.md","hash":"d3609107ddebd8c47f28d7c96fff3e67a87c8727","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/数据库/数据储存结构.md","hash":"b1db65a14a885bdf87b0b10c2f73ad3549c4f25f","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/数据库/数据库.md","hash":"0bd611ef965cc769a98c9eeeccc691fd07dc5329","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/数据库/关系型数据库设计：模式规范化.md","hash":"14ad4a855bd60893cedcd38efb7c3d8dfc3fdc65","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/智能计算系统/深度学习.md","hash":"4aba373300b8575cb5abf1b22801eedfb53d329a","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/智能计算系统/智能计算系统.md","hash":"9b78ebe5a1f2c66abe7d925e611147e9b4dd9930","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/智能计算系统/神经网络基础.md","hash":"bd41bb0a34ad8992f24d4e3c9267422d84c38cdb","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/智能计算系统/第三章作业.md","hash":"e5f26875df9516b95a3738dc4c50a7703823cc24","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Semaphores信号量.md","hash":"85fe61ee72813ba944ecbd44d9a33f3ab3b38a47","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/A2分析与简答.md","hash":"dc9b2b9deae1812995c8bc1be47545acf725bdde","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/智能计算系统/编程框架使用.md","hash":"a2acf4528be1cad372b44eaddca676a8a87da5c9","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/智能计算系统/编程框架机理.md","hash":"29a13a373c259337ab6db4f27983b9f94cf099af","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/A6分析与简答.md","hash":"75a3225c45be6b808d679384f94b36adda29e874","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/前馈神经网络.md","hash":"cde9f94540027ff892e3393619bc639b6a428135","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/图神经网络.md","hash":"f24757410fc0845d6f759faff2bbeee5d4d5d9f2","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/卷积神经网络.md","hash":"5f3a6239088ebaa07839baec075ebc37695c60eb","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/实验 用numpy搭建全连接神经网络用于手写数字识别.md","hash":"0df109f9e5e734417e17badcfcbffb1b1209aa65","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/强化学习.md","hash":"77c9eead0e01fe2df5567e2427047f365f53422b","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/循环神经网络.md","hash":"bf0409015be3ee070c931a91d8b7b69aa3b351aa","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/注意力机制.md","hash":"75145ba2cc69b0ec93aa1891a8fd0eb66adee8b5","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/生成对抗网络.md","hash":"624ded40e7db94ca14ff4a9e4d039afc2bae16c0","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/软件工程/UML.md","hash":"238674660b686cf5412f04f970eb1234c32a0c7d","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/软件工程/作业.md","hash":"3da50a38da7292c8289c4a85b9473bb6d45c49b1","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/软件工程/大作业 酒店温控计费系统 规划.md","hash":"a5a742409e3477b20a99b59f7b9c0a11c4f4031c","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/软件工程/软件生命周期.md","hash":"f0f04a4d2a0c199a7e0cfe52af574a94e8287bd5","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/生成模型.md","hash":"7244e20c9c848cf92236064a34c3c151227f3cf5","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/软件工程/面向对象分析.md","hash":"7f20d1e1d0e789683eceb24b35dbd0696a3d1c36","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/神经网络与深度学习.md","hash":"a0fab6960f97bf909aaaecfd50e7f493357b59cb","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/神经网络与深度学习/网络优化与正则化.md","hash":"eab54b4b53fb830b372f714590af3b16e1d6478e","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/软件工程/软件需求分析.md","hash":"daaf4cef6649a4a86bf7dde892e7cff2a2fa87f7","modified":1709353183792},{"_id":"source/_posts/Notes/课程/大三（上）/数据库/SQL高级.md","hash":"5303644980dba6450f9905b14a3c01bad7973c29","modified":1709353183791},{"_id":"source/_posts/Notes/课程/大三（上）/操作系统/Threads线程.md","hash":"4d7913b90ed123f6b88c7db762320fa4a6ecc57e","modified":1709353183791},{"_id":"public/archives/2024/03/index.html","hash":"8e938f5eab9cc2304e8068b9ebe45026fa3e761c","modified":1732672128096},{"_id":"public/2024/03/02/Notes/课程/自然语言处理/课程介绍/index.html","hash":"76a73f0a13b18a91fcf681c0408292d0b86c7114","modified":1710827052784},{"_id":"public/2024/01/07/Notes/课程/大三（上）/操作系统/内存/index.html","hash":"c429919cc002232ba58f2ae118c0a82df0bc6cd0","modified":1709353781724},{"_id":"public/2023/12/11/Notes/课程/大三（上）/神经网络与深度学习/A6分析与简答/index.html","hash":"e153c955e80993115ea5203ab584f44782a6f4e0","modified":1709353781724},{"_id":"public/2023/12/03/Notes/课程/大三（上）/数据库/数据储存结构/index.html","hash":"c809a7fc0324f3b4933ea5e59777782e6bdcc32f","modified":1709353781724},{"_id":"public/2023/11/28/Notes/课程/大三（上）/智能计算系统/编程框架机理/index.html","hash":"20c2f4abe04ffad054c7d4a87faf49af4478d671","modified":1709353781724},{"_id":"public/2023/11/20/Notes/课程/大三（上）/智能计算系统/第三章作业/index.html","hash":"bfeaeb6b8dbd145c9a19adabcb3863a8753100c4","modified":1709353781724},{"_id":"public/2023/11/16/Notes/课程/大三（上）/神经网络与深度学习/强化学习/index.html","hash":"37f7675a7b547cafe6531766b5ac04c70e91edd4","modified":1709353781724},{"_id":"public/2023/11/15/Notes/课程/大三（上）/数据库/关系型数据库设计：模式规范化/index.html","hash":"a7ab48e37ee0dc687190f32a44c6ab67a5686b57","modified":1709353781724},{"_id":"public/2023/11/09/Notes/课程/大三（上）/神经网络与深度学习/生成对抗网络/index.html","hash":"d0d76c57392d95836af0ce3cdb63fdb5a200cedc","modified":1709353781724},{"_id":"public/2023/11/09/Notes/课程/大三（上）/神经网络与深度学习/生成模型/index.html","hash":"032f1b308346b8572fc4846c8e5929b0e2a31ecd","modified":1709353781724},{"_id":"public/2023/11/09/Notes/课程/大三（上）/智能计算系统/编程框架使用/index.html","hash":"eb0ae50e386c2e39f812242cc0f8d639e397bf9d","modified":1709353781724},{"_id":"public/2023/11/03/Notes/课程/大三（上）/操作系统/Scheduling调度/index.html","hash":"af5d3be6f7adddac15a76fe3b1bfc9ea33221824","modified":1709353781724},{"_id":"public/2023/11/02/Notes/课程/大三（上）/操作系统/Processes进程/index.html","hash":"1e32b3151cc2a659247fa3c5956b919b02763888","modified":1709353781724},{"_id":"public/2023/11/01/Notes/课程/大三（上）/操作系统/Memory内存/index.html","hash":"09716eecc017fb747f1d483ee830ede164c8744a","modified":1709353781724},{"_id":"public/2023/10/30/Notes/课程/大三（上）/神经网络与深度学习/A2分析与简答/index.html","hash":"968b01161391ce2fbdd7f5279a7fd4bf180e8ee4","modified":1709353781724},{"_id":"public/2023/10/30/Notes/课程/大三（上）/数据库/使用E-R模型的数据库设计/index.html","hash":"ad389593a578e6f62e549a16a0766462391765c2","modified":1709353781724},{"_id":"public/2023/10/26/Notes/课程/大三（上）/神经网络与深度学习/图神经网络/index.html","hash":"b4755c64e4608e3ea080d6e8e82da57a20572488","modified":1709353781724},{"_id":"public/2023/11/02/Notes/课程/大三（上）/神经网络与深度学习/注意力机制/index.html","hash":"aeec058cc9fc054f69a208c56f1c02e1830005d3","modified":1709353781724},{"_id":"public/2023/10/20/Notes/课程/大三（上）/软件工程/面向对象分析/index.html","hash":"cd4c3ec0befc396c58e7e0a0e0155ae47e31e7ad","modified":1709353781724},{"_id":"public/2023/10/23/Notes/课程/大三（上）/数据库/SQL高级/index.html","hash":"d0c2707922b96d2d2bfafe5d2d128f7600b4ab9d","modified":1709353781724},{"_id":"public/2023/10/20/Notes/课程/大三（上）/软件工程/UML/index.html","hash":"a9d07b7150fce7213f9e3331e01c68d122d0c1b6","modified":1709353781724},{"_id":"public/2023/10/20/Notes/课程/大三（上）/软件工程/软件需求分析/index.html","hash":"ffc8c5613cf700a064c84ff15ab3fe07ba72c479","modified":1709353781724},{"_id":"public/2023/10/25/Notes/课程/大三（上）/操作系统/Deadlock死锁/index.html","hash":"4ee66b81bf06ad75680a20682f07ec109bded9a7","modified":1709353781724},{"_id":"public/2023/10/19/Notes/课程/大三（上）/神经网络与深度学习/循环神经网络/index.html","hash":"de6119e80b72e763b4b160a2593815735380307a","modified":1709353781724},{"_id":"public/2023/10/18/Notes/课程/大三（上）/操作系统/Monitor管程/index.html","hash":"b4c0ee1400dbae842a9fbf56170def0529a71b4a","modified":1709353781724},{"_id":"public/2023/10/16/Notes/课程/大三（上）/操作系统/Semaphores信号量/index.html","hash":"d1d469016d5f43ab11cf223009f32ba9b62b6d7a","modified":1709353781724},{"_id":"public/2023/10/12/Notes/课程/大三（上）/神经网络与深度学习/卷积神经网络/index.html","hash":"aa1ee8ad5b11598006014263916533bb235fff2e","modified":1709353781724},{"_id":"public/2023/10/10/Notes/课程/大三（上）/神经网络与深度学习/实验 用numpy搭建全连接神经网络用于手写数字识别/index.html","hash":"866dcd6edfe3300faaa428a7300e3711b15e0989","modified":1709353781724},{"_id":"public/2023/10/09/Notes/课程/大三（上）/操作系统/Threads线程/index.html","hash":"ca169b2048fcab059c3612c28828e70d5e13353b","modified":1709353781724},{"_id":"public/2023/10/09/Notes/课程/大三（上）/数据库/SQL中级/index.html","hash":"c7f145788e58fac5b6dc99257e9187b9c80f21ef","modified":1709353781724},{"_id":"public/2023/09/28/Notes/课程/大三（上）/神经网络与深度学习/网络优化与正则化/index.html","hash":"f424f25aef71690829be125d7276e0fa42f01d68","modified":1709353781724},{"_id":"public/2023/09/28/Notes/课程/大三（上）/智能计算系统/神经网络基础/index.html","hash":"ba4cff6871b949c2fcf2f7c5955ead8a135f242c","modified":1709353781724},{"_id":"public/2023/09/25/Notes/课程/大三（上）/数据库/SQL基础/index.html","hash":"fb701f94763d51b82a6043f4a8df1bfcd12167b0","modified":1709353781724},{"_id":"public/2023/09/24/Notes/课程/大三（上）/神经网络与深度学习/前馈神经网络/index.html","hash":"80b897c4fde41987ddf8f722d9517a874dd006a3","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/软件工程/软件生命周期/index.html","hash":"73a9b92dbfd90d094199e56b02d3b1e06a3fe6ac","modified":1709353781724},{"_id":"public/2023/10/19/Notes/课程/大三（上）/智能计算系统/深度学习/index.html","hash":"9af783b83fb4b26c587f73b33036f70a667ff47c","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/软件工程/大作业 酒店温控计费系统 规划/index.html","hash":"96516b7875b413acd4777ff7bef2200240585a76","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/软件工程/作业/index.html","hash":"022da9240877bc2c3df3c820047c4ca4439a81c2","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/神经网络与深度学习/神经网络与深度学习/index.html","hash":"cc925d0c3260c310ca3a48d0eda09b6bc940580c","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/智能计算系统/智能计算系统/index.html","hash":"5003dcff80f3f96f79940ede0324ddcdc0c02b35","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/操作系统/操作系统/index.html","hash":"3717cb294b30dd64ec9a11fd826b9d7879d85a16","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/数据库/数据库/index.html","hash":"9a4bb66c0c46af0ccbb025953a7d35a4ea704ee3","modified":1709353781724},{"_id":"public/2023/09/22/Notes/课程/大三（上）/操作系统/Blitz软件/index.html","hash":"2900562a0408f5044c9ab1dd182972ca3bfcadc5","modified":1709353781724},{"_id":"source/_posts/Notes/课程/嵌入式系统设计与应用/未命名.md","hash":"4f0de2fea51de1c8e9b7defcb99bc4e9402d114f","modified":1709353783750},{"_id":"source/_posts/Notes/课程/智能信息网络实验/未命名.md","hash":"a7e425d231a2739fa8af9b52bf98ee859c1942bd","modified":1709355685770},{"_id":"source/_posts/Notes/课程/计算机视觉/未命名.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1714031427414},{"_id":"source/_posts/Notes/课程/脑与认知科学基础/未命名.md","hash":"c9035bc5193612ad092ff26595b42b65a4c68696","modified":1709353783751},{"_id":"source/_posts/Notes/课程/语音信息处理/未命名.md","hash":"d124fe24f8cd8ca3b6eccdd416f96c4b2cafd8e4","modified":1709353783750},{"_id":"source/_posts/Notes/课程/项目管理与经济决策/未命名.md","hash":"27aab40a5013eb9c85abcf94e0b45e29f66eb296","modified":1709353783751},{"_id":"public/categories/Notes/课程/大三（上）/index.html","hash":"0701a10752e97a4519d5395dd7f562f39784fba8","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/page/2/index.html","hash":"d2e814d6c73847458ed8c5823b3ad049886b9a1a","modified":1709353781724},{"_id":"public/categories/Notes/课程/自然语言处理/index.html","hash":"d9e9ea3885c3a62c5581dc92fd7e831970df4da0","modified":1716050882142},{"_id":"public/categories/Notes/课程/大三（上）/page/5/index.html","hash":"7f5df521d47eb3a185a2ce5b035695a300d41b04","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/page/3/index.html","hash":"04254d1a51cae0a9c19fdef3d522a09921ecc8e4","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/page/4/index.html","hash":"d2e8e012e24cb2a35158e5dd77aa0a9cba5d7fae","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/操作系统/index.html","hash":"c0dbe823e3b5f015f23f13f1071c0f860a292446","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/智能计算系统/index.html","hash":"a29d78e534328d27fe833b995b5926c4b1213bcd","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/数据库/index.html","hash":"b81a89accc1fa3d796d08837e39ffc6456105338","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/软件工程/index.html","hash":"70771e0f41fccf14116226d9fb50c993072b6242","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/神经网络与深度学习/index.html","hash":"9941766458bc8df45f576489be25283d6220ed6d","modified":1709353781724},{"_id":"public/categories/Notes/课程/大三（上）/神经网络与深度学习/page/2/index.html","hash":"5c76759214db5544a0a01ea412f78ef722486e9d","modified":1709353781724},{"_id":"public/2024/03/02/Notes/课程/嵌入式系统设计与应用/未命名/index.html","hash":"4ca4e2b4b65bb55381a659c73de9fa2051a8c440","modified":1709556407713},{"_id":"public/2024/03/02/Notes/课程/脑与认知科学基础/未命名/index.html","hash":"eb9445dd9ebc3c18fae854bd46f98526a384f2a2","modified":1709354381756},{"_id":"public/2024/03/02/Notes/课程/计算机视觉/未命名/index.html","hash":"c1374109c8bc94b26032339d52bdc1552b37a082","modified":1709354381756},{"_id":"public/2024/03/02/Notes/课程/项目管理与经济决策/未命名/index.html","hash":"7766c8a01b174ea5f6a1c62c1756101f2325f514","modified":1709354381756},{"_id":"public/2024/03/02/Notes/课程/语音信息处理/未命名/index.html","hash":"d74bc293ac0870ddc3ba77bd7a4ed9394c0819a9","modified":1709354381756},{"_id":"public/2024/03/02/Notes/课程/智能信息网络实验/未命名/index.html","hash":"8d25e644cbffe8cc7c4f7e8e9e30ebeb0a41daf6","modified":1709356196823},{"_id":"public/categories/Notes/课程/脑与认知科学基础/index.html","hash":"836d16b33d6b3adb712719a350277e174ae1d5cf","modified":1710827052784},{"_id":"public/categories/Notes/课程/计算机视觉/index.html","hash":"7eb66123dd2ad45b040b0bb24679d38166fed988","modified":1715503045644},{"_id":"public/categories/Notes/课程/语音信息处理/index.html","hash":"158d7348cadd502ba505dff6a4d3e5131bbf6a65","modified":1713265819139},{"_id":"public/categories/Notes/课程/项目管理与经济决策/index.html","hash":"ab8e628545511260305b28eb81428b812676155a","modified":1710827052784},{"_id":"public/categories/Notes/课程/智能信息网络实验/index.html","hash":"28f3eceaa0ca92a060982c0a20334d1b46056a46","modified":1710827052784},{"_id":"public/categories/Notes/课程/嵌入式系统设计与应用/index.html","hash":"7d18f1696fe485cfb0d9e7b8b11cc80b234b9585","modified":1710827052784},{"_id":"source/_posts/Notes/考研/数学.md","hash":"3730633a0b0c4b75b305d5a16cdf57c2c0d07ef0","modified":1709556586315},{"_id":"public/archives/2024/page/2/index.html","hash":"6615c829433767c5253ccda88549e249a9099e3c","modified":1732672128096},{"_id":"public/2024/03/04/Notes/考研/数学/index.html","hash":"bd635385a443985ad1b8b08842561001117047ff","modified":1710827052784},{"_id":"source/_posts/Notes/花里胡哨/提高免疫力.md","hash":"ea040bda834085c8d735be15cc40ef1c6abf092b","modified":1709731064561},{"_id":"source/_posts/Notes/花里胡哨/BUPT抢课脚本.md","hash":"df7551f1cc6d779d0598dc52fd58e0fdfe0ad2df","modified":1709725966753},{"_id":"public/archives/page/9/index.html","hash":"00922b693dff33a0b4776d6923714ce523e941df","modified":1732672128096},{"_id":"public/page/9/index.html","hash":"46acb871f31f26a99dd5b816a9270534f3d61c85","modified":1732672128096},{"_id":"public/2024/03/04/Notes/花里胡哨/BUPT抢课脚本/index.html","hash":"bbfd9fea295b7b223d16eabaf0a6a32f9e86bcc4","modified":1709726521535},{"_id":"public/2024/03/06/Notes/花里胡哨/提高免疫力/index.html","hash":"2be1add93a6bcdbcf2347fa72e9bf39084998b3f","modified":1710827052784},{"_id":"public/categories/Notes/page/9/index.html","hash":"26cd5c67dd59ffa665fe7689c7eb990154c0ec3a","modified":1732672711732},{"_id":"source/_posts/Notes/课程/嵌入式系统设计与应用/课程介绍.md","hash":"3ed69628812670a4402b21d33747d9e1173d422d","modified":1710826465696},{"_id":"source/_posts/Notes/课程/智能信息网络实验/课程介绍.md","hash":"88bd4a44bc266c566efaff8f7b262d420260d078","modified":1710826476011},{"_id":"source/_posts/Notes/课程/脑与认知科学基础/课程介绍.md","hash":"2c23684caaf0ecfabce94b3259b219a178dc3cd3","modified":1710826462542},{"_id":"source/_posts/Notes/课程/计算机视觉/课程介绍.md","hash":"85a5a712138b7a4b0ca9999adc3124cc38f7c1d8","modified":1710826458960},{"_id":"source/_posts/Notes/课程/项目管理与经济决策/课程介绍.md","hash":"327702e44d08f7a5e357c1b05be89e9564d45f4a","modified":1710826469100},{"_id":"source/_posts/Notes/课程/语音信息处理/第一次实验.md","hash":"9d02f33eabd994ef035cc0f3f0f1bda4da2059c8","modified":1710827837655},{"_id":"source/_posts/Notes/课程/语音信息处理/课程介绍.md","hash":"5857d4641c2db5ad6193309036b969a22c3132a5","modified":1710826472195},{"_id":"public/2024/03/19/Notes/课程/语音信息处理/第一次实验/index.html","hash":"0c2f97deb2498a96e760f679dfeb081ca6fb371f","modified":1712027007793},{"_id":"public/2024/03/02/Notes/课程/脑与认知科学基础/课程介绍/index.html","hash":"6933763238565b590c51953c7edd5cfc469441c6","modified":1710827052784},{"_id":"public/2024/03/02/Notes/课程/嵌入式系统设计与应用/课程介绍/index.html","hash":"8609ae14cdbe22871f38534b634178bc76fc1135","modified":1710827052784},{"_id":"public/2024/03/02/Notes/课程/计算机视觉/课程介绍/index.html","hash":"15e8343edf012421c16c8ec98c20d9658638c134","modified":1710827052784},{"_id":"public/2024/03/02/Notes/课程/项目管理与经济决策/课程介绍/index.html","hash":"01cf355f5517a96e3a9c451b424e9af479120290","modified":1710827052784},{"_id":"public/2024/03/02/Notes/课程/语音信息处理/课程介绍/index.html","hash":"7efd44b92c07437109bd6eb53bd1d160ea92a869","modified":1710827052784},{"_id":"public/2024/03/02/Notes/课程/智能信息网络实验/课程介绍/index.html","hash":"4f436061218b693974d156935965f1ced4f2f09b","modified":1710827052784},{"_id":"public/archives/2024/03/page/2/index.html","hash":"9eb0ba7d2523badda290d95e1904a3be7f1ecaba","modified":1732672128096},{"_id":"source/_posts/Notes/课程/计算机视觉/钱币定位系统.md","hash":"8b4a6a4b9951760a088a54cf2a0994ff8784fe86","modified":1712029823386},{"_id":"public/archives/2024/04/index.html","hash":"b7d286a2bcd241772b91b12c587f6eb66dff6312","modified":1732672128096},{"_id":"public/2024/04/02/Notes/课程/计算机视觉/钱币定位系统/index.html","hash":"bc46f6390bd122771348c2ddb80bc18a730c53f9","modified":1712806495537},{"_id":"public/categories/Notes/课程/page/6/index.html","hash":"b21289919b268ee2dfbf5fc2248efb8048c83300","modified":1724638998994},{"_id":"source/_posts/Notes/课程/自然语言处理/构建汉语词向量实验.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1712805911780},{"_id":"public/2024/04/11/Notes/课程/自然语言处理/构建汉语词向量实验/index.html","hash":"e5f5c322c0d343b26ab06d30ea6a2b99bdc89809","modified":1712805919095},{"_id":"source/_posts/Notes/课程/自然语言处理/构建汉语词向量.md","hash":"d3932afa58e134f0a2fa31b2fe56363befb8564a","modified":1712815762384},{"_id":"public/2024/04/11/Notes/课程/自然语言处理/构建汉语词向量/index.html","hash":"5e1d2cace6df7654f5c40709ea5772c81576bd1d","modified":1713265219068},{"_id":"source/_posts/Notes/课程/语音信息处理/第二次实验.md","hash":"d073d4a69e37c4ba8e0f853fadf221efbc31353d","modified":1714371718506},{"_id":"public/2024/04/16/Notes/课程/语音信息处理/第二次实验/index.html","hash":"492fb218e1ea91f46682a1f71597bd33f11a4e50","modified":1714371796618},{"_id":"public/2024/04/25/Notes/课程/计算机视觉/未命名/index.html","hash":"12460e1b6b668a198fcdb6ed66e0cafdc49ef691","modified":1714031433693},{"_id":"source/_posts/Notes/课程/计算机视觉/词袋模型+SVM图像分类.md","hash":"4597aef89a81b451b812eb9cb9ff33da42374a0d","modified":1714320900754},{"_id":"public/2024/04/25/Notes/课程/计算机视觉/词袋模型+SVM图像分类/index.html","hash":"db42b1635b0a42ba6c15a6ece634877d77335c3b","modified":1715502385431},{"_id":"source/_posts/Notes/课程/计算机视觉/目标跟踪.md","hash":"777dc0d7682be121fe74286671cc43953bbcaa30","modified":1715502386556},{"_id":"public/archives/2024/05/index.html","hash":"89568b13b5dac678e634be591825228db187573e","modified":1732672128096},{"_id":"public/2024/05/12/Notes/课程/计算机视觉/目标跟踪/index.html","hash":"579f8637d061447819c3d62b21cc5dbfa1c674a9","modified":1716050307436},{"_id":"source/_posts/Notes/课程/自然语言处理/基于 Transformer 的命名实体识别.md","hash":"218e2c8f5a1ef4a8e27d6523addff4b4695d2951","modified":1716109894591},{"_id":"public/2024/05/19/Notes/课程/自然语言处理/基于 Transformer 的命名实体识别/index.html","hash":"e52ebc193ae3004e6a7c338ad22de603c66a5d62","modified":1724638418761},{"_id":"source/_posts/Notes/课程/专业实习/专业实习.md","hash":"8ff4f83e57ecde163499b139c4424710cba9d82c","modified":1724638446698},{"_id":"public/archives/2024/page/3/index.html","hash":"10185f815d5175d29ba3d965ddfb1b38c2fabc2c","modified":1732672128096},{"_id":"public/archives/2024/08/index.html","hash":"4deb9eb7478bba21c8b80021e460d6ab61434931","modified":1732672128096},{"_id":"public/2024/08/26/Notes/课程/专业实习/专业实习/index.html","hash":"d7e4ec18a7dc1baff30be1cdd55f1ac804c1c4d6","modified":1732672128096},{"_id":"public/categories/Notes/课程/专业实习/index.html","hash":"14157d8227e955ef66da3e0ca8623ca86203c28b","modified":1724638998994},{"_id":"source/_posts/Notes/毕设/论文调研.md","hash":"eae63d7d434c99b5843f6179ba44ab38a6abeb60","modified":1732672153711},{"_id":"public/archives/2024/11/index.html","hash":"5868c925fd210ba2fa2f86e94ec2e24bff9b3c37","modified":1732672128096},{"_id":"public/2024/11/27/Notes/毕设/论文调研/index.html","hash":"54991204ed4157ce3481e21ec00cf7299e1a745e","modified":1732672711732},{"_id":"public/categories/Notes/毕设/index.html","hash":"aebfc8bc02b7360b130eed74a0b9957c0ab108c1","modified":1732672711732}],"Category":[{"name":"Notes","_id":"cln1vnr4z0003tou5gonj1cg6"},{"name":"AI","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr56000jtou5f0jyf3fr"},{"name":"Git","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr57000ntou51p2o6tqt"},{"name":"Linux学习","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr58000utou5glr24ut0"},{"name":"Ob插件","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5h0027tou5caih55y1"},{"name":"旅游","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5i002gtou5aw4qh5lm"},{"name":"花里胡哨","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5k002vtou5hpnodhuy"},{"name":"编程","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5k0031tou5al0dbqcy"},{"name":"课程","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5l0036tou5h19r6mt8"},{"name":"论文","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5l003btou5eks44p4m"},{"name":"Django","parent":"cln1vnr5k0031tou5al0dbqcy","_id":"cln1vnr5q004otou56wefgn2d"},{"name":"MySQL","parent":"cln1vnr5k0031tou5al0dbqcy","_id":"cln1vnr5r004rtou51a9m0u2j"},{"name":"数据库","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"cln1vnr5r004wtou5dbq54a9x"},{"name":"软件工程","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"cln1vnr5s0055tou5afcjh52e"},{"name":"神经网络与深度学习","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"cln1vnr5u005otou56hckebov"},{"name":"操作系统","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"cln1vnr5u005rtou573qmaaap"},{"name":"智能计算系统","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"cln2g1kw70000m4u5bffv6ii9"},{"name":"Vue","parent":"cln1vnr5k0031tou5al0dbqcy","_id":"clno1xw0b0000hcu57dnvbwx1"},{"name":"训练计划","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"clo44vj7e00002su5hwqvf7sc"},{"name":"考研","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"clswlytm900005k8c4gmuc23c"},{"name":"自然语言处理","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9l3vyj0006uo8c852p2u0m"},{"name":"大三（上）","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9l3vyr0007uo8cfcr960my"},{"name":"操作系统","parent":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vyx000euo8ccw6whuki"},{"name":"数据库","parent":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz4000ouo8ce26x1g5q"},{"name":"智能计算系统","parent":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzf001euo8cae9ugysl"},{"name":"神经网络与深度学习","parent":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzg001luo8chay4btjb"},{"name":"软件工程","parent":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzk002muo8c600wd1r4"},{"name":"脑与认知科学基础","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9lgqx20000fg8c5t9xalcd"},{"name":"计算机视觉","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9lgqxc0004fg8caiu0b7wa"},{"name":"智能信息网络实验","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9lgqxe0005fg8c49xh2nh1"},{"name":"语音信息处理","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9lgqxe0007fg8cfh1e9arc"},{"name":"项目管理与经济决策","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9lgqxf000afg8c8jc22nzz"},{"name":"嵌入式系统设计与应用","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9lgqxf000dfg8c992n4ii9"},{"name":"专业实习","parent":"cln1vnr5l0036tou5h19r6mt8","_id":"cm0adj59f0000dg8c7qn58xcp"},{"name":"毕设","parent":"cln1vnr4z0003tou5gonj1cg6","_id":"cm3z8lhjp0000588c705rhuwx"}],"Data":[],"Page":[{"title":"about","date":"2023-09-20T11:24:18.000Z","layout":"about","_content":"### <div align=\"center\">I'm Yang, a student major AI🚀</div>  \n\n- ⚡ Collage student in Beijing\n\n\n- 🔭 Welcome to contact me\n  \n\n- 📫 Email: wangzhengyang@bupt.edu.cn","source":"about/index.md","raw":"---\ntitle: about\ndate: 2023-09-20 19:24:18\nlayout: about\n---\n### <div align=\"center\">I'm Yang, a student major AI🚀</div>  \n\n- ⚡ Collage student in Beijing\n\n\n- 🔭 Welcome to contact me\n  \n\n- 📫 Email: wangzhengyang@bupt.edu.cn","updated":"2023-09-27T15:03:46.322Z","path":"about/index.html","comments":1,"_id":"cln1vnr4p0000tou56rxi8p8w","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"I’m-Yang-a-student-major-AI🚀\"><a href=\"#I’m-Yang-a-student-major-AI🚀\" class=\"headerlink\" title=\"I’m Yang, a student major AI🚀\"></a><div align=\"center\">I’m Yang, a student major AI🚀</div></h3><ul>\n<li><p>⚡ Collage student in Beijing</p>\n</li>\n<li><p>🔭 Welcome to contact me</p>\n</li>\n<li><p>📫 Email: <a href=\"mailto:&#119;&#97;&#110;&#x67;&#122;&#104;&#101;&#110;&#103;&#x79;&#x61;&#110;&#103;&#x40;&#x62;&#x75;&#x70;&#116;&#46;&#101;&#100;&#x75;&#x2e;&#99;&#110;\">&#119;&#97;&#110;&#x67;&#122;&#104;&#101;&#110;&#103;&#x79;&#x61;&#110;&#103;&#x40;&#x62;&#x75;&#x70;&#116;&#46;&#101;&#100;&#x75;&#x2e;&#99;&#110;</a></p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"I’m-Yang-a-student-major-AI🚀\"><a href=\"#I’m-Yang-a-student-major-AI🚀\" class=\"headerlink\" title=\"I’m Yang, a student major AI🚀\"></a><div align=\"center\">I’m Yang, a student major AI🚀</div></h3><ul>\n<li><p>⚡ Collage student in Beijing</p>\n</li>\n<li><p>🔭 Welcome to contact me</p>\n</li>\n<li><p>📫 Email: <a href=\"mailto:&#119;&#97;&#110;&#x67;&#122;&#104;&#101;&#110;&#103;&#x79;&#x61;&#110;&#103;&#x40;&#x62;&#x75;&#x70;&#116;&#46;&#101;&#100;&#x75;&#x2e;&#99;&#110;\">&#119;&#97;&#110;&#x67;&#122;&#104;&#101;&#110;&#103;&#x79;&#x61;&#110;&#103;&#x40;&#x62;&#x75;&#x70;&#116;&#46;&#101;&#100;&#x75;&#x2e;&#99;&#110;</a></p>\n</li>\n</ul>\n"}],"Post":[{"title":"Git操作","_content":"\n[Git 教程 | 菜鸟教程](https://www.runoob.com/git/git-tutorial.html)\n### Git配置\nGit 提供了一个叫做 git config 的工具，专门用来配置或读取相应的工作环境变量。\n这些环境变量，决定了 Git 在各个环节的具体工作方式和行为。这些变量可以存放在以下三个不同的地方：\n\n- `/etc/gitconfig` 文件：系统中对所有用户都普遍适用的配置。若使用 `git config` 时用 `--system` 选项，读写的就是这个文件。\n- `~/.gitconfig` 文件：用户目录下的配置文件只适用于该用户。若使用 `git config` 时用 `--global` 选项，读写的就是这个文件。\n- 当前项目的 Git 目录中的配置文件（也就是工作目录中的 `.git/config` 文件）：这里的配置仅仅针对当前项目有效。每一个级别的配置都会覆盖上层的相同配置，所以 `.git/config` 里的配置会覆盖 `/etc/gitconfig` 中的同名变量。\n\n### Git创建仓库\n#### git init\n\nGit 使用 **git init** 命令来初始化一个 Git 仓库，Git 的很多命令都需要在 Git 的仓库中运行，所以 **git init** 是使用 Git 的第一个命令。\n\n在执行完成 **git init** 命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变。\n\n如果当前目录下有几个文件想要纳入版本控制，需要先用 git add 命令告诉 Git 开始对这些文件进行跟踪，然后提交：\n\n```\n$ git add *.c\n$ git add README\n$ git commit -m '初始化项目版本'\n```\n以上命令将目录下以 .c 结尾及 README 文件提交到仓库中。\n\n#### git clone\n\n我们使用 **git clone** 从现有 Git 仓库中拷贝项目\n克隆仓库的命令格式为：\n\n```\ngit clone <repo>\n```\n\n如果我们需要克隆到指定的目录，可以使用以下命令格式：\n\n```\ngit clone <repo> <directory>\n```\n\n**参数说明：**\n- **repo:** Git 仓库。\n- **directory:** 本地目录。\n\n### Git 基本操作\nGit 常用的是以下 6 个命令：**git clone**、**git push**、**git add** 、**git commit**、**git checkout**、**git pull**\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234622.png)\n\n#### Git工作区、暂存库和版本库\n\n- **工作区：** 电脑中的文件目录\n- **暂存区：** 英文叫 stage 或 index。一般存放在 .git 目录下的 index 文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n- **版本库：** 工作区有一个隐藏目录 .git，这个不算工作区，而是 Git 的版本库。\n\nUser在工作区修改文件，通过`git add`添加到暂存区，再通过`git commit`添加到版本库，再通过`git push`添加到远程仓库\n\n**说明：**\n- workspace：工作区\n- staging area：暂存区/缓存区\n- local repository：版本库或本地仓库\n- remote repository：远程仓库\n\n| 命令         | 说明                                     |\n| ------------ | ---------------------------------------- |\n| `git add`    | 添加文件到暂存区                         |\n| `git status` | 查看仓库当前的状态，显示有变更的文件。   |\n| `git diff`   | 比较文件的不同，即暂存区和工作区的差异。 |\n| `git commit` | 提交暂存区到本地仓库。                   |\n| `git reset`  | 回退版本。                               |\n| `git rm`     | 将文件从暂存区和工作区中删除。           |\n| `git mv`     | 移动或重命名工作区文件。                 |\n| `git remote` | 远程仓库操作                             |\n| `git fetch`  | 从远程获取代码库                         |\n| `git pull`   | 下载远程代码并合并                       |\n| `git push`   | 上传远程代码并合并                       |\n\n### Git 分支管理\n创建分支命令：\n\n```\ngit branch (branchname)\n```\n\n切换分支命令:\n\n```\ngit checkout (branchname)\n```\n\n当你切换分支的时候，Git 会用该分支的最后提交的快照替换你的工作目录的内容， 所以多个分支不需要多个目录。\n合并分支命令:\n\n```\ngit merge\n```\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234633.png)\n\n合并后删除无用分支：\n\n```\ngit branch -d newtest\n```\n\n","source":"_posts/Notes/Git/Git操作.md","raw":"---\ntitle: Git操作\ncategories:\n  - Notes\n  - Git\ntags:\n  - Git\n---\n\n[Git 教程 | 菜鸟教程](https://www.runoob.com/git/git-tutorial.html)\n### Git配置\nGit 提供了一个叫做 git config 的工具，专门用来配置或读取相应的工作环境变量。\n这些环境变量，决定了 Git 在各个环节的具体工作方式和行为。这些变量可以存放在以下三个不同的地方：\n\n- `/etc/gitconfig` 文件：系统中对所有用户都普遍适用的配置。若使用 `git config` 时用 `--system` 选项，读写的就是这个文件。\n- `~/.gitconfig` 文件：用户目录下的配置文件只适用于该用户。若使用 `git config` 时用 `--global` 选项，读写的就是这个文件。\n- 当前项目的 Git 目录中的配置文件（也就是工作目录中的 `.git/config` 文件）：这里的配置仅仅针对当前项目有效。每一个级别的配置都会覆盖上层的相同配置，所以 `.git/config` 里的配置会覆盖 `/etc/gitconfig` 中的同名变量。\n\n### Git创建仓库\n#### git init\n\nGit 使用 **git init** 命令来初始化一个 Git 仓库，Git 的很多命令都需要在 Git 的仓库中运行，所以 **git init** 是使用 Git 的第一个命令。\n\n在执行完成 **git init** 命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变。\n\n如果当前目录下有几个文件想要纳入版本控制，需要先用 git add 命令告诉 Git 开始对这些文件进行跟踪，然后提交：\n\n```\n$ git add *.c\n$ git add README\n$ git commit -m '初始化项目版本'\n```\n以上命令将目录下以 .c 结尾及 README 文件提交到仓库中。\n\n#### git clone\n\n我们使用 **git clone** 从现有 Git 仓库中拷贝项目\n克隆仓库的命令格式为：\n\n```\ngit clone <repo>\n```\n\n如果我们需要克隆到指定的目录，可以使用以下命令格式：\n\n```\ngit clone <repo> <directory>\n```\n\n**参数说明：**\n- **repo:** Git 仓库。\n- **directory:** 本地目录。\n\n### Git 基本操作\nGit 常用的是以下 6 个命令：**git clone**、**git push**、**git add** 、**git commit**、**git checkout**、**git pull**\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234622.png)\n\n#### Git工作区、暂存库和版本库\n\n- **工作区：** 电脑中的文件目录\n- **暂存区：** 英文叫 stage 或 index。一般存放在 .git 目录下的 index 文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n- **版本库：** 工作区有一个隐藏目录 .git，这个不算工作区，而是 Git 的版本库。\n\nUser在工作区修改文件，通过`git add`添加到暂存区，再通过`git commit`添加到版本库，再通过`git push`添加到远程仓库\n\n**说明：**\n- workspace：工作区\n- staging area：暂存区/缓存区\n- local repository：版本库或本地仓库\n- remote repository：远程仓库\n\n| 命令         | 说明                                     |\n| ------------ | ---------------------------------------- |\n| `git add`    | 添加文件到暂存区                         |\n| `git status` | 查看仓库当前的状态，显示有变更的文件。   |\n| `git diff`   | 比较文件的不同，即暂存区和工作区的差异。 |\n| `git commit` | 提交暂存区到本地仓库。                   |\n| `git reset`  | 回退版本。                               |\n| `git rm`     | 将文件从暂存区和工作区中删除。           |\n| `git mv`     | 移动或重命名工作区文件。                 |\n| `git remote` | 远程仓库操作                             |\n| `git fetch`  | 从远程获取代码库                         |\n| `git pull`   | 下载远程代码并合并                       |\n| `git push`   | 上传远程代码并合并                       |\n\n### Git 分支管理\n创建分支命令：\n\n```\ngit branch (branchname)\n```\n\n切换分支命令:\n\n```\ngit checkout (branchname)\n```\n\n当你切换分支的时候，Git 会用该分支的最后提交的快照替换你的工作目录的内容， 所以多个分支不需要多个目录。\n合并分支命令:\n\n```\ngit merge\n```\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234633.png)\n\n合并后删除无用分支：\n\n```\ngit branch -d newtest\n```\n\n","slug":"Notes/Git/Git操作","published":1,"date":"2023-09-19T12:38:33.932Z","updated":"2023-09-27T12:32:49.723Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr4x0002tou5bo0c25p2","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><a href=\"https://www.runoob.com/git/git-tutorial.html\">Git 教程 | 菜鸟教程</a></p>\n<h3 id=\"Git配置\"><a href=\"#Git配置\" class=\"headerlink\" title=\"Git配置\"></a>Git配置</h3><p>Git 提供了一个叫做 git config 的工具，专门用来配置或读取相应的工作环境变量。<br>这些环境变量，决定了 Git 在各个环节的具体工作方式和行为。这些变量可以存放在以下三个不同的地方：</p>\n<ul>\n<li><code>/etc/gitconfig</code> 文件：系统中对所有用户都普遍适用的配置。若使用 <code>git config</code> 时用 <code>--system</code> 选项，读写的就是这个文件。</li>\n<li><code>~/.gitconfig</code> 文件：用户目录下的配置文件只适用于该用户。若使用 <code>git config</code> 时用 <code>--global</code> 选项，读写的就是这个文件。</li>\n<li>当前项目的 Git 目录中的配置文件（也就是工作目录中的 <code>.git/config</code> 文件）：这里的配置仅仅针对当前项目有效。每一个级别的配置都会覆盖上层的相同配置，所以 <code>.git/config</code> 里的配置会覆盖 <code>/etc/gitconfig</code> 中的同名变量。</li>\n</ul>\n<h3 id=\"Git创建仓库\"><a href=\"#Git创建仓库\" class=\"headerlink\" title=\"Git创建仓库\"></a>Git创建仓库</h3><h4 id=\"git-init\"><a href=\"#git-init\" class=\"headerlink\" title=\"git init\"></a>git init</h4><p>Git 使用 <strong>git init</strong> 命令来初始化一个 Git 仓库，Git 的很多命令都需要在 Git 的仓库中运行，所以 <strong>git init</strong> 是使用 Git 的第一个命令。</p>\n<p>在执行完成 <strong>git init</strong> 命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变。</p>\n<p>如果当前目录下有几个文件想要纳入版本控制，需要先用 git add 命令告诉 Git 开始对这些文件进行跟踪，然后提交：</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\">$ git <span class=\"hljs-built_in\">add</span> *.c<br>$ git <span class=\"hljs-built_in\">add</span> README<br>$ git commit -m <span class=\"hljs-string\">&#x27;初始化项目版本&#x27;</span><br></code></pre></td></tr></table></figure>\n<p>以上命令将目录下以 .c 结尾及 README 文件提交到仓库中。</p>\n<h4 id=\"git-clone\"><a href=\"#git-clone\" class=\"headerlink\" title=\"git clone\"></a>git clone</h4><p>我们使用 <strong>git clone</strong> 从现有 Git 仓库中拷贝项目<br>克隆仓库的命令格式为：</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">git <span class=\"hljs-keyword\">clone</span> <span class=\"hljs-title\">&lt;repo</span>&gt;<br></code></pre></td></tr></table></figure>\n\n<p>如果我们需要克隆到指定的目录，可以使用以下命令格式：</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">git <span class=\"hljs-keyword\">clone</span> <span class=\"hljs-title\">&lt;repo</span>&gt; <span class=\"hljs-tag\">&lt;directory&gt;</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>参数说明：</strong></p>\n<ul>\n<li><strong>repo:</strong> Git 仓库。</li>\n<li><strong>directory:</strong> 本地目录。</li>\n</ul>\n<h3 id=\"Git-基本操作\"><a href=\"#Git-基本操作\" class=\"headerlink\" title=\"Git 基本操作\"></a>Git 基本操作</h3><p>Git 常用的是以下 6 个命令：<strong>git clone</strong>、<strong>git push</strong>、<strong>git add</strong> 、<strong>git commit</strong>、<strong>git checkout</strong>、<strong>git pull</strong><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234622.png\"></p>\n<h4 id=\"Git工作区、暂存库和版本库\"><a href=\"#Git工作区、暂存库和版本库\" class=\"headerlink\" title=\"Git工作区、暂存库和版本库\"></a>Git工作区、暂存库和版本库</h4><ul>\n<li><strong>工作区：</strong> 电脑中的文件目录</li>\n<li><strong>暂存区：</strong> 英文叫 stage 或 index。一般存放在 .git 目录下的 index 文件（.git&#x2F;index）中，所以我们把暂存区有时也叫作索引（index）。</li>\n<li><strong>版本库：</strong> 工作区有一个隐藏目录 .git，这个不算工作区，而是 Git 的版本库。</li>\n</ul>\n<p>User在工作区修改文件，通过<code>git add</code>添加到暂存区，再通过<code>git commit</code>添加到版本库，再通过<code>git push</code>添加到远程仓库</p>\n<p><strong>说明：</strong></p>\n<ul>\n<li>workspace：工作区</li>\n<li>staging area：暂存区&#x2F;缓存区</li>\n<li>local repository：版本库或本地仓库</li>\n<li>remote repository：远程仓库</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>命令</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>git add</code></td>\n<td>添加文件到暂存区</td>\n</tr>\n<tr>\n<td><code>git status</code></td>\n<td>查看仓库当前的状态，显示有变更的文件。</td>\n</tr>\n<tr>\n<td><code>git diff</code></td>\n<td>比较文件的不同，即暂存区和工作区的差异。</td>\n</tr>\n<tr>\n<td><code>git commit</code></td>\n<td>提交暂存区到本地仓库。</td>\n</tr>\n<tr>\n<td><code>git reset</code></td>\n<td>回退版本。</td>\n</tr>\n<tr>\n<td><code>git rm</code></td>\n<td>将文件从暂存区和工作区中删除。</td>\n</tr>\n<tr>\n<td><code>git mv</code></td>\n<td>移动或重命名工作区文件。</td>\n</tr>\n<tr>\n<td><code>git remote</code></td>\n<td>远程仓库操作</td>\n</tr>\n<tr>\n<td><code>git fetch</code></td>\n<td>从远程获取代码库</td>\n</tr>\n<tr>\n<td><code>git pull</code></td>\n<td>下载远程代码并合并</td>\n</tr>\n<tr>\n<td><code>git push</code></td>\n<td>上传远程代码并合并</td>\n</tr>\n</tbody></table>\n<h3 id=\"Git-分支管理\"><a href=\"#Git-分支管理\" class=\"headerlink\" title=\"Git 分支管理\"></a>Git 分支管理</h3><p>创建分支命令：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs java\">git <span class=\"hljs-title function_\">branch</span> <span class=\"hljs-params\">(branchname)</span><br></code></pre></td></tr></table></figure>\n\n<p>切换分支命令:</p>\n<figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gcode\">git checkout <span class=\"hljs-comment\">(branchname)</span><br></code></pre></td></tr></table></figure>\n\n<p>当你切换分支的时候，Git 会用该分支的最后提交的快照替换你的工作目录的内容， 所以多个分支不需要多个目录。<br>合并分支命令:</p>\n<figure class=\"highlight cos\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs cos\">git <span class=\"hljs-keyword\">merge</span><br></code></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234633.png\"></p>\n<p>合并后删除无用分支：</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ebnf\"><span class=\"hljs-attribute\">git branch -d newtest</span><br></code></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://www.runoob.com/git/git-tutorial.html\">Git 教程 | 菜鸟教程</a></p>\n<h3 id=\"Git配置\"><a href=\"#Git配置\" class=\"headerlink\" title=\"Git配置\"></a>Git配置</h3><p>Git 提供了一个叫做 git config 的工具，专门用来配置或读取相应的工作环境变量。<br>这些环境变量，决定了 Git 在各个环节的具体工作方式和行为。这些变量可以存放在以下三个不同的地方：</p>\n<ul>\n<li><code>/etc/gitconfig</code> 文件：系统中对所有用户都普遍适用的配置。若使用 <code>git config</code> 时用 <code>--system</code> 选项，读写的就是这个文件。</li>\n<li><code>~/.gitconfig</code> 文件：用户目录下的配置文件只适用于该用户。若使用 <code>git config</code> 时用 <code>--global</code> 选项，读写的就是这个文件。</li>\n<li>当前项目的 Git 目录中的配置文件（也就是工作目录中的 <code>.git/config</code> 文件）：这里的配置仅仅针对当前项目有效。每一个级别的配置都会覆盖上层的相同配置，所以 <code>.git/config</code> 里的配置会覆盖 <code>/etc/gitconfig</code> 中的同名变量。</li>\n</ul>\n<h3 id=\"Git创建仓库\"><a href=\"#Git创建仓库\" class=\"headerlink\" title=\"Git创建仓库\"></a>Git创建仓库</h3><h4 id=\"git-init\"><a href=\"#git-init\" class=\"headerlink\" title=\"git init\"></a>git init</h4><p>Git 使用 <strong>git init</strong> 命令来初始化一个 Git 仓库，Git 的很多命令都需要在 Git 的仓库中运行，所以 <strong>git init</strong> 是使用 Git 的第一个命令。</p>\n<p>在执行完成 <strong>git init</strong> 命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变。</p>\n<p>如果当前目录下有几个文件想要纳入版本控制，需要先用 git add 命令告诉 Git 开始对这些文件进行跟踪，然后提交：</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\">$ git <span class=\"hljs-built_in\">add</span> *.c<br>$ git <span class=\"hljs-built_in\">add</span> README<br>$ git commit -m <span class=\"hljs-string\">&#x27;初始化项目版本&#x27;</span><br></code></pre></td></tr></table></figure>\n<p>以上命令将目录下以 .c 结尾及 README 文件提交到仓库中。</p>\n<h4 id=\"git-clone\"><a href=\"#git-clone\" class=\"headerlink\" title=\"git clone\"></a>git clone</h4><p>我们使用 <strong>git clone</strong> 从现有 Git 仓库中拷贝项目<br>克隆仓库的命令格式为：</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">git <span class=\"hljs-keyword\">clone</span> <span class=\"hljs-title\">&lt;repo</span>&gt;<br></code></pre></td></tr></table></figure>\n\n<p>如果我们需要克隆到指定的目录，可以使用以下命令格式：</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">git <span class=\"hljs-keyword\">clone</span> <span class=\"hljs-title\">&lt;repo</span>&gt; <span class=\"hljs-tag\">&lt;directory&gt;</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>参数说明：</strong></p>\n<ul>\n<li><strong>repo:</strong> Git 仓库。</li>\n<li><strong>directory:</strong> 本地目录。</li>\n</ul>\n<h3 id=\"Git-基本操作\"><a href=\"#Git-基本操作\" class=\"headerlink\" title=\"Git 基本操作\"></a>Git 基本操作</h3><p>Git 常用的是以下 6 个命令：<strong>git clone</strong>、<strong>git push</strong>、<strong>git add</strong> 、<strong>git commit</strong>、<strong>git checkout</strong>、<strong>git pull</strong><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234622.png\"></p>\n<h4 id=\"Git工作区、暂存库和版本库\"><a href=\"#Git工作区、暂存库和版本库\" class=\"headerlink\" title=\"Git工作区、暂存库和版本库\"></a>Git工作区、暂存库和版本库</h4><ul>\n<li><strong>工作区：</strong> 电脑中的文件目录</li>\n<li><strong>暂存区：</strong> 英文叫 stage 或 index。一般存放在 .git 目录下的 index 文件（.git&#x2F;index）中，所以我们把暂存区有时也叫作索引（index）。</li>\n<li><strong>版本库：</strong> 工作区有一个隐藏目录 .git，这个不算工作区，而是 Git 的版本库。</li>\n</ul>\n<p>User在工作区修改文件，通过<code>git add</code>添加到暂存区，再通过<code>git commit</code>添加到版本库，再通过<code>git push</code>添加到远程仓库</p>\n<p><strong>说明：</strong></p>\n<ul>\n<li>workspace：工作区</li>\n<li>staging area：暂存区&#x2F;缓存区</li>\n<li>local repository：版本库或本地仓库</li>\n<li>remote repository：远程仓库</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>命令</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>git add</code></td>\n<td>添加文件到暂存区</td>\n</tr>\n<tr>\n<td><code>git status</code></td>\n<td>查看仓库当前的状态，显示有变更的文件。</td>\n</tr>\n<tr>\n<td><code>git diff</code></td>\n<td>比较文件的不同，即暂存区和工作区的差异。</td>\n</tr>\n<tr>\n<td><code>git commit</code></td>\n<td>提交暂存区到本地仓库。</td>\n</tr>\n<tr>\n<td><code>git reset</code></td>\n<td>回退版本。</td>\n</tr>\n<tr>\n<td><code>git rm</code></td>\n<td>将文件从暂存区和工作区中删除。</td>\n</tr>\n<tr>\n<td><code>git mv</code></td>\n<td>移动或重命名工作区文件。</td>\n</tr>\n<tr>\n<td><code>git remote</code></td>\n<td>远程仓库操作</td>\n</tr>\n<tr>\n<td><code>git fetch</code></td>\n<td>从远程获取代码库</td>\n</tr>\n<tr>\n<td><code>git pull</code></td>\n<td>下载远程代码并合并</td>\n</tr>\n<tr>\n<td><code>git push</code></td>\n<td>上传远程代码并合并</td>\n</tr>\n</tbody></table>\n<h3 id=\"Git-分支管理\"><a href=\"#Git-分支管理\" class=\"headerlink\" title=\"Git 分支管理\"></a>Git 分支管理</h3><p>创建分支命令：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs java\">git <span class=\"hljs-title function_\">branch</span> <span class=\"hljs-params\">(branchname)</span><br></code></pre></td></tr></table></figure>\n\n<p>切换分支命令:</p>\n<figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gcode\">git checkout <span class=\"hljs-comment\">(branchname)</span><br></code></pre></td></tr></table></figure>\n\n<p>当你切换分支的时候，Git 会用该分支的最后提交的快照替换你的工作目录的内容， 所以多个分支不需要多个目录。<br>合并分支命令:</p>\n<figure class=\"highlight cos\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs cos\">git <span class=\"hljs-keyword\">merge</span><br></code></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234633.png\"></p>\n<p>合并后删除无用分支：</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ebnf\"><span class=\"hljs-attribute\">git branch -d newtest</span><br></code></pre></td></tr></table></figure>\n\n"},{"title":"OBsidian Git使用","_content":"\n### Git使用\n第一次使用需要配置用户名和邮箱，这个数据是全局的，不需要重复配置\n```\ngit config --global user.name 用户名  # 配置用户名\ngit config --global user.email  # 配置邮箱\ngit config --global --list  # 检查是否配置成功\n```\n\n### 在Github上配置SSH key\n\n##### 原理\n\n>SSH登录安全性由非对称加密保证，产生密钥时，一次产生两个密钥，一个公钥，一个私钥，在git中一般命名为id_rsa.pub, id_rsa。\n  本地生成一个密钥对，其中公钥放到远程主机，私钥保存在本地\n  当本地主机需要登录远程主机时，本地主机向远程主机发送一个登录请求，远程收到消息后，随机生成一个字符串并用公钥加密，发回给本地。本地拿到该字符串，用存放在本地的私钥进行解密，再次发送到远程，远程比对该解密后的字符串与源字符串是否等同，如果等同则认证成功。\n  **ssh key的配置是针对每台主机的**。\n  目的是在Github上实现免密登录\n\n**第一步：检查本地主机是否已经存在ssh key**\n\n```\ncd ~/.ssh \n   ls \n   //看是否存在 id_rsa 和 id_rsa.pub文件，如果存在，说明已经有SSH Key\n```\n\n**第二步：如果不存在，生成ssh key**\n\n```\nssh-keygen -t rsa -C \"xxx@xxx.com\"\n//执行后一直回车即可\n```\n\n**第三步：获取ssh key公钥内容（id_rsa.pub）**\n\n```\ncd ~/.ssh\ncat id_rsa.pub\n```\n\n**第四步：在Github的Setting中添加key**\n**第五步：验证是否成功**\n\n```\nssh -T git@github.com\n```\n\n### 添加Github远程仓库：\n创建一个Github仓库用于存放笔记\n\n### 配置本地 Obsidian 仓库\n\n```\ntouch README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin \"替换为上图中本条命令位置提示的远端地址（形如 git@gitee.com:user/repo.git）\"\ngit push -u origin main\n```\n\n然后Obsidian Git成功同步笔记：）\n","source":"_posts/Notes/Git/Obsidian Git使用.md","raw":"---\ntitle: OBsidian Git使用\ncategories:\n  - Notes\n  - Git\ntags:\n  - Git\n  - Obsidian\n---\n\n### Git使用\n第一次使用需要配置用户名和邮箱，这个数据是全局的，不需要重复配置\n```\ngit config --global user.name 用户名  # 配置用户名\ngit config --global user.email  # 配置邮箱\ngit config --global --list  # 检查是否配置成功\n```\n\n### 在Github上配置SSH key\n\n##### 原理\n\n>SSH登录安全性由非对称加密保证，产生密钥时，一次产生两个密钥，一个公钥，一个私钥，在git中一般命名为id_rsa.pub, id_rsa。\n  本地生成一个密钥对，其中公钥放到远程主机，私钥保存在本地\n  当本地主机需要登录远程主机时，本地主机向远程主机发送一个登录请求，远程收到消息后，随机生成一个字符串并用公钥加密，发回给本地。本地拿到该字符串，用存放在本地的私钥进行解密，再次发送到远程，远程比对该解密后的字符串与源字符串是否等同，如果等同则认证成功。\n  **ssh key的配置是针对每台主机的**。\n  目的是在Github上实现免密登录\n\n**第一步：检查本地主机是否已经存在ssh key**\n\n```\ncd ~/.ssh \n   ls \n   //看是否存在 id_rsa 和 id_rsa.pub文件，如果存在，说明已经有SSH Key\n```\n\n**第二步：如果不存在，生成ssh key**\n\n```\nssh-keygen -t rsa -C \"xxx@xxx.com\"\n//执行后一直回车即可\n```\n\n**第三步：获取ssh key公钥内容（id_rsa.pub）**\n\n```\ncd ~/.ssh\ncat id_rsa.pub\n```\n\n**第四步：在Github的Setting中添加key**\n**第五步：验证是否成功**\n\n```\nssh -T git@github.com\n```\n\n### 添加Github远程仓库：\n创建一个Github仓库用于存放笔记\n\n### 配置本地 Obsidian 仓库\n\n```\ntouch README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin \"替换为上图中本条命令位置提示的远端地址（形如 git@gitee.com:user/repo.git）\"\ngit push -u origin main\n```\n\n然后Obsidian Git成功同步笔记：）\n","slug":"Notes/Git/Obsidian Git使用","published":1,"date":"2023-09-19T12:38:33.934Z","updated":"2023-09-27T12:33:08.777Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr500005tou5dhl327l4","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"Git使用\"><a href=\"#Git使用\" class=\"headerlink\" title=\"Git使用\"></a>Git使用</h3><p>第一次使用需要配置用户名和邮箱，这个数据是全局的，不需要重复配置</p>\n<figure class=\"highlight dsconfig\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs dsconfig\"><span class=\"hljs-string\">git</span> <span class=\"hljs-string\">config</span> <span class=\"hljs-built_in\">--global</span> <span class=\"hljs-string\">user</span>.<span class=\"hljs-string\">name</span> 用户名  <span class=\"hljs-comment\"># 配置用户名</span><br><span class=\"hljs-string\">git</span> <span class=\"hljs-string\">config</span> <span class=\"hljs-built_in\">--global</span> <span class=\"hljs-string\">user</span>.<span class=\"hljs-string\">email</span>  <span class=\"hljs-comment\"># 配置邮箱</span><br><span class=\"hljs-string\">git</span> <span class=\"hljs-string\">config</span> <span class=\"hljs-built_in\">--global</span> <span class=\"hljs-built_in\">--list</span>  <span class=\"hljs-comment\"># 检查是否配置成功</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"在Github上配置SSH-key\"><a href=\"#在Github上配置SSH-key\" class=\"headerlink\" title=\"在Github上配置SSH key\"></a>在Github上配置SSH key</h3><h5 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h5><blockquote>\n<p>SSH登录安全性由非对称加密保证，产生密钥时，一次产生两个密钥，一个公钥，一个私钥，在git中一般命名为id_rsa.pub, id_rsa。<br>  本地生成一个密钥对，其中公钥放到远程主机，私钥保存在本地<br>  当本地主机需要登录远程主机时，本地主机向远程主机发送一个登录请求，远程收到消息后，随机生成一个字符串并用公钥加密，发回给本地。本地拿到该字符串，用存放在本地的私钥进行解密，再次发送到远程，远程比对该解密后的字符串与源字符串是否等同，如果等同则认证成功。<br>  <strong>ssh key的配置是针对每台主机的</strong>。<br>  目的是在Github上实现免密登录</p>\n</blockquote>\n<p><strong>第一步：检查本地主机是否已经存在ssh key</strong></p>\n<figure class=\"highlight jboss-cli\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs jboss-cli\"><span class=\"hljs-keyword\">cd</span> ~<span class=\"hljs-string\">/.ssh</span> <br>   <span class=\"hljs-keyword\">ls</span> <br>   <span class=\"hljs-string\">//</span>看是否存在 id_rsa 和 id_rsa.pub文件，如果存在，说明已经有SSH Key<br></code></pre></td></tr></table></figure>\n\n<p><strong>第二步：如果不存在，生成ssh key</strong></p>\n<figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs awk\">ssh-keygen -t rsa -C <span class=\"hljs-string\">&quot;xxx@xxx.com&quot;</span><br><span class=\"hljs-regexp\">//</span>执行后一直回车即可<br></code></pre></td></tr></table></figure>\n\n<p><strong>第三步：获取ssh key公钥内容（id_rsa.pub）</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">cd</span> ~/.ssh<br><span class=\"hljs-built_in\">cat</span> id_rsa.pub<br></code></pre></td></tr></table></figure>\n\n<p><strong>第四步：在Github的Setting中添加key</strong><br><strong>第五步：验证是否成功</strong></p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nginx\"><span class=\"hljs-attribute\">ssh</span> -T git<span class=\"hljs-variable\">@github</span>.com<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"添加Github远程仓库：\"><a href=\"#添加Github远程仓库：\" class=\"headerlink\" title=\"添加Github远程仓库：\"></a>添加Github远程仓库：</h3><p>创建一个Github仓库用于存放笔记</p>\n<h3 id=\"配置本地-Obsidian-仓库\"><a href=\"#配置本地-Obsidian-仓库\" class=\"headerlink\" title=\"配置本地 Obsidian 仓库\"></a>配置本地 Obsidian 仓库</h3><figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">touch README.md<br>git init<br>git <span class=\"hljs-keyword\">add </span>README.md<br>git commit -m <span class=\"hljs-string\">&quot;first commit&quot;</span><br>git <span class=\"hljs-keyword\">branch </span>-M main<br>git remote <span class=\"hljs-keyword\">add </span><span class=\"hljs-keyword\">origin </span><span class=\"hljs-string\">&quot;替换为上图中本条命令位置提示的远端地址（形如 git@gitee.com:user/repo.git）&quot;</span><br>git push -u <span class=\"hljs-keyword\">origin </span>main<br></code></pre></td></tr></table></figure>\n\n<p>然后Obsidian Git成功同步笔记：）</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Git使用\"><a href=\"#Git使用\" class=\"headerlink\" title=\"Git使用\"></a>Git使用</h3><p>第一次使用需要配置用户名和邮箱，这个数据是全局的，不需要重复配置</p>\n<figure class=\"highlight dsconfig\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs dsconfig\"><span class=\"hljs-string\">git</span> <span class=\"hljs-string\">config</span> <span class=\"hljs-built_in\">--global</span> <span class=\"hljs-string\">user</span>.<span class=\"hljs-string\">name</span> 用户名  <span class=\"hljs-comment\"># 配置用户名</span><br><span class=\"hljs-string\">git</span> <span class=\"hljs-string\">config</span> <span class=\"hljs-built_in\">--global</span> <span class=\"hljs-string\">user</span>.<span class=\"hljs-string\">email</span>  <span class=\"hljs-comment\"># 配置邮箱</span><br><span class=\"hljs-string\">git</span> <span class=\"hljs-string\">config</span> <span class=\"hljs-built_in\">--global</span> <span class=\"hljs-built_in\">--list</span>  <span class=\"hljs-comment\"># 检查是否配置成功</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"在Github上配置SSH-key\"><a href=\"#在Github上配置SSH-key\" class=\"headerlink\" title=\"在Github上配置SSH key\"></a>在Github上配置SSH key</h3><h5 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h5><blockquote>\n<p>SSH登录安全性由非对称加密保证，产生密钥时，一次产生两个密钥，一个公钥，一个私钥，在git中一般命名为id_rsa.pub, id_rsa。<br>  本地生成一个密钥对，其中公钥放到远程主机，私钥保存在本地<br>  当本地主机需要登录远程主机时，本地主机向远程主机发送一个登录请求，远程收到消息后，随机生成一个字符串并用公钥加密，发回给本地。本地拿到该字符串，用存放在本地的私钥进行解密，再次发送到远程，远程比对该解密后的字符串与源字符串是否等同，如果等同则认证成功。<br>  <strong>ssh key的配置是针对每台主机的</strong>。<br>  目的是在Github上实现免密登录</p>\n</blockquote>\n<p><strong>第一步：检查本地主机是否已经存在ssh key</strong></p>\n<figure class=\"highlight jboss-cli\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs jboss-cli\"><span class=\"hljs-keyword\">cd</span> ~<span class=\"hljs-string\">/.ssh</span> <br>   <span class=\"hljs-keyword\">ls</span> <br>   <span class=\"hljs-string\">//</span>看是否存在 id_rsa 和 id_rsa.pub文件，如果存在，说明已经有SSH Key<br></code></pre></td></tr></table></figure>\n\n<p><strong>第二步：如果不存在，生成ssh key</strong></p>\n<figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs awk\">ssh-keygen -t rsa -C <span class=\"hljs-string\">&quot;xxx@xxx.com&quot;</span><br><span class=\"hljs-regexp\">//</span>执行后一直回车即可<br></code></pre></td></tr></table></figure>\n\n<p><strong>第三步：获取ssh key公钥内容（id_rsa.pub）</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">cd</span> ~/.ssh<br><span class=\"hljs-built_in\">cat</span> id_rsa.pub<br></code></pre></td></tr></table></figure>\n\n<p><strong>第四步：在Github的Setting中添加key</strong><br><strong>第五步：验证是否成功</strong></p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nginx\"><span class=\"hljs-attribute\">ssh</span> -T git<span class=\"hljs-variable\">@github</span>.com<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"添加Github远程仓库：\"><a href=\"#添加Github远程仓库：\" class=\"headerlink\" title=\"添加Github远程仓库：\"></a>添加Github远程仓库：</h3><p>创建一个Github仓库用于存放笔记</p>\n<h3 id=\"配置本地-Obsidian-仓库\"><a href=\"#配置本地-Obsidian-仓库\" class=\"headerlink\" title=\"配置本地 Obsidian 仓库\"></a>配置本地 Obsidian 仓库</h3><figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">touch README.md<br>git init<br>git <span class=\"hljs-keyword\">add </span>README.md<br>git commit -m <span class=\"hljs-string\">&quot;first commit&quot;</span><br>git <span class=\"hljs-keyword\">branch </span>-M main<br>git remote <span class=\"hljs-keyword\">add </span><span class=\"hljs-keyword\">origin </span><span class=\"hljs-string\">&quot;替换为上图中本条命令位置提示的远端地址（形如 git@gitee.com:user/repo.git）&quot;</span><br>git push -u <span class=\"hljs-keyword\">origin </span>main<br></code></pre></td></tr></table></figure>\n\n<p>然后Obsidian Git成功同步笔记：）</p>\n"},{"title":"AI绘画","update":null,"_content":"## stable diffusion 模型画任意要求\n\n飞桨： [文生图[多loRA,ControlNet预处理,高清v2,视频生成]v11_副本](https://aistudio.baidu.com/projectdetail/6665563)\nkaggle：[免费部署stable diffusion，白嫖32G GPU - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/643442494)\n## ai 二维码美化\n\n飞桨：[AI艺术二维码生成器]([【创意应用】AI艺术二维码生成 - 飞桨AI Studio (baidu.com)](https://aistudio.baidu.com/projectdetail/6452331))\n\n\n\n---\n\n#### Stable Diffusion使用\n[GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)在Github上下载压缩包并按照说明安装\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234607.png)\n\n**添加模型：** 在[Civitai | Stable Diffusion models, embeddings, LoRAs and more](https://civitai.com/)上下载模型，添加到相应文件夹。\n- Checkpoint:基本模型，下载到webui\\\\models\\\\Stable-diffusion\n- Lora:微调模型，下载到webui\\\\models\\\\Lora\n- 其他（待补充）\n\n**插件下载：**\n- 中文汉化：[GitHub - VinsonLaro/stable-diffusion-webui-chinese: stable-diffusion-webui 的汉化扩展](https://github.com/VinsonLaro/stable-diffusion-webui-chinese)作者写了非常详细的下载和使用教程\n- controlnet：[GitHub - lllyasviel/ControlNet-v1-1-nightly: Nightly release of ControlNet 1.1](https://github.com/lllyasviel/ControlNet-v1-1-nightly)同上的下载方式\n**注意：** 下载时可能会因为网络问题下载失败，挂梯子多尝试几次。下载失败后需要到webui\\\\tmp中删除文件夹，否则无法再次下载。\n\n**Controlnet使用：** 下载controlnet插件后，按照[GitHub - Mikubill/sd-webui-controlnet: WebUI extension for ControlNet](https://github.com/Mikubill/sd-webui-controlnet)提供的地址下载controlnet模型（14个），放到webui\\\\extensions\\\\sd-webui-controlnet\\\\models目录下\n此时在WebUI中即可看到controlnet界面，有许多参数可选项可调\n可参考b站教程：[“牛逼”的教程来了！一次学会AI二维码+艺术字+光影光效+创意Logo生成，绝对是B站最详细的Stable Diffusion特效设计流程教学！AI绘画进阶应用\\_哔哩哔哩\\_bilibili](https://www.bilibili.com/video/BV1gX4y1J7ei/?spm_id_from=333.788.recommend_more_video.-1&vd_source=7d4ddbfe6a66f2fbe94075935b693c57)\n\n**模型配置方案：** \nSD提供了太多的模型组合，在此记录一下模型搭配及其效果\n\n**提示词：** \n一些通用的提示词可以提高图像质量，特定的提示词可以指定生成内容","source":"_posts/Notes/AI/AI绘画.md","raw":"---\ntitle: AI绘画\ncategories:\n  - Notes\n  - AI\nupdate: \ntags:\n  - StableDiffusion\n---\n## stable diffusion 模型画任意要求\n\n飞桨： [文生图[多loRA,ControlNet预处理,高清v2,视频生成]v11_副本](https://aistudio.baidu.com/projectdetail/6665563)\nkaggle：[免费部署stable diffusion，白嫖32G GPU - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/643442494)\n## ai 二维码美化\n\n飞桨：[AI艺术二维码生成器]([【创意应用】AI艺术二维码生成 - 飞桨AI Studio (baidu.com)](https://aistudio.baidu.com/projectdetail/6452331))\n\n\n\n---\n\n#### Stable Diffusion使用\n[GitHub - AUTOMATIC1111/stable-diffusion-webui: Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)在Github上下载压缩包并按照说明安装\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234607.png)\n\n**添加模型：** 在[Civitai | Stable Diffusion models, embeddings, LoRAs and more](https://civitai.com/)上下载模型，添加到相应文件夹。\n- Checkpoint:基本模型，下载到webui\\\\models\\\\Stable-diffusion\n- Lora:微调模型，下载到webui\\\\models\\\\Lora\n- 其他（待补充）\n\n**插件下载：**\n- 中文汉化：[GitHub - VinsonLaro/stable-diffusion-webui-chinese: stable-diffusion-webui 的汉化扩展](https://github.com/VinsonLaro/stable-diffusion-webui-chinese)作者写了非常详细的下载和使用教程\n- controlnet：[GitHub - lllyasviel/ControlNet-v1-1-nightly: Nightly release of ControlNet 1.1](https://github.com/lllyasviel/ControlNet-v1-1-nightly)同上的下载方式\n**注意：** 下载时可能会因为网络问题下载失败，挂梯子多尝试几次。下载失败后需要到webui\\\\tmp中删除文件夹，否则无法再次下载。\n\n**Controlnet使用：** 下载controlnet插件后，按照[GitHub - Mikubill/sd-webui-controlnet: WebUI extension for ControlNet](https://github.com/Mikubill/sd-webui-controlnet)提供的地址下载controlnet模型（14个），放到webui\\\\extensions\\\\sd-webui-controlnet\\\\models目录下\n此时在WebUI中即可看到controlnet界面，有许多参数可选项可调\n可参考b站教程：[“牛逼”的教程来了！一次学会AI二维码+艺术字+光影光效+创意Logo生成，绝对是B站最详细的Stable Diffusion特效设计流程教学！AI绘画进阶应用\\_哔哩哔哩\\_bilibili](https://www.bilibili.com/video/BV1gX4y1J7ei/?spm_id_from=333.788.recommend_more_video.-1&vd_source=7d4ddbfe6a66f2fbe94075935b693c57)\n\n**模型配置方案：** \nSD提供了太多的模型组合，在此记录一下模型搭配及其效果\n\n**提示词：** \n一些通用的提示词可以提高图像质量，特定的提示词可以指定生成内容","slug":"Notes/AI/AI绘画","published":1,"date":"2023-09-21T16:36:34.332Z","updated":"2023-11-05T07:45:19.816Z","_id":"cln1vnr510006tou5048m8qov","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h2 id=\"stable-diffusion-模型画任意要求\"><a href=\"#stable-diffusion-模型画任意要求\" class=\"headerlink\" title=\"stable diffusion 模型画任意要求\"></a>stable diffusion 模型画任意要求</h2><p>飞桨： <a href=\"https://aistudio.baidu.com/projectdetail/6665563\">文生图[多loRA,ControlNet预处理,高清v2,视频生成]v11_副本</a><br>kaggle：<a href=\"https://zhuanlan.zhihu.com/p/643442494\">免费部署stable diffusion，白嫖32G GPU - 知乎 (zhihu.com)</a></p>\n<h2 id=\"ai-二维码美化\"><a href=\"#ai-二维码美化\" class=\"headerlink\" title=\"ai 二维码美化\"></a>ai 二维码美化</h2><p>飞桨：[AI艺术二维码生成器](<a href=\"https://aistudio.baidu.com/projectdetail/6452331\">【创意应用】AI艺术二维码生成 - 飞桨AI Studio (baidu.com)</a>)</p>\n<hr>\n<h4 id=\"Stable-Diffusion使用\"><a href=\"#Stable-Diffusion使用\" class=\"headerlink\" title=\"Stable Diffusion使用\"></a>Stable Diffusion使用</h4><p><a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">GitHub - AUTOMATIC1111&#x2F;stable-diffusion-webui: Stable Diffusion web UI</a>在Github上下载压缩包并按照说明安装<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234607.png\"></p>\n<p><strong>添加模型：</strong> 在<a href=\"https://civitai.com/\">Civitai | Stable Diffusion models, embeddings, LoRAs and more</a>上下载模型，添加到相应文件夹。</p>\n<ul>\n<li>Checkpoint:基本模型，下载到webui\\models\\Stable-diffusion</li>\n<li>Lora:微调模型，下载到webui\\models\\Lora</li>\n<li>其他（待补充）</li>\n</ul>\n<p><strong>插件下载：</strong></p>\n<ul>\n<li>中文汉化：<a href=\"https://github.com/VinsonLaro/stable-diffusion-webui-chinese\">GitHub - VinsonLaro&#x2F;stable-diffusion-webui-chinese: stable-diffusion-webui 的汉化扩展</a>作者写了非常详细的下载和使用教程</li>\n<li>controlnet：<a href=\"https://github.com/lllyasviel/ControlNet-v1-1-nightly\">GitHub - lllyasviel&#x2F;ControlNet-v1-1-nightly: Nightly release of ControlNet 1.1</a>同上的下载方式<br><strong>注意：</strong> 下载时可能会因为网络问题下载失败，挂梯子多尝试几次。下载失败后需要到webui\\tmp中删除文件夹，否则无法再次下载。</li>\n</ul>\n<p><strong>Controlnet使用：</strong> 下载controlnet插件后，按照<a href=\"https://github.com/Mikubill/sd-webui-controlnet\">GitHub - Mikubill&#x2F;sd-webui-controlnet: WebUI extension for ControlNet</a>提供的地址下载controlnet模型（14个），放到webui\\extensions\\sd-webui-controlnet\\models目录下<br>此时在WebUI中即可看到controlnet界面，有许多参数可选项可调<br>可参考b站教程：<a href=\"https://www.bilibili.com/video/BV1gX4y1J7ei/?spm_id_from=333.788.recommend_more_video.-1&vd_source=7d4ddbfe6a66f2fbe94075935b693c57\">“牛逼”的教程来了！一次学会AI二维码+艺术字+光影光效+创意Logo生成，绝对是B站最详细的Stable Diffusion特效设计流程教学！AI绘画进阶应用_哔哩哔哩_bilibili</a></p>\n<p><strong>模型配置方案：</strong><br>SD提供了太多的模型组合，在此记录一下模型搭配及其效果</p>\n<p><strong>提示词：</strong><br>一些通用的提示词可以提高图像质量，特定的提示词可以指定生成内容</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"stable-diffusion-模型画任意要求\"><a href=\"#stable-diffusion-模型画任意要求\" class=\"headerlink\" title=\"stable diffusion 模型画任意要求\"></a>stable diffusion 模型画任意要求</h2><p>飞桨： <a href=\"https://aistudio.baidu.com/projectdetail/6665563\">文生图[多loRA,ControlNet预处理,高清v2,视频生成]v11_副本</a><br>kaggle：<a href=\"https://zhuanlan.zhihu.com/p/643442494\">免费部署stable diffusion，白嫖32G GPU - 知乎 (zhihu.com)</a></p>\n<h2 id=\"ai-二维码美化\"><a href=\"#ai-二维码美化\" class=\"headerlink\" title=\"ai 二维码美化\"></a>ai 二维码美化</h2><p>飞桨：[AI艺术二维码生成器](<a href=\"https://aistudio.baidu.com/projectdetail/6452331\">【创意应用】AI艺术二维码生成 - 飞桨AI Studio (baidu.com)</a>)</p>\n<hr>\n<h4 id=\"Stable-Diffusion使用\"><a href=\"#Stable-Diffusion使用\" class=\"headerlink\" title=\"Stable Diffusion使用\"></a>Stable Diffusion使用</h4><p><a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">GitHub - AUTOMATIC1111&#x2F;stable-diffusion-webui: Stable Diffusion web UI</a>在Github上下载压缩包并按照说明安装<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234607.png\"></p>\n<p><strong>添加模型：</strong> 在<a href=\"https://civitai.com/\">Civitai | Stable Diffusion models, embeddings, LoRAs and more</a>上下载模型，添加到相应文件夹。</p>\n<ul>\n<li>Checkpoint:基本模型，下载到webui\\models\\Stable-diffusion</li>\n<li>Lora:微调模型，下载到webui\\models\\Lora</li>\n<li>其他（待补充）</li>\n</ul>\n<p><strong>插件下载：</strong></p>\n<ul>\n<li>中文汉化：<a href=\"https://github.com/VinsonLaro/stable-diffusion-webui-chinese\">GitHub - VinsonLaro&#x2F;stable-diffusion-webui-chinese: stable-diffusion-webui 的汉化扩展</a>作者写了非常详细的下载和使用教程</li>\n<li>controlnet：<a href=\"https://github.com/lllyasviel/ControlNet-v1-1-nightly\">GitHub - lllyasviel&#x2F;ControlNet-v1-1-nightly: Nightly release of ControlNet 1.1</a>同上的下载方式<br><strong>注意：</strong> 下载时可能会因为网络问题下载失败，挂梯子多尝试几次。下载失败后需要到webui\\tmp中删除文件夹，否则无法再次下载。</li>\n</ul>\n<p><strong>Controlnet使用：</strong> 下载controlnet插件后，按照<a href=\"https://github.com/Mikubill/sd-webui-controlnet\">GitHub - Mikubill&#x2F;sd-webui-controlnet: WebUI extension for ControlNet</a>提供的地址下载controlnet模型（14个），放到webui\\extensions\\sd-webui-controlnet\\models目录下<br>此时在WebUI中即可看到controlnet界面，有许多参数可选项可调<br>可参考b站教程：<a href=\"https://www.bilibili.com/video/BV1gX4y1J7ei/?spm_id_from=333.788.recommend_more_video.-1&vd_source=7d4ddbfe6a66f2fbe94075935b693c57\">“牛逼”的教程来了！一次学会AI二维码+艺术字+光影光效+创意Logo生成，绝对是B站最详细的Stable Diffusion特效设计流程教学！AI绘画进阶应用_哔哩哔哩_bilibili</a></p>\n<p><strong>模型配置方案：</strong><br>SD提供了太多的模型组合，在此记录一下模型搭配及其效果</p>\n<p><strong>提示词：</strong><br>一些通用的提示词可以提高图像质量，特定的提示词可以指定生成内容</p>\n"},{"title":"连接Github","date":"2023-09-24T05:22:26.422Z","_content":"### git连接远程仓库\n从github克隆仓库到本地\n```\ngit clone ssh\n```\n\n连接远程仓库\n```\ngit remote add origin ssh\n```\n\n### 分支管理\n本地新建分支后push到github\n```\ngit branch -v  # 查看分支列表\ngit branch 分支名  # 新建分支\ngit checkout 分支名  # 转到该分支\n```\n\n删除本地分支\n```\ngit branch -d 分支名  # 删除本地分支\ngit branch -D 分支名  # 强制删除本地分支\n```\n删除远程分支\n```\ngit push origin --delete 分支名  # 删除远程分支\n```","source":"_posts/Notes/Git/连接Github.md","raw":"---\ntitle: 连接Github\ncategories:\n  - Notes\n  - Git\ndate: \ntags:\n  - Github\n  - Git\n---\n### git连接远程仓库\n从github克隆仓库到本地\n```\ngit clone ssh\n```\n\n连接远程仓库\n```\ngit remote add origin ssh\n```\n\n### 分支管理\n本地新建分支后push到github\n```\ngit branch -v  # 查看分支列表\ngit branch 分支名  # 新建分支\ngit checkout 分支名  # 转到该分支\n```\n\n删除本地分支\n```\ngit branch -d 分支名  # 删除本地分支\ngit branch -D 分支名  # 强制删除本地分支\n```\n删除远程分支\n```\ngit push origin --delete 分支名  # 删除远程分支\n```","slug":"Notes/Git/连接Github","published":1,"updated":"2023-09-27T12:32:54.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr520007tou52vika6pq","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"git连接远程仓库\"><a href=\"#git连接远程仓库\" class=\"headerlink\" title=\"git连接远程仓库\"></a>git连接远程仓库</h3><p>从github克隆仓库到本地</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">git <span class=\"hljs-keyword\">clone</span> <span class=\"hljs-title\">ssh</span><br></code></pre></td></tr></table></figure>\n\n<p>连接远程仓库</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">git remote <span class=\"hljs-keyword\">add </span><span class=\"hljs-keyword\">origin </span>ssh<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"分支管理\"><a href=\"#分支管理\" class=\"headerlink\" title=\"分支管理\"></a>分支管理</h3><p>本地新建分支后push到github</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">git <span class=\"hljs-keyword\">branch </span>-v  <span class=\"hljs-comment\"># 查看分支列表</span><br>git <span class=\"hljs-keyword\">branch </span>分支名  <span class=\"hljs-comment\"># 新建分支</span><br>git checkout 分支名  <span class=\"hljs-comment\"># 转到该分支</span><br></code></pre></td></tr></table></figure>\n\n<p>删除本地分支</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">git <span class=\"hljs-keyword\">branch </span>-d 分支名  <span class=\"hljs-comment\"># 删除本地分支</span><br>git <span class=\"hljs-keyword\">branch </span>-D 分支名  <span class=\"hljs-comment\"># 强制删除本地分支</span><br></code></pre></td></tr></table></figure>\n<p>删除远程分支</p>\n<figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gauss\">git <span class=\"hljs-keyword\">push</span> origin --<span class=\"hljs-keyword\">delete</span> 分支名  <span class=\"hljs-meta\"># 删除远程分支</span><br></code></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"git连接远程仓库\"><a href=\"#git连接远程仓库\" class=\"headerlink\" title=\"git连接远程仓库\"></a>git连接远程仓库</h3><p>从github克隆仓库到本地</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">git <span class=\"hljs-keyword\">clone</span> <span class=\"hljs-title\">ssh</span><br></code></pre></td></tr></table></figure>\n\n<p>连接远程仓库</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">git remote <span class=\"hljs-keyword\">add </span><span class=\"hljs-keyword\">origin </span>ssh<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"分支管理\"><a href=\"#分支管理\" class=\"headerlink\" title=\"分支管理\"></a>分支管理</h3><p>本地新建分支后push到github</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">git <span class=\"hljs-keyword\">branch </span>-v  <span class=\"hljs-comment\"># 查看分支列表</span><br>git <span class=\"hljs-keyword\">branch </span>分支名  <span class=\"hljs-comment\"># 新建分支</span><br>git checkout 分支名  <span class=\"hljs-comment\"># 转到该分支</span><br></code></pre></td></tr></table></figure>\n\n<p>删除本地分支</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mipsasm\">git <span class=\"hljs-keyword\">branch </span>-d 分支名  <span class=\"hljs-comment\"># 删除本地分支</span><br>git <span class=\"hljs-keyword\">branch </span>-D 分支名  <span class=\"hljs-comment\"># 强制删除本地分支</span><br></code></pre></td></tr></table></figure>\n<p>删除远程分支</p>\n<figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gauss\">git <span class=\"hljs-keyword\">push</span> origin --<span class=\"hljs-keyword\">delete</span> 分支名  <span class=\"hljs-meta\"># 删除远程分支</span><br></code></pre></td></tr></table></figure>"},{"title":"Shell编程","_content":"打开文本编辑器(可以使用 vi/vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell）\n`#!` 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell\n保存为 test.sh，并 cd 到相应目录\n\n```\nchmod +x ./test.sh  #使脚本具有执行权限\n./test.sh  #执行脚本\n```\n\n","source":"_posts/Notes/Linux学习/Shell编程.md","raw":"---\ntitle: Shell编程\ncategories:\n  - Notes\n  - Linux学习\ntags:\n  - Linux\n---\n打开文本编辑器(可以使用 vi/vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell）\n`#!` 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell\n保存为 test.sh，并 cd 到相应目录\n\n```\nchmod +x ./test.sh  #使脚本具有执行权限\n./test.sh  #执行脚本\n```\n\n","slug":"Notes/Linux学习/Shell编程","published":1,"date":"2023-09-19T12:47:32.971Z","updated":"2023-09-27T12:33:48.737Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr53000btou5fi1u1wxe","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>打开文本编辑器(可以使用 vi&#x2F;vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell）<br><code>#!</code> 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell<br>保存为 test.sh，并 cd 到相应目录</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">chmod</span> +x ./test.sh  <span class=\"hljs-comment\">#使脚本具有执行权限</span><br>./test.sh  <span class=\"hljs-comment\">#执行脚本</span><br></code></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"","more":"<p>打开文本编辑器(可以使用 vi&#x2F;vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell）<br><code>#!</code> 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell<br>保存为 test.sh，并 cd 到相应目录</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">chmod</span> +x ./test.sh  <span class=\"hljs-comment\">#使脚本具有执行权限</span><br>./test.sh  <span class=\"hljs-comment\">#执行脚本</span><br></code></pre></td></tr></table></figure>\n\n"},{"title":"基本命令","_content":"命令的一般语法：命令 [-选项] [参数列表]\n\nls：列出当前目录下所有文件和文件夹\n\tls -a：显示所有文件夹（隐藏文件夹）\n\tls -l：显示文件属性\n\tls -la：结合前两个\nhelp：查看内建命令的作用及使用方法\n\t\t\t外部命令：命令 --help\ntype：查看指定命令是否为内建指令\nman：查阅操作手册（manual）\n","source":"_posts/Notes/Linux学习/基本命令.md","raw":"---\ntitle: 基本命令\ncategories:\n  - Notes\n  - Linux学习\ntags:\n  - Linux\n---\n命令的一般语法：命令 [-选项] [参数列表]\n\nls：列出当前目录下所有文件和文件夹\n\tls -a：显示所有文件夹（隐藏文件夹）\n\tls -l：显示文件属性\n\tls -la：结合前两个\nhelp：查看内建命令的作用及使用方法\n\t\t\t外部命令：命令 --help\ntype：查看指定命令是否为内建指令\nman：查阅操作手册（manual）\n","slug":"Notes/Linux学习/基本命令","published":1,"date":"2023-09-19T12:47:32.974Z","updated":"2023-09-27T12:33:28.253Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr54000ctou55wf55oh0","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>命令的一般语法：命令 [-选项] [参数列表]</p>\n<p>ls：列出当前目录下所有文件和文件夹<br>    ls -a：显示所有文件夹（隐藏文件夹）<br>    ls -l：显示文件属性<br>    ls -la：结合前两个<br>help：查看内建命令的作用及使用方法<br>            外部命令：命令 –help<br>type：查看指定命令是否为内建指令<br>man：查阅操作手册（manual）</p>\n","site":{"data":{}},"excerpt":"","more":"<p>命令的一般语法：命令 [-选项] [参数列表]</p>\n<p>ls：列出当前目录下所有文件和文件夹<br>    ls -a：显示所有文件夹（隐藏文件夹）<br>    ls -l：显示文件属性<br>    ls -la：结合前两个<br>help：查看内建命令的作用及使用方法<br>            外部命令：命令 –help<br>type：查看指定命令是否为内建指令<br>man：查阅操作手册（manual）</p>\n"},{"title":"文件系统","_content":"### 目录与路径\n\n| 符号  | 含义         |\n| ----- | ------------ |\n| /     | 根目录       |\n| /bin  | 可执行文件   |\n| /home | 用户的主目录 |\n\n**特殊目录符号：**\n\n| 目录         |                        释义| \n| ------------ | ---------------------- |\n| .            | 代表当前目录           |\n| ..           | 代表上层目录           |\n| -            | 代表上一个工作目录     |\n| ~            | 代表当前用户的主目录   |\n| ~accountname | 代表该name账户的主目录 |\n\n### PATH环境变量\n当前工作目录下的命令和程序可以直接在bash中执行，其他目录下的命令和程序无法执行。而PATH中的目录下的命令和程序可以在任何目录下执行。当执行命令时，会先在当前目录中寻找，如果存在即执行命令，如果不存在，则会在PATH中寻找并执行第一个相匹配的命令。\n```\n# 显示环境变量中的内容，每个路径用：隔开\necho $PATH      # $与PATH间没有空格\n\n# 添加路径到PATH环境变量\nexport PATH=命令行路径:$PATH\n```\n\n### 目录管理\n\n**显示当前工作目录：**\n\n```\n# pwd显示当前工作目录（print working directory）\npwd\n\n# 用help查看pwd命令的帮助信息\nhelp pwd\n```\n\n**查看目录与文件：**\n\n```\n# -l查看文件属性\nls -l\n\n# -h以方便阅读的单位显示文件尺寸\nls -lh\n\n# -S按照文件大小降序排列\nls -lS\n\n# -R可以递归地显示所有路径\nls -R\n```\n\n**常用目录操作：**\n\n```\n# 新建目录\nmkdir 文件名\n\n# 用-p创建多级目录\nmkdir -p 路径\n\n# 复制目录\ncp -r 源路径 目标路径\n\n# 移动目录\nmv 源路径 目标路径\n\n# 删除目录，加-r递归删除目录中的内容\nrm -r 路径\n```\n\n### 文件管理\n\n#### 文件类型：\n**普通文件类型**：\nLinux中最多的一种文件类型, 包括 纯文本文件(ASCII)；二进制文件(binary)；数据格式的文件(data);各种压缩文件.第一个属性为 [-]\n**目录文件**：\n就是目录， 能用 # cd 命令进入的。第一个属性为 [d]，例如 [drwxrwxrwx]\n**块设备文件**：\n块设备文件 ： 就是存储数据以供系统存取的接口设备，简单而言就是硬盘。例如一号硬盘的代码是 /dev/hda1等文件。第一个属性为 [b]\n**字符设备**：\n字符设备文件：即串行端口的接口设备，例如键盘、鼠标等等。第一个属性为 [c]\n**套接字文件**：\n这类文件通常用在网络数据连接。可以启动一个程序来监听客户端的要求，客户端就可以通过套接字来进行数据通信。第一个属性为 [s]，最常在 /var/run目录中看到这种文件类型\n**管道文件**：\nFIFO也是一种特殊的文件类型，它主要的目的是，解决多个程序同时存取一个文件所造成的错误。FIFO是first-in-first-out(先进先出)的缩写。第一个属性为 [p]\n**链接文件**：\n类似Windows下面的快捷方式。第一个属性为 [l]，例如 [lrwxrwxrwx]\n\n#### 管道\n\n管道是一系列将标准输入输出连接起来的进程\n\n**匿名管道：** 用符号“|”表示，用来连接多个命令，将前一个命令的输出导入第二个命令作为输入\n```\n# 查看ls中包含某个关键词的文件\nls -l | grep 关键词\n```\n\n**命名管道：**\n```\n# 用mkfifo命令创建命名管道（FIFO）\nmkfifo 管道文件名\n```\n\n#### 新建文件\n\n- touch：文件存在时，修改文件访问时间为当前时间，否则创建该文件\n- 用文本编辑器创建文件\n- 重定向方式创建文件：1）“>”操作符：覆盖源文件中已有内容  2）“>>”操作符：将新内容追加到源文件内容的后面\n\n| 命令            | 说明                                               |\n| --------------- | -------------------------------------------------- |\n| command > file  | 将输出重定向到 file。                              |\n| command < file  | 将输入重定向到 file。                              |\n| command >> file | 将输出以追加的方式重定向到 file。                  |\n| n > file        | 将文件描述符为 n 的文件重定向到 file。             |\n| n >> file       | 将文件描述符为 n 的文件以追加的方式重定向到 file。 |\n| n >& m          | 将输出文件 m 和 n 合并。                           |\n| n <& m          | 将输入文件 m 和 n 合并。                           |\n| << tag          | 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 |\n\n#### 复制、移动、删除文件\n\n```\n# 使用cp命令复制文件\ncp 文件名 新文件名\n\n# 用cp命令创建快捷方式（符号链接）\ncp -s 文件名 快捷方式名\n\n# 使用cp命令-p选项复制文件的完整属性\ncp -p 文件名 新文件名\n\n# 使用mv命令移动文件\nmv 文件名 指定路径\n\n# 使用mv命令-i选项移动文件时会提示是否覆盖\nmv -i 文件名 指定路径\n\n# 使用 mv命令-b选项移动文件时会自动备份重名文件之后直接移动\nmv -b 文件名 指定路径  # 备份\n\n```\n\n#### 搜索文件\n```\n# 使用which命令搜索PATH环境变量中包含的命令的具体路径\nwhich ls\n\n# 使用locate命令查找文件路径，会显示含该关键词的所有文件\nlocate 关键词\n\n# locate -c输出查找到的文件的个数\nlocate -c 关键词\n\n# locate -l输出查找到的前n个文档\nlocate -l n 关键词\n\n# find查找最近三天修改过的文件\nfind ~ -mtime -3\n\n# find查找某路径下属于某用户的所有文件\nfind 路径 -user 用户名\n\n# 根据文件名查找文件路径\nfind -name 文件名 # 需要输入完整文件名\n\n# 查找当前目录下所有管道类型文件\nfind -type p\n```\n\n#### 显示文件内容\n```\n# cat拼接两个文件\ncat 第一个文件 第二个文件 > 输出的文件  # 用重定向输出到指定文件，默认是输出到终端\n\n# 当cat一个文件时，则会输出文件内容\n cat 文件名\n \n# cat -n显示文件行号\ncat -n 文件名\n\n# more命令\nmore 文件名\n\n# head命令\nhead -n 5 文件名  # 输出文件前5行\nmore -c 100 文件名  # 输出文件前100个字符\n\n# tail命令\ntail -n 5 文件名  # 输出后5行\ntail -c +115 文件名  # 输出115行至最后\n\n# less命令\nless 文件名\n\n# grep命令可以实现关键词匹配\ngrep \"关键词\" 文件名\n```\n\n#### 文件压缩与备份\n```\n# 使用gzip进行文件压缩\ngzip 文件名\n\n# gzip -c：将压缩的内容输出到屏幕上，源文件不变，可以通过重定向处理输出的内容\n\n# 使用gzip解压缩\ngzip -d 文件名\n\n# bzip2 同gzip\n\n# tar命令可以将多个文件合并为一个压缩包\n-c:新建打包文件\n-t：查看打包文件中包含哪些文件\n-x：解包文件包\n-j：通过bzip2的支持进行压缩/解压缩\n-z：通过gzip的支持进行压缩/解压缩\n-C：指定解包目标路径\n-p：打包过程中保留源文件的属性和权限\n-v：输出打包过程中正在处理的文件名\n```\n","source":"_posts/Notes/Linux学习/文件系统.md","raw":"---\ntitle: 文件系统\ncategories:\n  - Notes\n  - Linux学习\ntags:\n  - Linux\n---\n### 目录与路径\n\n| 符号  | 含义         |\n| ----- | ------------ |\n| /     | 根目录       |\n| /bin  | 可执行文件   |\n| /home | 用户的主目录 |\n\n**特殊目录符号：**\n\n| 目录         |                        释义| \n| ------------ | ---------------------- |\n| .            | 代表当前目录           |\n| ..           | 代表上层目录           |\n| -            | 代表上一个工作目录     |\n| ~            | 代表当前用户的主目录   |\n| ~accountname | 代表该name账户的主目录 |\n\n### PATH环境变量\n当前工作目录下的命令和程序可以直接在bash中执行，其他目录下的命令和程序无法执行。而PATH中的目录下的命令和程序可以在任何目录下执行。当执行命令时，会先在当前目录中寻找，如果存在即执行命令，如果不存在，则会在PATH中寻找并执行第一个相匹配的命令。\n```\n# 显示环境变量中的内容，每个路径用：隔开\necho $PATH      # $与PATH间没有空格\n\n# 添加路径到PATH环境变量\nexport PATH=命令行路径:$PATH\n```\n\n### 目录管理\n\n**显示当前工作目录：**\n\n```\n# pwd显示当前工作目录（print working directory）\npwd\n\n# 用help查看pwd命令的帮助信息\nhelp pwd\n```\n\n**查看目录与文件：**\n\n```\n# -l查看文件属性\nls -l\n\n# -h以方便阅读的单位显示文件尺寸\nls -lh\n\n# -S按照文件大小降序排列\nls -lS\n\n# -R可以递归地显示所有路径\nls -R\n```\n\n**常用目录操作：**\n\n```\n# 新建目录\nmkdir 文件名\n\n# 用-p创建多级目录\nmkdir -p 路径\n\n# 复制目录\ncp -r 源路径 目标路径\n\n# 移动目录\nmv 源路径 目标路径\n\n# 删除目录，加-r递归删除目录中的内容\nrm -r 路径\n```\n\n### 文件管理\n\n#### 文件类型：\n**普通文件类型**：\nLinux中最多的一种文件类型, 包括 纯文本文件(ASCII)；二进制文件(binary)；数据格式的文件(data);各种压缩文件.第一个属性为 [-]\n**目录文件**：\n就是目录， 能用 # cd 命令进入的。第一个属性为 [d]，例如 [drwxrwxrwx]\n**块设备文件**：\n块设备文件 ： 就是存储数据以供系统存取的接口设备，简单而言就是硬盘。例如一号硬盘的代码是 /dev/hda1等文件。第一个属性为 [b]\n**字符设备**：\n字符设备文件：即串行端口的接口设备，例如键盘、鼠标等等。第一个属性为 [c]\n**套接字文件**：\n这类文件通常用在网络数据连接。可以启动一个程序来监听客户端的要求，客户端就可以通过套接字来进行数据通信。第一个属性为 [s]，最常在 /var/run目录中看到这种文件类型\n**管道文件**：\nFIFO也是一种特殊的文件类型，它主要的目的是，解决多个程序同时存取一个文件所造成的错误。FIFO是first-in-first-out(先进先出)的缩写。第一个属性为 [p]\n**链接文件**：\n类似Windows下面的快捷方式。第一个属性为 [l]，例如 [lrwxrwxrwx]\n\n#### 管道\n\n管道是一系列将标准输入输出连接起来的进程\n\n**匿名管道：** 用符号“|”表示，用来连接多个命令，将前一个命令的输出导入第二个命令作为输入\n```\n# 查看ls中包含某个关键词的文件\nls -l | grep 关键词\n```\n\n**命名管道：**\n```\n# 用mkfifo命令创建命名管道（FIFO）\nmkfifo 管道文件名\n```\n\n#### 新建文件\n\n- touch：文件存在时，修改文件访问时间为当前时间，否则创建该文件\n- 用文本编辑器创建文件\n- 重定向方式创建文件：1）“>”操作符：覆盖源文件中已有内容  2）“>>”操作符：将新内容追加到源文件内容的后面\n\n| 命令            | 说明                                               |\n| --------------- | -------------------------------------------------- |\n| command > file  | 将输出重定向到 file。                              |\n| command < file  | 将输入重定向到 file。                              |\n| command >> file | 将输出以追加的方式重定向到 file。                  |\n| n > file        | 将文件描述符为 n 的文件重定向到 file。             |\n| n >> file       | 将文件描述符为 n 的文件以追加的方式重定向到 file。 |\n| n >& m          | 将输出文件 m 和 n 合并。                           |\n| n <& m          | 将输入文件 m 和 n 合并。                           |\n| << tag          | 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 |\n\n#### 复制、移动、删除文件\n\n```\n# 使用cp命令复制文件\ncp 文件名 新文件名\n\n# 用cp命令创建快捷方式（符号链接）\ncp -s 文件名 快捷方式名\n\n# 使用cp命令-p选项复制文件的完整属性\ncp -p 文件名 新文件名\n\n# 使用mv命令移动文件\nmv 文件名 指定路径\n\n# 使用mv命令-i选项移动文件时会提示是否覆盖\nmv -i 文件名 指定路径\n\n# 使用 mv命令-b选项移动文件时会自动备份重名文件之后直接移动\nmv -b 文件名 指定路径  # 备份\n\n```\n\n#### 搜索文件\n```\n# 使用which命令搜索PATH环境变量中包含的命令的具体路径\nwhich ls\n\n# 使用locate命令查找文件路径，会显示含该关键词的所有文件\nlocate 关键词\n\n# locate -c输出查找到的文件的个数\nlocate -c 关键词\n\n# locate -l输出查找到的前n个文档\nlocate -l n 关键词\n\n# find查找最近三天修改过的文件\nfind ~ -mtime -3\n\n# find查找某路径下属于某用户的所有文件\nfind 路径 -user 用户名\n\n# 根据文件名查找文件路径\nfind -name 文件名 # 需要输入完整文件名\n\n# 查找当前目录下所有管道类型文件\nfind -type p\n```\n\n#### 显示文件内容\n```\n# cat拼接两个文件\ncat 第一个文件 第二个文件 > 输出的文件  # 用重定向输出到指定文件，默认是输出到终端\n\n# 当cat一个文件时，则会输出文件内容\n cat 文件名\n \n# cat -n显示文件行号\ncat -n 文件名\n\n# more命令\nmore 文件名\n\n# head命令\nhead -n 5 文件名  # 输出文件前5行\nmore -c 100 文件名  # 输出文件前100个字符\n\n# tail命令\ntail -n 5 文件名  # 输出后5行\ntail -c +115 文件名  # 输出115行至最后\n\n# less命令\nless 文件名\n\n# grep命令可以实现关键词匹配\ngrep \"关键词\" 文件名\n```\n\n#### 文件压缩与备份\n```\n# 使用gzip进行文件压缩\ngzip 文件名\n\n# gzip -c：将压缩的内容输出到屏幕上，源文件不变，可以通过重定向处理输出的内容\n\n# 使用gzip解压缩\ngzip -d 文件名\n\n# bzip2 同gzip\n\n# tar命令可以将多个文件合并为一个压缩包\n-c:新建打包文件\n-t：查看打包文件中包含哪些文件\n-x：解包文件包\n-j：通过bzip2的支持进行压缩/解压缩\n-z：通过gzip的支持进行压缩/解压缩\n-C：指定解包目标路径\n-p：打包过程中保留源文件的属性和权限\n-v：输出打包过程中正在处理的文件名\n```\n","slug":"Notes/Linux学习/文件系统","published":1,"date":"2023-09-19T12:47:32.975Z","updated":"2023-09-27T12:33:42.344Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr55000gtou5bknz2q2h","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"目录与路径\"><a href=\"#目录与路径\" class=\"headerlink\" title=\"目录与路径\"></a>目录与路径</h3><table>\n<thead>\n<tr>\n<th>符号</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&#x2F;</td>\n<td>根目录</td>\n</tr>\n<tr>\n<td>&#x2F;bin</td>\n<td>可执行文件</td>\n</tr>\n<tr>\n<td>&#x2F;home</td>\n<td>用户的主目录</td>\n</tr>\n</tbody></table>\n<p><strong>特殊目录符号：</strong></p>\n<table>\n<thead>\n<tr>\n<th>目录</th>\n<th>释义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>.</td>\n<td>代表当前目录</td>\n</tr>\n<tr>\n<td>..</td>\n<td>代表上层目录</td>\n</tr>\n<tr>\n<td>-</td>\n<td>代表上一个工作目录</td>\n</tr>\n<tr>\n<td>~</td>\n<td>代表当前用户的主目录</td>\n</tr>\n<tr>\n<td>~accountname</td>\n<td>代表该name账户的主目录</td>\n</tr>\n</tbody></table>\n<h3 id=\"PATH环境变量\"><a href=\"#PATH环境变量\" class=\"headerlink\" title=\"PATH环境变量\"></a>PATH环境变量</h3><p>当前工作目录下的命令和程序可以直接在bash中执行，其他目录下的命令和程序无法执行。而PATH中的目录下的命令和程序可以在任何目录下执行。当执行命令时，会先在当前目录中寻找，如果存在即执行命令，如果不存在，则会在PATH中寻找并执行第一个相匹配的命令。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 显示环境变量中的内容，每个路径用：隔开</span><br><span class=\"hljs-built_in\">echo</span> <span class=\"hljs-variable\">$PATH</span>      <span class=\"hljs-comment\"># $与PATH间没有空格</span><br><br><span class=\"hljs-comment\"># 添加路径到PATH环境变量</span><br><span class=\"hljs-built_in\">export</span> PATH=命令行路径:<span class=\"hljs-variable\">$PATH</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"目录管理\"><a href=\"#目录管理\" class=\"headerlink\" title=\"目录管理\"></a>目录管理</h3><p><strong>显示当前工作目录：</strong></p>\n<figure class=\"highlight mel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mel\"># <span class=\"hljs-keyword\">pwd</span>显示当前工作目录（<span class=\"hljs-keyword\">print</span> working directory）<br><span class=\"hljs-keyword\">pwd</span><br><br># 用<span class=\"hljs-keyword\">help</span>查看<span class=\"hljs-keyword\">pwd</span>命令的帮助信息<br><span class=\"hljs-keyword\">help</span> <span class=\"hljs-keyword\">pwd</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>查看目录与文件：</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># -l查看文件属性</span><br><span class=\"hljs-built_in\">ls</span> -l<br><br><span class=\"hljs-comment\"># -h以方便阅读的单位显示文件尺寸</span><br><span class=\"hljs-built_in\">ls</span> -lh<br><br><span class=\"hljs-comment\"># -S按照文件大小降序排列</span><br><span class=\"hljs-built_in\">ls</span> -lS<br><br><span class=\"hljs-comment\"># -R可以递归地显示所有路径</span><br><span class=\"hljs-built_in\">ls</span> -R<br></code></pre></td></tr></table></figure>\n\n<p><strong>常用目录操作：</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 新建目录</span><br><span class=\"hljs-built_in\">mkdir</span> 文件名<br><br><span class=\"hljs-comment\"># 用-p创建多级目录</span><br><span class=\"hljs-built_in\">mkdir</span> -p 路径<br><br><span class=\"hljs-comment\"># 复制目录</span><br><span class=\"hljs-built_in\">cp</span> -r 源路径 目标路径<br><br><span class=\"hljs-comment\"># 移动目录</span><br><span class=\"hljs-built_in\">mv</span> 源路径 目标路径<br><br><span class=\"hljs-comment\"># 删除目录，加-r递归删除目录中的内容</span><br><span class=\"hljs-built_in\">rm</span> -r 路径<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"文件管理\"><a href=\"#文件管理\" class=\"headerlink\" title=\"文件管理\"></a>文件管理</h3><h4 id=\"文件类型：\"><a href=\"#文件类型：\" class=\"headerlink\" title=\"文件类型：\"></a>文件类型：</h4><p><strong>普通文件类型</strong>：<br>Linux中最多的一种文件类型, 包括 纯文本文件(ASCII)；二进制文件(binary)；数据格式的文件(data);各种压缩文件.第一个属性为 [-]<br><strong>目录文件</strong>：<br>就是目录， 能用 # cd 命令进入的。第一个属性为 [d]，例如 [drwxrwxrwx]<br><strong>块设备文件</strong>：<br>块设备文件 ： 就是存储数据以供系统存取的接口设备，简单而言就是硬盘。例如一号硬盘的代码是 &#x2F;dev&#x2F;hda1等文件。第一个属性为 [b]<br><strong>字符设备</strong>：<br>字符设备文件：即串行端口的接口设备，例如键盘、鼠标等等。第一个属性为 [c]<br><strong>套接字文件</strong>：<br>这类文件通常用在网络数据连接。可以启动一个程序来监听客户端的要求，客户端就可以通过套接字来进行数据通信。第一个属性为 [s]，最常在 &#x2F;var&#x2F;run目录中看到这种文件类型<br><strong>管道文件</strong>：<br>FIFO也是一种特殊的文件类型，它主要的目的是，解决多个程序同时存取一个文件所造成的错误。FIFO是first-in-first-out(先进先出)的缩写。第一个属性为 [p]<br><strong>链接文件</strong>：<br>类似Windows下面的快捷方式。第一个属性为 [l]，例如 [lrwxrwxrwx]</p>\n<h4 id=\"管道\"><a href=\"#管道\" class=\"headerlink\" title=\"管道\"></a>管道</h4><p>管道是一系列将标准输入输出连接起来的进程</p>\n<p><strong>匿名管道：</strong> 用符号“|”表示，用来连接多个命令，将前一个命令的输出导入第二个命令作为输入</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\"># 查看<span class=\"hljs-keyword\">ls</span>中包含某个关键词的文件<br><span class=\"hljs-keyword\">ls</span> -<span class=\"hljs-keyword\">l</span> | <span class=\"hljs-keyword\">grep</span> 关键词<br></code></pre></td></tr></table></figure>\n\n<p><strong>命名管道：</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 用mkfifo命令创建命名管道（FIFO）</span><br><span class=\"hljs-built_in\">mkfifo</span> 管道文件名<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"新建文件\"><a href=\"#新建文件\" class=\"headerlink\" title=\"新建文件\"></a>新建文件</h4><ul>\n<li>touch：文件存在时，修改文件访问时间为当前时间，否则创建该文件</li>\n<li>用文本编辑器创建文件</li>\n<li>重定向方式创建文件：1）“&gt;”操作符：覆盖源文件中已有内容  2）“&gt;&gt;”操作符：将新内容追加到源文件内容的后面</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>命令</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>command &gt; file</td>\n<td>将输出重定向到 file。</td>\n</tr>\n<tr>\n<td>command &lt; file</td>\n<td>将输入重定向到 file。</td>\n</tr>\n<tr>\n<td>command &gt;&gt; file</td>\n<td>将输出以追加的方式重定向到 file。</td>\n</tr>\n<tr>\n<td>n &gt; file</td>\n<td>将文件描述符为 n 的文件重定向到 file。</td>\n</tr>\n<tr>\n<td>n &gt;&gt; file</td>\n<td>将文件描述符为 n 的文件以追加的方式重定向到 file。</td>\n</tr>\n<tr>\n<td>n &gt;&amp; m</td>\n<td>将输出文件 m 和 n 合并。</td>\n</tr>\n<tr>\n<td>n &lt;&amp; m</td>\n<td>将输入文件 m 和 n 合并。</td>\n</tr>\n<tr>\n<td>&lt;&lt; tag</td>\n<td>将开始标记 tag 和结束标记 tag 之间的内容作为输入。</td>\n</tr>\n</tbody></table>\n<h4 id=\"复制、移动、删除文件\"><a href=\"#复制、移动、删除文件\" class=\"headerlink\" title=\"复制、移动、删除文件\"></a>复制、移动、删除文件</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 使用cp命令复制文件</span><br><span class=\"hljs-built_in\">cp</span> 文件名 新文件名<br><br><span class=\"hljs-comment\"># 用cp命令创建快捷方式（符号链接）</span><br><span class=\"hljs-built_in\">cp</span> -s 文件名 快捷方式名<br><br><span class=\"hljs-comment\"># 使用cp命令-p选项复制文件的完整属性</span><br><span class=\"hljs-built_in\">cp</span> -p 文件名 新文件名<br><br><span class=\"hljs-comment\"># 使用mv命令移动文件</span><br><span class=\"hljs-built_in\">mv</span> 文件名 指定路径<br><br><span class=\"hljs-comment\"># 使用mv命令-i选项移动文件时会提示是否覆盖</span><br><span class=\"hljs-built_in\">mv</span> -i 文件名 指定路径<br><br><span class=\"hljs-comment\"># 使用 mv命令-b选项移动文件时会自动备份重名文件之后直接移动</span><br><span class=\"hljs-built_in\">mv</span> -b 文件名 指定路径  <span class=\"hljs-comment\"># 备份</span><br><br></code></pre></td></tr></table></figure>\n\n<h4 id=\"搜索文件\"><a href=\"#搜索文件\" class=\"headerlink\" title=\"搜索文件\"></a>搜索文件</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\"><span class=\"hljs-comment\"># 使用which命令搜索PATH环境变量中包含的命令的具体路径</span><br>which ls<br><br><span class=\"hljs-comment\"># 使用locate命令查找文件路径，会显示含该关键词的所有文件</span><br>locate 关键词<br><br><span class=\"hljs-comment\"># locate -c输出查找到的文件的个数</span><br>locate -c 关键词<br><br><span class=\"hljs-comment\"># locate -l输出查找到的前n个文档</span><br>locate -l n 关键词<br><br><span class=\"hljs-comment\"># find查找最近三天修改过的文件</span><br><span class=\"hljs-built_in\">find</span> ~ -mtime -3<br><br><span class=\"hljs-comment\"># find查找某路径下属于某用户的所有文件</span><br><span class=\"hljs-built_in\">find</span> 路径 -user 用户名<br><br><span class=\"hljs-comment\"># 根据文件名查找文件路径</span><br><span class=\"hljs-built_in\">find</span> -name 文件名 # 需要输入完整文件名<br><br><span class=\"hljs-comment\"># 查找当前目录下所有管道类型文件</span><br><span class=\"hljs-built_in\">find</span> -type p<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"显示文件内容\"><a href=\"#显示文件内容\" class=\"headerlink\" title=\"显示文件内容\"></a>显示文件内容</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># cat拼接两个文件</span><br><span class=\"hljs-built_in\">cat</span> 第一个文件 第二个文件 &gt; 输出的文件  <span class=\"hljs-comment\"># 用重定向输出到指定文件，默认是输出到终端</span><br><br><span class=\"hljs-comment\"># 当cat一个文件时，则会输出文件内容</span><br> <span class=\"hljs-built_in\">cat</span> 文件名<br> <br><span class=\"hljs-comment\"># cat -n显示文件行号</span><br><span class=\"hljs-built_in\">cat</span> -n 文件名<br><br><span class=\"hljs-comment\"># more命令</span><br>more 文件名<br><br><span class=\"hljs-comment\"># head命令</span><br><span class=\"hljs-built_in\">head</span> -n 5 文件名  <span class=\"hljs-comment\"># 输出文件前5行</span><br>more -c 100 文件名  <span class=\"hljs-comment\"># 输出文件前100个字符</span><br><br><span class=\"hljs-comment\"># tail命令</span><br><span class=\"hljs-built_in\">tail</span> -n 5 文件名  <span class=\"hljs-comment\"># 输出后5行</span><br><span class=\"hljs-built_in\">tail</span> -c +115 文件名  <span class=\"hljs-comment\"># 输出115行至最后</span><br><br><span class=\"hljs-comment\"># less命令</span><br>less 文件名<br><br><span class=\"hljs-comment\"># grep命令可以实现关键词匹配</span><br>grep <span class=\"hljs-string\">&quot;关键词&quot;</span> 文件名<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"文件压缩与备份\"><a href=\"#文件压缩与备份\" class=\"headerlink\" title=\"文件压缩与备份\"></a>文件压缩与备份</h4><figure class=\"highlight ldif\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ldif\"><span class=\"hljs-comment\"># 使用gzip进行文件压缩</span><br>gzip 文件名<br><br><span class=\"hljs-comment\"># gzip -c：将压缩的内容输出到屏幕上，源文件不变，可以通过重定向处理输出的内容</span><br><br><span class=\"hljs-comment\"># 使用gzip解压缩</span><br>gzip -d 文件名<br><br><span class=\"hljs-comment\"># bzip2 同gzip</span><br><br><span class=\"hljs-comment\"># tar命令可以将多个文件合并为一个压缩包</span><br><span class=\"hljs-literal\">-</span>c:新建打包文件<br><span class=\"hljs-literal\">-</span>t：查看打包文件中包含哪些文件<br><span class=\"hljs-literal\">-</span>x：解包文件包<br><span class=\"hljs-literal\">-</span>j：通过bzip2的支持进行压缩/解压缩<br><span class=\"hljs-literal\">-</span>z：通过gzip的支持进行压缩/解压缩<br><span class=\"hljs-literal\">-</span>C：指定解包目标路径<br><span class=\"hljs-literal\">-</span>p：打包过程中保留源文件的属性和权限<br><span class=\"hljs-literal\">-</span>v：输出打包过程中正在处理的文件名<br></code></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"目录与路径\"><a href=\"#目录与路径\" class=\"headerlink\" title=\"目录与路径\"></a>目录与路径</h3><table>\n<thead>\n<tr>\n<th>符号</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&#x2F;</td>\n<td>根目录</td>\n</tr>\n<tr>\n<td>&#x2F;bin</td>\n<td>可执行文件</td>\n</tr>\n<tr>\n<td>&#x2F;home</td>\n<td>用户的主目录</td>\n</tr>\n</tbody></table>\n<p><strong>特殊目录符号：</strong></p>\n<table>\n<thead>\n<tr>\n<th>目录</th>\n<th>释义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>.</td>\n<td>代表当前目录</td>\n</tr>\n<tr>\n<td>..</td>\n<td>代表上层目录</td>\n</tr>\n<tr>\n<td>-</td>\n<td>代表上一个工作目录</td>\n</tr>\n<tr>\n<td>~</td>\n<td>代表当前用户的主目录</td>\n</tr>\n<tr>\n<td>~accountname</td>\n<td>代表该name账户的主目录</td>\n</tr>\n</tbody></table>\n<h3 id=\"PATH环境变量\"><a href=\"#PATH环境变量\" class=\"headerlink\" title=\"PATH环境变量\"></a>PATH环境变量</h3><p>当前工作目录下的命令和程序可以直接在bash中执行，其他目录下的命令和程序无法执行。而PATH中的目录下的命令和程序可以在任何目录下执行。当执行命令时，会先在当前目录中寻找，如果存在即执行命令，如果不存在，则会在PATH中寻找并执行第一个相匹配的命令。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 显示环境变量中的内容，每个路径用：隔开</span><br><span class=\"hljs-built_in\">echo</span> <span class=\"hljs-variable\">$PATH</span>      <span class=\"hljs-comment\"># $与PATH间没有空格</span><br><br><span class=\"hljs-comment\"># 添加路径到PATH环境变量</span><br><span class=\"hljs-built_in\">export</span> PATH=命令行路径:<span class=\"hljs-variable\">$PATH</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"目录管理\"><a href=\"#目录管理\" class=\"headerlink\" title=\"目录管理\"></a>目录管理</h3><p><strong>显示当前工作目录：</strong></p>\n<figure class=\"highlight mel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs mel\"># <span class=\"hljs-keyword\">pwd</span>显示当前工作目录（<span class=\"hljs-keyword\">print</span> working directory）<br><span class=\"hljs-keyword\">pwd</span><br><br># 用<span class=\"hljs-keyword\">help</span>查看<span class=\"hljs-keyword\">pwd</span>命令的帮助信息<br><span class=\"hljs-keyword\">help</span> <span class=\"hljs-keyword\">pwd</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>查看目录与文件：</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># -l查看文件属性</span><br><span class=\"hljs-built_in\">ls</span> -l<br><br><span class=\"hljs-comment\"># -h以方便阅读的单位显示文件尺寸</span><br><span class=\"hljs-built_in\">ls</span> -lh<br><br><span class=\"hljs-comment\"># -S按照文件大小降序排列</span><br><span class=\"hljs-built_in\">ls</span> -lS<br><br><span class=\"hljs-comment\"># -R可以递归地显示所有路径</span><br><span class=\"hljs-built_in\">ls</span> -R<br></code></pre></td></tr></table></figure>\n\n<p><strong>常用目录操作：</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 新建目录</span><br><span class=\"hljs-built_in\">mkdir</span> 文件名<br><br><span class=\"hljs-comment\"># 用-p创建多级目录</span><br><span class=\"hljs-built_in\">mkdir</span> -p 路径<br><br><span class=\"hljs-comment\"># 复制目录</span><br><span class=\"hljs-built_in\">cp</span> -r 源路径 目标路径<br><br><span class=\"hljs-comment\"># 移动目录</span><br><span class=\"hljs-built_in\">mv</span> 源路径 目标路径<br><br><span class=\"hljs-comment\"># 删除目录，加-r递归删除目录中的内容</span><br><span class=\"hljs-built_in\">rm</span> -r 路径<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"文件管理\"><a href=\"#文件管理\" class=\"headerlink\" title=\"文件管理\"></a>文件管理</h3><h4 id=\"文件类型：\"><a href=\"#文件类型：\" class=\"headerlink\" title=\"文件类型：\"></a>文件类型：</h4><p><strong>普通文件类型</strong>：<br>Linux中最多的一种文件类型, 包括 纯文本文件(ASCII)；二进制文件(binary)；数据格式的文件(data);各种压缩文件.第一个属性为 [-]<br><strong>目录文件</strong>：<br>就是目录， 能用 # cd 命令进入的。第一个属性为 [d]，例如 [drwxrwxrwx]<br><strong>块设备文件</strong>：<br>块设备文件 ： 就是存储数据以供系统存取的接口设备，简单而言就是硬盘。例如一号硬盘的代码是 &#x2F;dev&#x2F;hda1等文件。第一个属性为 [b]<br><strong>字符设备</strong>：<br>字符设备文件：即串行端口的接口设备，例如键盘、鼠标等等。第一个属性为 [c]<br><strong>套接字文件</strong>：<br>这类文件通常用在网络数据连接。可以启动一个程序来监听客户端的要求，客户端就可以通过套接字来进行数据通信。第一个属性为 [s]，最常在 &#x2F;var&#x2F;run目录中看到这种文件类型<br><strong>管道文件</strong>：<br>FIFO也是一种特殊的文件类型，它主要的目的是，解决多个程序同时存取一个文件所造成的错误。FIFO是first-in-first-out(先进先出)的缩写。第一个属性为 [p]<br><strong>链接文件</strong>：<br>类似Windows下面的快捷方式。第一个属性为 [l]，例如 [lrwxrwxrwx]</p>\n<h4 id=\"管道\"><a href=\"#管道\" class=\"headerlink\" title=\"管道\"></a>管道</h4><p>管道是一系列将标准输入输出连接起来的进程</p>\n<p><strong>匿名管道：</strong> 用符号“|”表示，用来连接多个命令，将前一个命令的输出导入第二个命令作为输入</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\"># 查看<span class=\"hljs-keyword\">ls</span>中包含某个关键词的文件<br><span class=\"hljs-keyword\">ls</span> -<span class=\"hljs-keyword\">l</span> | <span class=\"hljs-keyword\">grep</span> 关键词<br></code></pre></td></tr></table></figure>\n\n<p><strong>命名管道：</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 用mkfifo命令创建命名管道（FIFO）</span><br><span class=\"hljs-built_in\">mkfifo</span> 管道文件名<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"新建文件\"><a href=\"#新建文件\" class=\"headerlink\" title=\"新建文件\"></a>新建文件</h4><ul>\n<li>touch：文件存在时，修改文件访问时间为当前时间，否则创建该文件</li>\n<li>用文本编辑器创建文件</li>\n<li>重定向方式创建文件：1）“&gt;”操作符：覆盖源文件中已有内容  2）“&gt;&gt;”操作符：将新内容追加到源文件内容的后面</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>命令</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>command &gt; file</td>\n<td>将输出重定向到 file。</td>\n</tr>\n<tr>\n<td>command &lt; file</td>\n<td>将输入重定向到 file。</td>\n</tr>\n<tr>\n<td>command &gt;&gt; file</td>\n<td>将输出以追加的方式重定向到 file。</td>\n</tr>\n<tr>\n<td>n &gt; file</td>\n<td>将文件描述符为 n 的文件重定向到 file。</td>\n</tr>\n<tr>\n<td>n &gt;&gt; file</td>\n<td>将文件描述符为 n 的文件以追加的方式重定向到 file。</td>\n</tr>\n<tr>\n<td>n &gt;&amp; m</td>\n<td>将输出文件 m 和 n 合并。</td>\n</tr>\n<tr>\n<td>n &lt;&amp; m</td>\n<td>将输入文件 m 和 n 合并。</td>\n</tr>\n<tr>\n<td>&lt;&lt; tag</td>\n<td>将开始标记 tag 和结束标记 tag 之间的内容作为输入。</td>\n</tr>\n</tbody></table>\n<h4 id=\"复制、移动、删除文件\"><a href=\"#复制、移动、删除文件\" class=\"headerlink\" title=\"复制、移动、删除文件\"></a>复制、移动、删除文件</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># 使用cp命令复制文件</span><br><span class=\"hljs-built_in\">cp</span> 文件名 新文件名<br><br><span class=\"hljs-comment\"># 用cp命令创建快捷方式（符号链接）</span><br><span class=\"hljs-built_in\">cp</span> -s 文件名 快捷方式名<br><br><span class=\"hljs-comment\"># 使用cp命令-p选项复制文件的完整属性</span><br><span class=\"hljs-built_in\">cp</span> -p 文件名 新文件名<br><br><span class=\"hljs-comment\"># 使用mv命令移动文件</span><br><span class=\"hljs-built_in\">mv</span> 文件名 指定路径<br><br><span class=\"hljs-comment\"># 使用mv命令-i选项移动文件时会提示是否覆盖</span><br><span class=\"hljs-built_in\">mv</span> -i 文件名 指定路径<br><br><span class=\"hljs-comment\"># 使用 mv命令-b选项移动文件时会自动备份重名文件之后直接移动</span><br><span class=\"hljs-built_in\">mv</span> -b 文件名 指定路径  <span class=\"hljs-comment\"># 备份</span><br><br></code></pre></td></tr></table></figure>\n\n<h4 id=\"搜索文件\"><a href=\"#搜索文件\" class=\"headerlink\" title=\"搜索文件\"></a>搜索文件</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\"><span class=\"hljs-comment\"># 使用which命令搜索PATH环境变量中包含的命令的具体路径</span><br>which ls<br><br><span class=\"hljs-comment\"># 使用locate命令查找文件路径，会显示含该关键词的所有文件</span><br>locate 关键词<br><br><span class=\"hljs-comment\"># locate -c输出查找到的文件的个数</span><br>locate -c 关键词<br><br><span class=\"hljs-comment\"># locate -l输出查找到的前n个文档</span><br>locate -l n 关键词<br><br><span class=\"hljs-comment\"># find查找最近三天修改过的文件</span><br><span class=\"hljs-built_in\">find</span> ~ -mtime -3<br><br><span class=\"hljs-comment\"># find查找某路径下属于某用户的所有文件</span><br><span class=\"hljs-built_in\">find</span> 路径 -user 用户名<br><br><span class=\"hljs-comment\"># 根据文件名查找文件路径</span><br><span class=\"hljs-built_in\">find</span> -name 文件名 # 需要输入完整文件名<br><br><span class=\"hljs-comment\"># 查找当前目录下所有管道类型文件</span><br><span class=\"hljs-built_in\">find</span> -type p<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"显示文件内容\"><a href=\"#显示文件内容\" class=\"headerlink\" title=\"显示文件内容\"></a>显示文件内容</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-comment\"># cat拼接两个文件</span><br><span class=\"hljs-built_in\">cat</span> 第一个文件 第二个文件 &gt; 输出的文件  <span class=\"hljs-comment\"># 用重定向输出到指定文件，默认是输出到终端</span><br><br><span class=\"hljs-comment\"># 当cat一个文件时，则会输出文件内容</span><br> <span class=\"hljs-built_in\">cat</span> 文件名<br> <br><span class=\"hljs-comment\"># cat -n显示文件行号</span><br><span class=\"hljs-built_in\">cat</span> -n 文件名<br><br><span class=\"hljs-comment\"># more命令</span><br>more 文件名<br><br><span class=\"hljs-comment\"># head命令</span><br><span class=\"hljs-built_in\">head</span> -n 5 文件名  <span class=\"hljs-comment\"># 输出文件前5行</span><br>more -c 100 文件名  <span class=\"hljs-comment\"># 输出文件前100个字符</span><br><br><span class=\"hljs-comment\"># tail命令</span><br><span class=\"hljs-built_in\">tail</span> -n 5 文件名  <span class=\"hljs-comment\"># 输出后5行</span><br><span class=\"hljs-built_in\">tail</span> -c +115 文件名  <span class=\"hljs-comment\"># 输出115行至最后</span><br><br><span class=\"hljs-comment\"># less命令</span><br>less 文件名<br><br><span class=\"hljs-comment\"># grep命令可以实现关键词匹配</span><br>grep <span class=\"hljs-string\">&quot;关键词&quot;</span> 文件名<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"文件压缩与备份\"><a href=\"#文件压缩与备份\" class=\"headerlink\" title=\"文件压缩与备份\"></a>文件压缩与备份</h4><figure class=\"highlight ldif\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ldif\"><span class=\"hljs-comment\"># 使用gzip进行文件压缩</span><br>gzip 文件名<br><br><span class=\"hljs-comment\"># gzip -c：将压缩的内容输出到屏幕上，源文件不变，可以通过重定向处理输出的内容</span><br><br><span class=\"hljs-comment\"># 使用gzip解压缩</span><br>gzip -d 文件名<br><br><span class=\"hljs-comment\"># bzip2 同gzip</span><br><br><span class=\"hljs-comment\"># tar命令可以将多个文件合并为一个压缩包</span><br><span class=\"hljs-literal\">-</span>c:新建打包文件<br><span class=\"hljs-literal\">-</span>t：查看打包文件中包含哪些文件<br><span class=\"hljs-literal\">-</span>x：解包文件包<br><span class=\"hljs-literal\">-</span>j：通过bzip2的支持进行压缩/解压缩<br><span class=\"hljs-literal\">-</span>z：通过gzip的支持进行压缩/解压缩<br><span class=\"hljs-literal\">-</span>C：指定解包目标路径<br><span class=\"hljs-literal\">-</span>p：打包过程中保留源文件的属性和权限<br><span class=\"hljs-literal\">-</span>v：输出打包过程中正在处理的文件名<br></code></pre></td></tr></table></figure>\n"},{"title":"文本编辑器","_content":"## Nano\n### 打开文件：\n在终端直接输入\n> nano 文件名 \n> 如：nano test\n\n如果当前目录不存在该文件，则会创建并打开\n\n### 写入文件\n直接在光标处输入\n\n### 保存文件\nCtrl+X退出，然后输入Y保存，新文件需要确认文件名，输入N则不保存直接退出\n\n\n---\n\n## Vim\n[精通 VIM ，此文就够了 - 知乎](https://zhuanlan.zhihu.com/p/68111471)\n[Linux vi/vim | 菜鸟教程](https://www.runoob.com/linux/linux-vim.html)\n\n基本上 vi/vim 共分为三种模式，**命令模式（Command Mode）、输入模式（Insert Mode）和命令行模式（Command-Line Mode）**。\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234707.png)\n### 输入模式 \n**像在文本文档中打字一样输入**\n\n在命令模式下按下 i 就进入了输入模式，使用 Esc 键可以返回到普通模式。\n\n在输入模式中，可以使用以下按键：\n- **字符按键以及Shift组合**，输入字符\n- **ENTER**，回车键，换行\n- **BACK SPACE**，退格键，删除光标前一个字符\n- **DEL**，删除键，删除光标后一个字符\n- **方向键**，在文本中移动光标\n- **HOME**/**END**，移动光标到行首/行尾\n- **Page Up**/**Page Down**，上/下翻页\n- **Insert**，切换光标为输入/替换模式，光标将变成竖线/下划线\n- **ESC**，退出输入模式，切换到命令模式\n\n### 命令模式（一般模式）\n**用户刚刚启动 vi/vim，便进入了命令模式。**\n\n此状态下敲击键盘动作会被 Vim 识别为命令，而非输入字符，比如我们此时按下 i，并不会输入一个字符，i 被当作了一个命令。\n\n以下是普通模式常用的几个命令：\n\n- i -- 切换到输入模式，在光标当前位置开始输入文本。\n- x -- 删除当前光标所在处的字符。\n- : -- 切换到底线命令模式，以在最底一行输入命令。\n- a -- 进入插入模式，在光标下一个位置开始输入文本。\n- o：在当前行的下方插入一个新行，并进入插入模式。\n- O -- 在当前行的上方插入一个新行，并进入插入模式。\n- dd -- 删除当前行。\n- yy -- 复制当前行。\n- p -- 粘贴剪贴板内容到光标下方。\n- P -- 粘贴剪贴板内容到光标上方。\n- u -- 撤销上一次操作。\n- Ctrl + r -- 重做上一次撤销的操作。\n- :w -- 保存文件。\n- :q -- 退出 Vim 编辑器。\n- :q! -- 强制退出Vim 编辑器，不保存修改。\n\n若想要编辑文本，只需要启动 Vim，进入了命令模式，按下 i 切换到输入模式即可。\n\n命令模式只有一些最基本的命令，因此仍要依靠**底线命令行模式**输入更多命令。\n\n### 底线命令模式（命令行模式）\n在命令模式下按下 :（英文冒号）就进入了底线命令模式。\n\n底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。\n\n在底线命令模式中，基本的命令有（已经省略了冒号）：\n\n- `:w`：保存文件。\n- `:q`：退出 Vim 编辑器。\n- `:wq`：保存文件并退出 Vim 编辑器。\n- `:q!`：强制退出Vim编辑器，不保存修改。\n\n按 ESC 键可随时退出底线命令模式。\n\n| 移动光标的方法     |                      |\n| ------------------ | -------------------- |\n| h 或 向左箭头键(←) | 光标向左移动一个字符 |\n| j 或 向下箭头键(↓) | 光标向下移动一个字符 |\n| k 或 向上箭头键(↑) | 光标向上移动一个字符 |\n| l 或 向右箭头键(→) | 光标向右移动一个字符 |\n\n| 搜索替换                                   |                                                                                                                                                                                                                                                 |     |\n| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --- |\n| /word                                      | 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用)                                                                                                                                        |     |\n| ?word                                      | 向光标之上寻找一个字符串名称为 word 的字符串。                                                                                                                                                                                                  |     |\n| n                                          | 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ |     |\n| N                                          | 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。                                                                                                                             |     |\n| :n1,n2s/word1/word2/g                      | n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：  <br>『:100,200s/vbird/VBIRD/g』。(常用)                                                    |     |\n| :1,$s/word1/word2/g 或 :%s/word1/word2/g   | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用)                                                                                                                                                                          |     |\n| :1,$s/word1/word2/gc 或 :%s/word1/word2/gc | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用)                                                                                                                 |     |\n\n\n| vim 环境的变更 |                                                    |     |\n| -------------- | -------------------------------------------------- | --- |\n| :set nu        | 显示行号，设定之后，会在每一行的前缀显示该行的行号 |     |\n| :set nonu      | 与 set nu 相反，为取消行号！                       |     |","source":"_posts/Notes/Linux学习/文本编辑器.md","raw":"---\ntitle: 文本编辑器\ncategories:\n  - Notes\n  - Linux学习\ntags:\n  - Linux\n---\n## Nano\n### 打开文件：\n在终端直接输入\n> nano 文件名 \n> 如：nano test\n\n如果当前目录不存在该文件，则会创建并打开\n\n### 写入文件\n直接在光标处输入\n\n### 保存文件\nCtrl+X退出，然后输入Y保存，新文件需要确认文件名，输入N则不保存直接退出\n\n\n---\n\n## Vim\n[精通 VIM ，此文就够了 - 知乎](https://zhuanlan.zhihu.com/p/68111471)\n[Linux vi/vim | 菜鸟教程](https://www.runoob.com/linux/linux-vim.html)\n\n基本上 vi/vim 共分为三种模式，**命令模式（Command Mode）、输入模式（Insert Mode）和命令行模式（Command-Line Mode）**。\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234707.png)\n### 输入模式 \n**像在文本文档中打字一样输入**\n\n在命令模式下按下 i 就进入了输入模式，使用 Esc 键可以返回到普通模式。\n\n在输入模式中，可以使用以下按键：\n- **字符按键以及Shift组合**，输入字符\n- **ENTER**，回车键，换行\n- **BACK SPACE**，退格键，删除光标前一个字符\n- **DEL**，删除键，删除光标后一个字符\n- **方向键**，在文本中移动光标\n- **HOME**/**END**，移动光标到行首/行尾\n- **Page Up**/**Page Down**，上/下翻页\n- **Insert**，切换光标为输入/替换模式，光标将变成竖线/下划线\n- **ESC**，退出输入模式，切换到命令模式\n\n### 命令模式（一般模式）\n**用户刚刚启动 vi/vim，便进入了命令模式。**\n\n此状态下敲击键盘动作会被 Vim 识别为命令，而非输入字符，比如我们此时按下 i，并不会输入一个字符，i 被当作了一个命令。\n\n以下是普通模式常用的几个命令：\n\n- i -- 切换到输入模式，在光标当前位置开始输入文本。\n- x -- 删除当前光标所在处的字符。\n- : -- 切换到底线命令模式，以在最底一行输入命令。\n- a -- 进入插入模式，在光标下一个位置开始输入文本。\n- o：在当前行的下方插入一个新行，并进入插入模式。\n- O -- 在当前行的上方插入一个新行，并进入插入模式。\n- dd -- 删除当前行。\n- yy -- 复制当前行。\n- p -- 粘贴剪贴板内容到光标下方。\n- P -- 粘贴剪贴板内容到光标上方。\n- u -- 撤销上一次操作。\n- Ctrl + r -- 重做上一次撤销的操作。\n- :w -- 保存文件。\n- :q -- 退出 Vim 编辑器。\n- :q! -- 强制退出Vim 编辑器，不保存修改。\n\n若想要编辑文本，只需要启动 Vim，进入了命令模式，按下 i 切换到输入模式即可。\n\n命令模式只有一些最基本的命令，因此仍要依靠**底线命令行模式**输入更多命令。\n\n### 底线命令模式（命令行模式）\n在命令模式下按下 :（英文冒号）就进入了底线命令模式。\n\n底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。\n\n在底线命令模式中，基本的命令有（已经省略了冒号）：\n\n- `:w`：保存文件。\n- `:q`：退出 Vim 编辑器。\n- `:wq`：保存文件并退出 Vim 编辑器。\n- `:q!`：强制退出Vim编辑器，不保存修改。\n\n按 ESC 键可随时退出底线命令模式。\n\n| 移动光标的方法     |                      |\n| ------------------ | -------------------- |\n| h 或 向左箭头键(←) | 光标向左移动一个字符 |\n| j 或 向下箭头键(↓) | 光标向下移动一个字符 |\n| k 或 向上箭头键(↑) | 光标向上移动一个字符 |\n| l 或 向右箭头键(→) | 光标向右移动一个字符 |\n\n| 搜索替换                                   |                                                                                                                                                                                                                                                 |     |\n| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --- |\n| /word                                      | 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用)                                                                                                                                        |     |\n| ?word                                      | 向光标之上寻找一个字符串名称为 word 的字符串。                                                                                                                                                                                                  |     |\n| n                                          | 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ |     |\n| N                                          | 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。                                                                                                                             |     |\n| :n1,n2s/word1/word2/g                      | n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：  <br>『:100,200s/vbird/VBIRD/g』。(常用)                                                    |     |\n| :1,$s/word1/word2/g 或 :%s/word1/word2/g   | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用)                                                                                                                                                                          |     |\n| :1,$s/word1/word2/gc 或 :%s/word1/word2/gc | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用)                                                                                                                 |     |\n\n\n| vim 环境的变更 |                                                    |     |\n| -------------- | -------------------------------------------------- | --- |\n| :set nu        | 显示行号，设定之后，会在每一行的前缀显示该行的行号 |     |\n| :set nonu      | 与 set nu 相反，为取消行号！                       |     |","slug":"Notes/Linux学习/文本编辑器","published":1,"date":"2023-09-19T12:47:32.976Z","updated":"2023-09-27T12:33:37.657Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr56000itou57jeqfejp","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h2 id=\"Nano\"><a href=\"#Nano\" class=\"headerlink\" title=\"Nano\"></a>Nano</h2><h3 id=\"打开文件：\"><a href=\"#打开文件：\" class=\"headerlink\" title=\"打开文件：\"></a>打开文件：</h3><p>在终端直接输入</p>\n<blockquote>\n<p>nano 文件名<br>如：nano test</p>\n</blockquote>\n<p>如果当前目录不存在该文件，则会创建并打开</p>\n<h3 id=\"写入文件\"><a href=\"#写入文件\" class=\"headerlink\" title=\"写入文件\"></a>写入文件</h3><p>直接在光标处输入</p>\n<h3 id=\"保存文件\"><a href=\"#保存文件\" class=\"headerlink\" title=\"保存文件\"></a>保存文件</h3><p>Ctrl+X退出，然后输入Y保存，新文件需要确认文件名，输入N则不保存直接退出</p>\n<hr>\n<h2 id=\"Vim\"><a href=\"#Vim\" class=\"headerlink\" title=\"Vim\"></a>Vim</h2><p><a href=\"https://zhuanlan.zhihu.com/p/68111471\">精通 VIM ，此文就够了 - 知乎</a><br><a href=\"https://www.runoob.com/linux/linux-vim.html\">Linux vi&#x2F;vim | 菜鸟教程</a></p>\n<p>基本上 vi&#x2F;vim 共分为三种模式，<strong>命令模式（Command Mode）、输入模式（Insert Mode）和命令行模式（Command-Line Mode）</strong>。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234707.png\"></p>\n<h3 id=\"输入模式\"><a href=\"#输入模式\" class=\"headerlink\" title=\"输入模式\"></a>输入模式</h3><p><strong>像在文本文档中打字一样输入</strong></p>\n<p>在命令模式下按下 i 就进入了输入模式，使用 Esc 键可以返回到普通模式。</p>\n<p>在输入模式中，可以使用以下按键：</p>\n<ul>\n<li><strong>字符按键以及Shift组合</strong>，输入字符</li>\n<li><strong>ENTER</strong>，回车键，换行</li>\n<li><strong>BACK SPACE</strong>，退格键，删除光标前一个字符</li>\n<li><strong>DEL</strong>，删除键，删除光标后一个字符</li>\n<li><strong>方向键</strong>，在文本中移动光标</li>\n<li><strong>HOME</strong>&#x2F;<strong>END</strong>，移动光标到行首&#x2F;行尾</li>\n<li><strong>Page Up</strong>&#x2F;<strong>Page Down</strong>，上&#x2F;下翻页</li>\n<li><strong>Insert</strong>，切换光标为输入&#x2F;替换模式，光标将变成竖线&#x2F;下划线</li>\n<li><strong>ESC</strong>，退出输入模式，切换到命令模式</li>\n</ul>\n<h3 id=\"命令模式（一般模式）\"><a href=\"#命令模式（一般模式）\" class=\"headerlink\" title=\"命令模式（一般模式）\"></a>命令模式（一般模式）</h3><p><strong>用户刚刚启动 vi&#x2F;vim，便进入了命令模式。</strong></p>\n<p>此状态下敲击键盘动作会被 Vim 识别为命令，而非输入字符，比如我们此时按下 i，并不会输入一个字符，i 被当作了一个命令。</p>\n<p>以下是普通模式常用的几个命令：</p>\n<ul>\n<li>i – 切换到输入模式，在光标当前位置开始输入文本。</li>\n<li>x – 删除当前光标所在处的字符。</li>\n<li>: – 切换到底线命令模式，以在最底一行输入命令。</li>\n<li>a – 进入插入模式，在光标下一个位置开始输入文本。</li>\n<li>o：在当前行的下方插入一个新行，并进入插入模式。</li>\n<li>O – 在当前行的上方插入一个新行，并进入插入模式。</li>\n<li>dd – 删除当前行。</li>\n<li>yy – 复制当前行。</li>\n<li>p – 粘贴剪贴板内容到光标下方。</li>\n<li>P – 粘贴剪贴板内容到光标上方。</li>\n<li>u – 撤销上一次操作。</li>\n<li>Ctrl + r – 重做上一次撤销的操作。</li>\n<li>:w – 保存文件。</li>\n<li>:q – 退出 Vim 编辑器。</li>\n<li>:q! – 强制退出Vim 编辑器，不保存修改。</li>\n</ul>\n<p>若想要编辑文本，只需要启动 Vim，进入了命令模式，按下 i 切换到输入模式即可。</p>\n<p>命令模式只有一些最基本的命令，因此仍要依靠<strong>底线命令行模式</strong>输入更多命令。</p>\n<h3 id=\"底线命令模式（命令行模式）\"><a href=\"#底线命令模式（命令行模式）\" class=\"headerlink\" title=\"底线命令模式（命令行模式）\"></a>底线命令模式（命令行模式）</h3><p>在命令模式下按下 :（英文冒号）就进入了底线命令模式。</p>\n<p>底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。</p>\n<p>在底线命令模式中，基本的命令有（已经省略了冒号）：</p>\n<ul>\n<li><code>:w</code>：保存文件。</li>\n<li><code>:q</code>：退出 Vim 编辑器。</li>\n<li><code>:wq</code>：保存文件并退出 Vim 编辑器。</li>\n<li><code>:q!</code>：强制退出Vim编辑器，不保存修改。</li>\n</ul>\n<p>按 ESC 键可随时退出底线命令模式。</p>\n<table>\n<thead>\n<tr>\n<th>移动光标的方法</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>h 或 向左箭头键(←)</td>\n<td>光标向左移动一个字符</td>\n</tr>\n<tr>\n<td>j 或 向下箭头键(↓)</td>\n<td>光标向下移动一个字符</td>\n</tr>\n<tr>\n<td>k 或 向上箭头键(↑)</td>\n<td>光标向上移动一个字符</td>\n</tr>\n<tr>\n<td>l 或 向右箭头键(→)</td>\n<td>光标向右移动一个字符</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>搜索替换</th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&#x2F;word</td>\n<td>向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 &#x2F;vbird 即可！ (常用)</td>\n<td></td>\n</tr>\n<tr>\n<td>?word</td>\n<td>向光标之上寻找一个字符串名称为 word 的字符串。</td>\n<td></td>\n</tr>\n<tr>\n<td>n</td>\n<td>这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 &#x2F;vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！</td>\n<td></td>\n</tr>\n<tr>\n<td>N</td>\n<td>这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 &#x2F;vbird 后，按下 N 则表示『向上』搜寻 vbird 。</td>\n<td></td>\n</tr>\n<tr>\n<td>:n1,n2s&#x2F;word1&#x2F;word2&#x2F;g</td>\n<td>n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：  <br>『:100,200s&#x2F;vbird&#x2F;VBIRD&#x2F;g』。(常用)</td>\n<td></td>\n</tr>\n<tr>\n<td>:1,$s&#x2F;word1&#x2F;word2&#x2F;g 或 :%s&#x2F;word1&#x2F;word2&#x2F;g</td>\n<td>从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用)</td>\n<td></td>\n</tr>\n<tr>\n<td>:1,$s&#x2F;word1&#x2F;word2&#x2F;gc 或 :%s&#x2F;word1&#x2F;word2&#x2F;gc</td>\n<td>从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用)</td>\n<td></td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>vim 环境的变更</th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>:set nu</td>\n<td>显示行号，设定之后，会在每一行的前缀显示该行的行号</td>\n<td></td>\n</tr>\n<tr>\n<td>:set nonu</td>\n<td>与 set nu 相反，为取消行号！</td>\n<td></td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Nano\"><a href=\"#Nano\" class=\"headerlink\" title=\"Nano\"></a>Nano</h2><h3 id=\"打开文件：\"><a href=\"#打开文件：\" class=\"headerlink\" title=\"打开文件：\"></a>打开文件：</h3><p>在终端直接输入</p>\n<blockquote>\n<p>nano 文件名<br>如：nano test</p>\n</blockquote>\n<p>如果当前目录不存在该文件，则会创建并打开</p>\n<h3 id=\"写入文件\"><a href=\"#写入文件\" class=\"headerlink\" title=\"写入文件\"></a>写入文件</h3><p>直接在光标处输入</p>\n<h3 id=\"保存文件\"><a href=\"#保存文件\" class=\"headerlink\" title=\"保存文件\"></a>保存文件</h3><p>Ctrl+X退出，然后输入Y保存，新文件需要确认文件名，输入N则不保存直接退出</p>\n<hr>\n<h2 id=\"Vim\"><a href=\"#Vim\" class=\"headerlink\" title=\"Vim\"></a>Vim</h2><p><a href=\"https://zhuanlan.zhihu.com/p/68111471\">精通 VIM ，此文就够了 - 知乎</a><br><a href=\"https://www.runoob.com/linux/linux-vim.html\">Linux vi&#x2F;vim | 菜鸟教程</a></p>\n<p>基本上 vi&#x2F;vim 共分为三种模式，<strong>命令模式（Command Mode）、输入模式（Insert Mode）和命令行模式（Command-Line Mode）</strong>。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234707.png\"></p>\n<h3 id=\"输入模式\"><a href=\"#输入模式\" class=\"headerlink\" title=\"输入模式\"></a>输入模式</h3><p><strong>像在文本文档中打字一样输入</strong></p>\n<p>在命令模式下按下 i 就进入了输入模式，使用 Esc 键可以返回到普通模式。</p>\n<p>在输入模式中，可以使用以下按键：</p>\n<ul>\n<li><strong>字符按键以及Shift组合</strong>，输入字符</li>\n<li><strong>ENTER</strong>，回车键，换行</li>\n<li><strong>BACK SPACE</strong>，退格键，删除光标前一个字符</li>\n<li><strong>DEL</strong>，删除键，删除光标后一个字符</li>\n<li><strong>方向键</strong>，在文本中移动光标</li>\n<li><strong>HOME</strong>&#x2F;<strong>END</strong>，移动光标到行首&#x2F;行尾</li>\n<li><strong>Page Up</strong>&#x2F;<strong>Page Down</strong>，上&#x2F;下翻页</li>\n<li><strong>Insert</strong>，切换光标为输入&#x2F;替换模式，光标将变成竖线&#x2F;下划线</li>\n<li><strong>ESC</strong>，退出输入模式，切换到命令模式</li>\n</ul>\n<h3 id=\"命令模式（一般模式）\"><a href=\"#命令模式（一般模式）\" class=\"headerlink\" title=\"命令模式（一般模式）\"></a>命令模式（一般模式）</h3><p><strong>用户刚刚启动 vi&#x2F;vim，便进入了命令模式。</strong></p>\n<p>此状态下敲击键盘动作会被 Vim 识别为命令，而非输入字符，比如我们此时按下 i，并不会输入一个字符，i 被当作了一个命令。</p>\n<p>以下是普通模式常用的几个命令：</p>\n<ul>\n<li>i – 切换到输入模式，在光标当前位置开始输入文本。</li>\n<li>x – 删除当前光标所在处的字符。</li>\n<li>: – 切换到底线命令模式，以在最底一行输入命令。</li>\n<li>a – 进入插入模式，在光标下一个位置开始输入文本。</li>\n<li>o：在当前行的下方插入一个新行，并进入插入模式。</li>\n<li>O – 在当前行的上方插入一个新行，并进入插入模式。</li>\n<li>dd – 删除当前行。</li>\n<li>yy – 复制当前行。</li>\n<li>p – 粘贴剪贴板内容到光标下方。</li>\n<li>P – 粘贴剪贴板内容到光标上方。</li>\n<li>u – 撤销上一次操作。</li>\n<li>Ctrl + r – 重做上一次撤销的操作。</li>\n<li>:w – 保存文件。</li>\n<li>:q – 退出 Vim 编辑器。</li>\n<li>:q! – 强制退出Vim 编辑器，不保存修改。</li>\n</ul>\n<p>若想要编辑文本，只需要启动 Vim，进入了命令模式，按下 i 切换到输入模式即可。</p>\n<p>命令模式只有一些最基本的命令，因此仍要依靠<strong>底线命令行模式</strong>输入更多命令。</p>\n<h3 id=\"底线命令模式（命令行模式）\"><a href=\"#底线命令模式（命令行模式）\" class=\"headerlink\" title=\"底线命令模式（命令行模式）\"></a>底线命令模式（命令行模式）</h3><p>在命令模式下按下 :（英文冒号）就进入了底线命令模式。</p>\n<p>底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。</p>\n<p>在底线命令模式中，基本的命令有（已经省略了冒号）：</p>\n<ul>\n<li><code>:w</code>：保存文件。</li>\n<li><code>:q</code>：退出 Vim 编辑器。</li>\n<li><code>:wq</code>：保存文件并退出 Vim 编辑器。</li>\n<li><code>:q!</code>：强制退出Vim编辑器，不保存修改。</li>\n</ul>\n<p>按 ESC 键可随时退出底线命令模式。</p>\n<table>\n<thead>\n<tr>\n<th>移动光标的方法</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>h 或 向左箭头键(←)</td>\n<td>光标向左移动一个字符</td>\n</tr>\n<tr>\n<td>j 或 向下箭头键(↓)</td>\n<td>光标向下移动一个字符</td>\n</tr>\n<tr>\n<td>k 或 向上箭头键(↑)</td>\n<td>光标向上移动一个字符</td>\n</tr>\n<tr>\n<td>l 或 向右箭头键(→)</td>\n<td>光标向右移动一个字符</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>搜索替换</th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&#x2F;word</td>\n<td>向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 &#x2F;vbird 即可！ (常用)</td>\n<td></td>\n</tr>\n<tr>\n<td>?word</td>\n<td>向光标之上寻找一个字符串名称为 word 的字符串。</td>\n<td></td>\n</tr>\n<tr>\n<td>n</td>\n<td>这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 &#x2F;vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！</td>\n<td></td>\n</tr>\n<tr>\n<td>N</td>\n<td>这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 &#x2F;vbird 后，按下 N 则表示『向上』搜寻 vbird 。</td>\n<td></td>\n</tr>\n<tr>\n<td>:n1,n2s&#x2F;word1&#x2F;word2&#x2F;g</td>\n<td>n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：  <br>『:100,200s&#x2F;vbird&#x2F;VBIRD&#x2F;g』。(常用)</td>\n<td></td>\n</tr>\n<tr>\n<td>:1,$s&#x2F;word1&#x2F;word2&#x2F;g 或 :%s&#x2F;word1&#x2F;word2&#x2F;g</td>\n<td>从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用)</td>\n<td></td>\n</tr>\n<tr>\n<td>:1,$s&#x2F;word1&#x2F;word2&#x2F;gc 或 :%s&#x2F;word1&#x2F;word2&#x2F;gc</td>\n<td>从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用)</td>\n<td></td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>vim 环境的变更</th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>:set nu</td>\n<td>显示行号，设定之后，会在每一行的前缀显示该行的行号</td>\n<td></td>\n</tr>\n<tr>\n<td>:set nonu</td>\n<td>与 set nu 相反，为取消行号！</td>\n<td></td>\n</tr>\n</tbody></table>\n"},{"title":"程序与进程管理","_content":"\n**实现在一个终端中执行多个任务**\n\n### 程序管理\n\nbash环境中，在命令最后添加 **&** 符号使命令切换到后台执行\n后台执行的程序的输出会直接输出在终端，干扰我们在终端中的编辑，需要重定向到适当位置\n```\n命令 > /tmp/name.log &\n```\n\n输出会保存在name.log中，使用cat可以查看\n\nCtrl + z可以将终端中的程序切换到后台，并输出工作号码\n\n使用jobs查看后台程序\n```\njobs\n\n# 使用-l选项显示后台程序的PID\njobs -l\n\n# 使用-r输出正在后台运行的程序\njobs -r\n\n# 使用-s输出所有已停止的后台程序\njobs -s\n```\n\n使用fg命令将后台程序切换到前台\n```\nfg  # 将最后一个移到后台的程序打开\n\nfg n  # 将工作号码为n的程序打开\n```\n\n使用bg启动后台程序\n```\nbg 1  # 启动工作号码为1的程序\n```\n\n删除后台程序\n程序在前台，Ctrl + C可以关闭程序\n```\nkill -1  # 重新读取参数配置\nkill -2  # 等于Ctrl + C\nkill -9  # 强制杀死一个程序\nkill -15  # 默认值，正常删除\n```\n\n\n### 进程管理\n\n#### 查看进程\nps命令：显示进程\n```\nps\nps -l  # 显示详细信息\n```\npstree命令\n```\npstree\npstree -p  # -p显示进程对应的PID\n```\ntop命令\n\n#### 系统资源查看\n查看内存使用量\n```\nfree \nfree -m   # -m使用MB为单位\n```\n查看系统及内核信息\n```\nuname -a  # 查看所有信息\n```","source":"_posts/Notes/Linux学习/程序与进程管理.md","raw":"---\ntitle: 程序与进程管理\ncategories:\n  - Notes\n  - Linux学习\ntags:\n  - Linux\n---\n\n**实现在一个终端中执行多个任务**\n\n### 程序管理\n\nbash环境中，在命令最后添加 **&** 符号使命令切换到后台执行\n后台执行的程序的输出会直接输出在终端，干扰我们在终端中的编辑，需要重定向到适当位置\n```\n命令 > /tmp/name.log &\n```\n\n输出会保存在name.log中，使用cat可以查看\n\nCtrl + z可以将终端中的程序切换到后台，并输出工作号码\n\n使用jobs查看后台程序\n```\njobs\n\n# 使用-l选项显示后台程序的PID\njobs -l\n\n# 使用-r输出正在后台运行的程序\njobs -r\n\n# 使用-s输出所有已停止的后台程序\njobs -s\n```\n\n使用fg命令将后台程序切换到前台\n```\nfg  # 将最后一个移到后台的程序打开\n\nfg n  # 将工作号码为n的程序打开\n```\n\n使用bg启动后台程序\n```\nbg 1  # 启动工作号码为1的程序\n```\n\n删除后台程序\n程序在前台，Ctrl + C可以关闭程序\n```\nkill -1  # 重新读取参数配置\nkill -2  # 等于Ctrl + C\nkill -9  # 强制杀死一个程序\nkill -15  # 默认值，正常删除\n```\n\n\n### 进程管理\n\n#### 查看进程\nps命令：显示进程\n```\nps\nps -l  # 显示详细信息\n```\npstree命令\n```\npstree\npstree -p  # -p显示进程对应的PID\n```\ntop命令\n\n#### 系统资源查看\n查看内存使用量\n```\nfree \nfree -m   # -m使用MB为单位\n```\n查看系统及内核信息\n```\nuname -a  # 查看所有信息\n```","slug":"Notes/Linux学习/程序与进程管理","published":1,"date":"2023-09-19T12:47:32.977Z","updated":"2023-09-27T12:33:21.281Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr56000ltou5b2dpd2bt","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><strong>实现在一个终端中执行多个任务</strong></p>\n<h3 id=\"程序管理\"><a href=\"#程序管理\" class=\"headerlink\" title=\"程序管理\"></a>程序管理</h3><p>bash环境中，在命令最后添加 <strong>&amp;</strong> 符号使命令切换到后台执行<br>后台执行的程序的输出会直接输出在终端，干扰我们在终端中的编辑，需要重定向到适当位置</p>\n<figure class=\"highlight arcade\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs arcade\">命令 &gt; <span class=\"hljs-regexp\">/tmp/</span>name.<span class=\"hljs-built_in\">log</span> &amp;<br></code></pre></td></tr></table></figure>\n\n<p>输出会保存在name.log中，使用cat可以查看</p>\n<p>Ctrl + z可以将终端中的程序切换到后台，并输出工作号码</p>\n<p>使用jobs查看后台程序</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">jobs</span><br><br><span class=\"hljs-comment\"># 使用-l选项显示后台程序的PID</span><br><span class=\"hljs-built_in\">jobs</span> -l<br><br><span class=\"hljs-comment\"># 使用-r输出正在后台运行的程序</span><br><span class=\"hljs-built_in\">jobs</span> -r<br><br><span class=\"hljs-comment\"># 使用-s输出所有已停止的后台程序</span><br><span class=\"hljs-built_in\">jobs</span> -s<br></code></pre></td></tr></table></figure>\n\n<p>使用fg命令将后台程序切换到前台</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">fg</span>  <span class=\"hljs-comment\"># 将最后一个移到后台的程序打开</span><br><br><span class=\"hljs-built_in\">fg</span> n  <span class=\"hljs-comment\"># 将工作号码为n的程序打开</span><br></code></pre></td></tr></table></figure>\n\n<p>使用bg启动后台程序</p>\n<figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs apache\"><span class=\"hljs-attribute\">bg</span> <span class=\"hljs-number\">1</span>  # 启动工作号码为<span class=\"hljs-number\">1</span>的程序<br></code></pre></td></tr></table></figure>\n\n<p>删除后台程序<br>程序在前台，Ctrl + C可以关闭程序</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">kill</span> -1  <span class=\"hljs-comment\"># 重新读取参数配置</span><br><span class=\"hljs-built_in\">kill</span> -2  <span class=\"hljs-comment\"># 等于Ctrl + C</span><br><span class=\"hljs-built_in\">kill</span> -9  <span class=\"hljs-comment\"># 强制杀死一个程序</span><br><span class=\"hljs-built_in\">kill</span> -15  <span class=\"hljs-comment\"># 默认值，正常删除</span><br></code></pre></td></tr></table></figure>\n\n\n<h3 id=\"进程管理\"><a href=\"#进程管理\" class=\"headerlink\" title=\"进程管理\"></a>进程管理</h3><h4 id=\"查看进程\"><a href=\"#查看进程\" class=\"headerlink\" title=\"查看进程\"></a>查看进程</h4><p>ps命令：显示进程</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs powershell\"><span class=\"hljs-built_in\">ps</span><br><span class=\"hljs-built_in\">ps</span> <span class=\"hljs-literal\">-l</span>  <span class=\"hljs-comment\"># 显示详细信息</span><br></code></pre></td></tr></table></figure>\n<p>pstree命令</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs css\">pstree<br>pstree -<span class=\"hljs-selector-tag\">p</span>  # -<span class=\"hljs-selector-tag\">p</span>显示进程对应的PID<br></code></pre></td></tr></table></figure>\n<p>top命令</p>\n<h4 id=\"系统资源查看\"><a href=\"#系统资源查看\" class=\"headerlink\" title=\"系统资源查看\"></a>系统资源查看</h4><p>查看内存使用量</p>\n<figure class=\"highlight gams\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gams\"><span class=\"hljs-keyword\">free</span> <br><span class=\"hljs-keyword\">free</span> -m   # -m使用MB为单位<br></code></pre></td></tr></table></figure>\n<p>查看系统及内核信息</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">uname</span> -a  <span class=\"hljs-comment\"># 查看所有信息</span><br></code></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p><strong>实现在一个终端中执行多个任务</strong></p>\n<h3 id=\"程序管理\"><a href=\"#程序管理\" class=\"headerlink\" title=\"程序管理\"></a>程序管理</h3><p>bash环境中，在命令最后添加 <strong>&amp;</strong> 符号使命令切换到后台执行<br>后台执行的程序的输出会直接输出在终端，干扰我们在终端中的编辑，需要重定向到适当位置</p>\n<figure class=\"highlight arcade\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs arcade\">命令 &gt; <span class=\"hljs-regexp\">/tmp/</span>name.<span class=\"hljs-built_in\">log</span> &amp;<br></code></pre></td></tr></table></figure>\n\n<p>输出会保存在name.log中，使用cat可以查看</p>\n<p>Ctrl + z可以将终端中的程序切换到后台，并输出工作号码</p>\n<p>使用jobs查看后台程序</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">jobs</span><br><br><span class=\"hljs-comment\"># 使用-l选项显示后台程序的PID</span><br><span class=\"hljs-built_in\">jobs</span> -l<br><br><span class=\"hljs-comment\"># 使用-r输出正在后台运行的程序</span><br><span class=\"hljs-built_in\">jobs</span> -r<br><br><span class=\"hljs-comment\"># 使用-s输出所有已停止的后台程序</span><br><span class=\"hljs-built_in\">jobs</span> -s<br></code></pre></td></tr></table></figure>\n\n<p>使用fg命令将后台程序切换到前台</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">fg</span>  <span class=\"hljs-comment\"># 将最后一个移到后台的程序打开</span><br><br><span class=\"hljs-built_in\">fg</span> n  <span class=\"hljs-comment\"># 将工作号码为n的程序打开</span><br></code></pre></td></tr></table></figure>\n\n<p>使用bg启动后台程序</p>\n<figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs apache\"><span class=\"hljs-attribute\">bg</span> <span class=\"hljs-number\">1</span>  # 启动工作号码为<span class=\"hljs-number\">1</span>的程序<br></code></pre></td></tr></table></figure>\n\n<p>删除后台程序<br>程序在前台，Ctrl + C可以关闭程序</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">kill</span> -1  <span class=\"hljs-comment\"># 重新读取参数配置</span><br><span class=\"hljs-built_in\">kill</span> -2  <span class=\"hljs-comment\"># 等于Ctrl + C</span><br><span class=\"hljs-built_in\">kill</span> -9  <span class=\"hljs-comment\"># 强制杀死一个程序</span><br><span class=\"hljs-built_in\">kill</span> -15  <span class=\"hljs-comment\"># 默认值，正常删除</span><br></code></pre></td></tr></table></figure>\n\n\n<h3 id=\"进程管理\"><a href=\"#进程管理\" class=\"headerlink\" title=\"进程管理\"></a>进程管理</h3><h4 id=\"查看进程\"><a href=\"#查看进程\" class=\"headerlink\" title=\"查看进程\"></a>查看进程</h4><p>ps命令：显示进程</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs powershell\"><span class=\"hljs-built_in\">ps</span><br><span class=\"hljs-built_in\">ps</span> <span class=\"hljs-literal\">-l</span>  <span class=\"hljs-comment\"># 显示详细信息</span><br></code></pre></td></tr></table></figure>\n<p>pstree命令</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs css\">pstree<br>pstree -<span class=\"hljs-selector-tag\">p</span>  # -<span class=\"hljs-selector-tag\">p</span>显示进程对应的PID<br></code></pre></td></tr></table></figure>\n<p>top命令</p>\n<h4 id=\"系统资源查看\"><a href=\"#系统资源查看\" class=\"headerlink\" title=\"系统资源查看\"></a>系统资源查看</h4><p>查看内存使用量</p>\n<figure class=\"highlight gams\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gams\"><span class=\"hljs-keyword\">free</span> <br><span class=\"hljs-keyword\">free</span> -m   # -m使用MB为单位<br></code></pre></td></tr></table></figure>\n<p>查看系统及内核信息</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-built_in\">uname</span> -a  <span class=\"hljs-comment\"># 查看所有信息</span><br></code></pre></td></tr></table></figure>"},{"title":"Advanced Tables表格绘制","update":null,"_content":"用 | 隔开内容，然后按TAB生成表格，Enter进入下一行\n\n示例：\n\n| Hotkey       | Action                      |\n| ------------ | --------------------------- |\n| Tab          | Next Cell                   |\n| Enter        | Next Row                    |\n| Shift+Tab    | Previous Cell               |\n| Ctrl+Shift+D | Open table controls sidebar |\n","source":"_posts/Notes/Ob插件/Advanced Tables 表格绘制.md","raw":"---\ntitle: Advanced Tables表格绘制\ncategories:\n  - Notes\n  - Ob插件\nupdate: \ntags:\n  - Obsidian\n---\n用 | 隔开内容，然后按TAB生成表格，Enter进入下一行\n\n示例：\n\n| Hotkey       | Action                      |\n| ------------ | --------------------------- |\n| Tab          | Next Cell                   |\n| Enter        | Next Row                    |\n| Shift+Tab    | Previous Cell               |\n| Ctrl+Shift+D | Open table controls sidebar |\n","slug":"Notes/Ob插件/Advanced Tables 表格绘制","published":1,"date":"2023-09-21T16:36:34.333Z","updated":"2023-09-27T12:34:35.346Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr57000mtou575vg8qzc","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>用 | 隔开内容，然后按TAB生成表格，Enter进入下一行</p>\n<p>示例：</p>\n<table>\n<thead>\n<tr>\n<th>Hotkey</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tab</td>\n<td>Next Cell</td>\n</tr>\n<tr>\n<td>Enter</td>\n<td>Next Row</td>\n</tr>\n<tr>\n<td>Shift+Tab</td>\n<td>Previous Cell</td>\n</tr>\n<tr>\n<td>Ctrl+Shift+D</td>\n<td>Open table controls sidebar</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"","more":"<p>用 | 隔开内容，然后按TAB生成表格，Enter进入下一行</p>\n<p>示例：</p>\n<table>\n<thead>\n<tr>\n<th>Hotkey</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tab</td>\n<td>Next Cell</td>\n</tr>\n<tr>\n<td>Enter</td>\n<td>Next Row</td>\n</tr>\n<tr>\n<td>Shift+Tab</td>\n<td>Previous Cell</td>\n</tr>\n<tr>\n<td>Ctrl+Shift+D</td>\n<td>Open table controls sidebar</td>\n</tr>\n</tbody></table>\n"},{"title":"景点","update":null,"_content":"远：\n>兵马俑 华清池 骊山 秦始皇陵\n\n晚上:\n>大唐不夜城 大唐芙蓉园 钟楼&鼓楼 \n\n城墙\n大雁塔 小雁塔\n~~陕西历史博物馆（只能越20号了 8.14 10点、11点、18点）~~\n碑林博物馆\n\n","source":"_posts/Notes/旅游/景点.md","raw":"---\ntitle: 景点\ncategories:\n  - Notes\n  - 旅游\nupdate: \ntags:\n  - 西安\n---\n远：\n>兵马俑 华清池 骊山 秦始皇陵\n\n晚上:\n>大唐不夜城 大唐芙蓉园 钟楼&鼓楼 \n\n城墙\n大雁塔 小雁塔\n~~陕西历史博物馆（只能越20号了 8.14 10点、11点、18点）~~\n碑林博物馆\n\n","slug":"Notes/旅游/景点","published":1,"date":"2023-09-21T16:36:34.335Z","updated":"2023-09-27T12:31:38.107Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr58000ptou53www8psm","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>远：</p>\n<blockquote>\n<p>兵马俑 华清池 骊山 秦始皇陵</p>\n</blockquote>\n<p>晚上:</p>\n<blockquote>\n<p>大唐不夜城 大唐芙蓉园 钟楼&amp;鼓楼 </p>\n</blockquote>\n<p>城墙<br>大雁塔 小雁塔<br><del>陕西历史博物馆（只能越20号了 8.14 10点、11点、18点）</del><br>碑林博物馆</p>\n","site":{"data":{}},"excerpt":"","more":"<p>远：</p>\n<blockquote>\n<p>兵马俑 华清池 骊山 秦始皇陵</p>\n</blockquote>\n<p>晚上:</p>\n<blockquote>\n<p>大唐不夜城 大唐芙蓉园 钟楼&amp;鼓楼 </p>\n</blockquote>\n<p>城墙<br>大雁塔 小雁塔<br><del>陕西历史博物馆（只能越20号了 8.14 10点、11点、18点）</del><br>碑林博物馆</p>\n"},{"title":"美食","update":null,"_content":"**1** 回民街、洒金桥（网红景点）可以去逛逛\n\n**2** 早餐：小南门早市\n\n**3** 泡馍和面馆 路边摊就行 赵记羊杂\n\n**4** 夜市：\n未央区六号大街 \n凤城六路和未央路交叉口\n碑林区卧龙巷机关小区\n","source":"_posts/Notes/旅游/美食.md","raw":"---\ntitle: 美食\ncategories:\n  - Notes\n  - 旅游\nupdate: \ntags:\n  - 西安\n---\n**1** 回民街、洒金桥（网红景点）可以去逛逛\n\n**2** 早餐：小南门早市\n\n**3** 泡馍和面馆 路边摊就行 赵记羊杂\n\n**4** 夜市：\n未央区六号大街 \n凤城六路和未央路交叉口\n碑林区卧龙巷机关小区\n","slug":"Notes/旅游/美食","published":1,"date":"2023-09-21T16:36:34.335Z","updated":"2023-09-27T12:31:42.877Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr58000rtou5e46f86mt","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><strong>1</strong> 回民街、洒金桥（网红景点）可以去逛逛</p>\n<p><strong>2</strong> 早餐：小南门早市</p>\n<p><strong>3</strong> 泡馍和面馆 路边摊就行 赵记羊杂</p>\n<p><strong>4</strong> 夜市：<br>未央区六号大街<br>凤城六路和未央路交叉口<br>碑林区卧龙巷机关小区</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>1</strong> 回民街、洒金桥（网红景点）可以去逛逛</p>\n<p><strong>2</strong> 早餐：小南门早市</p>\n<p><strong>3</strong> 泡馍和面馆 路边摊就行 赵记羊杂</p>\n<p><strong>4</strong> 夜市：<br>未央区六号大街<br>凤城六路和未央路交叉口<br>碑林区卧龙巷机关小区</p>\n"},{"title":"西安8.16-8.20","update":null,"_content":"## 17号\n\n早餐 小南门早市\n\n大雁塔 \n>音乐喷泉 12:00-12:10\n\n大唐不夜城 晚上去\n\n晚上 附近夜市 [[美食]]\n\n大唐芙蓉园 20:00\n## 18号\n\n兵马俑 华清池 骊山 ~~秦始皇陵~~\n\n午晚饭 [[美食]]\n\n## 19号\n\n碑林博物馆\n\n钟楼&鼓楼 **晚上**\n\n晚上 附近夜市\n\n大唐芙蓉园 20:00\n## 20号\n\n小雁塔 西安博物馆\n \n野球帝 livehouse\n\n~~陕西历史博物馆~~\n\n[[景点]]","source":"_posts/Notes/旅游/西安8.16-20.md","raw":"---\ntitle: 西安8.16-8.20\ncategories:\n  - Notes\n  - 旅游\nupdate: \ntags:\n  - 西安\n---\n## 17号\n\n早餐 小南门早市\n\n大雁塔 \n>音乐喷泉 12:00-12:10\n\n大唐不夜城 晚上去\n\n晚上 附近夜市 [[美食]]\n\n大唐芙蓉园 20:00\n## 18号\n\n兵马俑 华清池 骊山 ~~秦始皇陵~~\n\n午晚饭 [[美食]]\n\n## 19号\n\n碑林博物馆\n\n钟楼&鼓楼 **晚上**\n\n晚上 附近夜市\n\n大唐芙蓉园 20:00\n## 20号\n\n小雁塔 西安博物馆\n \n野球帝 livehouse\n\n~~陕西历史博物馆~~\n\n[[景点]]","slug":"Notes/旅游/西安8.16-20","published":1,"date":"2023-09-21T16:36:34.335Z","updated":"2023-09-27T12:31:48.879Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr59000wtou56isihdri","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h2 id=\"17号\"><a href=\"#17号\" class=\"headerlink\" title=\"17号\"></a>17号</h2><p>早餐 小南门早市</p>\n<p>大雁塔 </p>\n<blockquote>\n<p>音乐喷泉 12:00-12:10</p>\n</blockquote>\n<p>大唐不夜城 晚上去</p>\n<p>晚上 附近夜市 [[美食]]</p>\n<p>大唐芙蓉园 20:00</p>\n<h2 id=\"18号\"><a href=\"#18号\" class=\"headerlink\" title=\"18号\"></a>18号</h2><p>兵马俑 华清池 骊山 <del>秦始皇陵</del></p>\n<p>午晚饭 [[美食]]</p>\n<h2 id=\"19号\"><a href=\"#19号\" class=\"headerlink\" title=\"19号\"></a>19号</h2><p>碑林博物馆</p>\n<p>钟楼&amp;鼓楼 <strong>晚上</strong></p>\n<p>晚上 附近夜市</p>\n<p>大唐芙蓉园 20:00</p>\n<h2 id=\"20号\"><a href=\"#20号\" class=\"headerlink\" title=\"20号\"></a>20号</h2><p>小雁塔 西安博物馆</p>\n<p>野球帝 livehouse</p>\n<p><del>陕西历史博物馆</del></p>\n<p>[[景点]]</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"17号\"><a href=\"#17号\" class=\"headerlink\" title=\"17号\"></a>17号</h2><p>早餐 小南门早市</p>\n<p>大雁塔 </p>\n<blockquote>\n<p>音乐喷泉 12:00-12:10</p>\n</blockquote>\n<p>大唐不夜城 晚上去</p>\n<p>晚上 附近夜市 [[美食]]</p>\n<p>大唐芙蓉园 20:00</p>\n<h2 id=\"18号\"><a href=\"#18号\" class=\"headerlink\" title=\"18号\"></a>18号</h2><p>兵马俑 华清池 骊山 <del>秦始皇陵</del></p>\n<p>午晚饭 [[美食]]</p>\n<h2 id=\"19号\"><a href=\"#19号\" class=\"headerlink\" title=\"19号\"></a>19号</h2><p>碑林博物馆</p>\n<p>钟楼&amp;鼓楼 <strong>晚上</strong></p>\n<p>晚上 附近夜市</p>\n<p>大唐芙蓉园 20:00</p>\n<h2 id=\"20号\"><a href=\"#20号\" class=\"headerlink\" title=\"20号\"></a>20号</h2><p>小雁塔 西安博物馆</p>\n<p>野球帝 livehouse</p>\n<p><del>陕西历史博物馆</del></p>\n<p>[[景点]]</p>\n"},{"title":"PowellShell美化","update":null,"_content":"下载最新的PowerShell7.x.x\n\n根据oh my posh官网安装，结合网上的博客配置\n[Windows | Oh My Posh](https://ohmyposh.dev/docs/installation/windows)\n[oh-my-posh - 终端个性化工具 - 美化PowerShell / cmd - 好物分享，真香\\~\\_小码农场-小码农的博客-CSDN博客](https://blog.csdn.net/qq_21689457/article/details/129345662)\n\n安装字体\n先是按官方的指令下载不了，后来又成功了，然后根据给的config配置文件更改setting\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234327.png)\n\n根据官网安装主题,修改profile中的代码\n[Themes | Oh My Posh](https://ohmyposh.dev/docs/themes)这里有所有主题，更换时把profile的id改一下就行","source":"_posts/Notes/花里胡哨/PowerShell美化.md","raw":"---\ntitle: PowellShell美化\ncategories:\n  - Notes\n  - 花里胡哨\nupdate: \ntags:\n  - powershell\n---\n下载最新的PowerShell7.x.x\n\n根据oh my posh官网安装，结合网上的博客配置\n[Windows | Oh My Posh](https://ohmyposh.dev/docs/installation/windows)\n[oh-my-posh - 终端个性化工具 - 美化PowerShell / cmd - 好物分享，真香\\~\\_小码农场-小码农的博客-CSDN博客](https://blog.csdn.net/qq_21689457/article/details/129345662)\n\n安装字体\n先是按官方的指令下载不了，后来又成功了，然后根据给的config配置文件更改setting\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234327.png)\n\n根据官网安装主题,修改profile中的代码\n[Themes | Oh My Posh](https://ohmyposh.dev/docs/themes)这里有所有主题，更换时把profile的id改一下就行","slug":"Notes/花里胡哨/PowerShell美化","published":1,"date":"2023-09-21T16:36:34.334Z","updated":"2023-09-27T12:27:34.964Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr59000ztou58dyj1jgo","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>下载最新的PowerShell7.x.x</p>\n<p>根据oh my posh官网安装，结合网上的博客配置<br><a href=\"https://ohmyposh.dev/docs/installation/windows\">Windows | Oh My Posh</a><br><a href=\"https://blog.csdn.net/qq_21689457/article/details/129345662\">oh-my-posh - 终端个性化工具 - 美化PowerShell &#x2F; cmd - 好物分享，真香~_小码农场-小码农的博客-CSDN博客</a></p>\n<p>安装字体<br>先是按官方的指令下载不了，后来又成功了，然后根据给的config配置文件更改setting<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234327.png\"></p>\n<p>根据官网安装主题,修改profile中的代码<br><a href=\"https://ohmyposh.dev/docs/themes\">Themes | Oh My Posh</a>这里有所有主题，更换时把profile的id改一下就行</p>\n","site":{"data":{}},"excerpt":"","more":"<p>下载最新的PowerShell7.x.x</p>\n<p>根据oh my posh官网安装，结合网上的博客配置<br><a href=\"https://ohmyposh.dev/docs/installation/windows\">Windows | Oh My Posh</a><br><a href=\"https://blog.csdn.net/qq_21689457/article/details/129345662\">oh-my-posh - 终端个性化工具 - 美化PowerShell &#x2F; cmd - 好物分享，真香~_小码农场-小码农的博客-CSDN博客</a></p>\n<p>安装字体<br>先是按官方的指令下载不了，后来又成功了，然后根据给的config配置文件更改setting<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234327.png\"></p>\n<p>根据官网安装主题,修改profile中的代码<br><a href=\"https://ohmyposh.dev/docs/themes\">Themes | Oh My Posh</a>这里有所有主题，更换时把profile的id改一下就行</p>\n"},{"title":"WSL配置","_content":"\nWSL(Windows Subsystem for Linux)，Windows自带的Linux子系统\n\n## TODO \n- [ ] 输入bash没反应，重启后正常，待解决\n\n#### 安装过程及问题\n在Windows中开启该功能然后重启\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234736.png)\n\n> 可能出现以下问题\n> ![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234752.png)\n> 造成该问题的原因是WSL版本由原来的WSL1升级到WSL2后，内核没有升级，前往[微软WSL官网](https://docs.microsoft.com/zh-cn/windows/wsl/wsl2-kernel)下载安装适用于 x64 计算机的最新 WSL2 Linux 内核更新包即可。\n\n#### 迁移到D盘\n\n```text\n# 查看已经安装的虚拟机\nwsl -l -v\n# 关闭所有正在运行的虚拟机\nwsl --shutdown\n# 虚拟机文件导出\nwsl --export 虚拟机名称 保存路径\nwsl --export Ubuntu D:\\\\wsl-Ubuntu.tar\n# 注销原虚拟机\nwsl --unregister Ubuntu\n# 导入虚拟机文件\nwsl --import 虚拟机名称 目标路径 虚拟机文件路径 --version 2\nwsl --import Ubuntu D:\\\\WSL2\\\\Ubuntu D:\\\\wsl-Ubuntu.tar --version 2\n# 最后可以选择删除掉虚拟机文件，因为他已经没用了\n```\n\n**更改默认用户：**\n- 在powershell中输入`ubuntu.exe config --default-user 用户名`来将root用户改为普通用户\n- 输入`ubuntu config --default-user root`将普通用户改为默认用户，可以在忘记root密码时使用\n\n#### 配置python库环境\n在miniconda官网中找到下载命令，下载miniconda\n然后新建环境，在环境中下载相关依赖包\n#### Pycharm中使用WSL\n在Pycharm中添加解释器\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234812.png)\n选择WSL，选择配置好的环境\n\n#### 其他\nWSL中/mnt文件里是Windows文件的映射\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234824.png)\n\n在想转到Windows某目录下时需要cd到其映射后的目录\n","source":"_posts/Notes/编程/WSL配置.md","raw":"---\ntitle: WSL配置\ncategories:\n  - Notes\n  - 编程\ntags:\n  - WSL\n---\n\nWSL(Windows Subsystem for Linux)，Windows自带的Linux子系统\n\n## TODO \n- [ ] 输入bash没反应，重启后正常，待解决\n\n#### 安装过程及问题\n在Windows中开启该功能然后重启\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234736.png)\n\n> 可能出现以下问题\n> ![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234752.png)\n> 造成该问题的原因是WSL版本由原来的WSL1升级到WSL2后，内核没有升级，前往[微软WSL官网](https://docs.microsoft.com/zh-cn/windows/wsl/wsl2-kernel)下载安装适用于 x64 计算机的最新 WSL2 Linux 内核更新包即可。\n\n#### 迁移到D盘\n\n```text\n# 查看已经安装的虚拟机\nwsl -l -v\n# 关闭所有正在运行的虚拟机\nwsl --shutdown\n# 虚拟机文件导出\nwsl --export 虚拟机名称 保存路径\nwsl --export Ubuntu D:\\\\wsl-Ubuntu.tar\n# 注销原虚拟机\nwsl --unregister Ubuntu\n# 导入虚拟机文件\nwsl --import 虚拟机名称 目标路径 虚拟机文件路径 --version 2\nwsl --import Ubuntu D:\\\\WSL2\\\\Ubuntu D:\\\\wsl-Ubuntu.tar --version 2\n# 最后可以选择删除掉虚拟机文件，因为他已经没用了\n```\n\n**更改默认用户：**\n- 在powershell中输入`ubuntu.exe config --default-user 用户名`来将root用户改为普通用户\n- 输入`ubuntu config --default-user root`将普通用户改为默认用户，可以在忘记root密码时使用\n\n#### 配置python库环境\n在miniconda官网中找到下载命令，下载miniconda\n然后新建环境，在环境中下载相关依赖包\n#### Pycharm中使用WSL\n在Pycharm中添加解释器\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234812.png)\n选择WSL，选择配置好的环境\n\n#### 其他\nWSL中/mnt文件里是Windows文件的映射\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234824.png)\n\n在想转到Windows某目录下时需要cd到其映射后的目录\n","slug":"Notes/编程/WSL配置","published":1,"date":"2023-09-19T12:47:32.973Z","updated":"2023-09-27T12:35:27.361Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr5a0013tou54btlgbni","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>WSL(Windows Subsystem for Linux)，Windows自带的Linux子系统</p>\n<h2 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h2><ul>\n<li><input disabled=\"\" type=\"checkbox\"> 输入bash没反应，重启后正常，待解决</li>\n</ul>\n<h4 id=\"安装过程及问题\"><a href=\"#安装过程及问题\" class=\"headerlink\" title=\"安装过程及问题\"></a>安装过程及问题</h4><p>在Windows中开启该功能然后重启</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234736.png\"></p>\n<blockquote>\n<p>可能出现以下问题<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234752.png\"><br>造成该问题的原因是WSL版本由原来的WSL1升级到WSL2后，内核没有升级，前往<a href=\"https://docs.microsoft.com/zh-cn/windows/wsl/wsl2-kernel\">微软WSL官网</a>下载安装适用于 x64 计算机的最新 WSL2 Linux 内核更新包即可。</p>\n</blockquote>\n<h4 id=\"迁移到D盘\"><a href=\"#迁移到D盘\" class=\"headerlink\" title=\"迁移到D盘\"></a>迁移到D盘</h4><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\"># 查看已经安装的虚拟机<br>wsl -l -v<br># 关闭所有正在运行的虚拟机<br>wsl --shutdown<br># 虚拟机文件导出<br>wsl --export 虚拟机名称 保存路径<br>wsl --export Ubuntu D:\\\\wsl-Ubuntu.tar<br># 注销原虚拟机<br>wsl --unregister Ubuntu<br># 导入虚拟机文件<br>wsl --import 虚拟机名称 目标路径 虚拟机文件路径 --version 2<br>wsl --import Ubuntu D:\\\\WSL2\\\\Ubuntu D:\\\\wsl-Ubuntu.tar --version 2<br># 最后可以选择删除掉虚拟机文件，因为他已经没用了<br></code></pre></td></tr></table></figure>\n\n<p><strong>更改默认用户：</strong></p>\n<ul>\n<li>在powershell中输入<code>ubuntu.exe config --default-user 用户名</code>来将root用户改为普通用户</li>\n<li>输入<code>ubuntu config --default-user root</code>将普通用户改为默认用户，可以在忘记root密码时使用</li>\n</ul>\n<h4 id=\"配置python库环境\"><a href=\"#配置python库环境\" class=\"headerlink\" title=\"配置python库环境\"></a>配置python库环境</h4><p>在miniconda官网中找到下载命令，下载miniconda<br>然后新建环境，在环境中下载相关依赖包</p>\n<h4 id=\"Pycharm中使用WSL\"><a href=\"#Pycharm中使用WSL\" class=\"headerlink\" title=\"Pycharm中使用WSL\"></a>Pycharm中使用WSL</h4><p>在Pycharm中添加解释器<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234812.png\"><br>选择WSL，选择配置好的环境</p>\n<h4 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h4><p>WSL中&#x2F;mnt文件里是Windows文件的映射<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234824.png\"></p>\n<p>在想转到Windows某目录下时需要cd到其映射后的目录</p>\n","site":{"data":{}},"excerpt":"","more":"<p>WSL(Windows Subsystem for Linux)，Windows自带的Linux子系统</p>\n<h2 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h2><ul>\n<li><input disabled=\"\" type=\"checkbox\"> 输入bash没反应，重启后正常，待解决</li>\n</ul>\n<h4 id=\"安装过程及问题\"><a href=\"#安装过程及问题\" class=\"headerlink\" title=\"安装过程及问题\"></a>安装过程及问题</h4><p>在Windows中开启该功能然后重启</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234736.png\"></p>\n<blockquote>\n<p>可能出现以下问题<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234752.png\"><br>造成该问题的原因是WSL版本由原来的WSL1升级到WSL2后，内核没有升级，前往<a href=\"https://docs.microsoft.com/zh-cn/windows/wsl/wsl2-kernel\">微软WSL官网</a>下载安装适用于 x64 计算机的最新 WSL2 Linux 内核更新包即可。</p>\n</blockquote>\n<h4 id=\"迁移到D盘\"><a href=\"#迁移到D盘\" class=\"headerlink\" title=\"迁移到D盘\"></a>迁移到D盘</h4><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\"># 查看已经安装的虚拟机<br>wsl -l -v<br># 关闭所有正在运行的虚拟机<br>wsl --shutdown<br># 虚拟机文件导出<br>wsl --export 虚拟机名称 保存路径<br>wsl --export Ubuntu D:\\\\wsl-Ubuntu.tar<br># 注销原虚拟机<br>wsl --unregister Ubuntu<br># 导入虚拟机文件<br>wsl --import 虚拟机名称 目标路径 虚拟机文件路径 --version 2<br>wsl --import Ubuntu D:\\\\WSL2\\\\Ubuntu D:\\\\wsl-Ubuntu.tar --version 2<br># 最后可以选择删除掉虚拟机文件，因为他已经没用了<br></code></pre></td></tr></table></figure>\n\n<p><strong>更改默认用户：</strong></p>\n<ul>\n<li>在powershell中输入<code>ubuntu.exe config --default-user 用户名</code>来将root用户改为普通用户</li>\n<li>输入<code>ubuntu config --default-user root</code>将普通用户改为默认用户，可以在忘记root密码时使用</li>\n</ul>\n<h4 id=\"配置python库环境\"><a href=\"#配置python库环境\" class=\"headerlink\" title=\"配置python库环境\"></a>配置python库环境</h4><p>在miniconda官网中找到下载命令，下载miniconda<br>然后新建环境，在环境中下载相关依赖包</p>\n<h4 id=\"Pycharm中使用WSL\"><a href=\"#Pycharm中使用WSL\" class=\"headerlink\" title=\"Pycharm中使用WSL\"></a>Pycharm中使用WSL</h4><p>在Pycharm中添加解释器<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234812.png\"><br>选择WSL，选择配置好的环境</p>\n<h4 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h4><p>WSL中&#x2F;mnt文件里是Windows文件的映射<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234824.png\"></p>\n<p>在想转到Windows某目录下时需要cd到其映射后的目录</p>\n"},{"title":"hexo+Obsidian+github笔记博客","update":null,"_content":"之前的Obsidian笔记都存在本地仓库，github page业已荒废，遂着手将ob笔记同步到blog上并实现自动更新\n\n[使用 Hexo+GitHub 搭建个人免费博客教程（小白向） - 知乎](https://zhuanlan.zhihu.com/p/60578464)\n[Obsidian+Git完美维护Hexo博客 - 知乎](https://zhuanlan.zhihu.com/p/554333805)\n### 安装hexo并建立博客\n\n之前的网页源代码已经丢失，于是重新搭建网页\n\n#### 本地建站\n首先新建一个本地文件夹（D:\\\\myweb），在文件夹内打开git bash，安装hexo\n```\nnpm install -g hexo-cli\n```\n然后便可以新建一个网页了\n```text\nhexo init      # 初始化\nnpm install    # 安装组件\nhexo g   # 生成页面\nhexo s   # 启动预览，在本地\n```\n#### 部署到Github Page\n**安装 hexo-deployer-git**\n```text\nnpm install hexo-deployer-git --save\n```\n然后**修改  _ config.yml**  文件末尾的 Deployment 部分\n```text\ndeploy:\n  type: git\n  repository: git@github.com:用户名/用户名.github.io.git\n  branch: main\n```\n完成后运行 `hexo d` 将网站上传部署到 GitHub Pages\n\n### Obsidian实现笔记与博客同步\n[javascript - hexo配合github action 自动构建（多种形式） - 前端与算法 - SegmentFault 思否](https://segmentfault.com/a/1190000040767893)\n[github action 部署 hexo踩坑记录 - 知乎](https://zhuanlan.zhihu.com/p/626270948)\n[GitHub自动部署HEXO个人博客 - 知乎](https://zhuanlan.zhihu.com/p/441558922)\n参考了很多文章，解决了无数bug\n\n#### 前置工作\n首先需要两个GitHub仓库，一个(Obsidian-Notes)用来和Obsidian Git远程连接，同步文章内容，然后在push时使用Github Action执行hexo deploy来将部署好的网页传输到另一个仓库。另一个(zhengyangWang1)用来接受第一个仓库部署好的内容，作为Github Page的仓库。\n\n首先先把本地存放博客框架的文件夹（D:\\\\myweb\\\\web）用git连接到第一个仓库,将内容同步。\n\n#### 给两个仓库配置密钥\n```\nssh-keygen -f github-deploy-key # 在git Bash中执行，生成秘钥\n```\n会在当前目录中生成两个文件：\n- 私钥文件 `github-deploy-key`\n- 公钥文件 `github-deploy-key.pub`\n\n复制私钥文件中**所有内容**，在第一个项目仓库， Settings -> Secrets and variables -> Actions页面上点击New repository secret 添加。\n在 Name 输入框填写 HEXO_DEPLOY_PRI。\n在 Value 输入框填写 github-deploy-key文件**所有内容**\n**注**：所有内容包括第一行和最后一行不太像私钥内容的东西，曾在此踩坑\n\n在第二个仓库Setting-> deploy key中配置公钥。在 Title 输入框填写 HEXO_DEPLOY_PUB，在 Key 输入框填写 github-deploy-key.pub 文件内容，勾选 Allow write access 选项。\n\n#### 配置第一个仓库的Github Actions\n在第一个仓库的本地文件夹找到.github文件，在文件夹下创建workflows/deploy.yml，文件夹名称必须是workflows，这个是github action的文件夹，下面的yml名称可以任意，为执行文件。\n\nyml中的内容如下（改了无数bug）：\n```\nname: deploy  # name任意\n\non:  # 当执行push操作时，触发该action\n  push:\n    branches:\n    - main\n\nenv:\n  GIT_USER: zhengyangWang1  # git的用户名\n  GIT_EMAIL: wangzhengyang@bupt.edu.cn  # git的邮箱\n  THEME_REPO: fluid-dev/hexo-theme-fluid  # 主题\n  THEME_BRANCH: main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest  # 环境，使用Ubuntu\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          ref: main\n\n      - name: 安装 Node  # 这个格式是正确的，使用build那个会报错\n        uses: actions/setup-node@v3\n        with:\n          node-version: \"20.x\"\n\n      - name: 配置 环境变量\n        env:\n          HEXO_DEPLOY_PRI: ${{secrets.HEXO_DEPLOY_PRI}}\n        run: |\n          mkdir -p ~/.ssh/\n          echo \"$HEXO_DEPLOY_PRI\" > ~/.ssh/id_rsa\n          chmod 600 ~/.ssh/id_rsa\n          ssh-keyscan github.com >> ~/.ssh/known_hosts\n          git config --global user.name $GIT_USER\n          git config --global user.email $GIT_EMAIL\n      - name: 安装 Hexo\n        run: |\n          npm install hexo-cli -g\n          npm install\n      \n      - name: 生成静态文件\n        run: |\n          rm -rf .deploy_git  # .deploy_git报错遂添加此行和下一行\n          npm install hexo-deployer-git --save\n          hexo clean\n          hexo generate\n      - name: 部署到Github page\n        run: |\n          hexo deploy\n```\n\n本来还有部署主题文件的代码，但因为deploy时报错所以暂时删去，以后搞清楚作用在处理\n\n配置好后push一下，看到deploy成功方可。\n\n### 配置\n[Hexo + Obsidian + Git 完美的博客部署与编辑方案 - 掘金](https://juejin.cn/post/7120189614660255781)\n[Obsidian+Git完美维护Hexo博客 - 知乎](https://zhuanlan.zhihu.com/p/554333805)\n#### 插件Folder foucs mode\n在ob中打开hexo博客的文件夹，发现文件太多，都是些平时不会浏览的配置文件，所以下载ob插件Folder foucs mode，可以将左侧文件列表聚焦到想要的地方（\\_posts文件夹下）\n\n#### 适配hexo格式\n使用hexo new创建笔记会在文件头加入一段模板，这样才会在部署时部署到网页。在ob中，打开核心插件中的模板功能，并将模板文件夹位置设为Template。新建一个Template文件夹，存放一个hexo文件头模板文件。当创建文件时，点击左侧功能栏中的插入模板即可插入文件头。\n\n#### 自动生成分类\nHexo写日志，通常我们都需要维护一个front-matter信息，包括`title`、`date`。博客多了，为了方便日志分类，一般还需要设置`categories`。\n[GitHub - xu-song/hexo-auto-category: Generate categories automatically for each post in Hexo](https://github.com/xu-song/hexo-auto-category)自动生成categories\n安装：\n```\nnpm install hexo-auto-category --save\n```\n在站点根目录下的`_config.yml`添加:\n```\nauto_category:  \n enable: true  \n depth:\n```\n\n**利用Git钩子函数触发更新:**\n这个插件只有执行`hexo generate`时才会去读取文件夹并更新所有文章的Front-matter分类信息，所以我们可以利用[Git的钩子函数](https://link.zhihu.com/?target=https%3A//git-scm.com/book/zh/v2/%25E8%2587%25AA%25E5%25AE%259A%25E4%25B9%2589-Git-Git-%25E9%2592%25A9%25E5%25AD%2590%23_git_hooks)，在commit的时候先执行下`hexo generate`，这样就能实现自动更新了。\n在`.git/hooks`目录下新建一个`pre-commit`文件，也可以执行`touch pre-commit`命令新建该文件,将如下命令写到文件里\n```bash\n#!/bin/sh\nhexo generate && git add .\n```\n\n#### PicGo+Github图床\n[使用Github+picGo搭建图床，保姆级教程来了 - 知乎](https://zhuanlan.zhihu.com/p/489236769)\n\n#### 博客时间问题\n- [ ] 待解决 ","source":"_posts/Notes/花里胡哨/hexo+Obsidian+github博客.md","raw":"---\ntitle: hexo+Obsidian+github笔记博客\ncategories:\n  - Notes\n  - 花里胡哨\nupdate: \ntags:\n  - hexo\n  - Github\n  - Obsidian\n---\n之前的Obsidian笔记都存在本地仓库，github page业已荒废，遂着手将ob笔记同步到blog上并实现自动更新\n\n[使用 Hexo+GitHub 搭建个人免费博客教程（小白向） - 知乎](https://zhuanlan.zhihu.com/p/60578464)\n[Obsidian+Git完美维护Hexo博客 - 知乎](https://zhuanlan.zhihu.com/p/554333805)\n### 安装hexo并建立博客\n\n之前的网页源代码已经丢失，于是重新搭建网页\n\n#### 本地建站\n首先新建一个本地文件夹（D:\\\\myweb），在文件夹内打开git bash，安装hexo\n```\nnpm install -g hexo-cli\n```\n然后便可以新建一个网页了\n```text\nhexo init      # 初始化\nnpm install    # 安装组件\nhexo g   # 生成页面\nhexo s   # 启动预览，在本地\n```\n#### 部署到Github Page\n**安装 hexo-deployer-git**\n```text\nnpm install hexo-deployer-git --save\n```\n然后**修改  _ config.yml**  文件末尾的 Deployment 部分\n```text\ndeploy:\n  type: git\n  repository: git@github.com:用户名/用户名.github.io.git\n  branch: main\n```\n完成后运行 `hexo d` 将网站上传部署到 GitHub Pages\n\n### Obsidian实现笔记与博客同步\n[javascript - hexo配合github action 自动构建（多种形式） - 前端与算法 - SegmentFault 思否](https://segmentfault.com/a/1190000040767893)\n[github action 部署 hexo踩坑记录 - 知乎](https://zhuanlan.zhihu.com/p/626270948)\n[GitHub自动部署HEXO个人博客 - 知乎](https://zhuanlan.zhihu.com/p/441558922)\n参考了很多文章，解决了无数bug\n\n#### 前置工作\n首先需要两个GitHub仓库，一个(Obsidian-Notes)用来和Obsidian Git远程连接，同步文章内容，然后在push时使用Github Action执行hexo deploy来将部署好的网页传输到另一个仓库。另一个(zhengyangWang1)用来接受第一个仓库部署好的内容，作为Github Page的仓库。\n\n首先先把本地存放博客框架的文件夹（D:\\\\myweb\\\\web）用git连接到第一个仓库,将内容同步。\n\n#### 给两个仓库配置密钥\n```\nssh-keygen -f github-deploy-key # 在git Bash中执行，生成秘钥\n```\n会在当前目录中生成两个文件：\n- 私钥文件 `github-deploy-key`\n- 公钥文件 `github-deploy-key.pub`\n\n复制私钥文件中**所有内容**，在第一个项目仓库， Settings -> Secrets and variables -> Actions页面上点击New repository secret 添加。\n在 Name 输入框填写 HEXO_DEPLOY_PRI。\n在 Value 输入框填写 github-deploy-key文件**所有内容**\n**注**：所有内容包括第一行和最后一行不太像私钥内容的东西，曾在此踩坑\n\n在第二个仓库Setting-> deploy key中配置公钥。在 Title 输入框填写 HEXO_DEPLOY_PUB，在 Key 输入框填写 github-deploy-key.pub 文件内容，勾选 Allow write access 选项。\n\n#### 配置第一个仓库的Github Actions\n在第一个仓库的本地文件夹找到.github文件，在文件夹下创建workflows/deploy.yml，文件夹名称必须是workflows，这个是github action的文件夹，下面的yml名称可以任意，为执行文件。\n\nyml中的内容如下（改了无数bug）：\n```\nname: deploy  # name任意\n\non:  # 当执行push操作时，触发该action\n  push:\n    branches:\n    - main\n\nenv:\n  GIT_USER: zhengyangWang1  # git的用户名\n  GIT_EMAIL: wangzhengyang@bupt.edu.cn  # git的邮箱\n  THEME_REPO: fluid-dev/hexo-theme-fluid  # 主题\n  THEME_BRANCH: main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest  # 环境，使用Ubuntu\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          ref: main\n\n      - name: 安装 Node  # 这个格式是正确的，使用build那个会报错\n        uses: actions/setup-node@v3\n        with:\n          node-version: \"20.x\"\n\n      - name: 配置 环境变量\n        env:\n          HEXO_DEPLOY_PRI: ${{secrets.HEXO_DEPLOY_PRI}}\n        run: |\n          mkdir -p ~/.ssh/\n          echo \"$HEXO_DEPLOY_PRI\" > ~/.ssh/id_rsa\n          chmod 600 ~/.ssh/id_rsa\n          ssh-keyscan github.com >> ~/.ssh/known_hosts\n          git config --global user.name $GIT_USER\n          git config --global user.email $GIT_EMAIL\n      - name: 安装 Hexo\n        run: |\n          npm install hexo-cli -g\n          npm install\n      \n      - name: 生成静态文件\n        run: |\n          rm -rf .deploy_git  # .deploy_git报错遂添加此行和下一行\n          npm install hexo-deployer-git --save\n          hexo clean\n          hexo generate\n      - name: 部署到Github page\n        run: |\n          hexo deploy\n```\n\n本来还有部署主题文件的代码，但因为deploy时报错所以暂时删去，以后搞清楚作用在处理\n\n配置好后push一下，看到deploy成功方可。\n\n### 配置\n[Hexo + Obsidian + Git 完美的博客部署与编辑方案 - 掘金](https://juejin.cn/post/7120189614660255781)\n[Obsidian+Git完美维护Hexo博客 - 知乎](https://zhuanlan.zhihu.com/p/554333805)\n#### 插件Folder foucs mode\n在ob中打开hexo博客的文件夹，发现文件太多，都是些平时不会浏览的配置文件，所以下载ob插件Folder foucs mode，可以将左侧文件列表聚焦到想要的地方（\\_posts文件夹下）\n\n#### 适配hexo格式\n使用hexo new创建笔记会在文件头加入一段模板，这样才会在部署时部署到网页。在ob中，打开核心插件中的模板功能，并将模板文件夹位置设为Template。新建一个Template文件夹，存放一个hexo文件头模板文件。当创建文件时，点击左侧功能栏中的插入模板即可插入文件头。\n\n#### 自动生成分类\nHexo写日志，通常我们都需要维护一个front-matter信息，包括`title`、`date`。博客多了，为了方便日志分类，一般还需要设置`categories`。\n[GitHub - xu-song/hexo-auto-category: Generate categories automatically for each post in Hexo](https://github.com/xu-song/hexo-auto-category)自动生成categories\n安装：\n```\nnpm install hexo-auto-category --save\n```\n在站点根目录下的`_config.yml`添加:\n```\nauto_category:  \n enable: true  \n depth:\n```\n\n**利用Git钩子函数触发更新:**\n这个插件只有执行`hexo generate`时才会去读取文件夹并更新所有文章的Front-matter分类信息，所以我们可以利用[Git的钩子函数](https://link.zhihu.com/?target=https%3A//git-scm.com/book/zh/v2/%25E8%2587%25AA%25E5%25AE%259A%25E4%25B9%2589-Git-Git-%25E9%2592%25A9%25E5%25AD%2590%23_git_hooks)，在commit的时候先执行下`hexo generate`，这样就能实现自动更新了。\n在`.git/hooks`目录下新建一个`pre-commit`文件，也可以执行`touch pre-commit`命令新建该文件,将如下命令写到文件里\n```bash\n#!/bin/sh\nhexo generate && git add .\n```\n\n#### PicGo+Github图床\n[使用Github+picGo搭建图床，保姆级教程来了 - 知乎](https://zhuanlan.zhihu.com/p/489236769)\n\n#### 博客时间问题\n- [ ] 待解决 ","slug":"Notes/花里胡哨/hexo+Obsidian+github博客","published":1,"date":"2023-09-21T16:36:34.334Z","updated":"2023-09-27T12:30:21.126Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr5c001htou5cnq3208o","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>之前的Obsidian笔记都存在本地仓库，github page业已荒废，遂着手将ob笔记同步到blog上并实现自动更新</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/60578464\">使用 Hexo+GitHub 搭建个人免费博客教程（小白向） - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/554333805\">Obsidian+Git完美维护Hexo博客 - 知乎</a></p>\n<h3 id=\"安装hexo并建立博客\"><a href=\"#安装hexo并建立博客\" class=\"headerlink\" title=\"安装hexo并建立博客\"></a>安装hexo并建立博客</h3><p>之前的网页源代码已经丢失，于是重新搭建网页</p>\n<h4 id=\"本地建站\"><a href=\"#本地建站\" class=\"headerlink\" title=\"本地建站\"></a>本地建站</h4><p>首先新建一个本地文件夹（D:\\myweb），在文件夹内打开git bash，安装hexo</p>\n<figure class=\"highlight avrasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs avrasm\">npm install -g hexo-<span class=\"hljs-keyword\">cli</span><br></code></pre></td></tr></table></figure>\n<p>然后便可以新建一个网页了</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">hexo init      # 初始化<br>npm install    # 安装组件<br>hexo g   # 生成页面<br>hexo s   # 启动预览，在本地<br></code></pre></td></tr></table></figure>\n<h4 id=\"部署到Github-Page\"><a href=\"#部署到Github-Page\" class=\"headerlink\" title=\"部署到Github Page\"></a>部署到Github Page</h4><p><strong>安装 hexo-deployer-git</strong></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">npm install hexo-deployer-git --save<br></code></pre></td></tr></table></figure>\n<p>然后<strong>修改  _ config.yml</strong>  文件末尾的 Deployment 部分</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">deploy:<br>  type: git<br>  repository: git@github.com:用户名/用户名.github.io.git<br>  branch: main<br></code></pre></td></tr></table></figure>\n<p>完成后运行 <code>hexo d</code> 将网站上传部署到 GitHub Pages</p>\n<h3 id=\"Obsidian实现笔记与博客同步\"><a href=\"#Obsidian实现笔记与博客同步\" class=\"headerlink\" title=\"Obsidian实现笔记与博客同步\"></a>Obsidian实现笔记与博客同步</h3><p><a href=\"https://segmentfault.com/a/1190000040767893\">javascript - hexo配合github action 自动构建（多种形式） - 前端与算法 - SegmentFault 思否</a><br><a href=\"https://zhuanlan.zhihu.com/p/626270948\">github action 部署 hexo踩坑记录 - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/441558922\">GitHub自动部署HEXO个人博客 - 知乎</a><br>参考了很多文章，解决了无数bug</p>\n<h4 id=\"前置工作\"><a href=\"#前置工作\" class=\"headerlink\" title=\"前置工作\"></a>前置工作</h4><p>首先需要两个GitHub仓库，一个(Obsidian-Notes)用来和Obsidian Git远程连接，同步文章内容，然后在push时使用Github Action执行hexo deploy来将部署好的网页传输到另一个仓库。另一个(zhengyangWang1)用来接受第一个仓库部署好的内容，作为Github Page的仓库。</p>\n<p>首先先把本地存放博客框架的文件夹（D:\\myweb\\web）用git连接到第一个仓库,将内容同步。</p>\n<h4 id=\"给两个仓库配置密钥\"><a href=\"#给两个仓库配置密钥\" class=\"headerlink\" title=\"给两个仓库配置密钥\"></a>给两个仓库配置密钥</h4><figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gauss\">ssh-keygen -f github-deploy-<span class=\"hljs-built_in\">key</span> <span class=\"hljs-meta\"># 在git Bash中执行，生成秘钥</span><br></code></pre></td></tr></table></figure>\n<p>会在当前目录中生成两个文件：</p>\n<ul>\n<li>私钥文件 <code>github-deploy-key</code></li>\n<li>公钥文件 <code>github-deploy-key.pub</code></li>\n</ul>\n<p>复制私钥文件中<strong>所有内容</strong>，在第一个项目仓库， Settings -&gt; Secrets and variables -&gt; Actions页面上点击New repository secret 添加。<br>在 Name 输入框填写 HEXO_DEPLOY_PRI。<br>在 Value 输入框填写 github-deploy-key文件<strong>所有内容</strong><br><strong>注</strong>：所有内容包括第一行和最后一行不太像私钥内容的东西，曾在此踩坑</p>\n<p>在第二个仓库Setting-&gt; deploy key中配置公钥。在 Title 输入框填写 HEXO_DEPLOY_PUB，在 Key 输入框填写 github-deploy-key.pub 文件内容，勾选 Allow write access 选项。</p>\n<h4 id=\"配置第一个仓库的Github-Actions\"><a href=\"#配置第一个仓库的Github-Actions\" class=\"headerlink\" title=\"配置第一个仓库的Github Actions\"></a>配置第一个仓库的Github Actions</h4><p>在第一个仓库的本地文件夹找到.github文件，在文件夹下创建workflows&#x2F;deploy.yml，文件夹名称必须是workflows，这个是github action的文件夹，下面的yml名称可以任意，为执行文件。</p>\n<p>yml中的内容如下（改了无数bug）：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">deploy</span>  <span class=\"hljs-comment\"># name任意</span><br><br><span class=\"hljs-attr\">on:</span>  <span class=\"hljs-comment\"># 当执行push操作时，触发该action</span><br>  <span class=\"hljs-attr\">push:</span><br>    <span class=\"hljs-attr\">branches:</span><br>    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">main</span><br><br><span class=\"hljs-attr\">env:</span><br>  <span class=\"hljs-attr\">GIT_USER:</span> <span class=\"hljs-string\">zhengyangWang1</span>  <span class=\"hljs-comment\"># git的用户名</span><br>  <span class=\"hljs-attr\">GIT_EMAIL:</span> <span class=\"hljs-string\">wangzhengyang@bupt.edu.cn</span>  <span class=\"hljs-comment\"># git的邮箱</span><br>  <span class=\"hljs-attr\">THEME_REPO:</span> <span class=\"hljs-string\">fluid-dev/hexo-theme-fluid</span>  <span class=\"hljs-comment\"># 主题</span><br>  <span class=\"hljs-attr\">THEME_BRANCH:</span> <span class=\"hljs-string\">main</span><br><br><span class=\"hljs-attr\">jobs:</span><br>  <span class=\"hljs-attr\">deploy:</span><br>    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>  <span class=\"hljs-comment\"># 环境，使用Ubuntu</span><br><br>    <span class=\"hljs-attr\">steps:</span><br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span><br>        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span><br>        <span class=\"hljs-attr\">with:</span><br>          <span class=\"hljs-attr\">ref:</span> <span class=\"hljs-string\">main</span><br><br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">安装</span> <span class=\"hljs-string\">Node</span>  <span class=\"hljs-comment\"># 这个格式是正确的，使用build那个会报错</span><br>        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/setup-node@v3</span><br>        <span class=\"hljs-attr\">with:</span><br>          <span class=\"hljs-attr\">node-version:</span> <span class=\"hljs-string\">&quot;20.x&quot;</span><br><br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">配置</span> <span class=\"hljs-string\">环境变量</span><br>        <span class=\"hljs-attr\">env:</span><br>          <span class=\"hljs-attr\">HEXO_DEPLOY_PRI:</span> <span class=\"hljs-string\">$&#123;&#123;secrets.HEXO_DEPLOY_PRI&#125;&#125;</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br><span class=\"hljs-string\">          mkdir -p ~/.ssh/</span><br><span class=\"hljs-string\">          echo &quot;$HEXO_DEPLOY_PRI&quot; &gt; ~/.ssh/id_rsa</span><br><span class=\"hljs-string\">          chmod 600 ~/.ssh/id_rsa</span><br><span class=\"hljs-string\">          ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts</span><br><span class=\"hljs-string\">          git config --global user.name $GIT_USER</span><br><span class=\"hljs-string\">          git config --global user.email $GIT_EMAIL</span><br><span class=\"hljs-string\"></span>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">安装</span> <span class=\"hljs-string\">Hexo</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br><span class=\"hljs-string\">          npm install hexo-cli -g</span><br><span class=\"hljs-string\">          npm install</span><br><span class=\"hljs-string\"></span>      <br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">生成静态文件</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br><span class=\"hljs-string\">          rm -rf .deploy_git  # .deploy_git报错遂添加此行和下一行</span><br><span class=\"hljs-string\">          npm install hexo-deployer-git --save</span><br><span class=\"hljs-string\">          hexo clean</span><br><span class=\"hljs-string\">          hexo generate</span><br><span class=\"hljs-string\"></span>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">部署到Github</span> <span class=\"hljs-string\">page</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br>          <span class=\"hljs-string\">hexo</span> <span class=\"hljs-string\">deploy</span><br></code></pre></td></tr></table></figure>\n\n<p>本来还有部署主题文件的代码，但因为deploy时报错所以暂时删去，以后搞清楚作用在处理</p>\n<p>配置好后push一下，看到deploy成功方可。</p>\n<h3 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p><a href=\"https://juejin.cn/post/7120189614660255781\">Hexo + Obsidian + Git 完美的博客部署与编辑方案 - 掘金</a><br><a href=\"https://zhuanlan.zhihu.com/p/554333805\">Obsidian+Git完美维护Hexo博客 - 知乎</a></p>\n<h4 id=\"插件Folder-foucs-mode\"><a href=\"#插件Folder-foucs-mode\" class=\"headerlink\" title=\"插件Folder foucs mode\"></a>插件Folder foucs mode</h4><p>在ob中打开hexo博客的文件夹，发现文件太多，都是些平时不会浏览的配置文件，所以下载ob插件Folder foucs mode，可以将左侧文件列表聚焦到想要的地方（_posts文件夹下）</p>\n<h4 id=\"适配hexo格式\"><a href=\"#适配hexo格式\" class=\"headerlink\" title=\"适配hexo格式\"></a>适配hexo格式</h4><p>使用hexo new创建笔记会在文件头加入一段模板，这样才会在部署时部署到网页。在ob中，打开核心插件中的模板功能，并将模板文件夹位置设为Template。新建一个Template文件夹，存放一个hexo文件头模板文件。当创建文件时，点击左侧功能栏中的插入模板即可插入文件头。</p>\n<h4 id=\"自动生成分类\"><a href=\"#自动生成分类\" class=\"headerlink\" title=\"自动生成分类\"></a>自动生成分类</h4><p>Hexo写日志，通常我们都需要维护一个front-matter信息，包括<code>title</code>、<code>date</code>。博客多了，为了方便日志分类，一般还需要设置<code>categories</code>。<br><a href=\"https://github.com/xu-song/hexo-auto-category\">GitHub - xu-song&#x2F;hexo-auto-category: Generate categories automatically for each post in Hexo</a>自动生成categories<br>安装：</p>\n<figure class=\"highlight scss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scss\">npm install hexo-auto-category <span class=\"hljs-attr\">--save</span><br></code></pre></td></tr></table></figure>\n<p>在站点根目录下的<code>_config.yml</code>添加:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs yaml\"><span class=\"hljs-attr\">auto_category:</span>  <br> <span class=\"hljs-attr\">enable:</span> <span class=\"hljs-literal\">true</span>  <br> <span class=\"hljs-attr\">depth:</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>利用Git钩子函数触发更新:</strong><br>这个插件只有执行<code>hexo generate</code>时才会去读取文件夹并更新所有文章的Front-matter分类信息，所以我们可以利用<a href=\"https://link.zhihu.com/?target=https://git-scm.com/book/zh/v2/%25E8%2587%25AA%25E5%25AE%259A%25E4%25B9%2589-Git-Git-%25E9%2592%25A9%25E5%25AD%2590%23_git_hooks\">Git的钩子函数</a>，在commit的时候先执行下<code>hexo generate</code>，这样就能实现自动更新了。<br>在<code>.git/hooks</code>目录下新建一个<code>pre-commit</code>文件，也可以执行<code>touch pre-commit</code>命令新建该文件,将如下命令写到文件里</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-meta\">#!/bin/sh</span><br>hexo generate &amp;&amp; git add .<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"PicGo-Github图床\"><a href=\"#PicGo-Github图床\" class=\"headerlink\" title=\"PicGo+Github图床\"></a>PicGo+Github图床</h4><p><a href=\"https://zhuanlan.zhihu.com/p/489236769\">使用Github+picGo搭建图床，保姆级教程来了 - 知乎</a></p>\n<h4 id=\"博客时间问题\"><a href=\"#博客时间问题\" class=\"headerlink\" title=\"博客时间问题\"></a>博客时间问题</h4><ul>\n<li><input disabled=\"\" type=\"checkbox\"> 待解决</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>之前的Obsidian笔记都存在本地仓库，github page业已荒废，遂着手将ob笔记同步到blog上并实现自动更新</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/60578464\">使用 Hexo+GitHub 搭建个人免费博客教程（小白向） - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/554333805\">Obsidian+Git完美维护Hexo博客 - 知乎</a></p>\n<h3 id=\"安装hexo并建立博客\"><a href=\"#安装hexo并建立博客\" class=\"headerlink\" title=\"安装hexo并建立博客\"></a>安装hexo并建立博客</h3><p>之前的网页源代码已经丢失，于是重新搭建网页</p>\n<h4 id=\"本地建站\"><a href=\"#本地建站\" class=\"headerlink\" title=\"本地建站\"></a>本地建站</h4><p>首先新建一个本地文件夹（D:\\myweb），在文件夹内打开git bash，安装hexo</p>\n<figure class=\"highlight avrasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs avrasm\">npm install -g hexo-<span class=\"hljs-keyword\">cli</span><br></code></pre></td></tr></table></figure>\n<p>然后便可以新建一个网页了</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">hexo init      # 初始化<br>npm install    # 安装组件<br>hexo g   # 生成页面<br>hexo s   # 启动预览，在本地<br></code></pre></td></tr></table></figure>\n<h4 id=\"部署到Github-Page\"><a href=\"#部署到Github-Page\" class=\"headerlink\" title=\"部署到Github Page\"></a>部署到Github Page</h4><p><strong>安装 hexo-deployer-git</strong></p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">npm install hexo-deployer-git --save<br></code></pre></td></tr></table></figure>\n<p>然后<strong>修改  _ config.yml</strong>  文件末尾的 Deployment 部分</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">deploy:<br>  type: git<br>  repository: git@github.com:用户名/用户名.github.io.git<br>  branch: main<br></code></pre></td></tr></table></figure>\n<p>完成后运行 <code>hexo d</code> 将网站上传部署到 GitHub Pages</p>\n<h3 id=\"Obsidian实现笔记与博客同步\"><a href=\"#Obsidian实现笔记与博客同步\" class=\"headerlink\" title=\"Obsidian实现笔记与博客同步\"></a>Obsidian实现笔记与博客同步</h3><p><a href=\"https://segmentfault.com/a/1190000040767893\">javascript - hexo配合github action 自动构建（多种形式） - 前端与算法 - SegmentFault 思否</a><br><a href=\"https://zhuanlan.zhihu.com/p/626270948\">github action 部署 hexo踩坑记录 - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/441558922\">GitHub自动部署HEXO个人博客 - 知乎</a><br>参考了很多文章，解决了无数bug</p>\n<h4 id=\"前置工作\"><a href=\"#前置工作\" class=\"headerlink\" title=\"前置工作\"></a>前置工作</h4><p>首先需要两个GitHub仓库，一个(Obsidian-Notes)用来和Obsidian Git远程连接，同步文章内容，然后在push时使用Github Action执行hexo deploy来将部署好的网页传输到另一个仓库。另一个(zhengyangWang1)用来接受第一个仓库部署好的内容，作为Github Page的仓库。</p>\n<p>首先先把本地存放博客框架的文件夹（D:\\myweb\\web）用git连接到第一个仓库,将内容同步。</p>\n<h4 id=\"给两个仓库配置密钥\"><a href=\"#给两个仓库配置密钥\" class=\"headerlink\" title=\"给两个仓库配置密钥\"></a>给两个仓库配置密钥</h4><figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gauss\">ssh-keygen -f github-deploy-<span class=\"hljs-built_in\">key</span> <span class=\"hljs-meta\"># 在git Bash中执行，生成秘钥</span><br></code></pre></td></tr></table></figure>\n<p>会在当前目录中生成两个文件：</p>\n<ul>\n<li>私钥文件 <code>github-deploy-key</code></li>\n<li>公钥文件 <code>github-deploy-key.pub</code></li>\n</ul>\n<p>复制私钥文件中<strong>所有内容</strong>，在第一个项目仓库， Settings -&gt; Secrets and variables -&gt; Actions页面上点击New repository secret 添加。<br>在 Name 输入框填写 HEXO_DEPLOY_PRI。<br>在 Value 输入框填写 github-deploy-key文件<strong>所有内容</strong><br><strong>注</strong>：所有内容包括第一行和最后一行不太像私钥内容的东西，曾在此踩坑</p>\n<p>在第二个仓库Setting-&gt; deploy key中配置公钥。在 Title 输入框填写 HEXO_DEPLOY_PUB，在 Key 输入框填写 github-deploy-key.pub 文件内容，勾选 Allow write access 选项。</p>\n<h4 id=\"配置第一个仓库的Github-Actions\"><a href=\"#配置第一个仓库的Github-Actions\" class=\"headerlink\" title=\"配置第一个仓库的Github Actions\"></a>配置第一个仓库的Github Actions</h4><p>在第一个仓库的本地文件夹找到.github文件，在文件夹下创建workflows&#x2F;deploy.yml，文件夹名称必须是workflows，这个是github action的文件夹，下面的yml名称可以任意，为执行文件。</p>\n<p>yml中的内容如下（改了无数bug）：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">deploy</span>  <span class=\"hljs-comment\"># name任意</span><br><br><span class=\"hljs-attr\">on:</span>  <span class=\"hljs-comment\"># 当执行push操作时，触发该action</span><br>  <span class=\"hljs-attr\">push:</span><br>    <span class=\"hljs-attr\">branches:</span><br>    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">main</span><br><br><span class=\"hljs-attr\">env:</span><br>  <span class=\"hljs-attr\">GIT_USER:</span> <span class=\"hljs-string\">zhengyangWang1</span>  <span class=\"hljs-comment\"># git的用户名</span><br>  <span class=\"hljs-attr\">GIT_EMAIL:</span> <span class=\"hljs-string\">wangzhengyang@bupt.edu.cn</span>  <span class=\"hljs-comment\"># git的邮箱</span><br>  <span class=\"hljs-attr\">THEME_REPO:</span> <span class=\"hljs-string\">fluid-dev/hexo-theme-fluid</span>  <span class=\"hljs-comment\"># 主题</span><br>  <span class=\"hljs-attr\">THEME_BRANCH:</span> <span class=\"hljs-string\">main</span><br><br><span class=\"hljs-attr\">jobs:</span><br>  <span class=\"hljs-attr\">deploy:</span><br>    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>  <span class=\"hljs-comment\"># 环境，使用Ubuntu</span><br><br>    <span class=\"hljs-attr\">steps:</span><br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span><br>        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span><br>        <span class=\"hljs-attr\">with:</span><br>          <span class=\"hljs-attr\">ref:</span> <span class=\"hljs-string\">main</span><br><br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">安装</span> <span class=\"hljs-string\">Node</span>  <span class=\"hljs-comment\"># 这个格式是正确的，使用build那个会报错</span><br>        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/setup-node@v3</span><br>        <span class=\"hljs-attr\">with:</span><br>          <span class=\"hljs-attr\">node-version:</span> <span class=\"hljs-string\">&quot;20.x&quot;</span><br><br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">配置</span> <span class=\"hljs-string\">环境变量</span><br>        <span class=\"hljs-attr\">env:</span><br>          <span class=\"hljs-attr\">HEXO_DEPLOY_PRI:</span> <span class=\"hljs-string\">$&#123;&#123;secrets.HEXO_DEPLOY_PRI&#125;&#125;</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br><span class=\"hljs-string\">          mkdir -p ~/.ssh/</span><br><span class=\"hljs-string\">          echo &quot;$HEXO_DEPLOY_PRI&quot; &gt; ~/.ssh/id_rsa</span><br><span class=\"hljs-string\">          chmod 600 ~/.ssh/id_rsa</span><br><span class=\"hljs-string\">          ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts</span><br><span class=\"hljs-string\">          git config --global user.name $GIT_USER</span><br><span class=\"hljs-string\">          git config --global user.email $GIT_EMAIL</span><br><span class=\"hljs-string\"></span>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">安装</span> <span class=\"hljs-string\">Hexo</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br><span class=\"hljs-string\">          npm install hexo-cli -g</span><br><span class=\"hljs-string\">          npm install</span><br><span class=\"hljs-string\"></span>      <br>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">生成静态文件</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br><span class=\"hljs-string\">          rm -rf .deploy_git  # .deploy_git报错遂添加此行和下一行</span><br><span class=\"hljs-string\">          npm install hexo-deployer-git --save</span><br><span class=\"hljs-string\">          hexo clean</span><br><span class=\"hljs-string\">          hexo generate</span><br><span class=\"hljs-string\"></span>      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">部署到Github</span> <span class=\"hljs-string\">page</span><br>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|</span><br>          <span class=\"hljs-string\">hexo</span> <span class=\"hljs-string\">deploy</span><br></code></pre></td></tr></table></figure>\n\n<p>本来还有部署主题文件的代码，但因为deploy时报错所以暂时删去，以后搞清楚作用在处理</p>\n<p>配置好后push一下，看到deploy成功方可。</p>\n<h3 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p><a href=\"https://juejin.cn/post/7120189614660255781\">Hexo + Obsidian + Git 完美的博客部署与编辑方案 - 掘金</a><br><a href=\"https://zhuanlan.zhihu.com/p/554333805\">Obsidian+Git完美维护Hexo博客 - 知乎</a></p>\n<h4 id=\"插件Folder-foucs-mode\"><a href=\"#插件Folder-foucs-mode\" class=\"headerlink\" title=\"插件Folder foucs mode\"></a>插件Folder foucs mode</h4><p>在ob中打开hexo博客的文件夹，发现文件太多，都是些平时不会浏览的配置文件，所以下载ob插件Folder foucs mode，可以将左侧文件列表聚焦到想要的地方（_posts文件夹下）</p>\n<h4 id=\"适配hexo格式\"><a href=\"#适配hexo格式\" class=\"headerlink\" title=\"适配hexo格式\"></a>适配hexo格式</h4><p>使用hexo new创建笔记会在文件头加入一段模板，这样才会在部署时部署到网页。在ob中，打开核心插件中的模板功能，并将模板文件夹位置设为Template。新建一个Template文件夹，存放一个hexo文件头模板文件。当创建文件时，点击左侧功能栏中的插入模板即可插入文件头。</p>\n<h4 id=\"自动生成分类\"><a href=\"#自动生成分类\" class=\"headerlink\" title=\"自动生成分类\"></a>自动生成分类</h4><p>Hexo写日志，通常我们都需要维护一个front-matter信息，包括<code>title</code>、<code>date</code>。博客多了，为了方便日志分类，一般还需要设置<code>categories</code>。<br><a href=\"https://github.com/xu-song/hexo-auto-category\">GitHub - xu-song&#x2F;hexo-auto-category: Generate categories automatically for each post in Hexo</a>自动生成categories<br>安装：</p>\n<figure class=\"highlight scss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scss\">npm install hexo-auto-category <span class=\"hljs-attr\">--save</span><br></code></pre></td></tr></table></figure>\n<p>在站点根目录下的<code>_config.yml</code>添加:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs yaml\"><span class=\"hljs-attr\">auto_category:</span>  <br> <span class=\"hljs-attr\">enable:</span> <span class=\"hljs-literal\">true</span>  <br> <span class=\"hljs-attr\">depth:</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>利用Git钩子函数触发更新:</strong><br>这个插件只有执行<code>hexo generate</code>时才会去读取文件夹并更新所有文章的Front-matter分类信息，所以我们可以利用<a href=\"https://link.zhihu.com/?target=https://git-scm.com/book/zh/v2/%25E8%2587%25AA%25E5%25AE%259A%25E4%25B9%2589-Git-Git-%25E9%2592%25A9%25E5%25AD%2590%23_git_hooks\">Git的钩子函数</a>，在commit的时候先执行下<code>hexo generate</code>，这样就能实现自动更新了。<br>在<code>.git/hooks</code>目录下新建一个<code>pre-commit</code>文件，也可以执行<code>touch pre-commit</code>命令新建该文件,将如下命令写到文件里</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs bash\"><span class=\"hljs-meta\">#!/bin/sh</span><br>hexo generate &amp;&amp; git add .<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"PicGo-Github图床\"><a href=\"#PicGo-Github图床\" class=\"headerlink\" title=\"PicGo+Github图床\"></a>PicGo+Github图床</h4><p><a href=\"https://zhuanlan.zhihu.com/p/489236769\">使用Github+picGo搭建图床，保姆级教程来了 - 知乎</a></p>\n<h4 id=\"博客时间问题\"><a href=\"#博客时间问题\" class=\"headerlink\" title=\"博客时间问题\"></a>博客时间问题</h4><ul>\n<li><input disabled=\"\" type=\"checkbox\"> 待解决</li>\n</ul>\n"},{"title":"安装MySQL","date":"2023-09-27T12:18:27.514Z","_content":"### 安装配置\n[2023 年 MySQL 8.0 安装配置 最简易（保姆级）\\_mysql8.0安装配置教程\\_mobeicanyue的博客-CSDN博客](https://blog.csdn.net/m0_52559040/article/details/121843945)\n安装时需要输入一个密码作为mysql的root权限密码，需要牢记\n\n### 开始使用\n在Power Shell中输入：\n```\nmysql -u root -p\n```\n然后输入密码进入mysql\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230927202531.png)\n\n","source":"_posts/Notes/编程/MySQL/安装MySQL.md","raw":"---\ntitle: 安装MySQL\ncategories:\n  - Notes\n  - 编程\n  - MySQL\ntags:\n  - 数据库\n  - MySQL\ndate:\n---\n### 安装配置\n[2023 年 MySQL 8.0 安装配置 最简易（保姆级）\\_mysql8.0安装配置教程\\_mobeicanyue的博客-CSDN博客](https://blog.csdn.net/m0_52559040/article/details/121843945)\n安装时需要输入一个密码作为mysql的root权限密码，需要牢记\n\n### 开始使用\n在Power Shell中输入：\n```\nmysql -u root -p\n```\n然后输入密码进入mysql\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230927202531.png)\n\n","slug":"Notes/编程/MySQL/安装MySQL","published":1,"updated":"2023-09-27T12:35:12.198Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cln1vnr5d001ktou54gp9gg5i","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"安装配置\"><a href=\"#安装配置\" class=\"headerlink\" title=\"安装配置\"></a>安装配置</h3><p><a href=\"https://blog.csdn.net/m0_52559040/article/details/121843945\">2023 年 MySQL 8.0 安装配置 最简易（保姆级）_mysql8.0安装配置教程_mobeicanyue的博客-CSDN博客</a><br>安装时需要输入一个密码作为mysql的root权限密码，需要牢记</p>\n<h3 id=\"开始使用\"><a href=\"#开始使用\" class=\"headerlink\" title=\"开始使用\"></a>开始使用</h3><p>在Power Shell中输入：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs css\">mysql -u root -<span class=\"hljs-selector-tag\">p</span><br></code></pre></td></tr></table></figure>\n<p>然后输入密码进入mysql<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230927202531.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"安装配置\"><a href=\"#安装配置\" class=\"headerlink\" title=\"安装配置\"></a>安装配置</h3><p><a href=\"https://blog.csdn.net/m0_52559040/article/details/121843945\">2023 年 MySQL 8.0 安装配置 最简易（保姆级）_mysql8.0安装配置教程_mobeicanyue的博客-CSDN博客</a><br>安装时需要输入一个密码作为mysql的root权限密码，需要牢记</p>\n<h3 id=\"开始使用\"><a href=\"#开始使用\" class=\"headerlink\" title=\"开始使用\"></a>开始使用</h3><p>在Power Shell中输入：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs css\">mysql -u root -<span class=\"hljs-selector-tag\">p</span><br></code></pre></td></tr></table></figure>\n<p>然后输入密码进入mysql<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230927202531.png\" alt=\"image.png\"></p>\n"},{"title":"MySQL基本使用","date":"2023-09-27T12:35:59.293Z","_content":"[[SQL基础]]\n[mysql入门教程——基本操作\\_mysql使用教程-CSDN博客](https://blog.csdn.net/qq_43323867/article/details/107433570)\n\n### 数据库操作\n\n#### 查询数据库\n```\n-- 查询所有数据库\nSHOW DATABASES;\n-- 查询某个数据库的编码\nSHOW CREATE DATABASE test;\n```\n\n#### 创建数据库\n```\n-- 创建库\nCREATE DATABASE demo1;\n-- 创建数据库的时候指定编码表\n-- GB2312、GBK、GB18030、UTF-8(unicode)、ISO-8859-1(拉丁文)\nCREATE DATABASE demo2 CHARACTER SET gbk;\n-- 修改库的校对规则\nCREATE DATABASE demo3 CHARACTER SET utf8 COLLATE utf8_bin;\n```\n\n#### 删除数据库\n```\n-- 删除数据库\nDROP DATABASE demo4;\n```\n\n修改数据库\n```\n-- 修改数据库编码表\nALTER DATABASE demo3 CHARACTER SET gbk;\n```\n\n### 数据表操作\n在操作数据表之前，一定要记住切换到某个库下\n```\n-- 切换数据\nuse 库名;\n```\n\n#### 创建数据表\n```\n-- 创建表\nCREATE TABLE tb_user(\n  id INT,\n  username VARCHAR(10),\n  age INT\n);\n```\n\n#### 删除数据表\n```\n-- 删除表\ndrop table 表名;\n```\n\n#### 查询表结构\n`desc` 表名; 查看表结构\n`show tables` ; 查看当前库内所有表名\n`show create table` 表名; 查看建表语句和字符集\n\n#### 表中插入数据\n语法：insert into 表名 (列名，列名，列名…) values (值，值，值…);","source":"_posts/Notes/编程/MySQL/MySQL基本使用.md","raw":"---\ntitle: MySQL基本使用\ncategories:\n  - Notes\n  - 编程\n  - MySQL\ntags:\n  - MySQL\n  - 数据库\ndate:\n---\n[[SQL基础]]\n[mysql入门教程——基本操作\\_mysql使用教程-CSDN博客](https://blog.csdn.net/qq_43323867/article/details/107433570)\n\n### 数据库操作\n\n#### 查询数据库\n```\n-- 查询所有数据库\nSHOW DATABASES;\n-- 查询某个数据库的编码\nSHOW CREATE DATABASE test;\n```\n\n#### 创建数据库\n```\n-- 创建库\nCREATE DATABASE demo1;\n-- 创建数据库的时候指定编码表\n-- GB2312、GBK、GB18030、UTF-8(unicode)、ISO-8859-1(拉丁文)\nCREATE DATABASE demo2 CHARACTER SET gbk;\n-- 修改库的校对规则\nCREATE DATABASE demo3 CHARACTER SET utf8 COLLATE utf8_bin;\n```\n\n#### 删除数据库\n```\n-- 删除数据库\nDROP DATABASE demo4;\n```\n\n修改数据库\n```\n-- 修改数据库编码表\nALTER DATABASE demo3 CHARACTER SET gbk;\n```\n\n### 数据表操作\n在操作数据表之前，一定要记住切换到某个库下\n```\n-- 切换数据\nuse 库名;\n```\n\n#### 创建数据表\n```\n-- 创建表\nCREATE TABLE tb_user(\n  id INT,\n  username VARCHAR(10),\n  age INT\n);\n```\n\n#### 删除数据表\n```\n-- 删除表\ndrop table 表名;\n```\n\n#### 查询表结构\n`desc` 表名; 查看表结构\n`show tables` ; 查看当前库内所有表名\n`show create table` 表名; 查看建表语句和字符集\n\n#### 表中插入数据\n语法：insert into 表名 (列名，列名，列名…) values (值，值，值…);","slug":"Notes/编程/MySQL/MySQL基本使用","published":1,"updated":"2023-10-09T03:35:40.611Z","_id":"cln1vnr5e001rtou51zes7wqs","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>[[SQL基础]]<br><a href=\"https://blog.csdn.net/qq_43323867/article/details/107433570\">mysql入门教程——基本操作_mysql使用教程-CSDN博客</a></p>\n<h3 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h3><h4 id=\"查询数据库\"><a href=\"#查询数据库\" class=\"headerlink\" title=\"查询数据库\"></a>查询数据库</h4><figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 查询所有数据库</span><br><span class=\"hljs-keyword\">SHOW</span> DATABASES;<br><span class=\"hljs-comment\">-- 查询某个数据库的编码</span><br><span class=\"hljs-keyword\">SHOW</span> <span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> test;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h4><figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 创建库</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> demo1;<br><span class=\"hljs-comment\">-- 创建数据库的时候指定编码表</span><br><span class=\"hljs-comment\">-- GB2312、GBK、GB18030、UTF-8(unicode)、ISO-8859-1(拉丁文)</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> demo2 <span class=\"hljs-type\">CHARACTER</span> <span class=\"hljs-keyword\">SET</span> gbk;<br><span class=\"hljs-comment\">-- 修改库的校对规则</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> demo3 <span class=\"hljs-type\">CHARACTER</span> <span class=\"hljs-keyword\">SET</span> utf8 <span class=\"hljs-keyword\">COLLATE</span> utf8_bin;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h4><figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 删除数据库</span><br><span class=\"hljs-keyword\">DROP</span> <span class=\"hljs-keyword\">DATABASE</span> demo4;<br></code></pre></td></tr></table></figure>\n\n<p>修改数据库</p>\n<figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 修改数据库编码表</span><br><span class=\"hljs-keyword\">ALTER</span> <span class=\"hljs-keyword\">DATABASE</span> demo3 <span class=\"hljs-type\">CHARACTER</span> <span class=\"hljs-keyword\">SET</span> gbk;<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"数据表操作\"><a href=\"#数据表操作\" class=\"headerlink\" title=\"数据表操作\"></a>数据表操作</h3><p>在操作数据表之前，一定要记住切换到某个库下</p>\n<figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ada\"><span class=\"hljs-comment\">-- 切换数据</span><br><span class=\"hljs-keyword\">use</span> 库名;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"创建数据表\"><a href=\"#创建数据表\" class=\"headerlink\" title=\"创建数据表\"></a>创建数据表</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs sql\"><span class=\"hljs-comment\">-- 创建表</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">TABLE</span> tb_user(<br>  id <span class=\"hljs-type\">INT</span>,<br>  username <span class=\"hljs-type\">VARCHAR</span>(<span class=\"hljs-number\">10</span>),<br>  age <span class=\"hljs-type\">INT</span><br>);<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"删除数据表\"><a href=\"#删除数据表\" class=\"headerlink\" title=\"删除数据表\"></a>删除数据表</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs sql\"><span class=\"hljs-comment\">-- 删除表</span><br><span class=\"hljs-keyword\">drop</span> <span class=\"hljs-keyword\">table</span> 表名;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"查询表结构\"><a href=\"#查询表结构\" class=\"headerlink\" title=\"查询表结构\"></a>查询表结构</h4><p><code>desc</code> 表名; 查看表结构<br><code>show tables</code> ; 查看当前库内所有表名<br><code>show create table</code> 表名; 查看建表语句和字符集</p>\n<h4 id=\"表中插入数据\"><a href=\"#表中插入数据\" class=\"headerlink\" title=\"表中插入数据\"></a>表中插入数据</h4><p>语法：insert into 表名 (列名，列名，列名…) values (值，值，值…);</p>\n","site":{"data":{}},"excerpt":"","more":"<p>[[SQL基础]]<br><a href=\"https://blog.csdn.net/qq_43323867/article/details/107433570\">mysql入门教程——基本操作_mysql使用教程-CSDN博客</a></p>\n<h3 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h3><h4 id=\"查询数据库\"><a href=\"#查询数据库\" class=\"headerlink\" title=\"查询数据库\"></a>查询数据库</h4><figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 查询所有数据库</span><br><span class=\"hljs-keyword\">SHOW</span> DATABASES;<br><span class=\"hljs-comment\">-- 查询某个数据库的编码</span><br><span class=\"hljs-keyword\">SHOW</span> <span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> test;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h4><figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 创建库</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> demo1;<br><span class=\"hljs-comment\">-- 创建数据库的时候指定编码表</span><br><span class=\"hljs-comment\">-- GB2312、GBK、GB18030、UTF-8(unicode)、ISO-8859-1(拉丁文)</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> demo2 <span class=\"hljs-type\">CHARACTER</span> <span class=\"hljs-keyword\">SET</span> gbk;<br><span class=\"hljs-comment\">-- 修改库的校对规则</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">DATABASE</span> demo3 <span class=\"hljs-type\">CHARACTER</span> <span class=\"hljs-keyword\">SET</span> utf8 <span class=\"hljs-keyword\">COLLATE</span> utf8_bin;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h4><figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 删除数据库</span><br><span class=\"hljs-keyword\">DROP</span> <span class=\"hljs-keyword\">DATABASE</span> demo4;<br></code></pre></td></tr></table></figure>\n\n<p>修改数据库</p>\n<figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-comment\">-- 修改数据库编码表</span><br><span class=\"hljs-keyword\">ALTER</span> <span class=\"hljs-keyword\">DATABASE</span> demo3 <span class=\"hljs-type\">CHARACTER</span> <span class=\"hljs-keyword\">SET</span> gbk;<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"数据表操作\"><a href=\"#数据表操作\" class=\"headerlink\" title=\"数据表操作\"></a>数据表操作</h3><p>在操作数据表之前，一定要记住切换到某个库下</p>\n<figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ada\"><span class=\"hljs-comment\">-- 切换数据</span><br><span class=\"hljs-keyword\">use</span> 库名;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"创建数据表\"><a href=\"#创建数据表\" class=\"headerlink\" title=\"创建数据表\"></a>创建数据表</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs sql\"><span class=\"hljs-comment\">-- 创建表</span><br><span class=\"hljs-keyword\">CREATE</span> <span class=\"hljs-keyword\">TABLE</span> tb_user(<br>  id <span class=\"hljs-type\">INT</span>,<br>  username <span class=\"hljs-type\">VARCHAR</span>(<span class=\"hljs-number\">10</span>),<br>  age <span class=\"hljs-type\">INT</span><br>);<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"删除数据表\"><a href=\"#删除数据表\" class=\"headerlink\" title=\"删除数据表\"></a>删除数据表</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs sql\"><span class=\"hljs-comment\">-- 删除表</span><br><span class=\"hljs-keyword\">drop</span> <span class=\"hljs-keyword\">table</span> 表名;<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"查询表结构\"><a href=\"#查询表结构\" class=\"headerlink\" title=\"查询表结构\"></a>查询表结构</h4><p><code>desc</code> 表名; 查看表结构<br><code>show tables</code> ; 查看当前库内所有表名<br><code>show create table</code> 表名; 查看建表语句和字符集</p>\n<h4 id=\"表中插入数据\"><a href=\"#表中插入数据\" class=\"headerlink\" title=\"表中插入数据\"></a>表中插入数据</h4><p>语法：insert into 表名 (列名，列名，列名…) values (值，值，值…);</p>\n"},{"title":"通过MySQL Workbench操作","date":"2023-10-06T04:06:52.872Z","_content":"### 创建数据库以及表\n#### 新建数据库\n点击button栏的“create a new schema in the connected server”\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125516.png)\n\n在视图中间区域出现的标签页中，填写新建数据库的名字同时可以选择数据库的字符集和校对规则。点击“Apply”。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125554.png)\n\n工具会给出一个提示窗口，该窗口中可看到刚才通过视图创建数据库的动作对应的SQL 语句。点击“Apply”。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125652.png)\n执行完创建数据库操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库成功。点击“Finish”![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125738.png)\n此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库。展开数据库名字左侧的箭头，可看到如下图所示的数据库“testdb”![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125801.png)\n\n#### 新建表\n在上图中选中“testdb”的Table，右键选择“Create Table”。在中间工作区域新出现的标签窗口中填写表以及字段的相关信息。①填写表的名字及字符集和校对规则。②填写所有的字段名字以及字段的数据类型和相关约束。此处的约束缩写与③处的全称对应。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125857.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125950.png)\n\n填好信息后，点击“Apply”，工具会弹出提示窗口，在该窗口中可看到刚才通过视图创建表的动作对应的SQL 语句。点击“Apply”。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130449.png)\n执行完创建表操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库表成功。点击“Finish”。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130508.png)\n此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库表。展开表名字左侧的箭头，可看到如下图所示的字段![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130524.png)\n### 查看数据\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006143655.png)\n点击想查看的表\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006144930.png)\n\n可以在此修改表的内容\n\n### 导出和导入sql脚本\n#### 导出\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145032.png)\n\n进入如下界面，勾选想要导出的数据表，并选择适当的选项\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145328.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145437.png)\n\n点击上方的`Export Progress`，进入导出界面开始导出，完成后可以在指定路径下看到sql脚本生成。\n\n#### 导入\n新建一个数据库，双击选中（名称会加粗）![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145753.png)\n在`File`中点击`Open SQL Script`,打开指定sql脚本\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145846.png)\n点击框中的按钮执行\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006150022.png)\n\n执行结束后，返回home界面重新进入数据库（更新一下数据，否则导入的数据显示不出来），可以看到导入的数据。\n\n","source":"_posts/Notes/编程/MySQL/通过MySQL Workbench操作.md","raw":"---\ntitle: 通过MySQL Workbench操作\ncategories:\n  - Notes\n  - 编程\n  - MySQL\ntags:\n  - 数据库\n  - MySQL\ndate:\n---\n### 创建数据库以及表\n#### 新建数据库\n点击button栏的“create a new schema in the connected server”\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125516.png)\n\n在视图中间区域出现的标签页中，填写新建数据库的名字同时可以选择数据库的字符集和校对规则。点击“Apply”。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125554.png)\n\n工具会给出一个提示窗口，该窗口中可看到刚才通过视图创建数据库的动作对应的SQL 语句。点击“Apply”。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125652.png)\n执行完创建数据库操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库成功。点击“Finish”![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125738.png)\n此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库。展开数据库名字左侧的箭头，可看到如下图所示的数据库“testdb”![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125801.png)\n\n#### 新建表\n在上图中选中“testdb”的Table，右键选择“Create Table”。在中间工作区域新出现的标签窗口中填写表以及字段的相关信息。①填写表的名字及字符集和校对规则。②填写所有的字段名字以及字段的数据类型和相关约束。此处的约束缩写与③处的全称对应。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125857.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125950.png)\n\n填好信息后，点击“Apply”，工具会弹出提示窗口，在该窗口中可看到刚才通过视图创建表的动作对应的SQL 语句。点击“Apply”。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130449.png)\n执行完创建表操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库表成功。点击“Finish”。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130508.png)\n此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库表。展开表名字左侧的箭头，可看到如下图所示的字段![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130524.png)\n### 查看数据\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006143655.png)\n点击想查看的表\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006144930.png)\n\n可以在此修改表的内容\n\n### 导出和导入sql脚本\n#### 导出\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145032.png)\n\n进入如下界面，勾选想要导出的数据表，并选择适当的选项\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145328.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145437.png)\n\n点击上方的`Export Progress`，进入导出界面开始导出，完成后可以在指定路径下看到sql脚本生成。\n\n#### 导入\n新建一个数据库，双击选中（名称会加粗）![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145753.png)\n在`File`中点击`Open SQL Script`,打开指定sql脚本\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145846.png)\n点击框中的按钮执行\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006150022.png)\n\n执行结束后，返回home界面重新进入数据库（更新一下数据，否则导入的数据显示不出来），可以看到导入的数据。\n\n","slug":"Notes/编程/MySQL/通过MySQL Workbench操作","published":1,"updated":"2023-10-06T07:01:33.494Z","_id":"clne436w9000028u5dbw0h1th","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"创建数据库以及表\"><a href=\"#创建数据库以及表\" class=\"headerlink\" title=\"创建数据库以及表\"></a>创建数据库以及表</h3><h4 id=\"新建数据库\"><a href=\"#新建数据库\" class=\"headerlink\" title=\"新建数据库\"></a>新建数据库</h4><p>点击button栏的“create a new schema in the connected server”<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125516.png\" alt=\"image.png\"></p>\n<p>在视图中间区域出现的标签页中，填写新建数据库的名字同时可以选择数据库的字符集和校对规则。点击“Apply”。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125554.png\" alt=\"image.png\"></p>\n<p>工具会给出一个提示窗口，该窗口中可看到刚才通过视图创建数据库的动作对应的SQL 语句。点击“Apply”。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125652.png\" alt=\"image.png\"><br>执行完创建数据库操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库成功。点击“Finish”<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125738.png\" alt=\"image.png\"><br>此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库。展开数据库名字左侧的箭头，可看到如下图所示的数据库“testdb”<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125801.png\" alt=\"image.png\"></p>\n<h4 id=\"新建表\"><a href=\"#新建表\" class=\"headerlink\" title=\"新建表\"></a>新建表</h4><p>在上图中选中“testdb”的Table，右键选择“Create Table”。在中间工作区域新出现的标签窗口中填写表以及字段的相关信息。①填写表的名字及字符集和校对规则。②填写所有的字段名字以及字段的数据类型和相关约束。此处的约束缩写与③处的全称对应。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125857.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125950.png\" alt=\"image.png\"></p>\n<p>填好信息后，点击“Apply”，工具会弹出提示窗口，在该窗口中可看到刚才通过视图创建表的动作对应的SQL 语句。点击“Apply”。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130449.png\" alt=\"image.png\"><br>执行完创建表操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库表成功。点击“Finish”。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130508.png\" alt=\"image.png\"><br>此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库表。展开表名字左侧的箭头，可看到如下图所示的字段<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130524.png\" alt=\"image.png\"></p>\n<h3 id=\"查看数据\"><a href=\"#查看数据\" class=\"headerlink\" title=\"查看数据\"></a>查看数据</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006143655.png\" alt=\"image.png\"><br>点击想查看的表<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006144930.png\" alt=\"image.png\"></p>\n<p>可以在此修改表的内容</p>\n<h3 id=\"导出和导入sql脚本\"><a href=\"#导出和导入sql脚本\" class=\"headerlink\" title=\"导出和导入sql脚本\"></a>导出和导入sql脚本</h3><h4 id=\"导出\"><a href=\"#导出\" class=\"headerlink\" title=\"导出\"></a>导出</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145032.png\" alt=\"image.png\"></p>\n<p>进入如下界面，勾选想要导出的数据表，并选择适当的选项<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145328.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145437.png\" alt=\"image.png\"></p>\n<p>点击上方的<code>Export Progress</code>，进入导出界面开始导出，完成后可以在指定路径下看到sql脚本生成。</p>\n<h4 id=\"导入\"><a href=\"#导入\" class=\"headerlink\" title=\"导入\"></a>导入</h4><p>新建一个数据库，双击选中（名称会加粗）<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145753.png\" alt=\"image.png\"><br>在<code>File</code>中点击<code>Open SQL Script</code>,打开指定sql脚本<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145846.png\" alt=\"image.png\"><br>点击框中的按钮执行<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006150022.png\" alt=\"image.png\"></p>\n<p>执行结束后，返回home界面重新进入数据库（更新一下数据，否则导入的数据显示不出来），可以看到导入的数据。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"创建数据库以及表\"><a href=\"#创建数据库以及表\" class=\"headerlink\" title=\"创建数据库以及表\"></a>创建数据库以及表</h3><h4 id=\"新建数据库\"><a href=\"#新建数据库\" class=\"headerlink\" title=\"新建数据库\"></a>新建数据库</h4><p>点击button栏的“create a new schema in the connected server”<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125516.png\" alt=\"image.png\"></p>\n<p>在视图中间区域出现的标签页中，填写新建数据库的名字同时可以选择数据库的字符集和校对规则。点击“Apply”。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125554.png\" alt=\"image.png\"></p>\n<p>工具会给出一个提示窗口，该窗口中可看到刚才通过视图创建数据库的动作对应的SQL 语句。点击“Apply”。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125652.png\" alt=\"image.png\"><br>执行完创建数据库操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库成功。点击“Finish”<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125738.png\" alt=\"image.png\"><br>此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库。展开数据库名字左侧的箭头，可看到如下图所示的数据库“testdb”<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125801.png\" alt=\"image.png\"></p>\n<h4 id=\"新建表\"><a href=\"#新建表\" class=\"headerlink\" title=\"新建表\"></a>新建表</h4><p>在上图中选中“testdb”的Table，右键选择“Create Table”。在中间工作区域新出现的标签窗口中填写表以及字段的相关信息。①填写表的名字及字符集和校对规则。②填写所有的字段名字以及字段的数据类型和相关约束。此处的约束缩写与③处的全称对应。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125857.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006125950.png\" alt=\"image.png\"></p>\n<p>填好信息后，点击“Apply”，工具会弹出提示窗口，在该窗口中可看到刚才通过视图创建表的动作对应的SQL 语句。点击“Apply”。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130449.png\" alt=\"image.png\"><br>执行完创建表操作后，工具会给出执行结果。看到如下图①所示的提示，表示创建数据库表成功。点击“Finish”。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130508.png\" alt=\"image.png\"><br>此时可看到左侧数据库导航窗口中出现了刚刚新建的数据库表。展开表名字左侧的箭头，可看到如下图所示的字段<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006130524.png\" alt=\"image.png\"></p>\n<h3 id=\"查看数据\"><a href=\"#查看数据\" class=\"headerlink\" title=\"查看数据\"></a>查看数据</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006143655.png\" alt=\"image.png\"><br>点击想查看的表<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006144930.png\" alt=\"image.png\"></p>\n<p>可以在此修改表的内容</p>\n<h3 id=\"导出和导入sql脚本\"><a href=\"#导出和导入sql脚本\" class=\"headerlink\" title=\"导出和导入sql脚本\"></a>导出和导入sql脚本</h3><h4 id=\"导出\"><a href=\"#导出\" class=\"headerlink\" title=\"导出\"></a>导出</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145032.png\" alt=\"image.png\"></p>\n<p>进入如下界面，勾选想要导出的数据表，并选择适当的选项<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145328.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145437.png\" alt=\"image.png\"></p>\n<p>点击上方的<code>Export Progress</code>，进入导出界面开始导出，完成后可以在指定路径下看到sql脚本生成。</p>\n<h4 id=\"导入\"><a href=\"#导入\" class=\"headerlink\" title=\"导入\"></a>导入</h4><p>新建一个数据库，双击选中（名称会加粗）<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145753.png\" alt=\"image.png\"><br>在<code>File</code>中点击<code>Open SQL Script</code>,打开指定sql脚本<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006145846.png\" alt=\"image.png\"><br>点击框中的按钮执行<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231006150022.png\" alt=\"image.png\"></p>\n<p>执行结束后，返回home界面重新进入数据库（更新一下数据，否则导入的数据显示不出来），可以看到导入的数据。</p>\n"},{"title":"Django操作MySQL","date":"2023-09-27T15:37:46.021Z","_content":"### 安装第三方模块\n```\npip install mysqlclient\n```\n\n### ORM\nORM全称是：Object Relational Mapping(对象关系映射)，其主要作用是在编程中，把面向对象的概念跟数据库中表的概念对应起来。\n在Django中的应用\n- 创建、修改、删除数据库中的表\n- 操作表中的数据\n\n### 创建数据库\n- 启动MySQL服务\n- 自带工具创建数据库\n```\ncreate datebase 数据库名;  # 创建数据库\nshow databases;  # 显示数据库列表\n```\n\n### Django连接数据库\n在setting中修改默认的数据库`DATABASES`，Django默认的数据库是sqlite，将其改为mysql\n```\nDATABASES = {  \n    'default': {  \n        \"ENGINE\": \"django.db.backends.mysql\",  \n        \"NAME\": \"table1\",  # 数据库名字  \n        \"USER\": 'root',  # 用户名  \n        'PASSWORD': 'Wzy030530',  \n        'HOST': '127.0.0.1',  # 主机  \n        'PORT': '3306',  \n    }  \n}\n```\n\n注：\n```\nSELECT User FROM mysql.user;  # 查看mysql的用户名列表\n```\n\n### 创建删除和修改表\n在app的models文件下写入类 文件结构见[[Django基础]]\n```\nclass User(models.Model):  \n    # 用户身份证号 主键  \n    user_id = models.CharField('user_id', primary_key=True, max_length=30)  \n    # 用户姓名  \n    user_name = models.CharField('user_name', max_length=20, null=False)  \n    \n    class Meta:  \n        db_table = 'User'\n```\n\nmodels修改完后，在终端中执行\n```\npython manage.py makemigrations\npython manage.py migrate\n```\n即可在数据库中看到表\n\n修改和删除表，只需要修改models中的类，然后执行上面两条命令即可","source":"_posts/Notes/编程/Django/Django操作MySQL.md","raw":"---\ntitle: Django操作MySQL\ncategories:\n  - Notes\n  - 编程\n  - Django\ntags:\n  - MySQL\n  - Python\ndate:\n---\n### 安装第三方模块\n```\npip install mysqlclient\n```\n\n### ORM\nORM全称是：Object Relational Mapping(对象关系映射)，其主要作用是在编程中，把面向对象的概念跟数据库中表的概念对应起来。\n在Django中的应用\n- 创建、修改、删除数据库中的表\n- 操作表中的数据\n\n### 创建数据库\n- 启动MySQL服务\n- 自带工具创建数据库\n```\ncreate datebase 数据库名;  # 创建数据库\nshow databases;  # 显示数据库列表\n```\n\n### Django连接数据库\n在setting中修改默认的数据库`DATABASES`，Django默认的数据库是sqlite，将其改为mysql\n```\nDATABASES = {  \n    'default': {  \n        \"ENGINE\": \"django.db.backends.mysql\",  \n        \"NAME\": \"table1\",  # 数据库名字  \n        \"USER\": 'root',  # 用户名  \n        'PASSWORD': 'Wzy030530',  \n        'HOST': '127.0.0.1',  # 主机  \n        'PORT': '3306',  \n    }  \n}\n```\n\n注：\n```\nSELECT User FROM mysql.user;  # 查看mysql的用户名列表\n```\n\n### 创建删除和修改表\n在app的models文件下写入类 文件结构见[[Django基础]]\n```\nclass User(models.Model):  \n    # 用户身份证号 主键  \n    user_id = models.CharField('user_id', primary_key=True, max_length=30)  \n    # 用户姓名  \n    user_name = models.CharField('user_name', max_length=20, null=False)  \n    \n    class Meta:  \n        db_table = 'User'\n```\n\nmodels修改完后，在终端中执行\n```\npython manage.py makemigrations\npython manage.py migrate\n```\n即可在数据库中看到表\n\n修改和删除表，只需要修改models中的类，然后执行上面两条命令即可","slug":"Notes/编程/Django/Django操作MySQL","published":1,"updated":"2023-10-24T14:50:21.245Z","_id":"clnjr0wpf0000rou5hi9ibg4a","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"安装第三方模块\"><a href=\"#安装第三方模块\" class=\"headerlink\" title=\"安装第三方模块\"></a>安装第三方模块</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs cmake\">pip <span class=\"hljs-keyword\">install</span> mysqlclient<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"ORM\"><a href=\"#ORM\" class=\"headerlink\" title=\"ORM\"></a>ORM</h3><p>ORM全称是：Object Relational Mapping(对象关系映射)，其主要作用是在编程中，把面向对象的概念跟数据库中表的概念对应起来。<br>在Django中的应用</p>\n<ul>\n<li>创建、修改、删除数据库中的表</li>\n<li>操作表中的数据</li>\n</ul>\n<h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><ul>\n<li>启动MySQL服务</li>\n<li>自带工具创建数据库<figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gauss\"><span class=\"hljs-keyword\">create</span> datebase 数据库名;  <span class=\"hljs-meta\"># 创建数据库</span><br><span class=\"hljs-keyword\">show</span> databases;  <span class=\"hljs-meta\"># 显示数据库列表</span><br></code></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"Django连接数据库\"><a href=\"#Django连接数据库\" class=\"headerlink\" title=\"Django连接数据库\"></a>Django连接数据库</h3><p>在setting中修改默认的数据库<code>DATABASES</code>，Django默认的数据库是sqlite，将其改为mysql</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs 1c\">DATABASES = &#123;  <br>    &#x27;default&#x27;: &#123;  <br>        <span class=\"hljs-string\">&quot;ENGINE&quot;</span>: <span class=\"hljs-string\">&quot;django.db.backends.mysql&quot;</span>,  <br>        <span class=\"hljs-string\">&quot;NAME&quot;</span>: <span class=\"hljs-string\">&quot;table1&quot;</span>,  <span class=\"hljs-meta\"># 数据库名字  </span><br>        <span class=\"hljs-string\">&quot;USER&quot;</span>: &#x27;root&#x27;,  <span class=\"hljs-meta\"># 用户名  </span><br>        &#x27;PASSWORD&#x27;: &#x27;Wzy<span class=\"hljs-number\">030530</span>&#x27;,  <br>        &#x27;HOST&#x27;: &#x27;127.0.0.1&#x27;,  <span class=\"hljs-meta\"># 主机  </span><br>        &#x27;PORT&#x27;: &#x27;<span class=\"hljs-number\">3306</span>&#x27;,  <br>    &#125;  <br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>注：</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">SELECT <span class=\"hljs-keyword\">User</span> <span class=\"hljs-title\">FROM</span> mysql.user;  <span class=\"hljs-comment\"># 查看mysql的用户名列表</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"创建删除和修改表\"><a href=\"#创建删除和修改表\" class=\"headerlink\" title=\"创建删除和修改表\"></a>创建删除和修改表</h3><p>在app的models文件下写入类 文件结构见[[Django基础]]</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\">class User(models.Model):  <br>    # 用户身份证号 主键  <br>    user_id = models.CharField(<span class=\"hljs-string\">&#x27;user_id&#x27;</span>, <span class=\"hljs-attribute\">primary_key</span>=<span class=\"hljs-literal\">True</span>, <span class=\"hljs-attribute\">max_length</span>=30)  <br>    # 用户姓名  <br>    user_name = models.CharField(<span class=\"hljs-string\">&#x27;user_name&#x27;</span>, <span class=\"hljs-attribute\">max_length</span>=20, <span class=\"hljs-attribute\">null</span>=<span class=\"hljs-literal\">False</span>)  <br>    <br>    class Meta:  <br>        db_table = <span class=\"hljs-string\">&#x27;User&#x27;</span><br></code></pre></td></tr></table></figure>\n\n<p>models修改完后，在终端中执行</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\"><span class=\"hljs-keyword\">python</span> manage.<span class=\"hljs-keyword\">py</span> makemigrations<br><span class=\"hljs-keyword\">python</span> manage.<span class=\"hljs-keyword\">py</span> migrate<br></code></pre></td></tr></table></figure>\n<p>即可在数据库中看到表</p>\n<p>修改和删除表，只需要修改models中的类，然后执行上面两条命令即可</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"安装第三方模块\"><a href=\"#安装第三方模块\" class=\"headerlink\" title=\"安装第三方模块\"></a>安装第三方模块</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs cmake\">pip <span class=\"hljs-keyword\">install</span> mysqlclient<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"ORM\"><a href=\"#ORM\" class=\"headerlink\" title=\"ORM\"></a>ORM</h3><p>ORM全称是：Object Relational Mapping(对象关系映射)，其主要作用是在编程中，把面向对象的概念跟数据库中表的概念对应起来。<br>在Django中的应用</p>\n<ul>\n<li>创建、修改、删除数据库中的表</li>\n<li>操作表中的数据</li>\n</ul>\n<h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><ul>\n<li>启动MySQL服务</li>\n<li>自带工具创建数据库<figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gauss\"><span class=\"hljs-keyword\">create</span> datebase 数据库名;  <span class=\"hljs-meta\"># 创建数据库</span><br><span class=\"hljs-keyword\">show</span> databases;  <span class=\"hljs-meta\"># 显示数据库列表</span><br></code></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"Django连接数据库\"><a href=\"#Django连接数据库\" class=\"headerlink\" title=\"Django连接数据库\"></a>Django连接数据库</h3><p>在setting中修改默认的数据库<code>DATABASES</code>，Django默认的数据库是sqlite，将其改为mysql</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs 1c\">DATABASES = &#123;  <br>    &#x27;default&#x27;: &#123;  <br>        <span class=\"hljs-string\">&quot;ENGINE&quot;</span>: <span class=\"hljs-string\">&quot;django.db.backends.mysql&quot;</span>,  <br>        <span class=\"hljs-string\">&quot;NAME&quot;</span>: <span class=\"hljs-string\">&quot;table1&quot;</span>,  <span class=\"hljs-meta\"># 数据库名字  </span><br>        <span class=\"hljs-string\">&quot;USER&quot;</span>: &#x27;root&#x27;,  <span class=\"hljs-meta\"># 用户名  </span><br>        &#x27;PASSWORD&#x27;: &#x27;Wzy<span class=\"hljs-number\">030530</span>&#x27;,  <br>        &#x27;HOST&#x27;: &#x27;127.0.0.1&#x27;,  <span class=\"hljs-meta\"># 主机  </span><br>        &#x27;PORT&#x27;: &#x27;<span class=\"hljs-number\">3306</span>&#x27;,  <br>    &#125;  <br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>注：</p>\n<figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs crmsh\">SELECT <span class=\"hljs-keyword\">User</span> <span class=\"hljs-title\">FROM</span> mysql.user;  <span class=\"hljs-comment\"># 查看mysql的用户名列表</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"创建删除和修改表\"><a href=\"#创建删除和修改表\" class=\"headerlink\" title=\"创建删除和修改表\"></a>创建删除和修改表</h3><p>在app的models文件下写入类 文件结构见[[Django基础]]</p>\n<figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs routeros\">class User(models.Model):  <br>    # 用户身份证号 主键  <br>    user_id = models.CharField(<span class=\"hljs-string\">&#x27;user_id&#x27;</span>, <span class=\"hljs-attribute\">primary_key</span>=<span class=\"hljs-literal\">True</span>, <span class=\"hljs-attribute\">max_length</span>=30)  <br>    # 用户姓名  <br>    user_name = models.CharField(<span class=\"hljs-string\">&#x27;user_name&#x27;</span>, <span class=\"hljs-attribute\">max_length</span>=20, <span class=\"hljs-attribute\">null</span>=<span class=\"hljs-literal\">False</span>)  <br>    <br>    class Meta:  <br>        db_table = <span class=\"hljs-string\">&#x27;User&#x27;</span><br></code></pre></td></tr></table></figure>\n\n<p>models修改完后，在终端中执行</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\"><span class=\"hljs-keyword\">python</span> manage.<span class=\"hljs-keyword\">py</span> makemigrations<br><span class=\"hljs-keyword\">python</span> manage.<span class=\"hljs-keyword\">py</span> migrate<br></code></pre></td></tr></table></figure>\n<p>即可在数据库中看到表</p>\n<p>修改和删除表，只需要修改models中的类，然后执行上面两条命令即可</p>\n"},{"title":"三分化","date":"2023-10-24T09:18:18.606Z","_content":"![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171908.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171917.png)\n","source":"_posts/Notes/训练计划/三分化.md","raw":"---\ntitle: 三分化\ncategories:\n  - Notes\n  - 训练计划\ntags:\n  - 训练\n  - 三分化\ndate:\n---\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171908.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171917.png)\n","slug":"Notes/训练计划/三分化","published":1,"updated":"2023-10-24T09:28:50.237Z","_id":"clo44jtk40000g0u5dfmkejl5","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171908.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171917.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171908.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231024171917.png\" alt=\"image.png\"></p>\n"},{"title":"Django基础","date":"2023-09-23T05:21:34.187Z","_content":"### 创建第一个项目\n```\ndjango-admin startprojct HelloWorld\n```\n创建一个Django项目名为HelloWorld，目录结构如下\n```\n$ cd HelloWorld/\n$ tree\n.\n|-- HelloWorld\n|   |-- __init__.py\n|   |-- asgi.py\n|   |-- settings.py\n|   |-- urls.py\n|   `-- wsgi.py\n`-- manage.py\n```\n目录说明：\n- **HelloWorld:** 项目的容器。\n- **manage.py:** 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。\n- **HelloWorld/__init__.py:** 一个空文件，告诉 Python 该目录是一个 Python 包。\n- **HelloWorld/asgi.py:** 一个 ASGI 兼容的 Web 服务器的入口，以便运行你的项目。\n- **HelloWorld/settings.py:** 该 Django 项目的设置/配置。\n- **HelloWorld/urls.py:** 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站\"目录\"。\n- **HelloWorld/wsgi.py:** 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。\n\n#### 启动服务器\n```\npython3 manage.py runserver 0.0.0.0:8000\n```\n0.0.0.0 让其它电脑可连接到开发服务器，8000 为端口号。如果不说明，那么端口号默认为 8000\n\n>第一次打开网页出现报错：DisallowedHost at Invaild HTTP_HOST header： '0.0.0.0:8000'. You may need to add '0.0.0.0' to ALLOWED_HOSTS. \n>解决：在配置文件settings.py中找到`ALLOWED_HOSTS`，将‘0.0.0.0’添加到其中\n\n#### 视图和URL配置\n在`HelloWorld/HelloWorld/`目录下新建一个`views.py`文件，输入\n```\nfrom django.http import HttpResponse\n \ndef hello(request):\n    return HttpResponse(\"Hello world ! \")\n```\n\n在urls.py文件中写入\n```\nfrom django.urls import re_path as url  \n# from django.conf.urls import url 这段代码已经过时，上面的re_path取代了之前版本的url  \nfrom . import views  \n  \nurlpatterns = [  \n    url(r'^$', views.hello),  \n]\n```\n可在网页中看到“Hello world！”的输出\n\n**path（）函数**:\n```\npath(route, view, kwargs=None, name=None)\n```\n\n### 创建app\n```\npython3 manage.py startapp app1  # 创建app1\n```\n\n目录结构如下：\n```\n.\n├── HelloWorld\n│   ├── __init__.py\n│   ├── __pycache__\n│   │   ├── __init__.cpython-38.pyc\n│   │   ├── settings.cpython-38.pyc\n│   │   ├── urls.cpython-38.pyc\n│   │   ├── views.cpython-38.pyc\n│   │   └── wsgi.cpython-38.pyc\n│   ├── asgi.py\n│   ├── settings.py\n│   ├── urls.py  # 配置app用的url\n│   ├── views.py\n│   └── wsgi.py\n├── app1\n│   ├── __init__.py\n│   ├── admin.py\n│   ├── apps.py\n│   ├── migrations\n│   │   └── __init__.py\n│   ├── models.py  # 对数据库进行操作\n│   ├── tests.py\n│   └── views.py  # 视图函数\n├── db.sqlite3\n└── manage.py\n```\n\n**编写URL和视图函数的对应关系**\n```\nurlpatterns = [  \n    path('something/', views.something)  \n]\n```\n\n**编写视图函数**\n```\ndef something(request)\n\treturn HttpResponse(\"xxxxx\")\n```\n\n**启动django项目**\n- 命令行启动\n```\npython manage.py runserver\n```\n\n在app1下新建一个templates目录,写入HTML模板,可以在views中用`render`函数直接获取\n```\nreturn render(request, 'something.html')\n```","source":"_posts/Notes/编程/Django/Django基础.md","raw":"---\ntitle: Django基础\ncategories:\n  - Notes\n  - 编程\n  - Django\ndate: \ntags:\n  - 软件工程\n  - Python\n---\n### 创建第一个项目\n```\ndjango-admin startprojct HelloWorld\n```\n创建一个Django项目名为HelloWorld，目录结构如下\n```\n$ cd HelloWorld/\n$ tree\n.\n|-- HelloWorld\n|   |-- __init__.py\n|   |-- asgi.py\n|   |-- settings.py\n|   |-- urls.py\n|   `-- wsgi.py\n`-- manage.py\n```\n目录说明：\n- **HelloWorld:** 项目的容器。\n- **manage.py:** 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。\n- **HelloWorld/__init__.py:** 一个空文件，告诉 Python 该目录是一个 Python 包。\n- **HelloWorld/asgi.py:** 一个 ASGI 兼容的 Web 服务器的入口，以便运行你的项目。\n- **HelloWorld/settings.py:** 该 Django 项目的设置/配置。\n- **HelloWorld/urls.py:** 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站\"目录\"。\n- **HelloWorld/wsgi.py:** 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。\n\n#### 启动服务器\n```\npython3 manage.py runserver 0.0.0.0:8000\n```\n0.0.0.0 让其它电脑可连接到开发服务器，8000 为端口号。如果不说明，那么端口号默认为 8000\n\n>第一次打开网页出现报错：DisallowedHost at Invaild HTTP_HOST header： '0.0.0.0:8000'. You may need to add '0.0.0.0' to ALLOWED_HOSTS. \n>解决：在配置文件settings.py中找到`ALLOWED_HOSTS`，将‘0.0.0.0’添加到其中\n\n#### 视图和URL配置\n在`HelloWorld/HelloWorld/`目录下新建一个`views.py`文件，输入\n```\nfrom django.http import HttpResponse\n \ndef hello(request):\n    return HttpResponse(\"Hello world ! \")\n```\n\n在urls.py文件中写入\n```\nfrom django.urls import re_path as url  \n# from django.conf.urls import url 这段代码已经过时，上面的re_path取代了之前版本的url  \nfrom . import views  \n  \nurlpatterns = [  \n    url(r'^$', views.hello),  \n]\n```\n可在网页中看到“Hello world！”的输出\n\n**path（）函数**:\n```\npath(route, view, kwargs=None, name=None)\n```\n\n### 创建app\n```\npython3 manage.py startapp app1  # 创建app1\n```\n\n目录结构如下：\n```\n.\n├── HelloWorld\n│   ├── __init__.py\n│   ├── __pycache__\n│   │   ├── __init__.cpython-38.pyc\n│   │   ├── settings.cpython-38.pyc\n│   │   ├── urls.cpython-38.pyc\n│   │   ├── views.cpython-38.pyc\n│   │   └── wsgi.cpython-38.pyc\n│   ├── asgi.py\n│   ├── settings.py\n│   ├── urls.py  # 配置app用的url\n│   ├── views.py\n│   └── wsgi.py\n├── app1\n│   ├── __init__.py\n│   ├── admin.py\n│   ├── apps.py\n│   ├── migrations\n│   │   └── __init__.py\n│   ├── models.py  # 对数据库进行操作\n│   ├── tests.py\n│   └── views.py  # 视图函数\n├── db.sqlite3\n└── manage.py\n```\n\n**编写URL和视图函数的对应关系**\n```\nurlpatterns = [  \n    path('something/', views.something)  \n]\n```\n\n**编写视图函数**\n```\ndef something(request)\n\treturn HttpResponse(\"xxxxx\")\n```\n\n**启动django项目**\n- 命令行启动\n```\npython manage.py runserver\n```\n\n在app1下新建一个templates目录,写入HTML模板,可以在views中用`render`函数直接获取\n```\nreturn render(request, 'something.html')\n```","slug":"Notes/编程/Django/Django基础","published":1,"updated":"2023-10-24T14:50:27.294Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clo4gan3f00009cu580hn4ios","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"创建第一个项目\"><a href=\"#创建第一个项目\" class=\"headerlink\" title=\"创建第一个项目\"></a>创建第一个项目</h3><figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ebnf\"><span class=\"hljs-attribute\">django-admin startprojct HelloWorld</span><br></code></pre></td></tr></table></figure>\n<p>创建一个Django项目名为HelloWorld，目录结构如下</p>\n<figure class=\"highlight gherkin\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gherkin\">$ cd HelloWorld/<br>$ tree<br>.<br>|<span class=\"hljs-string\">-- HelloWorld</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- __init__.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- asgi.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- settings.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- urls.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   `-- wsgi.py</span><br><span class=\"hljs-string\">`-- manage.py</span><br></code></pre></td></tr></table></figure>\n<p>目录说明：</p>\n<ul>\n<li><strong>HelloWorld:</strong> 项目的容器。</li>\n<li><strong>manage.py:</strong> 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。</li>\n<li><strong>HelloWorld&#x2F;<strong>init</strong>.py:</strong> 一个空文件，告诉 Python 该目录是一个 Python 包。</li>\n<li><strong>HelloWorld&#x2F;asgi.py:</strong> 一个 ASGI 兼容的 Web 服务器的入口，以便运行你的项目。</li>\n<li><strong>HelloWorld&#x2F;settings.py:</strong> 该 Django 项目的设置&#x2F;配置。</li>\n<li><strong>HelloWorld&#x2F;urls.py:</strong> 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站”目录”。</li>\n<li><strong>HelloWorld&#x2F;wsgi.py:</strong> 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。</li>\n</ul>\n<h4 id=\"启动服务器\"><a href=\"#启动服务器\" class=\"headerlink\" title=\"启动服务器\"></a>启动服务器</h4><figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nginx\"><span class=\"hljs-attribute\">python3</span> manage.py runserver <span class=\"hljs-number\">0.0.0.0:8000</span><br></code></pre></td></tr></table></figure>\n<p>0.0.0.0 让其它电脑可连接到开发服务器，8000 为端口号。如果不说明，那么端口号默认为 8000</p>\n<blockquote>\n<p>第一次打开网页出现报错：DisallowedHost at Invaild HTTP_HOST header： ‘0.0.0.0:8000’. You may need to add ‘0.0.0.0’ to ALLOWED_HOSTS.<br>解决：在配置文件settings.py中找到<code>ALLOWED_HOSTS</code>，将‘0.0.0.0’添加到其中</p>\n</blockquote>\n<h4 id=\"视图和URL配置\"><a href=\"#视图和URL配置\" class=\"headerlink\" title=\"视图和URL配置\"></a>视图和URL配置</h4><p>在<code>HelloWorld/HelloWorld/</code>目录下新建一个<code>views.py</code>文件，输入</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> django.http <span class=\"hljs-keyword\">import</span> HttpResponse<br> <br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">hello</span>(<span class=\"hljs-params\">request</span>):<br>    <span class=\"hljs-keyword\">return</span> HttpResponse(<span class=\"hljs-string\">&quot;Hello world ! &quot;</span>)<br></code></pre></td></tr></table></figure>\n\n<p>在urls.py文件中写入</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> django.urls <span class=\"hljs-keyword\">import</span> re_path <span class=\"hljs-keyword\">as</span> url  <br><span class=\"hljs-comment\"># from django.conf.urls import url 这段代码已经过时，上面的re_path取代了之前版本的url  </span><br><span class=\"hljs-keyword\">from</span> . <span class=\"hljs-keyword\">import</span> views  <br>  <br>urlpatterns = [  <br>    url(<span class=\"hljs-string\">r&#x27;^$&#x27;</span>, views.hello),  <br>]<br></code></pre></td></tr></table></figure>\n<p>可在网页中看到“Hello world！”的输出</p>\n<p><strong>path（）函数</strong>:</p>\n<figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\">path(route, <span class=\"hljs-keyword\">view</span>, kwargs=<span class=\"hljs-keyword\">None</span>, <span class=\"hljs-type\">name</span>=<span class=\"hljs-keyword\">None</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"创建app\"><a href=\"#创建app\" class=\"headerlink\" title=\"创建app\"></a>创建app</h3><figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nginx\"><span class=\"hljs-attribute\">python3</span> manage.py startapp app1  <span class=\"hljs-comment\"># 创建app1</span><br></code></pre></td></tr></table></figure>\n\n<p>目录结构如下：</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\">.<br>├── HelloWorld<br>│   ├── __init__.<span class=\"hljs-keyword\">py</span><br>│   ├── __pycache__<br>│   │   ├── __init__.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   ├── settings.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   ├── urls.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   ├── views.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   └── wsgi.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   ├── asgi.<span class=\"hljs-keyword\">py</span><br>│   ├── settings.<span class=\"hljs-keyword\">py</span><br>│   ├── urls.<span class=\"hljs-keyword\">py</span>  # 配置app用的url<br>│   ├── views.<span class=\"hljs-keyword\">py</span><br>│   └── wsgi.<span class=\"hljs-keyword\">py</span><br>├── app1<br>│   ├── __init__.<span class=\"hljs-keyword\">py</span><br>│   ├── admin.<span class=\"hljs-keyword\">py</span><br>│   ├── apps.<span class=\"hljs-keyword\">py</span><br>│   ├── migrations<br>│   │   └── __init__.<span class=\"hljs-keyword\">py</span><br>│   ├── models.<span class=\"hljs-keyword\">py</span>  # 对数据库进行操作<br>│   ├── tests.<span class=\"hljs-keyword\">py</span><br>│   └── views.<span class=\"hljs-keyword\">py</span>  # 视图函数<br>├── db.sqlite3<br>└── manage.<span class=\"hljs-keyword\">py</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>编写URL和视图函数的对应关系</strong></p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ini\"><span class=\"hljs-attr\">urlpatterns</span> = [  <br>    path(<span class=\"hljs-string\">&#x27;something/&#x27;</span>, views.something)  <br>]<br></code></pre></td></tr></table></figure>\n\n<p><strong>编写视图函数</strong></p>\n<figure class=\"highlight isbl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs isbl\"><span class=\"hljs-variable\">def</span> <span class=\"hljs-function\"><span class=\"hljs-title\">something</span>(<span class=\"hljs-variable\">request</span>)</span><br>\t<span class=\"hljs-variable\">return</span> <span class=\"hljs-function\"><span class=\"hljs-title\">HttpResponse</span>(<span class=\"hljs-string\">&quot;xxxxx&quot;</span>)</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>启动django项目</strong></p>\n<ul>\n<li>命令行启动<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\"><span class=\"hljs-keyword\">python</span> manage.<span class=\"hljs-keyword\">py</span> runserver<br></code></pre></td></tr></table></figure></li>\n</ul>\n<p>在app1下新建一个templates目录,写入HTML模板,可以在views中用<code>render</code>函数直接获取</p>\n<figure class=\"highlight kotlin\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs kotlin\"><span class=\"hljs-keyword\">return</span> render(request, <span class=\"hljs-string\">&#x27;something.html&#x27;</span>)<br></code></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"创建第一个项目\"><a href=\"#创建第一个项目\" class=\"headerlink\" title=\"创建第一个项目\"></a>创建第一个项目</h3><figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ebnf\"><span class=\"hljs-attribute\">django-admin startprojct HelloWorld</span><br></code></pre></td></tr></table></figure>\n<p>创建一个Django项目名为HelloWorld，目录结构如下</p>\n<figure class=\"highlight gherkin\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs gherkin\">$ cd HelloWorld/<br>$ tree<br>.<br>|<span class=\"hljs-string\">-- HelloWorld</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- __init__.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- asgi.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- settings.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   </span>|<span class=\"hljs-string\">-- urls.py</span><br><span class=\"hljs-string\"></span>|<span class=\"hljs-string\">   `-- wsgi.py</span><br><span class=\"hljs-string\">`-- manage.py</span><br></code></pre></td></tr></table></figure>\n<p>目录说明：</p>\n<ul>\n<li><strong>HelloWorld:</strong> 项目的容器。</li>\n<li><strong>manage.py:</strong> 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。</li>\n<li><strong>HelloWorld&#x2F;<strong>init</strong>.py:</strong> 一个空文件，告诉 Python 该目录是一个 Python 包。</li>\n<li><strong>HelloWorld&#x2F;asgi.py:</strong> 一个 ASGI 兼容的 Web 服务器的入口，以便运行你的项目。</li>\n<li><strong>HelloWorld&#x2F;settings.py:</strong> 该 Django 项目的设置&#x2F;配置。</li>\n<li><strong>HelloWorld&#x2F;urls.py:</strong> 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站”目录”。</li>\n<li><strong>HelloWorld&#x2F;wsgi.py:</strong> 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。</li>\n</ul>\n<h4 id=\"启动服务器\"><a href=\"#启动服务器\" class=\"headerlink\" title=\"启动服务器\"></a>启动服务器</h4><figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nginx\"><span class=\"hljs-attribute\">python3</span> manage.py runserver <span class=\"hljs-number\">0.0.0.0:8000</span><br></code></pre></td></tr></table></figure>\n<p>0.0.0.0 让其它电脑可连接到开发服务器，8000 为端口号。如果不说明，那么端口号默认为 8000</p>\n<blockquote>\n<p>第一次打开网页出现报错：DisallowedHost at Invaild HTTP_HOST header： ‘0.0.0.0:8000’. You may need to add ‘0.0.0.0’ to ALLOWED_HOSTS.<br>解决：在配置文件settings.py中找到<code>ALLOWED_HOSTS</code>，将‘0.0.0.0’添加到其中</p>\n</blockquote>\n<h4 id=\"视图和URL配置\"><a href=\"#视图和URL配置\" class=\"headerlink\" title=\"视图和URL配置\"></a>视图和URL配置</h4><p>在<code>HelloWorld/HelloWorld/</code>目录下新建一个<code>views.py</code>文件，输入</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> django.http <span class=\"hljs-keyword\">import</span> HttpResponse<br> <br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">hello</span>(<span class=\"hljs-params\">request</span>):<br>    <span class=\"hljs-keyword\">return</span> HttpResponse(<span class=\"hljs-string\">&quot;Hello world ! &quot;</span>)<br></code></pre></td></tr></table></figure>\n\n<p>在urls.py文件中写入</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">from</span> django.urls <span class=\"hljs-keyword\">import</span> re_path <span class=\"hljs-keyword\">as</span> url  <br><span class=\"hljs-comment\"># from django.conf.urls import url 这段代码已经过时，上面的re_path取代了之前版本的url  </span><br><span class=\"hljs-keyword\">from</span> . <span class=\"hljs-keyword\">import</span> views  <br>  <br>urlpatterns = [  <br>    url(<span class=\"hljs-string\">r&#x27;^$&#x27;</span>, views.hello),  <br>]<br></code></pre></td></tr></table></figure>\n<p>可在网页中看到“Hello world！”的输出</p>\n<p><strong>path（）函数</strong>:</p>\n<figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\">path(route, <span class=\"hljs-keyword\">view</span>, kwargs=<span class=\"hljs-keyword\">None</span>, <span class=\"hljs-type\">name</span>=<span class=\"hljs-keyword\">None</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"创建app\"><a href=\"#创建app\" class=\"headerlink\" title=\"创建app\"></a>创建app</h3><figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nginx\"><span class=\"hljs-attribute\">python3</span> manage.py startapp app1  <span class=\"hljs-comment\"># 创建app1</span><br></code></pre></td></tr></table></figure>\n\n<p>目录结构如下：</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\">.<br>├── HelloWorld<br>│   ├── __init__.<span class=\"hljs-keyword\">py</span><br>│   ├── __pycache__<br>│   │   ├── __init__.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   ├── settings.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   ├── urls.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   ├── views.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   │   └── wsgi.cpython-<span class=\"hljs-number\">38</span>.pyc<br>│   ├── asgi.<span class=\"hljs-keyword\">py</span><br>│   ├── settings.<span class=\"hljs-keyword\">py</span><br>│   ├── urls.<span class=\"hljs-keyword\">py</span>  # 配置app用的url<br>│   ├── views.<span class=\"hljs-keyword\">py</span><br>│   └── wsgi.<span class=\"hljs-keyword\">py</span><br>├── app1<br>│   ├── __init__.<span class=\"hljs-keyword\">py</span><br>│   ├── admin.<span class=\"hljs-keyword\">py</span><br>│   ├── apps.<span class=\"hljs-keyword\">py</span><br>│   ├── migrations<br>│   │   └── __init__.<span class=\"hljs-keyword\">py</span><br>│   ├── models.<span class=\"hljs-keyword\">py</span>  # 对数据库进行操作<br>│   ├── tests.<span class=\"hljs-keyword\">py</span><br>│   └── views.<span class=\"hljs-keyword\">py</span>  # 视图函数<br>├── db.sqlite3<br>└── manage.<span class=\"hljs-keyword\">py</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>编写URL和视图函数的对应关系</strong></p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ini\"><span class=\"hljs-attr\">urlpatterns</span> = [  <br>    path(<span class=\"hljs-string\">&#x27;something/&#x27;</span>, views.something)  <br>]<br></code></pre></td></tr></table></figure>\n\n<p><strong>编写视图函数</strong></p>\n<figure class=\"highlight isbl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs isbl\"><span class=\"hljs-variable\">def</span> <span class=\"hljs-function\"><span class=\"hljs-title\">something</span>(<span class=\"hljs-variable\">request</span>)</span><br>\t<span class=\"hljs-variable\">return</span> <span class=\"hljs-function\"><span class=\"hljs-title\">HttpResponse</span>(<span class=\"hljs-string\">&quot;xxxxx&quot;</span>)</span><br></code></pre></td></tr></table></figure>\n\n<p><strong>启动django项目</strong></p>\n<ul>\n<li>命令行启动<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs vim\"><span class=\"hljs-keyword\">python</span> manage.<span class=\"hljs-keyword\">py</span> runserver<br></code></pre></td></tr></table></figure></li>\n</ul>\n<p>在app1下新建一个templates目录,写入HTML模板,可以在views中用<code>render</code>函数直接获取</p>\n<figure class=\"highlight kotlin\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs kotlin\"><span class=\"hljs-keyword\">return</span> render(request, <span class=\"hljs-string\">&#x27;something.html&#x27;</span>)<br></code></pre></td></tr></table></figure>"},{"title":"扩散模型论文todo","update":null,"_content":"检查句号后面空格\n\n\n论文检查：\n- [x] 论文中公式全部检查  公式文字部分也对应检查\n- [x] 图表全部检查  图表文字部分也对应检查\n- [x] 参考文献检查  **20和29参考文献重点检查**\n- [x] 学历补充\n- [x] word格式整理好\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203092747.png)\n![d5304720bd3b25b687d5db36f180fb3.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/d5304720bd3b25b687d5db36f180fb3.jpg)\n\n\n检查论文，12.4之前\n\n\n\n\n\n\n对比学习生成 **hardsample**\n自动数据增强\n识别出假数据\nhttps://arxiv.org/pdf/2207.00148.pdf\nhttps://arxiv.org/pdf/2010.04592v2.pdf\nhttps://epubs.siam.org/doi/epdf/10.1137/1.9781611977653.ch19\nhttps://ojs.aaai.org/index.php/AAAI/article/view/26071\nhttps://arxiv.org/pdf/2303.15161v3.pdf\nhttps://www.sciencedirect.com/science/article/abs/pii/S0031320323002121\nhttps://paperswithcode.com/paper/hard-sample-guided-hybrid-contrast-learning\nhttps://paperswithcode.com/paper/hard-sample-aware-noise-robust-learning-for\n### NEW\n\n![ff4ca0be44cbc807008ad5e9d025a40.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/ff4ca0be44cbc807008ad5e9d025a40.jpg)\n\n- [ ] 表一加个表头\n\n- [ ] 表三添加：auc已经到很高的数值，差距拉不开，意义不大\n\n- [ ] 所有数据换成通信数据,电信场景不同任务，网络优化、运维、用户进行实验，验证在不同任务下的效果\n\n- [ ] 基于文章写一个ppt\n12-15页有效内容\n\n11.29\nppt要求：刷一下ppt格式\n参考p8做p11\n\n背景 相关工作\n方法介绍\n数据集介绍\n哪几个实验\n\n用对比学习做GAN的判别器\n1. hardsample的生成结果进行对比\n2. hardsample怎么生成，普通生成也怎么生成\n3. 对这些样本构造一个图，从图中计算hardsample\n\n1. 基于对比学习\n2. DDPM在通信的应用\n3. 隐私保护的深入探索 看论文\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127150317.png)\n\n### IDEA\n#### 基于误分样本的数据重构\n之前调节分类器的参数，来提升评估效果\n现在可以尝试==调节生成模型的参数==来提升生成模型的效果\n\n#### 难分类样本\n难以分类但是分类正确的样本，提升权重\n之前是使用全部原始样本生成生成数据，然后从生成数据中挑选误分类样本。\n现在先评估原始数据，挑选出难分类样本，生成难分类样本生成数据，进行评估或者和原始数据融合\n\n#### 对比学习\n数据抽样\n\n将原始数据划分为正样本和负样本，可知正样本很少，存在严重的不平衡\n按照对比学习的思路，可以尝试在全部样本中进行两两一组的随机抽样，两个样本标签相同为正样本对，不同为负样本对\n\n#### 误分类样本数据重构 +数据规模\n考虑到真实数据量比较小\n我们可以生成大量数据\n以真实数据训练分类器，提取生成误分样本，并和一些生成样本融合，（相当于提升了数据规模）\n生成两份数量都为N的数据（N为原始数据规模的10-100倍），一份通过分类器得到误分类样本，与另一份融合，共同用于训练分类器\n\n#### 不同生成模型数据融合\n将扩散生成模型和 GAN 模型生成的数据进行融合用于共同训练分类器\n\n\n---\n## 融合生成数据和原始数据的特征表示？\n\n使用自适应的权重机制\n\n---\n\n- [x] 只考虑真实&生成，修改文字\n\n- [x] 增加大量的生成数据  50倍 30倍 70倍\n\n- [x] 表格：真实数据一条，每个模型的生成数据3条，绘成表格\n\n- [x] 相关工作：把数据增强放前面，客户流失放后面，重在罗列，要写哪篇文章\n- [x] 数据集来源换成网址\n- [x] 隐私部分表格修改\n\n---\n\n- [ ] 审稿意见标粗的 <font color=\"#ff0000\">其他联合损失函数</font>\n- [x] 表格格式问题\n- [x] 弱分类器强分类器表格加粗好结果\n- [x] 4.1生成数据量对效果的影响\n- [ ] 相关工作的文献引用\n- [x] 写修改说明，合并到一起\n\n\n---\n\n\n- [x] 相关工作第二部分\n- [ ] 引言部分的参考文献\n- [x] 加上LR的误分类，另外数据是不是贴反了@程梓航\n- [x] 序号\n- [ ] 方法介绍开始一直检查到\n- [x] 修改说明中 损失函数 数据量增大\n- [ ] 修改说明合完发过去\n- [ ] 4.1绿的复制 表四复制\n- [ ] 引言 和 相关工作【】里加参考文献\n\n\n- [ ] 图标边框\n- [x] 合参考文献 数据集划分 误分类\n- [ ] 公式 摘要\n\n\n有问题的论文序号：2，4，8，10，15，16，18，28，34，35\n## 生成数据量对效果的影响\n\n为了验证大规模的生成数据对效果的影响，我们使用扩散模型生成了数据集训练数据量10倍、50倍和100倍的数据，用来训练分类器Catboost，训练结果如下表所示。在电信数据集中，大量样本并不能显著提升分类器的性能，反而使结果略微下降。在银行数据集和电商数据集中，大规模样本使分类器性能得到提升，尤其是电商数据的表现提升显著，超越了真实数据的效果。三者都在数据规模达到原训练样本100倍时取得最好的效果。这表明通过增大数据集规模来提升模型效果在某些场景下是可行的。\n\n| real   | 0.8072 | 0.7388 | 0.8546 |     |     |        | 0.9701 | 0.9456 | 0.9948 |     |     |        | 0.9401 | 0.8737 | 0.9528 |\n| --------------- | ------ | ------ | ------ | ------ | --- | --- | ------ | ------ | ------ | ------ | --- | --- | ------ | ------ | ------ | ------ |\n| syn 1:5_fakeALL | 0.8051 | 0.7367 | 0.8542 |        |     |     | 0.9589 | 0.9241 | 0.9836 |        |     |     | 0.9049 | 0.8076 | 0.8893 |        |\n| 10x_fakeall     | 0.7277 | 0.7072 | 0.8384 |        |     |     | 0.9589 | 0.9239 | 0.9855 |        |     |     | 0.9331 | 0.8646 | 0.9511 |        |\n| 30x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     | 0.9322 | 0.8652 | 0.9556 |        |\n| 50x_fakeall     | 0.7227 | 0.702  | 0.844  |        |     |     | 0.9608 | 0.9279 | 0.9872 |        |     |     | 0.9349 | 0.8708 | 0.9543 |        |\n| 70x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     |        |        |        |        |\n| 100x_fakeall    | 0.7277 | 0.7076 | 0.8442 |        |     |     | 0.9623 | 0.9312 | 0.9882 |        |     |     | 0.934  | 0.8687 | 0.9562 |        |\n\n## 相关工作\n\n#### 数据增强方法\n数据增强是一种基于有限数据生成更多有效数据来扩充训练集，从而使得训练集训练出的模型效果得到提升的方法。目前，许多数据增强方法在图像生成等领域应用广泛。常用的方法有综合采样人工合成数据算法SMOTE[12]，无监督学习方法VAE（Variational Autoencoder）[32]，GAN[34]及其多种变体，以及扩散模型（Denoising Diffusion Probabilistic Model）[48]等。\nXiaojun Wu利用改进的 SMOTE 处理流失数据，结合过采样和欠采样方法解决流失数据的不平衡问题，对电子商务客户流失进行预测。（# E-commerce customer churn prediction based on improved SMOTE and AdaBoost）TVAE[33]在VAE的基础上提出了一个新的集成框架，结合深度度量学习（deep metric learning）来学习 VAE 中的潜在嵌入。MedGAN将一个自编码器与GAN结合，可以在图像层面实现端到端的生成任务，可以生成连续和离散变量，被用于EHR（electronic health record）数据的生成。CrGAN-Cnet将Cramér Distance和Cross-Net architecture整合到算法中，用于航空公司乘客姓名记录生成（Airline Passenger Name Record Generation），此外还能生成数据来填补数据表格中的缺失值。IRGAN首次将在CV领域中广泛应用的GAN引入到信息检索，利用强化学习，创造性地解决了GAN在离散领域的适用问题。TableGAN将分类器和信息损失引入GAN，并在生成器、鉴别器和分类器中应用卷积神经网络（CNN）。另外，在表格数据增强中，隐私保护能力也是一个重要的因素。PATE-GAN提出了一个生成具有差分隐私保证数据的框架。\n上述基于GAN的算法在生成特定类别数据上存在很大不足，而Conditional GAN（CGAN）可以有效控制生成数据，因而被广泛应用。CGAN在生成器和判别器的输入中增加了额外的条件信息，限制了生成数据的生成类型。CW-GAN将Wasserstein距离应用到CGAN中，利用条件向量对少数类进行过采样，以解决表格数据生成不平衡的问题。CTGAN将 PacGAN结构集成到它的鉴别器中，并使用 WGAN 损失加上梯度惩罚来训练一个条件 GAN 框架。CTAB-GAN通过数据编码连续和分类变量的混合数据类型，以及对长尾连续变量有效建模。\n2020年，DDPM（Denoising Diffusion Probabilistic Model）被提出，被称为扩散模型，在图像生成领域广泛应用。扩散模型包括两个过程：前向过程和反向过程，其中前向过程又称为扩散过程。无论是前向过程还是反向过程都是一个参数化的马尔可夫链。后来学者们改进DDPM（引用Improved Denoising Diffusion Probabilistic Models），进一步增强其生成效果。\n\n#### 客户流失预估： \n在过去的研究中，学者们对客户流失预测在多种领域进行了大量的探索，包括电信客户流失、银行理财客户流失、新兴互联网行业客户流失，如电商、直播、旅游产品等。预测方法也逐渐成熟，形成了以机器学习算法为主流的现状。从发展历程上主要分为两个阶段。\n第一阶段是传统统计学预测方法，主要包括决策树（decision tree, DT）、逻辑回归（logistic regression，LR）、贝叶斯分类器（bayesian）、支持向量机（support vector machine，SVM）等算法。早在2007年，Luo Bin就使用决策树来预测手机服务中的客户流失（# Customer Churn Prediction Based on the Decision Tree in Personal Handyphone System Service）。同年，针对实际客户流失数据中正负样本数量不平衡而且数据量大的特点,Ying Weiyun提出带有不同类权重参数的支持向量机算法CW-SVM来预测客户流失,通过调整类权重参数改变分类面位置,提高算法分类准确性[6]。近些年电子商务发展迅速，客户激增，Qiu Yanfang使用逻辑回归来预测电子商务场景下的客户流失[4]。在[3]中，Hemlata Jain将逻辑回归和logit boost结合起来，在美国电信公司Orange的数据集上表现良好。在[5]中，Guangli Nie分别使用决策树和逻辑回归对某银行信用卡用户流失数据做预测。Arno De Caigny在其论文中将决策树和逻辑回归结合提出了一种新的混合算法，logit leaf model (LLM) ，以更好地对数据进行分类。（# A new hybrid classification algorithm for customer churn prediction based on logistic regression and decision trees）。这一阶段的客户流失预测方法主要集中在将这些基础算法和手动特征工程结合对客户流失进行建模[1-3, 6-7]。\n第二阶段是客户流失预测场景中集成学习算法的垄断和深度学习的初步探索。随着集成学习的引入，随机森林（random forest，RF）、梯度提升决策树（gradient boosting decision tree，GBDT）、Adaboost和Stack等方法被大量引入到对客户流失预测中，包括在电信、银行、互联网等场景。特别是GBDT，由于其算法具有很好的性能被客户流失预测广泛应用。\nYaya Xie提出了一种基于随机森林的学习方法，称为改进的平衡随机森林(improved balanced random forests，IBRF)，通过改变类的分布和对少数类的错误分类施加更高的惩罚来迭代学习最佳特征，并将该方法应用于某银行客户流失数据集。（# Customer churn prediction using improved balanced random forests）Liang Jiafu提出了一种基于梯度提升决策树算法(GBDT)和逻辑回归(LR)算法的用户流失预防模型,对参数特征进行调整,对已有移动用户流失数据进行计算,识别速度和准确率均拥有好的效果[8]。Zengyuan Wu提出 PCA-AdaBoost 模型，采用主成分分析来减少数据维度，使用AdaBoost对多个决策树进行级联，以最小化不平衡数据的影响。该模型在kaggle的电子商务数据集上证明了模型的有效性。（# A PCA-AdaBoost model for E-commerce customer churn prediction）\n在深度学习领域，Ebru Pekel Ozmen改进卷积神经网络算法，提出了一种新的混合扩展卷积决策树模型(ECDT)，将模型应用于零售业员工流失预测的数据集[10]。[11]提出了一个利用交易数据预测银行客户流失的框架,将GRU和bi-directional LSTM应用于客户流失预测。这一阶段的客户流失方法开始尝试将一些机器学习方法集成起来，或是使用流行的深度学习方法，用更复杂的模型学习和预测流失数据。\n整体上，客户流失预测以机器学习算法为主，并发展出集成学习等不同模型，深度学习在该领域也有一定应用。预测领域也从电信用户流失逐渐发展到银行信用卡用户流失、电商用户流失等多个领域。\n\n\n\n\n\n\n## 工作\n**1** 多处需要增加引用文献，例如：Theil’s U 统计量，Correlation Ratio，SHAP（SHapley Additive exPlanations）\n解决：Theil’s U 统计量，Correlation Ratio可以直接引用CTABGAN\n\t\t  SHAP： [A Unified Approach to Interpreting Model Predictions]([[1705.07874v2] A Unified Approach to Interpreting Model Predictions (arxiv.org)](https://arxiv.org/abs/1705.07874v2))\n\t\t  \n**2** 论文之中提到的 kaggle 数据集可以给出相应的链接。 @ 正阳 （找链接，试着参考论文里）\n[Telco Customer Churn | Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)\n[Credit Card customers | Kaggle](https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers)\n[直播电商数据集_数据集-阿里云天池 (aliyun.com)](https://tianchi.aliyun.com/dataset/124814)\n\n3 电信数据集，每个模型找五条\n\nREAL：\n\nMale  0 No No  2  Yes  No  DSL  Yes  Yes  No  No  No  No  Month-to-month  Yes  Mailed check  53.85  108.15\n\nFemale  0  No No 2 Yes No Fiber optic No No No No No No Month-to-month Yes Electronic check 70.7 151.65 \n\nFemale 0 No No 8 Yes Yes Fiber optic No No Yes No Yes Yes Month-to-month Yes Electronic check 99.65 820.5\n\nMale 1 No No 1 No No phone service DSL No No Yes No No Yes Month-to-month Yes Electronic check 39.65 39.65\n\nMale 0 No No 49 Yes Yes Fiber optic No Yes Yes No Yes Yes Month-to-month Yes Bank transfer(automatic) 103.7 5036.3\n\nDDPM：\n'Male' '1' 'Yes' 'No'  4.0 'Yes' 'No' 'Fiber optic' 'No' 'No' 'Yes' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check'\n  92.81641778046422 347.77240745447966\n  \n 'Male' '0' 'Yes' 'No' 1.0 'Yes' 'Yes' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check' \n  95.8100172795924 135.30996333735123\n\n 'Female' '0' 'No' 'No' 12.89970293412554 'Yes' 'No' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Bank transfer (automatic)'\n   90.24851621598897 1131.933374934577\n  \n 'Male' '0' 'No' 'No' 15.0 'Yes' 'Yes' 'Fiber optic' 'No' 'Yes' 'No' 'No'\n  'No' 'Yes' 'Month-to-month' 'Yes' 'Credit card (automatic)' \n  89.23206588556003 1370.882200227259\n \n 'Female' '1' 'No' 'No' 1.0 'Yes' 'Yes' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'No' 'No' 'Month-to-month' 'Yes' 'Electronic check' \n  75.70813933778831 75.53547210781328\n\nSMOTE：\n'Female' '0' 'No' 'No' 6.058020806965098 'Yes' 'No' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check' \n  89.30362630043531 581.5998093801566\n  \n 'Female' '0' 'Yes' 'No' 34.16695916300951 'Yes' 'Yes' 'Fiber optic' 'Yes' 'No' 'Yes' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check' \n  102.47469287884003 3553.063238271951\n  \n 'Female' '0' 'No' 'No' 10.409424239667828 'Yes' 'Yes' 'DSL' 'No' 'No' 'No' 'No' 'No' 'Yes'\n  'Month-to-month' 'No' 'Electronic check' \n  72.89683257375917 749.7780225143397\n  \n 'Male' '0' 'Yes' 'No' 68.0 'Yes' 'Yes' 'Fiber optic' 'Yes' 'Yes' 'Yes' 'Yes'\n  'Yes' 'Yes' 'Two year' 'Yes' 'Credit card (automatic)' \n  111.84516691088298 7754.865079775263\n  \n 'Male' '0' 'No' 'No' 57.53490684507468 'No' 'No phone service' 'DSL' 'No' 'Yes' 'Yes'\n  'No' 'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check'\n   52.08537000221106 2966.5298068815946\n\nCTABGAN：\n'Female' '1' 'Yes' 'No' 25.813646159053306 'Yes' 'No' 'Fiber optic' 'No' 'Yes' 'No' 'No'\n  'No' 'No' 'Month-to-month' 'Yes' 'Credit card (automatic)'\n   72.4520499161782 107.12076519599646\n  \n 'Male' '0' 'No' 'Yes' 7.578428442708454 'Yes' 'No' 'DSL' 'No' 'No' 'No' 'No' 'No' 'No'\n  'Month-to-month' 'No' 'Mailed check' \n  45.84371280675952 105.69947914250504\n  \n 'Male' '0' 'No' 'No'  1.9809866925261472 'Yes' 'Yes' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'No' 'Month-to-month' 'Yes' 'Bank transfer (automatic)'\n   81.46474915190095 40.94928790189158\n  \n 'Female' '1' 'No' 'Yes' 26.57383830887195 'Yes' 'No phone service' 'DSL' 'Yes' 'Yes'\n  'Yes' 'No' 'Yes' 'Yes' 'Month-to-month' 'No' 'Electronic check'\n   57.69859110911275 781.0754735958174\n  \n 'Male' '0' 'Yes' 'No'  28.8168545923753 'Yes' 'Yes' 'DSL' 'No' 'Yes' 'No' 'Yes' 'Yes'\n  'Yes' 'Month-to-month' 'No' 'Credit card (automatic)' \n  77.30306219710255 756.2990340865898\n\n\n\n\nIn the field of data mining, there are commonly encountered issues of data imbalance and inadequate protection of user privacy data. These issues can have a detrimental impact on the accuracy of model predictions and the applicability of models in privacy-preserving scenarios. Therefore, the generation of new data has emerged as a prominent solution in addressing these challenges.  \nHowever, generating high-quality data poses certain challenges in the field of data mining, which primarily deals with structured data characterized by a high number of dimensions and unrelated features.  \nConsidering the successful application of diffusion models in tasks such as image generation, this study attempts to apply diffusion models to the task of customer churn prediction.  \nThis paper employs Gaussian diffusion models and polynomial diffusion models to generate data for numerical and categorical features in churn data. It also conducts research and analysis on the predictive performance and privacy protection capabilities of these models.  \nWe conducted extensive experiments on customer churn data from multiple domains to explore the potential of merging synthetic data with real data for reconstruction.  \nThe experimental results indicate that the adoption of the diffusion model enables the generation of high-quality data. Furthermore, the generated data exhibits improvements across various customer churn prediction methods, with a particular advantage observed for weak classifiers. This approach also effectively mitigates the issue of data imbalance.  \nMoreover, the data generated by the diffusion model exhibits a distribution that is closer to real data, thereby possessing potential value in preserving customer privacy.\n\n\n\n\n","source":"_posts/Notes/论文/扩散模型论文todo.md","raw":"---\ntitle: 扩散模型论文todo\ncategories:\n  - Notes\n  - 论文\nupdate: \ntags:\n  - 扩散模型\n---\n检查句号后面空格\n\n\n论文检查：\n- [x] 论文中公式全部检查  公式文字部分也对应检查\n- [x] 图表全部检查  图表文字部分也对应检查\n- [x] 参考文献检查  **20和29参考文献重点检查**\n- [x] 学历补充\n- [x] word格式整理好\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203092747.png)\n![d5304720bd3b25b687d5db36f180fb3.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/d5304720bd3b25b687d5db36f180fb3.jpg)\n\n\n检查论文，12.4之前\n\n\n\n\n\n\n对比学习生成 **hardsample**\n自动数据增强\n识别出假数据\nhttps://arxiv.org/pdf/2207.00148.pdf\nhttps://arxiv.org/pdf/2010.04592v2.pdf\nhttps://epubs.siam.org/doi/epdf/10.1137/1.9781611977653.ch19\nhttps://ojs.aaai.org/index.php/AAAI/article/view/26071\nhttps://arxiv.org/pdf/2303.15161v3.pdf\nhttps://www.sciencedirect.com/science/article/abs/pii/S0031320323002121\nhttps://paperswithcode.com/paper/hard-sample-guided-hybrid-contrast-learning\nhttps://paperswithcode.com/paper/hard-sample-aware-noise-robust-learning-for\n### NEW\n\n![ff4ca0be44cbc807008ad5e9d025a40.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/ff4ca0be44cbc807008ad5e9d025a40.jpg)\n\n- [ ] 表一加个表头\n\n- [ ] 表三添加：auc已经到很高的数值，差距拉不开，意义不大\n\n- [ ] 所有数据换成通信数据,电信场景不同任务，网络优化、运维、用户进行实验，验证在不同任务下的效果\n\n- [ ] 基于文章写一个ppt\n12-15页有效内容\n\n11.29\nppt要求：刷一下ppt格式\n参考p8做p11\n\n背景 相关工作\n方法介绍\n数据集介绍\n哪几个实验\n\n用对比学习做GAN的判别器\n1. hardsample的生成结果进行对比\n2. hardsample怎么生成，普通生成也怎么生成\n3. 对这些样本构造一个图，从图中计算hardsample\n\n1. 基于对比学习\n2. DDPM在通信的应用\n3. 隐私保护的深入探索 看论文\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127150317.png)\n\n### IDEA\n#### 基于误分样本的数据重构\n之前调节分类器的参数，来提升评估效果\n现在可以尝试==调节生成模型的参数==来提升生成模型的效果\n\n#### 难分类样本\n难以分类但是分类正确的样本，提升权重\n之前是使用全部原始样本生成生成数据，然后从生成数据中挑选误分类样本。\n现在先评估原始数据，挑选出难分类样本，生成难分类样本生成数据，进行评估或者和原始数据融合\n\n#### 对比学习\n数据抽样\n\n将原始数据划分为正样本和负样本，可知正样本很少，存在严重的不平衡\n按照对比学习的思路，可以尝试在全部样本中进行两两一组的随机抽样，两个样本标签相同为正样本对，不同为负样本对\n\n#### 误分类样本数据重构 +数据规模\n考虑到真实数据量比较小\n我们可以生成大量数据\n以真实数据训练分类器，提取生成误分样本，并和一些生成样本融合，（相当于提升了数据规模）\n生成两份数量都为N的数据（N为原始数据规模的10-100倍），一份通过分类器得到误分类样本，与另一份融合，共同用于训练分类器\n\n#### 不同生成模型数据融合\n将扩散生成模型和 GAN 模型生成的数据进行融合用于共同训练分类器\n\n\n---\n## 融合生成数据和原始数据的特征表示？\n\n使用自适应的权重机制\n\n---\n\n- [x] 只考虑真实&生成，修改文字\n\n- [x] 增加大量的生成数据  50倍 30倍 70倍\n\n- [x] 表格：真实数据一条，每个模型的生成数据3条，绘成表格\n\n- [x] 相关工作：把数据增强放前面，客户流失放后面，重在罗列，要写哪篇文章\n- [x] 数据集来源换成网址\n- [x] 隐私部分表格修改\n\n---\n\n- [ ] 审稿意见标粗的 <font color=\"#ff0000\">其他联合损失函数</font>\n- [x] 表格格式问题\n- [x] 弱分类器强分类器表格加粗好结果\n- [x] 4.1生成数据量对效果的影响\n- [ ] 相关工作的文献引用\n- [x] 写修改说明，合并到一起\n\n\n---\n\n\n- [x] 相关工作第二部分\n- [ ] 引言部分的参考文献\n- [x] 加上LR的误分类，另外数据是不是贴反了@程梓航\n- [x] 序号\n- [ ] 方法介绍开始一直检查到\n- [x] 修改说明中 损失函数 数据量增大\n- [ ] 修改说明合完发过去\n- [ ] 4.1绿的复制 表四复制\n- [ ] 引言 和 相关工作【】里加参考文献\n\n\n- [ ] 图标边框\n- [x] 合参考文献 数据集划分 误分类\n- [ ] 公式 摘要\n\n\n有问题的论文序号：2，4，8，10，15，16，18，28，34，35\n## 生成数据量对效果的影响\n\n为了验证大规模的生成数据对效果的影响，我们使用扩散模型生成了数据集训练数据量10倍、50倍和100倍的数据，用来训练分类器Catboost，训练结果如下表所示。在电信数据集中，大量样本并不能显著提升分类器的性能，反而使结果略微下降。在银行数据集和电商数据集中，大规模样本使分类器性能得到提升，尤其是电商数据的表现提升显著，超越了真实数据的效果。三者都在数据规模达到原训练样本100倍时取得最好的效果。这表明通过增大数据集规模来提升模型效果在某些场景下是可行的。\n\n| real   | 0.8072 | 0.7388 | 0.8546 |     |     |        | 0.9701 | 0.9456 | 0.9948 |     |     |        | 0.9401 | 0.8737 | 0.9528 |\n| --------------- | ------ | ------ | ------ | ------ | --- | --- | ------ | ------ | ------ | ------ | --- | --- | ------ | ------ | ------ | ------ |\n| syn 1:5_fakeALL | 0.8051 | 0.7367 | 0.8542 |        |     |     | 0.9589 | 0.9241 | 0.9836 |        |     |     | 0.9049 | 0.8076 | 0.8893 |        |\n| 10x_fakeall     | 0.7277 | 0.7072 | 0.8384 |        |     |     | 0.9589 | 0.9239 | 0.9855 |        |     |     | 0.9331 | 0.8646 | 0.9511 |        |\n| 30x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     | 0.9322 | 0.8652 | 0.9556 |        |\n| 50x_fakeall     | 0.7227 | 0.702  | 0.844  |        |     |     | 0.9608 | 0.9279 | 0.9872 |        |     |     | 0.9349 | 0.8708 | 0.9543 |        |\n| 70x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     |        |        |        |        |\n| 100x_fakeall    | 0.7277 | 0.7076 | 0.8442 |        |     |     | 0.9623 | 0.9312 | 0.9882 |        |     |     | 0.934  | 0.8687 | 0.9562 |        |\n\n## 相关工作\n\n#### 数据增强方法\n数据增强是一种基于有限数据生成更多有效数据来扩充训练集，从而使得训练集训练出的模型效果得到提升的方法。目前，许多数据增强方法在图像生成等领域应用广泛。常用的方法有综合采样人工合成数据算法SMOTE[12]，无监督学习方法VAE（Variational Autoencoder）[32]，GAN[34]及其多种变体，以及扩散模型（Denoising Diffusion Probabilistic Model）[48]等。\nXiaojun Wu利用改进的 SMOTE 处理流失数据，结合过采样和欠采样方法解决流失数据的不平衡问题，对电子商务客户流失进行预测。（# E-commerce customer churn prediction based on improved SMOTE and AdaBoost）TVAE[33]在VAE的基础上提出了一个新的集成框架，结合深度度量学习（deep metric learning）来学习 VAE 中的潜在嵌入。MedGAN将一个自编码器与GAN结合，可以在图像层面实现端到端的生成任务，可以生成连续和离散变量，被用于EHR（electronic health record）数据的生成。CrGAN-Cnet将Cramér Distance和Cross-Net architecture整合到算法中，用于航空公司乘客姓名记录生成（Airline Passenger Name Record Generation），此外还能生成数据来填补数据表格中的缺失值。IRGAN首次将在CV领域中广泛应用的GAN引入到信息检索，利用强化学习，创造性地解决了GAN在离散领域的适用问题。TableGAN将分类器和信息损失引入GAN，并在生成器、鉴别器和分类器中应用卷积神经网络（CNN）。另外，在表格数据增强中，隐私保护能力也是一个重要的因素。PATE-GAN提出了一个生成具有差分隐私保证数据的框架。\n上述基于GAN的算法在生成特定类别数据上存在很大不足，而Conditional GAN（CGAN）可以有效控制生成数据，因而被广泛应用。CGAN在生成器和判别器的输入中增加了额外的条件信息，限制了生成数据的生成类型。CW-GAN将Wasserstein距离应用到CGAN中，利用条件向量对少数类进行过采样，以解决表格数据生成不平衡的问题。CTGAN将 PacGAN结构集成到它的鉴别器中，并使用 WGAN 损失加上梯度惩罚来训练一个条件 GAN 框架。CTAB-GAN通过数据编码连续和分类变量的混合数据类型，以及对长尾连续变量有效建模。\n2020年，DDPM（Denoising Diffusion Probabilistic Model）被提出，被称为扩散模型，在图像生成领域广泛应用。扩散模型包括两个过程：前向过程和反向过程，其中前向过程又称为扩散过程。无论是前向过程还是反向过程都是一个参数化的马尔可夫链。后来学者们改进DDPM（引用Improved Denoising Diffusion Probabilistic Models），进一步增强其生成效果。\n\n#### 客户流失预估： \n在过去的研究中，学者们对客户流失预测在多种领域进行了大量的探索，包括电信客户流失、银行理财客户流失、新兴互联网行业客户流失，如电商、直播、旅游产品等。预测方法也逐渐成熟，形成了以机器学习算法为主流的现状。从发展历程上主要分为两个阶段。\n第一阶段是传统统计学预测方法，主要包括决策树（decision tree, DT）、逻辑回归（logistic regression，LR）、贝叶斯分类器（bayesian）、支持向量机（support vector machine，SVM）等算法。早在2007年，Luo Bin就使用决策树来预测手机服务中的客户流失（# Customer Churn Prediction Based on the Decision Tree in Personal Handyphone System Service）。同年，针对实际客户流失数据中正负样本数量不平衡而且数据量大的特点,Ying Weiyun提出带有不同类权重参数的支持向量机算法CW-SVM来预测客户流失,通过调整类权重参数改变分类面位置,提高算法分类准确性[6]。近些年电子商务发展迅速，客户激增，Qiu Yanfang使用逻辑回归来预测电子商务场景下的客户流失[4]。在[3]中，Hemlata Jain将逻辑回归和logit boost结合起来，在美国电信公司Orange的数据集上表现良好。在[5]中，Guangli Nie分别使用决策树和逻辑回归对某银行信用卡用户流失数据做预测。Arno De Caigny在其论文中将决策树和逻辑回归结合提出了一种新的混合算法，logit leaf model (LLM) ，以更好地对数据进行分类。（# A new hybrid classification algorithm for customer churn prediction based on logistic regression and decision trees）。这一阶段的客户流失预测方法主要集中在将这些基础算法和手动特征工程结合对客户流失进行建模[1-3, 6-7]。\n第二阶段是客户流失预测场景中集成学习算法的垄断和深度学习的初步探索。随着集成学习的引入，随机森林（random forest，RF）、梯度提升决策树（gradient boosting decision tree，GBDT）、Adaboost和Stack等方法被大量引入到对客户流失预测中，包括在电信、银行、互联网等场景。特别是GBDT，由于其算法具有很好的性能被客户流失预测广泛应用。\nYaya Xie提出了一种基于随机森林的学习方法，称为改进的平衡随机森林(improved balanced random forests，IBRF)，通过改变类的分布和对少数类的错误分类施加更高的惩罚来迭代学习最佳特征，并将该方法应用于某银行客户流失数据集。（# Customer churn prediction using improved balanced random forests）Liang Jiafu提出了一种基于梯度提升决策树算法(GBDT)和逻辑回归(LR)算法的用户流失预防模型,对参数特征进行调整,对已有移动用户流失数据进行计算,识别速度和准确率均拥有好的效果[8]。Zengyuan Wu提出 PCA-AdaBoost 模型，采用主成分分析来减少数据维度，使用AdaBoost对多个决策树进行级联，以最小化不平衡数据的影响。该模型在kaggle的电子商务数据集上证明了模型的有效性。（# A PCA-AdaBoost model for E-commerce customer churn prediction）\n在深度学习领域，Ebru Pekel Ozmen改进卷积神经网络算法，提出了一种新的混合扩展卷积决策树模型(ECDT)，将模型应用于零售业员工流失预测的数据集[10]。[11]提出了一个利用交易数据预测银行客户流失的框架,将GRU和bi-directional LSTM应用于客户流失预测。这一阶段的客户流失方法开始尝试将一些机器学习方法集成起来，或是使用流行的深度学习方法，用更复杂的模型学习和预测流失数据。\n整体上，客户流失预测以机器学习算法为主，并发展出集成学习等不同模型，深度学习在该领域也有一定应用。预测领域也从电信用户流失逐渐发展到银行信用卡用户流失、电商用户流失等多个领域。\n\n\n\n\n\n\n## 工作\n**1** 多处需要增加引用文献，例如：Theil’s U 统计量，Correlation Ratio，SHAP（SHapley Additive exPlanations）\n解决：Theil’s U 统计量，Correlation Ratio可以直接引用CTABGAN\n\t\t  SHAP： [A Unified Approach to Interpreting Model Predictions]([[1705.07874v2] A Unified Approach to Interpreting Model Predictions (arxiv.org)](https://arxiv.org/abs/1705.07874v2))\n\t\t  \n**2** 论文之中提到的 kaggle 数据集可以给出相应的链接。 @ 正阳 （找链接，试着参考论文里）\n[Telco Customer Churn | Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)\n[Credit Card customers | Kaggle](https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers)\n[直播电商数据集_数据集-阿里云天池 (aliyun.com)](https://tianchi.aliyun.com/dataset/124814)\n\n3 电信数据集，每个模型找五条\n\nREAL：\n\nMale  0 No No  2  Yes  No  DSL  Yes  Yes  No  No  No  No  Month-to-month  Yes  Mailed check  53.85  108.15\n\nFemale  0  No No 2 Yes No Fiber optic No No No No No No Month-to-month Yes Electronic check 70.7 151.65 \n\nFemale 0 No No 8 Yes Yes Fiber optic No No Yes No Yes Yes Month-to-month Yes Electronic check 99.65 820.5\n\nMale 1 No No 1 No No phone service DSL No No Yes No No Yes Month-to-month Yes Electronic check 39.65 39.65\n\nMale 0 No No 49 Yes Yes Fiber optic No Yes Yes No Yes Yes Month-to-month Yes Bank transfer(automatic) 103.7 5036.3\n\nDDPM：\n'Male' '1' 'Yes' 'No'  4.0 'Yes' 'No' 'Fiber optic' 'No' 'No' 'Yes' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check'\n  92.81641778046422 347.77240745447966\n  \n 'Male' '0' 'Yes' 'No' 1.0 'Yes' 'Yes' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check' \n  95.8100172795924 135.30996333735123\n\n 'Female' '0' 'No' 'No' 12.89970293412554 'Yes' 'No' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Bank transfer (automatic)'\n   90.24851621598897 1131.933374934577\n  \n 'Male' '0' 'No' 'No' 15.0 'Yes' 'Yes' 'Fiber optic' 'No' 'Yes' 'No' 'No'\n  'No' 'Yes' 'Month-to-month' 'Yes' 'Credit card (automatic)' \n  89.23206588556003 1370.882200227259\n \n 'Female' '1' 'No' 'No' 1.0 'Yes' 'Yes' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'No' 'No' 'Month-to-month' 'Yes' 'Electronic check' \n  75.70813933778831 75.53547210781328\n\nSMOTE：\n'Female' '0' 'No' 'No' 6.058020806965098 'Yes' 'No' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check' \n  89.30362630043531 581.5998093801566\n  \n 'Female' '0' 'Yes' 'No' 34.16695916300951 'Yes' 'Yes' 'Fiber optic' 'Yes' 'No' 'Yes' 'No'\n  'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check' \n  102.47469287884003 3553.063238271951\n  \n 'Female' '0' 'No' 'No' 10.409424239667828 'Yes' 'Yes' 'DSL' 'No' 'No' 'No' 'No' 'No' 'Yes'\n  'Month-to-month' 'No' 'Electronic check' \n  72.89683257375917 749.7780225143397\n  \n 'Male' '0' 'Yes' 'No' 68.0 'Yes' 'Yes' 'Fiber optic' 'Yes' 'Yes' 'Yes' 'Yes'\n  'Yes' 'Yes' 'Two year' 'Yes' 'Credit card (automatic)' \n  111.84516691088298 7754.865079775263\n  \n 'Male' '0' 'No' 'No' 57.53490684507468 'No' 'No phone service' 'DSL' 'No' 'Yes' 'Yes'\n  'No' 'Yes' 'Yes' 'Month-to-month' 'Yes' 'Electronic check'\n   52.08537000221106 2966.5298068815946\n\nCTABGAN：\n'Female' '1' 'Yes' 'No' 25.813646159053306 'Yes' 'No' 'Fiber optic' 'No' 'Yes' 'No' 'No'\n  'No' 'No' 'Month-to-month' 'Yes' 'Credit card (automatic)'\n   72.4520499161782 107.12076519599646\n  \n 'Male' '0' 'No' 'Yes' 7.578428442708454 'Yes' 'No' 'DSL' 'No' 'No' 'No' 'No' 'No' 'No'\n  'Month-to-month' 'No' 'Mailed check' \n  45.84371280675952 105.69947914250504\n  \n 'Male' '0' 'No' 'No'  1.9809866925261472 'Yes' 'Yes' 'Fiber optic' 'No' 'No' 'No' 'No'\n  'Yes' 'No' 'Month-to-month' 'Yes' 'Bank transfer (automatic)'\n   81.46474915190095 40.94928790189158\n  \n 'Female' '1' 'No' 'Yes' 26.57383830887195 'Yes' 'No phone service' 'DSL' 'Yes' 'Yes'\n  'Yes' 'No' 'Yes' 'Yes' 'Month-to-month' 'No' 'Electronic check'\n   57.69859110911275 781.0754735958174\n  \n 'Male' '0' 'Yes' 'No'  28.8168545923753 'Yes' 'Yes' 'DSL' 'No' 'Yes' 'No' 'Yes' 'Yes'\n  'Yes' 'Month-to-month' 'No' 'Credit card (automatic)' \n  77.30306219710255 756.2990340865898\n\n\n\n\nIn the field of data mining, there are commonly encountered issues of data imbalance and inadequate protection of user privacy data. These issues can have a detrimental impact on the accuracy of model predictions and the applicability of models in privacy-preserving scenarios. Therefore, the generation of new data has emerged as a prominent solution in addressing these challenges.  \nHowever, generating high-quality data poses certain challenges in the field of data mining, which primarily deals with structured data characterized by a high number of dimensions and unrelated features.  \nConsidering the successful application of diffusion models in tasks such as image generation, this study attempts to apply diffusion models to the task of customer churn prediction.  \nThis paper employs Gaussian diffusion models and polynomial diffusion models to generate data for numerical and categorical features in churn data. It also conducts research and analysis on the predictive performance and privacy protection capabilities of these models.  \nWe conducted extensive experiments on customer churn data from multiple domains to explore the potential of merging synthetic data with real data for reconstruction.  \nThe experimental results indicate that the adoption of the diffusion model enables the generation of high-quality data. Furthermore, the generated data exhibits improvements across various customer churn prediction methods, with a particular advantage observed for weak classifiers. This approach also effectively mitigates the issue of data imbalance.  \nMoreover, the data generated by the diffusion model exhibits a distribution that is closer to real data, thereby possessing potential value in preserving customer privacy.\n\n\n\n\n","slug":"Notes/论文/扩散模型论文todo","published":1,"date":"2023-09-21T16:36:34.336Z","updated":"2024-01-18T04:59:15.158Z","_id":"clo5xc63m000008u57cfg4n7d","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>检查句号后面空格</p>\n<p>论文检查：</p>\n<ul>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 论文中公式全部检查  公式文字部分也对应检查</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 图表全部检查  图表文字部分也对应检查</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 参考文献检查  <strong>20和29参考文献重点检查</strong></li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 学历补充</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> word格式整理好</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203092747.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/d5304720bd3b25b687d5db36f180fb3.jpg\" alt=\"d5304720bd3b25b687d5db36f180fb3.jpg\"></p>\n<p>检查论文，12.4之前</p>\n<p>对比学习生成 <strong>hardsample</strong><br>自动数据增强<br>识别出假数据<br><a href=\"https://arxiv.org/pdf/2207.00148.pdf\">https://arxiv.org/pdf/2207.00148.pdf</a><br><a href=\"https://arxiv.org/pdf/2010.04592v2.pdf\">https://arxiv.org/pdf/2010.04592v2.pdf</a><br><a href=\"https://epubs.siam.org/doi/epdf/10.1137/1.9781611977653.ch19\">https://epubs.siam.org/doi/epdf/10.1137/1.9781611977653.ch19</a><br><a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/26071\">https://ojs.aaai.org/index.php/AAAI/article/view/26071</a><br><a href=\"https://arxiv.org/pdf/2303.15161v3.pdf\">https://arxiv.org/pdf/2303.15161v3.pdf</a><br><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0031320323002121\">https://www.sciencedirect.com/science/article/abs/pii/S0031320323002121</a><br><a href=\"https://paperswithcode.com/paper/hard-sample-guided-hybrid-contrast-learning\">https://paperswithcode.com/paper/hard-sample-guided-hybrid-contrast-learning</a><br><a href=\"https://paperswithcode.com/paper/hard-sample-aware-noise-robust-learning-for\">https://paperswithcode.com/paper/hard-sample-aware-noise-robust-learning-for</a></p>\n<h3 id=\"NEW\"><a href=\"#NEW\" class=\"headerlink\" title=\"NEW\"></a>NEW</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/ff4ca0be44cbc807008ad5e9d025a40.jpg\" alt=\"ff4ca0be44cbc807008ad5e9d025a40.jpg\"></p>\n<ul>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n表一加个表头</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n表三添加：auc已经到很高的数值，差距拉不开，意义不大</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n所有数据换成通信数据,电信场景不同任务，网络优化、运维、用户进行实验，验证在不同任务下的效果</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n基于文章写一个ppt<br>12-15页有效内容</p>\n</li>\n</ul>\n<p>11.29<br>ppt要求：刷一下ppt格式<br>参考p8做p11</p>\n<p>背景 相关工作<br>方法介绍<br>数据集介绍<br>哪几个实验</p>\n<p>用对比学习做GAN的判别器</p>\n<ol>\n<li><p>hardsample的生成结果进行对比</p>\n</li>\n<li><p>hardsample怎么生成，普通生成也怎么生成</p>\n</li>\n<li><p>对这些样本构造一个图，从图中计算hardsample</p>\n</li>\n<li><p>基于对比学习</p>\n</li>\n<li><p>DDPM在通信的应用</p>\n</li>\n<li><p>隐私保护的深入探索 看论文<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127150317.png\" alt=\"image.png\"></p>\n</li>\n</ol>\n<h3 id=\"IDEA\"><a href=\"#IDEA\" class=\"headerlink\" title=\"IDEA\"></a>IDEA</h3><h4 id=\"基于误分样本的数据重构\"><a href=\"#基于误分样本的数据重构\" class=\"headerlink\" title=\"基于误分样本的数据重构\"></a>基于误分样本的数据重构</h4><p>之前调节分类器的参数，来提升评估效果<br>现在可以尝试&#x3D;&#x3D;调节生成模型的参数&#x3D;&#x3D;来提升生成模型的效果</p>\n<h4 id=\"难分类样本\"><a href=\"#难分类样本\" class=\"headerlink\" title=\"难分类样本\"></a>难分类样本</h4><p>难以分类但是分类正确的样本，提升权重<br>之前是使用全部原始样本生成生成数据，然后从生成数据中挑选误分类样本。<br>现在先评估原始数据，挑选出难分类样本，生成难分类样本生成数据，进行评估或者和原始数据融合</p>\n<h4 id=\"对比学习\"><a href=\"#对比学习\" class=\"headerlink\" title=\"对比学习\"></a>对比学习</h4><p>数据抽样</p>\n<p>将原始数据划分为正样本和负样本，可知正样本很少，存在严重的不平衡<br>按照对比学习的思路，可以尝试在全部样本中进行两两一组的随机抽样，两个样本标签相同为正样本对，不同为负样本对</p>\n<h4 id=\"误分类样本数据重构-数据规模\"><a href=\"#误分类样本数据重构-数据规模\" class=\"headerlink\" title=\"误分类样本数据重构 +数据规模\"></a>误分类样本数据重构 +数据规模</h4><p>考虑到真实数据量比较小<br>我们可以生成大量数据<br>以真实数据训练分类器，提取生成误分样本，并和一些生成样本融合，（相当于提升了数据规模）<br>生成两份数量都为N的数据（N为原始数据规模的10-100倍），一份通过分类器得到误分类样本，与另一份融合，共同用于训练分类器</p>\n<h4 id=\"不同生成模型数据融合\"><a href=\"#不同生成模型数据融合\" class=\"headerlink\" title=\"不同生成模型数据融合\"></a>不同生成模型数据融合</h4><p>将扩散生成模型和 GAN 模型生成的数据进行融合用于共同训练分类器</p>\n<hr>\n<h2 id=\"融合生成数据和原始数据的特征表示？\"><a href=\"#融合生成数据和原始数据的特征表示？\" class=\"headerlink\" title=\"融合生成数据和原始数据的特征表示？\"></a>融合生成数据和原始数据的特征表示？</h2><p>使用自适应的权重机制</p>\n<hr>\n<ul>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n只考虑真实&amp;生成，修改文字</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n增加大量的生成数据  50倍 30倍 70倍</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n表格：真实数据一条，每个模型的生成数据3条，绘成表格</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n相关工作：把数据增强放前面，客户流失放后面，重在罗列，要写哪篇文章</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n数据集来源换成网址</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n隐私部分表格修改</p>\n</li>\n</ul>\n<hr>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> 审稿意见标粗的 <font color=\"#ff0000\">其他联合损失函数</font></li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 表格格式问题</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 弱分类器强分类器表格加粗好结果</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 4.1生成数据量对效果的影响</li>\n<li><input disabled=\"\" type=\"checkbox\"> 相关工作的文献引用</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 写修改说明，合并到一起</li>\n</ul>\n<hr>\n<ul>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n相关工作第二部分</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n引言部分的参考文献</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n加上LR的误分类，另外数据是不是贴反了@程梓航</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n序号</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n方法介绍开始一直检查到</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n修改说明中 损失函数 数据量增大</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n修改说明合完发过去</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n4.1绿的复制 表四复制</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n引言 和 相关工作【】里加参考文献</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n图标边框</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n合参考文献 数据集划分 误分类</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n公式 摘要</p>\n</li>\n</ul>\n<p>有问题的论文序号：2，4，8，10，15，16，18，28，34，35</p>\n<h2 id=\"生成数据量对效果的影响\"><a href=\"#生成数据量对效果的影响\" class=\"headerlink\" title=\"生成数据量对效果的影响\"></a>生成数据量对效果的影响</h2><p>为了验证大规模的生成数据对效果的影响，我们使用扩散模型生成了数据集训练数据量10倍、50倍和100倍的数据，用来训练分类器Catboost，训练结果如下表所示。在电信数据集中，大量样本并不能显著提升分类器的性能，反而使结果略微下降。在银行数据集和电商数据集中，大规模样本使分类器性能得到提升，尤其是电商数据的表现提升显著，超越了真实数据的效果。三者都在数据规模达到原训练样本100倍时取得最好的效果。这表明通过增大数据集规模来提升模型效果在某些场景下是可行的。</p>\n<p>| real   | 0.8072 | 0.7388 | 0.8546 |     |     |        | 0.9701 | 0.9456 | 0.9948 |     |     |        | 0.9401 | 0.8737 | 0.9528 |<br>| ————— | —— | —— | —— | —— | — | — | —— | —— | —— | —— | — | — | —— | —— | —— | —— |<br>| syn 1:5_fakeALL | 0.8051 | 0.7367 | 0.8542 |        |     |     | 0.9589 | 0.9241 | 0.9836 |        |     |     | 0.9049 | 0.8076 | 0.8893 |        |<br>| 10x_fakeall     | 0.7277 | 0.7072 | 0.8384 |        |     |     | 0.9589 | 0.9239 | 0.9855 |        |     |     | 0.9331 | 0.8646 | 0.9511 |        |<br>| 30x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     | 0.9322 | 0.8652 | 0.9556 |        |<br>| 50x_fakeall     | 0.7227 | 0.702  | 0.844  |        |     |     | 0.9608 | 0.9279 | 0.9872 |        |     |     | 0.9349 | 0.8708 | 0.9543 |        |<br>| 70x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     |        |        |        |        |<br>| 100x_fakeall    | 0.7277 | 0.7076 | 0.8442 |        |     |     | 0.9623 | 0.9312 | 0.9882 |        |     |     | 0.934  | 0.8687 | 0.9562 |        |</p>\n<h2 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h2><h4 id=\"数据增强方法\"><a href=\"#数据增强方法\" class=\"headerlink\" title=\"数据增强方法\"></a>数据增强方法</h4><p>数据增强是一种基于有限数据生成更多有效数据来扩充训练集，从而使得训练集训练出的模型效果得到提升的方法。目前，许多数据增强方法在图像生成等领域应用广泛。常用的方法有综合采样人工合成数据算法SMOTE[12]，无监督学习方法VAE（Variational Autoencoder）[32]，GAN[34]及其多种变体，以及扩散模型（Denoising Diffusion Probabilistic Model）[48]等。<br>Xiaojun Wu利用改进的 SMOTE 处理流失数据，结合过采样和欠采样方法解决流失数据的不平衡问题，对电子商务客户流失进行预测。（# E-commerce customer churn prediction based on improved SMOTE and AdaBoost）TVAE[33]在VAE的基础上提出了一个新的集成框架，结合深度度量学习（deep metric learning）来学习 VAE 中的潜在嵌入。MedGAN将一个自编码器与GAN结合，可以在图像层面实现端到端的生成任务，可以生成连续和离散变量，被用于EHR（electronic health record）数据的生成。CrGAN-Cnet将Cramér Distance和Cross-Net architecture整合到算法中，用于航空公司乘客姓名记录生成（Airline Passenger Name Record Generation），此外还能生成数据来填补数据表格中的缺失值。IRGAN首次将在CV领域中广泛应用的GAN引入到信息检索，利用强化学习，创造性地解决了GAN在离散领域的适用问题。TableGAN将分类器和信息损失引入GAN，并在生成器、鉴别器和分类器中应用卷积神经网络（CNN）。另外，在表格数据增强中，隐私保护能力也是一个重要的因素。PATE-GAN提出了一个生成具有差分隐私保证数据的框架。<br>上述基于GAN的算法在生成特定类别数据上存在很大不足，而Conditional GAN（CGAN）可以有效控制生成数据，因而被广泛应用。CGAN在生成器和判别器的输入中增加了额外的条件信息，限制了生成数据的生成类型。CW-GAN将Wasserstein距离应用到CGAN中，利用条件向量对少数类进行过采样，以解决表格数据生成不平衡的问题。CTGAN将 PacGAN结构集成到它的鉴别器中，并使用 WGAN 损失加上梯度惩罚来训练一个条件 GAN 框架。CTAB-GAN通过数据编码连续和分类变量的混合数据类型，以及对长尾连续变量有效建模。<br>2020年，DDPM（Denoising Diffusion Probabilistic Model）被提出，被称为扩散模型，在图像生成领域广泛应用。扩散模型包括两个过程：前向过程和反向过程，其中前向过程又称为扩散过程。无论是前向过程还是反向过程都是一个参数化的马尔可夫链。后来学者们改进DDPM（引用Improved Denoising Diffusion Probabilistic Models），进一步增强其生成效果。</p>\n<h4 id=\"客户流失预估：\"><a href=\"#客户流失预估：\" class=\"headerlink\" title=\"客户流失预估：\"></a>客户流失预估：</h4><p>在过去的研究中，学者们对客户流失预测在多种领域进行了大量的探索，包括电信客户流失、银行理财客户流失、新兴互联网行业客户流失，如电商、直播、旅游产品等。预测方法也逐渐成熟，形成了以机器学习算法为主流的现状。从发展历程上主要分为两个阶段。<br>第一阶段是传统统计学预测方法，主要包括决策树（decision tree, DT）、逻辑回归（logistic regression，LR）、贝叶斯分类器（bayesian）、支持向量机（support vector machine，SVM）等算法。早在2007年，Luo Bin就使用决策树来预测手机服务中的客户流失（# Customer Churn Prediction Based on the Decision Tree in Personal Handyphone System Service）。同年，针对实际客户流失数据中正负样本数量不平衡而且数据量大的特点,Ying Weiyun提出带有不同类权重参数的支持向量机算法CW-SVM来预测客户流失,通过调整类权重参数改变分类面位置,提高算法分类准确性[6]。近些年电子商务发展迅速，客户激增，Qiu Yanfang使用逻辑回归来预测电子商务场景下的客户流失[4]。在[3]中，Hemlata Jain将逻辑回归和logit boost结合起来，在美国电信公司Orange的数据集上表现良好。在[5]中，Guangli Nie分别使用决策树和逻辑回归对某银行信用卡用户流失数据做预测。Arno De Caigny在其论文中将决策树和逻辑回归结合提出了一种新的混合算法，logit leaf model (LLM) ，以更好地对数据进行分类。（# A new hybrid classification algorithm for customer churn prediction based on logistic regression and decision trees）。这一阶段的客户流失预测方法主要集中在将这些基础算法和手动特征工程结合对客户流失进行建模[1-3, 6-7]。<br>第二阶段是客户流失预测场景中集成学习算法的垄断和深度学习的初步探索。随着集成学习的引入，随机森林（random forest，RF）、梯度提升决策树（gradient boosting decision tree，GBDT）、Adaboost和Stack等方法被大量引入到对客户流失预测中，包括在电信、银行、互联网等场景。特别是GBDT，由于其算法具有很好的性能被客户流失预测广泛应用。<br>Yaya Xie提出了一种基于随机森林的学习方法，称为改进的平衡随机森林(improved balanced random forests，IBRF)，通过改变类的分布和对少数类的错误分类施加更高的惩罚来迭代学习最佳特征，并将该方法应用于某银行客户流失数据集。（# Customer churn prediction using improved balanced random forests）Liang Jiafu提出了一种基于梯度提升决策树算法(GBDT)和逻辑回归(LR)算法的用户流失预防模型,对参数特征进行调整,对已有移动用户流失数据进行计算,识别速度和准确率均拥有好的效果[8]。Zengyuan Wu提出 PCA-AdaBoost 模型，采用主成分分析来减少数据维度，使用AdaBoost对多个决策树进行级联，以最小化不平衡数据的影响。该模型在kaggle的电子商务数据集上证明了模型的有效性。（# A PCA-AdaBoost model for E-commerce customer churn prediction）<br>在深度学习领域，Ebru Pekel Ozmen改进卷积神经网络算法，提出了一种新的混合扩展卷积决策树模型(ECDT)，将模型应用于零售业员工流失预测的数据集[10]。[11]提出了一个利用交易数据预测银行客户流失的框架,将GRU和bi-directional LSTM应用于客户流失预测。这一阶段的客户流失方法开始尝试将一些机器学习方法集成起来，或是使用流行的深度学习方法，用更复杂的模型学习和预测流失数据。<br>整体上，客户流失预测以机器学习算法为主，并发展出集成学习等不同模型，深度学习在该领域也有一定应用。预测领域也从电信用户流失逐渐发展到银行信用卡用户流失、电商用户流失等多个领域。</p>\n<h2 id=\"工作\"><a href=\"#工作\" class=\"headerlink\" title=\"工作\"></a>工作</h2><p><strong>1</strong> 多处需要增加引用文献，例如：Theil’s U 统计量，Correlation Ratio，SHAP（SHapley Additive exPlanations）<br>解决：Theil’s U 统计量，Correlation Ratio可以直接引用CTABGAN<br>          SHAP： [A Unified Approach to Interpreting Model Predictions](<a href=\"https://arxiv.org/abs/1705.07874v2\">[1705.07874v2] A Unified Approach to Interpreting Model Predictions (arxiv.org)</a>)</p>\n<p><strong>2</strong> 论文之中提到的 kaggle 数据集可以给出相应的链接。 @ 正阳 （找链接，试着参考论文里）<br><a href=\"https://www.kaggle.com/datasets/blastchar/telco-customer-churn\">Telco Customer Churn | Kaggle</a><br><a href=\"https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers\">Credit Card customers | Kaggle</a><br><a href=\"https://tianchi.aliyun.com/dataset/124814\">直播电商数据集_数据集-阿里云天池 (aliyun.com)</a></p>\n<p>3 电信数据集，每个模型找五条</p>\n<p>REAL：</p>\n<p>Male  0 No No  2  Yes  No  DSL  Yes  Yes  No  No  No  No  Month-to-month  Yes  Mailed check  53.85  108.15</p>\n<p>Female  0  No No 2 Yes No Fiber optic No No No No No No Month-to-month Yes Electronic check 70.7 151.65 </p>\n<p>Female 0 No No 8 Yes Yes Fiber optic No No Yes No Yes Yes Month-to-month Yes Electronic check 99.65 820.5</p>\n<p>Male 1 No No 1 No No phone service DSL No No Yes No No Yes Month-to-month Yes Electronic check 39.65 39.65</p>\n<p>Male 0 No No 49 Yes Yes Fiber optic No Yes Yes No Yes Yes Month-to-month Yes Bank transfer(automatic) 103.7 5036.3</p>\n<p>DDPM：<br>‘Male’ ‘1’ ‘Yes’ ‘No’  4.0 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘No’ ‘Yes’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  92.81641778046422 347.77240745447966</p>\n<p> ‘Male’ ‘0’ ‘Yes’ ‘No’ 1.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  95.8100172795924 135.30996333735123</p>\n<p> ‘Female’ ‘0’ ‘No’ ‘No’ 12.89970293412554 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Bank transfer (automatic)’<br>   90.24851621598897 1131.933374934577</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘No’ 15.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘Yes’ ‘No’ ‘No’<br>  ‘No’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Credit card (automatic)’<br>  89.23206588556003 1370.882200227259</p>\n<p> ‘Female’ ‘1’ ‘No’ ‘No’ 1.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘No’ ‘No’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  75.70813933778831 75.53547210781328</p>\n<p>SMOTE：<br>‘Female’ ‘0’ ‘No’ ‘No’ 6.058020806965098 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  89.30362630043531 581.5998093801566</p>\n<p> ‘Female’ ‘0’ ‘Yes’ ‘No’ 34.16695916300951 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘Yes’ ‘No’ ‘Yes’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  102.47469287884003 3553.063238271951</p>\n<p> ‘Female’ ‘0’ ‘No’ ‘No’ 10.409424239667828 ‘Yes’ ‘Yes’ ‘DSL’ ‘No’ ‘No’ ‘No’ ‘No’ ‘No’ ‘Yes’<br>  ‘Month-to-month’ ‘No’ ‘Electronic check’<br>  72.89683257375917 749.7780225143397</p>\n<p> ‘Male’ ‘0’ ‘Yes’ ‘No’ 68.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘Yes’ ‘Yes’ ‘Yes’ ‘Yes’<br>  ‘Yes’ ‘Yes’ ‘Two year’ ‘Yes’ ‘Credit card (automatic)’<br>  111.84516691088298 7754.865079775263</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘No’ 57.53490684507468 ‘No’ ‘No phone service’ ‘DSL’ ‘No’ ‘Yes’ ‘Yes’<br>  ‘No’ ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>   52.08537000221106 2966.5298068815946</p>\n<p>CTABGAN：<br>‘Female’ ‘1’ ‘Yes’ ‘No’ 25.813646159053306 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘Yes’ ‘No’ ‘No’<br>  ‘No’ ‘No’ ‘Month-to-month’ ‘Yes’ ‘Credit card (automatic)’<br>   72.4520499161782 107.12076519599646</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘Yes’ 7.578428442708454 ‘Yes’ ‘No’ ‘DSL’ ‘No’ ‘No’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Month-to-month’ ‘No’ ‘Mailed check’<br>  45.84371280675952 105.69947914250504</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘No’  1.9809866925261472 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘No’ ‘Month-to-month’ ‘Yes’ ‘Bank transfer (automatic)’<br>   81.46474915190095 40.94928790189158</p>\n<p> ‘Female’ ‘1’ ‘No’ ‘Yes’ 26.57383830887195 ‘Yes’ ‘No phone service’ ‘DSL’ ‘Yes’ ‘Yes’<br>  ‘Yes’ ‘No’ ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘No’ ‘Electronic check’<br>   57.69859110911275 781.0754735958174</p>\n<p> ‘Male’ ‘0’ ‘Yes’ ‘No’  28.8168545923753 ‘Yes’ ‘Yes’ ‘DSL’ ‘No’ ‘Yes’ ‘No’ ‘Yes’ ‘Yes’<br>  ‘Yes’ ‘Month-to-month’ ‘No’ ‘Credit card (automatic)’<br>  77.30306219710255 756.2990340865898</p>\n<p>In the field of data mining, there are commonly encountered issues of data imbalance and inadequate protection of user privacy data. These issues can have a detrimental impact on the accuracy of model predictions and the applicability of models in privacy-preserving scenarios. Therefore, the generation of new data has emerged as a prominent solution in addressing these challenges.<br>However, generating high-quality data poses certain challenges in the field of data mining, which primarily deals with structured data characterized by a high number of dimensions and unrelated features.<br>Considering the successful application of diffusion models in tasks such as image generation, this study attempts to apply diffusion models to the task of customer churn prediction.<br>This paper employs Gaussian diffusion models and polynomial diffusion models to generate data for numerical and categorical features in churn data. It also conducts research and analysis on the predictive performance and privacy protection capabilities of these models.<br>We conducted extensive experiments on customer churn data from multiple domains to explore the potential of merging synthetic data with real data for reconstruction.<br>The experimental results indicate that the adoption of the diffusion model enables the generation of high-quality data. Furthermore, the generated data exhibits improvements across various customer churn prediction methods, with a particular advantage observed for weak classifiers. This approach also effectively mitigates the issue of data imbalance.<br>Moreover, the data generated by the diffusion model exhibits a distribution that is closer to real data, thereby possessing potential value in preserving customer privacy.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>检查句号后面空格</p>\n<p>论文检查：</p>\n<ul>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 论文中公式全部检查  公式文字部分也对应检查</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 图表全部检查  图表文字部分也对应检查</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 参考文献检查  <strong>20和29参考文献重点检查</strong></li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 学历补充</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> word格式整理好</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203092747.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/d5304720bd3b25b687d5db36f180fb3.jpg\" alt=\"d5304720bd3b25b687d5db36f180fb3.jpg\"></p>\n<p>检查论文，12.4之前</p>\n<p>对比学习生成 <strong>hardsample</strong><br>自动数据增强<br>识别出假数据<br><a href=\"https://arxiv.org/pdf/2207.00148.pdf\">https://arxiv.org/pdf/2207.00148.pdf</a><br><a href=\"https://arxiv.org/pdf/2010.04592v2.pdf\">https://arxiv.org/pdf/2010.04592v2.pdf</a><br><a href=\"https://epubs.siam.org/doi/epdf/10.1137/1.9781611977653.ch19\">https://epubs.siam.org/doi/epdf/10.1137/1.9781611977653.ch19</a><br><a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/26071\">https://ojs.aaai.org/index.php/AAAI/article/view/26071</a><br><a href=\"https://arxiv.org/pdf/2303.15161v3.pdf\">https://arxiv.org/pdf/2303.15161v3.pdf</a><br><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0031320323002121\">https://www.sciencedirect.com/science/article/abs/pii/S0031320323002121</a><br><a href=\"https://paperswithcode.com/paper/hard-sample-guided-hybrid-contrast-learning\">https://paperswithcode.com/paper/hard-sample-guided-hybrid-contrast-learning</a><br><a href=\"https://paperswithcode.com/paper/hard-sample-aware-noise-robust-learning-for\">https://paperswithcode.com/paper/hard-sample-aware-noise-robust-learning-for</a></p>\n<h3 id=\"NEW\"><a href=\"#NEW\" class=\"headerlink\" title=\"NEW\"></a>NEW</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/ff4ca0be44cbc807008ad5e9d025a40.jpg\" alt=\"ff4ca0be44cbc807008ad5e9d025a40.jpg\"></p>\n<ul>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n表一加个表头</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n表三添加：auc已经到很高的数值，差距拉不开，意义不大</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n所有数据换成通信数据,电信场景不同任务，网络优化、运维、用户进行实验，验证在不同任务下的效果</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n基于文章写一个ppt<br>12-15页有效内容</p>\n</li>\n</ul>\n<p>11.29<br>ppt要求：刷一下ppt格式<br>参考p8做p11</p>\n<p>背景 相关工作<br>方法介绍<br>数据集介绍<br>哪几个实验</p>\n<p>用对比学习做GAN的判别器</p>\n<ol>\n<li><p>hardsample的生成结果进行对比</p>\n</li>\n<li><p>hardsample怎么生成，普通生成也怎么生成</p>\n</li>\n<li><p>对这些样本构造一个图，从图中计算hardsample</p>\n</li>\n<li><p>基于对比学习</p>\n</li>\n<li><p>DDPM在通信的应用</p>\n</li>\n<li><p>隐私保护的深入探索 看论文<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127150317.png\" alt=\"image.png\"></p>\n</li>\n</ol>\n<h3 id=\"IDEA\"><a href=\"#IDEA\" class=\"headerlink\" title=\"IDEA\"></a>IDEA</h3><h4 id=\"基于误分样本的数据重构\"><a href=\"#基于误分样本的数据重构\" class=\"headerlink\" title=\"基于误分样本的数据重构\"></a>基于误分样本的数据重构</h4><p>之前调节分类器的参数，来提升评估效果<br>现在可以尝试&#x3D;&#x3D;调节生成模型的参数&#x3D;&#x3D;来提升生成模型的效果</p>\n<h4 id=\"难分类样本\"><a href=\"#难分类样本\" class=\"headerlink\" title=\"难分类样本\"></a>难分类样本</h4><p>难以分类但是分类正确的样本，提升权重<br>之前是使用全部原始样本生成生成数据，然后从生成数据中挑选误分类样本。<br>现在先评估原始数据，挑选出难分类样本，生成难分类样本生成数据，进行评估或者和原始数据融合</p>\n<h4 id=\"对比学习\"><a href=\"#对比学习\" class=\"headerlink\" title=\"对比学习\"></a>对比学习</h4><p>数据抽样</p>\n<p>将原始数据划分为正样本和负样本，可知正样本很少，存在严重的不平衡<br>按照对比学习的思路，可以尝试在全部样本中进行两两一组的随机抽样，两个样本标签相同为正样本对，不同为负样本对</p>\n<h4 id=\"误分类样本数据重构-数据规模\"><a href=\"#误分类样本数据重构-数据规模\" class=\"headerlink\" title=\"误分类样本数据重构 +数据规模\"></a>误分类样本数据重构 +数据规模</h4><p>考虑到真实数据量比较小<br>我们可以生成大量数据<br>以真实数据训练分类器，提取生成误分样本，并和一些生成样本融合，（相当于提升了数据规模）<br>生成两份数量都为N的数据（N为原始数据规模的10-100倍），一份通过分类器得到误分类样本，与另一份融合，共同用于训练分类器</p>\n<h4 id=\"不同生成模型数据融合\"><a href=\"#不同生成模型数据融合\" class=\"headerlink\" title=\"不同生成模型数据融合\"></a>不同生成模型数据融合</h4><p>将扩散生成模型和 GAN 模型生成的数据进行融合用于共同训练分类器</p>\n<hr>\n<h2 id=\"融合生成数据和原始数据的特征表示？\"><a href=\"#融合生成数据和原始数据的特征表示？\" class=\"headerlink\" title=\"融合生成数据和原始数据的特征表示？\"></a>融合生成数据和原始数据的特征表示？</h2><p>使用自适应的权重机制</p>\n<hr>\n<ul>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n只考虑真实&amp;生成，修改文字</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n增加大量的生成数据  50倍 30倍 70倍</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n表格：真实数据一条，每个模型的生成数据3条，绘成表格</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n相关工作：把数据增强放前面，客户流失放后面，重在罗列，要写哪篇文章</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n数据集来源换成网址</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n隐私部分表格修改</p>\n</li>\n</ul>\n<hr>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> 审稿意见标粗的 <font color=\"#ff0000\">其他联合损失函数</font></li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 表格格式问题</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 弱分类器强分类器表格加粗好结果</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 4.1生成数据量对效果的影响</li>\n<li><input disabled=\"\" type=\"checkbox\"> 相关工作的文献引用</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 写修改说明，合并到一起</li>\n</ul>\n<hr>\n<ul>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n相关工作第二部分</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n引言部分的参考文献</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n加上LR的误分类，另外数据是不是贴反了@程梓航</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n序号</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n方法介绍开始一直检查到</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n修改说明中 损失函数 数据量增大</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n修改说明合完发过去</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n4.1绿的复制 表四复制</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n引言 和 相关工作【】里加参考文献</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n图标边框</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n合参考文献 数据集划分 误分类</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n公式 摘要</p>\n</li>\n</ul>\n<p>有问题的论文序号：2，4，8，10，15，16，18，28，34，35</p>\n<h2 id=\"生成数据量对效果的影响\"><a href=\"#生成数据量对效果的影响\" class=\"headerlink\" title=\"生成数据量对效果的影响\"></a>生成数据量对效果的影响</h2><p>为了验证大规模的生成数据对效果的影响，我们使用扩散模型生成了数据集训练数据量10倍、50倍和100倍的数据，用来训练分类器Catboost，训练结果如下表所示。在电信数据集中，大量样本并不能显著提升分类器的性能，反而使结果略微下降。在银行数据集和电商数据集中，大规模样本使分类器性能得到提升，尤其是电商数据的表现提升显著，超越了真实数据的效果。三者都在数据规模达到原训练样本100倍时取得最好的效果。这表明通过增大数据集规模来提升模型效果在某些场景下是可行的。</p>\n<p>| real   | 0.8072 | 0.7388 | 0.8546 |     |     |        | 0.9701 | 0.9456 | 0.9948 |     |     |        | 0.9401 | 0.8737 | 0.9528 |<br>| ————— | —— | —— | —— | —— | — | — | —— | —— | —— | —— | — | — | —— | —— | —— | —— |<br>| syn 1:5_fakeALL | 0.8051 | 0.7367 | 0.8542 |        |     |     | 0.9589 | 0.9241 | 0.9836 |        |     |     | 0.9049 | 0.8076 | 0.8893 |        |<br>| 10x_fakeall     | 0.7277 | 0.7072 | 0.8384 |        |     |     | 0.9589 | 0.9239 | 0.9855 |        |     |     | 0.9331 | 0.8646 | 0.9511 |        |<br>| 30x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     | 0.9322 | 0.8652 | 0.9556 |        |<br>| 50x_fakeall     | 0.7227 | 0.702  | 0.844  |        |     |     | 0.9608 | 0.9279 | 0.9872 |        |     |     | 0.9349 | 0.8708 | 0.9543 |        |<br>| 70x_fakeall     |        |        |        |        |     |     |        |        |        |        |     |     |        |        |        |        |<br>| 100x_fakeall    | 0.7277 | 0.7076 | 0.8442 |        |     |     | 0.9623 | 0.9312 | 0.9882 |        |     |     | 0.934  | 0.8687 | 0.9562 |        |</p>\n<h2 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h2><h4 id=\"数据增强方法\"><a href=\"#数据增强方法\" class=\"headerlink\" title=\"数据增强方法\"></a>数据增强方法</h4><p>数据增强是一种基于有限数据生成更多有效数据来扩充训练集，从而使得训练集训练出的模型效果得到提升的方法。目前，许多数据增强方法在图像生成等领域应用广泛。常用的方法有综合采样人工合成数据算法SMOTE[12]，无监督学习方法VAE（Variational Autoencoder）[32]，GAN[34]及其多种变体，以及扩散模型（Denoising Diffusion Probabilistic Model）[48]等。<br>Xiaojun Wu利用改进的 SMOTE 处理流失数据，结合过采样和欠采样方法解决流失数据的不平衡问题，对电子商务客户流失进行预测。（# E-commerce customer churn prediction based on improved SMOTE and AdaBoost）TVAE[33]在VAE的基础上提出了一个新的集成框架，结合深度度量学习（deep metric learning）来学习 VAE 中的潜在嵌入。MedGAN将一个自编码器与GAN结合，可以在图像层面实现端到端的生成任务，可以生成连续和离散变量，被用于EHR（electronic health record）数据的生成。CrGAN-Cnet将Cramér Distance和Cross-Net architecture整合到算法中，用于航空公司乘客姓名记录生成（Airline Passenger Name Record Generation），此外还能生成数据来填补数据表格中的缺失值。IRGAN首次将在CV领域中广泛应用的GAN引入到信息检索，利用强化学习，创造性地解决了GAN在离散领域的适用问题。TableGAN将分类器和信息损失引入GAN，并在生成器、鉴别器和分类器中应用卷积神经网络（CNN）。另外，在表格数据增强中，隐私保护能力也是一个重要的因素。PATE-GAN提出了一个生成具有差分隐私保证数据的框架。<br>上述基于GAN的算法在生成特定类别数据上存在很大不足，而Conditional GAN（CGAN）可以有效控制生成数据，因而被广泛应用。CGAN在生成器和判别器的输入中增加了额外的条件信息，限制了生成数据的生成类型。CW-GAN将Wasserstein距离应用到CGAN中，利用条件向量对少数类进行过采样，以解决表格数据生成不平衡的问题。CTGAN将 PacGAN结构集成到它的鉴别器中，并使用 WGAN 损失加上梯度惩罚来训练一个条件 GAN 框架。CTAB-GAN通过数据编码连续和分类变量的混合数据类型，以及对长尾连续变量有效建模。<br>2020年，DDPM（Denoising Diffusion Probabilistic Model）被提出，被称为扩散模型，在图像生成领域广泛应用。扩散模型包括两个过程：前向过程和反向过程，其中前向过程又称为扩散过程。无论是前向过程还是反向过程都是一个参数化的马尔可夫链。后来学者们改进DDPM（引用Improved Denoising Diffusion Probabilistic Models），进一步增强其生成效果。</p>\n<h4 id=\"客户流失预估：\"><a href=\"#客户流失预估：\" class=\"headerlink\" title=\"客户流失预估：\"></a>客户流失预估：</h4><p>在过去的研究中，学者们对客户流失预测在多种领域进行了大量的探索，包括电信客户流失、银行理财客户流失、新兴互联网行业客户流失，如电商、直播、旅游产品等。预测方法也逐渐成熟，形成了以机器学习算法为主流的现状。从发展历程上主要分为两个阶段。<br>第一阶段是传统统计学预测方法，主要包括决策树（decision tree, DT）、逻辑回归（logistic regression，LR）、贝叶斯分类器（bayesian）、支持向量机（support vector machine，SVM）等算法。早在2007年，Luo Bin就使用决策树来预测手机服务中的客户流失（# Customer Churn Prediction Based on the Decision Tree in Personal Handyphone System Service）。同年，针对实际客户流失数据中正负样本数量不平衡而且数据量大的特点,Ying Weiyun提出带有不同类权重参数的支持向量机算法CW-SVM来预测客户流失,通过调整类权重参数改变分类面位置,提高算法分类准确性[6]。近些年电子商务发展迅速，客户激增，Qiu Yanfang使用逻辑回归来预测电子商务场景下的客户流失[4]。在[3]中，Hemlata Jain将逻辑回归和logit boost结合起来，在美国电信公司Orange的数据集上表现良好。在[5]中，Guangli Nie分别使用决策树和逻辑回归对某银行信用卡用户流失数据做预测。Arno De Caigny在其论文中将决策树和逻辑回归结合提出了一种新的混合算法，logit leaf model (LLM) ，以更好地对数据进行分类。（# A new hybrid classification algorithm for customer churn prediction based on logistic regression and decision trees）。这一阶段的客户流失预测方法主要集中在将这些基础算法和手动特征工程结合对客户流失进行建模[1-3, 6-7]。<br>第二阶段是客户流失预测场景中集成学习算法的垄断和深度学习的初步探索。随着集成学习的引入，随机森林（random forest，RF）、梯度提升决策树（gradient boosting decision tree，GBDT）、Adaboost和Stack等方法被大量引入到对客户流失预测中，包括在电信、银行、互联网等场景。特别是GBDT，由于其算法具有很好的性能被客户流失预测广泛应用。<br>Yaya Xie提出了一种基于随机森林的学习方法，称为改进的平衡随机森林(improved balanced random forests，IBRF)，通过改变类的分布和对少数类的错误分类施加更高的惩罚来迭代学习最佳特征，并将该方法应用于某银行客户流失数据集。（# Customer churn prediction using improved balanced random forests）Liang Jiafu提出了一种基于梯度提升决策树算法(GBDT)和逻辑回归(LR)算法的用户流失预防模型,对参数特征进行调整,对已有移动用户流失数据进行计算,识别速度和准确率均拥有好的效果[8]。Zengyuan Wu提出 PCA-AdaBoost 模型，采用主成分分析来减少数据维度，使用AdaBoost对多个决策树进行级联，以最小化不平衡数据的影响。该模型在kaggle的电子商务数据集上证明了模型的有效性。（# A PCA-AdaBoost model for E-commerce customer churn prediction）<br>在深度学习领域，Ebru Pekel Ozmen改进卷积神经网络算法，提出了一种新的混合扩展卷积决策树模型(ECDT)，将模型应用于零售业员工流失预测的数据集[10]。[11]提出了一个利用交易数据预测银行客户流失的框架,将GRU和bi-directional LSTM应用于客户流失预测。这一阶段的客户流失方法开始尝试将一些机器学习方法集成起来，或是使用流行的深度学习方法，用更复杂的模型学习和预测流失数据。<br>整体上，客户流失预测以机器学习算法为主，并发展出集成学习等不同模型，深度学习在该领域也有一定应用。预测领域也从电信用户流失逐渐发展到银行信用卡用户流失、电商用户流失等多个领域。</p>\n<h2 id=\"工作\"><a href=\"#工作\" class=\"headerlink\" title=\"工作\"></a>工作</h2><p><strong>1</strong> 多处需要增加引用文献，例如：Theil’s U 统计量，Correlation Ratio，SHAP（SHapley Additive exPlanations）<br>解决：Theil’s U 统计量，Correlation Ratio可以直接引用CTABGAN<br>          SHAP： [A Unified Approach to Interpreting Model Predictions](<a href=\"https://arxiv.org/abs/1705.07874v2\">[1705.07874v2] A Unified Approach to Interpreting Model Predictions (arxiv.org)</a>)</p>\n<p><strong>2</strong> 论文之中提到的 kaggle 数据集可以给出相应的链接。 @ 正阳 （找链接，试着参考论文里）<br><a href=\"https://www.kaggle.com/datasets/blastchar/telco-customer-churn\">Telco Customer Churn | Kaggle</a><br><a href=\"https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers\">Credit Card customers | Kaggle</a><br><a href=\"https://tianchi.aliyun.com/dataset/124814\">直播电商数据集_数据集-阿里云天池 (aliyun.com)</a></p>\n<p>3 电信数据集，每个模型找五条</p>\n<p>REAL：</p>\n<p>Male  0 No No  2  Yes  No  DSL  Yes  Yes  No  No  No  No  Month-to-month  Yes  Mailed check  53.85  108.15</p>\n<p>Female  0  No No 2 Yes No Fiber optic No No No No No No Month-to-month Yes Electronic check 70.7 151.65 </p>\n<p>Female 0 No No 8 Yes Yes Fiber optic No No Yes No Yes Yes Month-to-month Yes Electronic check 99.65 820.5</p>\n<p>Male 1 No No 1 No No phone service DSL No No Yes No No Yes Month-to-month Yes Electronic check 39.65 39.65</p>\n<p>Male 0 No No 49 Yes Yes Fiber optic No Yes Yes No Yes Yes Month-to-month Yes Bank transfer(automatic) 103.7 5036.3</p>\n<p>DDPM：<br>‘Male’ ‘1’ ‘Yes’ ‘No’  4.0 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘No’ ‘Yes’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  92.81641778046422 347.77240745447966</p>\n<p> ‘Male’ ‘0’ ‘Yes’ ‘No’ 1.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  95.8100172795924 135.30996333735123</p>\n<p> ‘Female’ ‘0’ ‘No’ ‘No’ 12.89970293412554 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Bank transfer (automatic)’<br>   90.24851621598897 1131.933374934577</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘No’ 15.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘Yes’ ‘No’ ‘No’<br>  ‘No’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Credit card (automatic)’<br>  89.23206588556003 1370.882200227259</p>\n<p> ‘Female’ ‘1’ ‘No’ ‘No’ 1.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘No’ ‘No’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  75.70813933778831 75.53547210781328</p>\n<p>SMOTE：<br>‘Female’ ‘0’ ‘No’ ‘No’ 6.058020806965098 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  89.30362630043531 581.5998093801566</p>\n<p> ‘Female’ ‘0’ ‘Yes’ ‘No’ 34.16695916300951 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘Yes’ ‘No’ ‘Yes’ ‘No’<br>  ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>  102.47469287884003 3553.063238271951</p>\n<p> ‘Female’ ‘0’ ‘No’ ‘No’ 10.409424239667828 ‘Yes’ ‘Yes’ ‘DSL’ ‘No’ ‘No’ ‘No’ ‘No’ ‘No’ ‘Yes’<br>  ‘Month-to-month’ ‘No’ ‘Electronic check’<br>  72.89683257375917 749.7780225143397</p>\n<p> ‘Male’ ‘0’ ‘Yes’ ‘No’ 68.0 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘Yes’ ‘Yes’ ‘Yes’ ‘Yes’<br>  ‘Yes’ ‘Yes’ ‘Two year’ ‘Yes’ ‘Credit card (automatic)’<br>  111.84516691088298 7754.865079775263</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘No’ 57.53490684507468 ‘No’ ‘No phone service’ ‘DSL’ ‘No’ ‘Yes’ ‘Yes’<br>  ‘No’ ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘Yes’ ‘Electronic check’<br>   52.08537000221106 2966.5298068815946</p>\n<p>CTABGAN：<br>‘Female’ ‘1’ ‘Yes’ ‘No’ 25.813646159053306 ‘Yes’ ‘No’ ‘Fiber optic’ ‘No’ ‘Yes’ ‘No’ ‘No’<br>  ‘No’ ‘No’ ‘Month-to-month’ ‘Yes’ ‘Credit card (automatic)’<br>   72.4520499161782 107.12076519599646</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘Yes’ 7.578428442708454 ‘Yes’ ‘No’ ‘DSL’ ‘No’ ‘No’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Month-to-month’ ‘No’ ‘Mailed check’<br>  45.84371280675952 105.69947914250504</p>\n<p> ‘Male’ ‘0’ ‘No’ ‘No’  1.9809866925261472 ‘Yes’ ‘Yes’ ‘Fiber optic’ ‘No’ ‘No’ ‘No’ ‘No’<br>  ‘Yes’ ‘No’ ‘Month-to-month’ ‘Yes’ ‘Bank transfer (automatic)’<br>   81.46474915190095 40.94928790189158</p>\n<p> ‘Female’ ‘1’ ‘No’ ‘Yes’ 26.57383830887195 ‘Yes’ ‘No phone service’ ‘DSL’ ‘Yes’ ‘Yes’<br>  ‘Yes’ ‘No’ ‘Yes’ ‘Yes’ ‘Month-to-month’ ‘No’ ‘Electronic check’<br>   57.69859110911275 781.0754735958174</p>\n<p> ‘Male’ ‘0’ ‘Yes’ ‘No’  28.8168545923753 ‘Yes’ ‘Yes’ ‘DSL’ ‘No’ ‘Yes’ ‘No’ ‘Yes’ ‘Yes’<br>  ‘Yes’ ‘Month-to-month’ ‘No’ ‘Credit card (automatic)’<br>  77.30306219710255 756.2990340865898</p>\n<p>In the field of data mining, there are commonly encountered issues of data imbalance and inadequate protection of user privacy data. These issues can have a detrimental impact on the accuracy of model predictions and the applicability of models in privacy-preserving scenarios. Therefore, the generation of new data has emerged as a prominent solution in addressing these challenges.<br>However, generating high-quality data poses certain challenges in the field of data mining, which primarily deals with structured data characterized by a high number of dimensions and unrelated features.<br>Considering the successful application of diffusion models in tasks such as image generation, this study attempts to apply diffusion models to the task of customer churn prediction.<br>This paper employs Gaussian diffusion models and polynomial diffusion models to generate data for numerical and categorical features in churn data. It also conducts research and analysis on the predictive performance and privacy protection capabilities of these models.<br>We conducted extensive experiments on customer churn data from multiple domains to explore the potential of merging synthetic data with real data for reconstruction.<br>The experimental results indicate that the adoption of the diffusion model enables the generation of high-quality data. Furthermore, the generated data exhibits improvements across various customer churn prediction methods, with a particular advantage observed for weak classifiers. This approach also effectively mitigates the issue of data imbalance.<br>Moreover, the data generated by the diffusion model exhibits a distribution that is closer to real data, thereby possessing potential value in preserving customer privacy.</p>\n"},{"title":"Django模板","_content":"### 加载静态文件\n[如何管理静态文件（如图片、JavaScript、CSS） | Django 文档 | Django](https://docs.djangoproject.com/zh-hans/4.2/howto/static-files/)\n[django在html中显示图片【实测成功】\\_django 显示图片-CSDN博客](https://blog.csdn.net/weixin_41529093/article/details/115653070)\n[Django基础篇-模板加载静态文件-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/1465931)\n在Django的templates中，我们可以存放html文件，但**静态文件**如图片、CSS和Javascript不能与html放在同一目录下。需要在app目录下新建一个static文件夹，用来存放静态文件。\n#### 修改settings文件\n确保`settings`文件中`INSTALLED_APPS`包含了`django.contrib.staticfiles`，并需要在`settings`中添加`STATIC_URL = \"static/\"`来定义静态文件的链接。\n除了在 apps 中使用 `static/` 目录，你可以在配置文件中定义一个目录列表 ([`STATICFILES_DIRS`](https://docs.djangoproject.com/zh-hans/4.2/ref/settings/#std-setting-STATICFILES_DIRS)) ，Django 会从中寻找静态文件：\n```\nSTATICFILES_DIRS = [\n    BASE_DIR / \"static\",\n    \"/var/www/static/\",\n]\n```\n\n#### 修改html文件\n在html中需要添加一行`{% load static %}`在调用静态文件之前。\n然后在调用静态文件时，使用`{% static '文件路径' %}`\n\n\n### 前后端通信\n#### 表单传参\n[使用表单 | Django 文档 | Django](https://docs.djangoproject.com/zh-hans/4.2/topics/forms/)\n前端可以使用表单将参数传递给后端接收\n```\n<form id=\"表单名\" action=\"{% url '接收表单的函数' %}\" method=\"post\" >\n\t{% csrf_token %} \n\t<!-- ... 其他表单字段 ... --> \n\t<button type=\"submit\">提交</button> \n</form>\n```\nurl中的函数需要在views中定义，用来接受表单中的信息并进行处理，接收方式常用`get`和`post`\n\nDjango使用CSRF令牌保护机制，用于防止恶意网站利用用户的登录状态进行伪造请求。在表单中需要添加`{% csrf_token %}`标签，它会生成一个包含 CSRF 令牌的隐藏字段。\n\n使用javascript函数返回布尔值，可以控制表单是否被提交：\n```\n<button type=\"submit\" onclick=\"return submitForm()\">提交</button>\n```","source":"_posts/Notes/编程/Django/Django模板.md","raw":"---\ncategories:\n  - Notes\n  - 编程\n  - Django\ntitle: Django模板\ntags:\n  - Python\n  - 软件工程\n---\n### 加载静态文件\n[如何管理静态文件（如图片、JavaScript、CSS） | Django 文档 | Django](https://docs.djangoproject.com/zh-hans/4.2/howto/static-files/)\n[django在html中显示图片【实测成功】\\_django 显示图片-CSDN博客](https://blog.csdn.net/weixin_41529093/article/details/115653070)\n[Django基础篇-模板加载静态文件-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/1465931)\n在Django的templates中，我们可以存放html文件，但**静态文件**如图片、CSS和Javascript不能与html放在同一目录下。需要在app目录下新建一个static文件夹，用来存放静态文件。\n#### 修改settings文件\n确保`settings`文件中`INSTALLED_APPS`包含了`django.contrib.staticfiles`，并需要在`settings`中添加`STATIC_URL = \"static/\"`来定义静态文件的链接。\n除了在 apps 中使用 `static/` 目录，你可以在配置文件中定义一个目录列表 ([`STATICFILES_DIRS`](https://docs.djangoproject.com/zh-hans/4.2/ref/settings/#std-setting-STATICFILES_DIRS)) ，Django 会从中寻找静态文件：\n```\nSTATICFILES_DIRS = [\n    BASE_DIR / \"static\",\n    \"/var/www/static/\",\n]\n```\n\n#### 修改html文件\n在html中需要添加一行`{% load static %}`在调用静态文件之前。\n然后在调用静态文件时，使用`{% static '文件路径' %}`\n\n\n### 前后端通信\n#### 表单传参\n[使用表单 | Django 文档 | Django](https://docs.djangoproject.com/zh-hans/4.2/topics/forms/)\n前端可以使用表单将参数传递给后端接收\n```\n<form id=\"表单名\" action=\"{% url '接收表单的函数' %}\" method=\"post\" >\n\t{% csrf_token %} \n\t<!-- ... 其他表单字段 ... --> \n\t<button type=\"submit\">提交</button> \n</form>\n```\nurl中的函数需要在views中定义，用来接受表单中的信息并进行处理，接收方式常用`get`和`post`\n\nDjango使用CSRF令牌保护机制，用于防止恶意网站利用用户的登录状态进行伪造请求。在表单中需要添加`{% csrf_token %}`标签，它会生成一个包含 CSRF 令牌的隐藏字段。\n\n使用javascript函数返回布尔值，可以控制表单是否被提交：\n```\n<button type=\"submit\" onclick=\"return submitForm()\">提交</button>\n```","slug":"Notes/编程/Django/Django模板","published":1,"date":"2023-10-27T15:59:08.367Z","updated":"2023-11-03T07:12:09.437Z","_id":"clo8stmx80000548cbgrv32y9","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"加载静态文件\"><a href=\"#加载静态文件\" class=\"headerlink\" title=\"加载静态文件\"></a>加载静态文件</h3><p><a href=\"https://docs.djangoproject.com/zh-hans/4.2/howto/static-files/\">如何管理静态文件（如图片、JavaScript、CSS） | Django 文档 | Django</a><br><a href=\"https://blog.csdn.net/weixin_41529093/article/details/115653070\">django在html中显示图片【实测成功】_django 显示图片-CSDN博客</a><br><a href=\"https://cloud.tencent.com/developer/article/1465931\">Django基础篇-模板加载静态文件-腾讯云开发者社区-腾讯云</a><br>在Django的templates中，我们可以存放html文件，但<strong>静态文件</strong>如图片、CSS和Javascript不能与html放在同一目录下。需要在app目录下新建一个static文件夹，用来存放静态文件。</p>\n<h4 id=\"修改settings文件\"><a href=\"#修改settings文件\" class=\"headerlink\" title=\"修改settings文件\"></a>修改settings文件</h4><p>确保<code>settings</code>文件中<code>INSTALLED_APPS</code>包含了<code>django.contrib.staticfiles</code>，并需要在<code>settings</code>中添加<code>STATIC_URL = &quot;static/&quot;</code>来定义静态文件的链接。<br>除了在 apps 中使用 <code>static/</code> 目录，你可以在配置文件中定义一个目录列表 (<a href=\"https://docs.djangoproject.com/zh-hans/4.2/ref/settings/#std-setting-STATICFILES_DIRS\"><code>STATICFILES_DIRS</code></a>) ，Django 会从中寻找静态文件：</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ini\"><span class=\"hljs-attr\">STATICFILES_DIRS</span> = [<br>    BASE_DIR / <span class=\"hljs-string\">&quot;static&quot;</span>,<br>    <span class=\"hljs-string\">&quot;/var/www/static/&quot;</span>,<br>]<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"修改html文件\"><a href=\"#修改html文件\" class=\"headerlink\" title=\"修改html文件\"></a>修改html文件</h4><p>在html中需要添加一行<code>&#123;% load static %&#125;</code>在调用静态文件之前。<br>然后在调用静态文件时，使用<code>&#123;% static '文件路径' %&#125;</code></p>\n<h3 id=\"前后端通信\"><a href=\"#前后端通信\" class=\"headerlink\" title=\"前后端通信\"></a>前后端通信</h3><h4 id=\"表单传参\"><a href=\"#表单传参\" class=\"headerlink\" title=\"表单传参\"></a>表单传参</h4><p><a href=\"https://docs.djangoproject.com/zh-hans/4.2/topics/forms/\">使用表单 | Django 文档 | Django</a><br>前端可以使用表单将参数传递给后端接收</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs django\"><span class=\"language-xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">form</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;表单名&quot;</span> <span class=\"hljs-attr\">action</span>=<span class=\"hljs-string\">&quot;</span></span></span><span class=\"hljs-template-tag\">&#123;% <span class=\"hljs-name\"><span class=\"hljs-name\">url</span></span> &#x27;接收表单的函数&#x27; %&#125;</span><span class=\"language-xml\"><span class=\"hljs-tag\"><span class=\"hljs-string\">&quot;</span> <span class=\"hljs-attr\">method</span>=<span class=\"hljs-string\">&quot;post&quot;</span> &gt;</span></span><br><span class=\"language-xml\">\t</span><span class=\"hljs-template-tag\">&#123;% <span class=\"hljs-name\"><span class=\"hljs-name\">csrf_token</span></span> %&#125;</span><span class=\"language-xml\"> </span><br><span class=\"language-xml\">\t<span class=\"hljs-comment\">&lt;!-- ... 其他表单字段 ... --&gt;</span> </span><br><span class=\"language-xml\">\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">button</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;submit&quot;</span>&gt;</span>提交<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">button</span>&gt;</span> </span><br><span class=\"language-xml\"><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">form</span>&gt;</span></span><br></code></pre></td></tr></table></figure>\n<p>url中的函数需要在views中定义，用来接受表单中的信息并进行处理，接收方式常用<code>get</code>和<code>post</code></p>\n<p>Django使用CSRF令牌保护机制，用于防止恶意网站利用用户的登录状态进行伪造请求。在表单中需要添加<code>&#123;% csrf_token %&#125;</code>标签，它会生成一个包含 CSRF 令牌的隐藏字段。</p>\n<p>使用javascript函数返回布尔值，可以控制表单是否被提交：</p>\n<figure class=\"highlight hsp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs hsp\">&lt;<span class=\"hljs-keyword\">button</span> type=<span class=\"hljs-string\">&quot;submit&quot;</span> <span class=\"hljs-keyword\">onclick</span>=<span class=\"hljs-string\">&quot;return submitForm()&quot;</span>&gt;提交&lt;/<span class=\"hljs-keyword\">button</span>&gt;<br></code></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"加载静态文件\"><a href=\"#加载静态文件\" class=\"headerlink\" title=\"加载静态文件\"></a>加载静态文件</h3><p><a href=\"https://docs.djangoproject.com/zh-hans/4.2/howto/static-files/\">如何管理静态文件（如图片、JavaScript、CSS） | Django 文档 | Django</a><br><a href=\"https://blog.csdn.net/weixin_41529093/article/details/115653070\">django在html中显示图片【实测成功】_django 显示图片-CSDN博客</a><br><a href=\"https://cloud.tencent.com/developer/article/1465931\">Django基础篇-模板加载静态文件-腾讯云开发者社区-腾讯云</a><br>在Django的templates中，我们可以存放html文件，但<strong>静态文件</strong>如图片、CSS和Javascript不能与html放在同一目录下。需要在app目录下新建一个static文件夹，用来存放静态文件。</p>\n<h4 id=\"修改settings文件\"><a href=\"#修改settings文件\" class=\"headerlink\" title=\"修改settings文件\"></a>修改settings文件</h4><p>确保<code>settings</code>文件中<code>INSTALLED_APPS</code>包含了<code>django.contrib.staticfiles</code>，并需要在<code>settings</code>中添加<code>STATIC_URL = &quot;static/&quot;</code>来定义静态文件的链接。<br>除了在 apps 中使用 <code>static/</code> 目录，你可以在配置文件中定义一个目录列表 (<a href=\"https://docs.djangoproject.com/zh-hans/4.2/ref/settings/#std-setting-STATICFILES_DIRS\"><code>STATICFILES_DIRS</code></a>) ，Django 会从中寻找静态文件：</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ini\"><span class=\"hljs-attr\">STATICFILES_DIRS</span> = [<br>    BASE_DIR / <span class=\"hljs-string\">&quot;static&quot;</span>,<br>    <span class=\"hljs-string\">&quot;/var/www/static/&quot;</span>,<br>]<br></code></pre></td></tr></table></figure>\n\n<h4 id=\"修改html文件\"><a href=\"#修改html文件\" class=\"headerlink\" title=\"修改html文件\"></a>修改html文件</h4><p>在html中需要添加一行<code>&#123;% load static %&#125;</code>在调用静态文件之前。<br>然后在调用静态文件时，使用<code>&#123;% static '文件路径' %&#125;</code></p>\n<h3 id=\"前后端通信\"><a href=\"#前后端通信\" class=\"headerlink\" title=\"前后端通信\"></a>前后端通信</h3><h4 id=\"表单传参\"><a href=\"#表单传参\" class=\"headerlink\" title=\"表单传参\"></a>表单传参</h4><p><a href=\"https://docs.djangoproject.com/zh-hans/4.2/topics/forms/\">使用表单 | Django 文档 | Django</a><br>前端可以使用表单将参数传递给后端接收</p>\n<figure class=\"highlight django\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs django\"><span class=\"language-xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">form</span> <span class=\"hljs-attr\">id</span>=<span class=\"hljs-string\">&quot;表单名&quot;</span> <span class=\"hljs-attr\">action</span>=<span class=\"hljs-string\">&quot;</span></span></span><span class=\"hljs-template-tag\">&#123;% <span class=\"hljs-name\"><span class=\"hljs-name\">url</span></span> &#x27;接收表单的函数&#x27; %&#125;</span><span class=\"language-xml\"><span class=\"hljs-tag\"><span class=\"hljs-string\">&quot;</span> <span class=\"hljs-attr\">method</span>=<span class=\"hljs-string\">&quot;post&quot;</span> &gt;</span></span><br><span class=\"language-xml\">\t</span><span class=\"hljs-template-tag\">&#123;% <span class=\"hljs-name\"><span class=\"hljs-name\">csrf_token</span></span> %&#125;</span><span class=\"language-xml\"> </span><br><span class=\"language-xml\">\t<span class=\"hljs-comment\">&lt;!-- ... 其他表单字段 ... --&gt;</span> </span><br><span class=\"language-xml\">\t<span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">button</span> <span class=\"hljs-attr\">type</span>=<span class=\"hljs-string\">&quot;submit&quot;</span>&gt;</span>提交<span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">button</span>&gt;</span> </span><br><span class=\"language-xml\"><span class=\"hljs-tag\">&lt;/<span class=\"hljs-name\">form</span>&gt;</span></span><br></code></pre></td></tr></table></figure>\n<p>url中的函数需要在views中定义，用来接受表单中的信息并进行处理，接收方式常用<code>get</code>和<code>post</code></p>\n<p>Django使用CSRF令牌保护机制，用于防止恶意网站利用用户的登录状态进行伪造请求。在表单中需要添加<code>&#123;% csrf_token %&#125;</code>标签，它会生成一个包含 CSRF 令牌的隐藏字段。</p>\n<p>使用javascript函数返回布尔值，可以控制表单是否被提交：</p>\n<figure class=\"highlight hsp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs hsp\">&lt;<span class=\"hljs-keyword\">button</span> type=<span class=\"hljs-string\">&quot;submit&quot;</span> <span class=\"hljs-keyword\">onclick</span>=<span class=\"hljs-string\">&quot;return submitForm()&quot;</span>&gt;提交&lt;/<span class=\"hljs-keyword\">button</span>&gt;<br></code></pre></td></tr></table></figure>"},{"title":"扩散模型调研","date":"2023-11-01T01:17:04.297Z","_content":"\n12.27 \n调研AI发展中的表格类数据研究趋势\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231227125526.png)\n\n\n\n\n\n看扩散模型[超详细的扩散模型（Diffusion Models）原理+代码 - 知乎](https://zhuanlan.zhihu.com/p/624221952)\n看TabDDPM\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104173325.png)\n\n看Improved DDPM\n看Generative models for tabular problems、ddpm diffusion model相关检索。\n\n### Accelerating Diffusion Models via Early Stop of the Diffusion Process\n#### 2022 CVPR [2205.12524.pdf](https://arxiv.org/pdf/2205.12524.pdf)\n在实际应用中，DDPM往往需要数百甚至数千个去噪步骤才能从高斯噪声中获得高质量的样本，从而导致推理效率极低。\n在这项工作中，我们提出了一种针对 DDPM 的原则性加速策略，称为 Early-Stopped DDPM (ES-DDPM)。关键思想是尽早停止扩散过程，其中仅考虑少数初始扩散步骤，并且反向降噪过程从非高斯分布开始。 ES-DDPM中进一步**采用强大的预训练生成模型，如GAN和VAE，通过扩散从预训练生成模型获得的样本**，可以有效地实现对目标非高斯分布的采样。通过这种方式，所需的去噪步骤数量显着减少。\n\n为了最大化数据对数似然 log p(x0)，我们需要最小化两个损失项：LDDPM 和LVAE。我们将证明，这意味着我们可以分别训练 ES-DDPM 和变分自动编码器（VAE），然后将它们组合在一起以获得新的生成模型。\n分别训练ES-DDPM和VAE后，我们可以将它们组合起来形成新的生成模型。我们按照方程 5 生成样本，如图 1 所示：首先从标准高斯分布中采样 z，然后使用 VAE 的解码器生成图像 fφ(z)。接下来，从公式 11 中描述的分布中采样 xT '。最后，使用 ES-DDPM 从 pθ(xt−1|xt) 中采样 xt−1，其中 t = T ', T ' − 1, · · · , 1 ，并输出生成的图像x0。注意，在上面的采样过程中，我们只需要使用解码器fφ(z)，而没有使用编码器qψ(z|x0)。这意味着我们不仅可以将 ES-DDPM 与 VAE 结合起来，还可以与任何可以将潜在代码 z 映射到干净图像的生成模型（例如 GAN）结合起来。由于 GAN 训练完成后，它的生成器非常类似于 VAE 的解码器，只是生成器的编码器是未知的，但我们根本不需要编码器从组合模型生成样本。因此，我们也可以将 fφ(z) 视为经过训练的 GAN 的生成器。\n\n### Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\n### 2022 ICLR [Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality | OpenReview](https://openreview.net/forum?id=VFBjuF8HEp)\n**通过区分样本质量进行快速采样**\n我们引入可微扩散采样器搜索（DDSS）：一种通过区分样本质量分数来优化任何预训练扩散模型的快速采样器的方法。我们还提出了广义高斯扩散模型（GGDM），这是一系列用于扩散模型的灵活非马尔可夫采样器。我们表明，通过梯度下降最大化样本质量分数来优化 GGDM 采样器的自由度可以提高样本质量。我们的优化过程使用重参数化技巧和梯度重物化通过采样过程进行反向传播。\n\n### Learning to Efﬁciently Sample from Diffusion Probabilistic Models\n**引入了一种动态规划算法，减少推理步数，进行快速采样**\n在这项工作中，我们将推理调度路径的选择视为一个独立的优化问题，其中我们尝试学习最佳调度。我们的方法依赖于动态规划算法，在给定 K 个细化步骤的固定预算和预先训练的 DDPM 的情况下，我们找到最大化优化目标 (ELBO) 的时间步集。作为优化目标，ELBO 具有关键的可分解性属性：总 ELBO 是各个 KL 项的总和，对于任何两个推理路径，如果时间步 (s, t) 连续出现在两者中，则它们共享一个共同的 KL 项，因此承认记忆化。\n引入了一种动态规划算法，该算法基于 ELBO 为 K 细化步骤的所有可能计算预算找到最佳推理路径。该算法搜索 T > K 个时间步长，仅需要 O(T ) 神经网络前向传递。它只需要对预训练的DDPM 应用一次，不需要训练或重新训练DDPM，并且适用于时间离散和时间连续的DDPM。\n\n\n### Deep Equilibrium Approaches to Diffusion Models\n\n从不同的角度（深度）平衡（DEQ）定点模型来研究扩散模型。具体来说，我们扩展了最近的去噪扩散隐式模型（DDIM）[68]，并将整个采样链建模为联合多变量定点系统。该设置提供了扩散和平衡模型的优雅统一，并显示出以下优点：1）单图像采样，因为它用并行采样过程取代了全串行典型采样过程； 2）模型反演，我们可以利用 DEQ 设置中的快速梯度来更快地找到生成给定图像的噪声。该方法也是正交的，因此与用于减少采样时间或改进模型反演的其他方法互补。\n\n### Improved Denoising Diffusion Probabilistic Models\n在本文中，我们证明 DDPM 可以实现与其他基于似然的模型竞争的对数似然，即使在 ImageNet 等高多样性数据集上也是如此。为了更紧密地优化变分下界（VLB），我们使用简单的重新参数化和混合学习目标来学习逆向过程方差，该混合学习目标将 VLB 与 Ho 等人的简化目标相结合。我们令人惊讶地发现，通过我们的混合目标，我们的模型比直接优化对数似然获得的模型获得了更好的对数似然，并且发现后一个目标在训练过程中具有更多的梯度噪声。我们证明了一种简单的重要性采样技术可以减少这种噪声，并使我们能够比混合目标获得更好的对数似然。在将学习到的方差合并到我们的模型中后，我们惊讶地发现我们可以用很少的时间从我们的模型中**以更少的步骤进行采样**。","source":"_posts/Notes/论文/扩散模型调研.md","raw":"---\ntitle: 扩散模型调研\ncategories:\n  - Notes\n  - 论文\ntags:\n  - 扩散模型\ndate:\n---\n\n12.27 \n调研AI发展中的表格类数据研究趋势\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231227125526.png)\n\n\n\n\n\n看扩散模型[超详细的扩散模型（Diffusion Models）原理+代码 - 知乎](https://zhuanlan.zhihu.com/p/624221952)\n看TabDDPM\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104173325.png)\n\n看Improved DDPM\n看Generative models for tabular problems、ddpm diffusion model相关检索。\n\n### Accelerating Diffusion Models via Early Stop of the Diffusion Process\n#### 2022 CVPR [2205.12524.pdf](https://arxiv.org/pdf/2205.12524.pdf)\n在实际应用中，DDPM往往需要数百甚至数千个去噪步骤才能从高斯噪声中获得高质量的样本，从而导致推理效率极低。\n在这项工作中，我们提出了一种针对 DDPM 的原则性加速策略，称为 Early-Stopped DDPM (ES-DDPM)。关键思想是尽早停止扩散过程，其中仅考虑少数初始扩散步骤，并且反向降噪过程从非高斯分布开始。 ES-DDPM中进一步**采用强大的预训练生成模型，如GAN和VAE，通过扩散从预训练生成模型获得的样本**，可以有效地实现对目标非高斯分布的采样。通过这种方式，所需的去噪步骤数量显着减少。\n\n为了最大化数据对数似然 log p(x0)，我们需要最小化两个损失项：LDDPM 和LVAE。我们将证明，这意味着我们可以分别训练 ES-DDPM 和变分自动编码器（VAE），然后将它们组合在一起以获得新的生成模型。\n分别训练ES-DDPM和VAE后，我们可以将它们组合起来形成新的生成模型。我们按照方程 5 生成样本，如图 1 所示：首先从标准高斯分布中采样 z，然后使用 VAE 的解码器生成图像 fφ(z)。接下来，从公式 11 中描述的分布中采样 xT '。最后，使用 ES-DDPM 从 pθ(xt−1|xt) 中采样 xt−1，其中 t = T ', T ' − 1, · · · , 1 ，并输出生成的图像x0。注意，在上面的采样过程中，我们只需要使用解码器fφ(z)，而没有使用编码器qψ(z|x0)。这意味着我们不仅可以将 ES-DDPM 与 VAE 结合起来，还可以与任何可以将潜在代码 z 映射到干净图像的生成模型（例如 GAN）结合起来。由于 GAN 训练完成后，它的生成器非常类似于 VAE 的解码器，只是生成器的编码器是未知的，但我们根本不需要编码器从组合模型生成样本。因此，我们也可以将 fφ(z) 视为经过训练的 GAN 的生成器。\n\n### Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\n### 2022 ICLR [Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality | OpenReview](https://openreview.net/forum?id=VFBjuF8HEp)\n**通过区分样本质量进行快速采样**\n我们引入可微扩散采样器搜索（DDSS）：一种通过区分样本质量分数来优化任何预训练扩散模型的快速采样器的方法。我们还提出了广义高斯扩散模型（GGDM），这是一系列用于扩散模型的灵活非马尔可夫采样器。我们表明，通过梯度下降最大化样本质量分数来优化 GGDM 采样器的自由度可以提高样本质量。我们的优化过程使用重参数化技巧和梯度重物化通过采样过程进行反向传播。\n\n### Learning to Efﬁciently Sample from Diffusion Probabilistic Models\n**引入了一种动态规划算法，减少推理步数，进行快速采样**\n在这项工作中，我们将推理调度路径的选择视为一个独立的优化问题，其中我们尝试学习最佳调度。我们的方法依赖于动态规划算法，在给定 K 个细化步骤的固定预算和预先训练的 DDPM 的情况下，我们找到最大化优化目标 (ELBO) 的时间步集。作为优化目标，ELBO 具有关键的可分解性属性：总 ELBO 是各个 KL 项的总和，对于任何两个推理路径，如果时间步 (s, t) 连续出现在两者中，则它们共享一个共同的 KL 项，因此承认记忆化。\n引入了一种动态规划算法，该算法基于 ELBO 为 K 细化步骤的所有可能计算预算找到最佳推理路径。该算法搜索 T > K 个时间步长，仅需要 O(T ) 神经网络前向传递。它只需要对预训练的DDPM 应用一次，不需要训练或重新训练DDPM，并且适用于时间离散和时间连续的DDPM。\n\n\n### Deep Equilibrium Approaches to Diffusion Models\n\n从不同的角度（深度）平衡（DEQ）定点模型来研究扩散模型。具体来说，我们扩展了最近的去噪扩散隐式模型（DDIM）[68]，并将整个采样链建模为联合多变量定点系统。该设置提供了扩散和平衡模型的优雅统一，并显示出以下优点：1）单图像采样，因为它用并行采样过程取代了全串行典型采样过程； 2）模型反演，我们可以利用 DEQ 设置中的快速梯度来更快地找到生成给定图像的噪声。该方法也是正交的，因此与用于减少采样时间或改进模型反演的其他方法互补。\n\n### Improved Denoising Diffusion Probabilistic Models\n在本文中，我们证明 DDPM 可以实现与其他基于似然的模型竞争的对数似然，即使在 ImageNet 等高多样性数据集上也是如此。为了更紧密地优化变分下界（VLB），我们使用简单的重新参数化和混合学习目标来学习逆向过程方差，该混合学习目标将 VLB 与 Ho 等人的简化目标相结合。我们令人惊讶地发现，通过我们的混合目标，我们的模型比直接优化对数似然获得的模型获得了更好的对数似然，并且发现后一个目标在训练过程中具有更多的梯度噪声。我们证明了一种简单的重要性采样技术可以减少这种噪声，并使我们能够比混合目标获得更好的对数似然。在将学习到的方差合并到我们的模型中后，我们惊讶地发现我们可以用很少的时间从我们的模型中**以更少的步骤进行采样**。","slug":"Notes/论文/扩散模型调研","published":1,"updated":"2023-12-27T05:00:31.442Z","_id":"clof2n7700000rk8c4g6sf4ry","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>12.27<br>调研AI发展中的表格类数据研究趋势<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231227125526.png\" alt=\"image.png\"></p>\n<p>看扩散模型<a href=\"https://zhuanlan.zhihu.com/p/624221952\">超详细的扩散模型（Diffusion Models）原理+代码 - 知乎</a><br>看TabDDPM<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104173325.png\" alt=\"image.png\"></p>\n<p>看Improved DDPM<br>看Generative models for tabular problems、ddpm diffusion model相关检索。</p>\n<h3 id=\"Accelerating-Diffusion-Models-via-Early-Stop-of-the-Diffusion-Process\"><a href=\"#Accelerating-Diffusion-Models-via-Early-Stop-of-the-Diffusion-Process\" class=\"headerlink\" title=\"Accelerating Diffusion Models via Early Stop of the Diffusion Process\"></a>Accelerating Diffusion Models via Early Stop of the Diffusion Process</h3><h4 id=\"2022-CVPR-2205-12524-pdf\"><a href=\"#2022-CVPR-2205-12524-pdf\" class=\"headerlink\" title=\"2022 CVPR 2205.12524.pdf\"></a>2022 CVPR <a href=\"https://arxiv.org/pdf/2205.12524.pdf\">2205.12524.pdf</a></h4><p>在实际应用中，DDPM往往需要数百甚至数千个去噪步骤才能从高斯噪声中获得高质量的样本，从而导致推理效率极低。<br>在这项工作中，我们提出了一种针对 DDPM 的原则性加速策略，称为 Early-Stopped DDPM (ES-DDPM)。关键思想是尽早停止扩散过程，其中仅考虑少数初始扩散步骤，并且反向降噪过程从非高斯分布开始。 ES-DDPM中进一步<strong>采用强大的预训练生成模型，如GAN和VAE，通过扩散从预训练生成模型获得的样本</strong>，可以有效地实现对目标非高斯分布的采样。通过这种方式，所需的去噪步骤数量显着减少。</p>\n<p>为了最大化数据对数似然 log p(x0)，我们需要最小化两个损失项：LDDPM 和LVAE。我们将证明，这意味着我们可以分别训练 ES-DDPM 和变分自动编码器（VAE），然后将它们组合在一起以获得新的生成模型。<br>分别训练ES-DDPM和VAE后，我们可以将它们组合起来形成新的生成模型。我们按照方程 5 生成样本，如图 1 所示：首先从标准高斯分布中采样 z，然后使用 VAE 的解码器生成图像 fφ(z)。接下来，从公式 11 中描述的分布中采样 xT ‘。最后，使用 ES-DDPM 从 pθ(xt−1|xt) 中采样 xt−1，其中 t &#x3D; T ‘, T ‘ − 1, · · · , 1 ，并输出生成的图像x0。注意，在上面的采样过程中，我们只需要使用解码器fφ(z)，而没有使用编码器qψ(z|x0)。这意味着我们不仅可以将 ES-DDPM 与 VAE 结合起来，还可以与任何可以将潜在代码 z 映射到干净图像的生成模型（例如 GAN）结合起来。由于 GAN 训练完成后，它的生成器非常类似于 VAE 的解码器，只是生成器的编码器是未知的，但我们根本不需要编码器从组合模型生成样本。因此，我们也可以将 fφ(z) 视为经过训练的 GAN 的生成器。</p>\n<h3 id=\"Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality\"><a href=\"#Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality\" class=\"headerlink\" title=\"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\"></a>Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality</h3><h3 id=\"2022-ICLR-Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality-OpenReview\"><a href=\"#2022-ICLR-Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality-OpenReview\" class=\"headerlink\" title=\"2022 ICLR Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality | OpenReview\"></a>2022 ICLR <a href=\"https://openreview.net/forum?id=VFBjuF8HEp\">Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality | OpenReview</a></h3><p><strong>通过区分样本质量进行快速采样</strong><br>我们引入可微扩散采样器搜索（DDSS）：一种通过区分样本质量分数来优化任何预训练扩散模型的快速采样器的方法。我们还提出了广义高斯扩散模型（GGDM），这是一系列用于扩散模型的灵活非马尔可夫采样器。我们表明，通过梯度下降最大化样本质量分数来优化 GGDM 采样器的自由度可以提高样本质量。我们的优化过程使用重参数化技巧和梯度重物化通过采样过程进行反向传播。</p>\n<h3 id=\"Learning-to-Efﬁciently-Sample-from-Diffusion-Probabilistic-Models\"><a href=\"#Learning-to-Efﬁciently-Sample-from-Diffusion-Probabilistic-Models\" class=\"headerlink\" title=\"Learning to Efﬁciently Sample from Diffusion Probabilistic Models\"></a>Learning to Efﬁciently Sample from Diffusion Probabilistic Models</h3><p><strong>引入了一种动态规划算法，减少推理步数，进行快速采样</strong><br>在这项工作中，我们将推理调度路径的选择视为一个独立的优化问题，其中我们尝试学习最佳调度。我们的方法依赖于动态规划算法，在给定 K 个细化步骤的固定预算和预先训练的 DDPM 的情况下，我们找到最大化优化目标 (ELBO) 的时间步集。作为优化目标，ELBO 具有关键的可分解性属性：总 ELBO 是各个 KL 项的总和，对于任何两个推理路径，如果时间步 (s, t) 连续出现在两者中，则它们共享一个共同的 KL 项，因此承认记忆化。<br>引入了一种动态规划算法，该算法基于 ELBO 为 K 细化步骤的所有可能计算预算找到最佳推理路径。该算法搜索 T &gt; K 个时间步长，仅需要 O(T ) 神经网络前向传递。它只需要对预训练的DDPM 应用一次，不需要训练或重新训练DDPM，并且适用于时间离散和时间连续的DDPM。</p>\n<h3 id=\"Deep-Equilibrium-Approaches-to-Diffusion-Models\"><a href=\"#Deep-Equilibrium-Approaches-to-Diffusion-Models\" class=\"headerlink\" title=\"Deep Equilibrium Approaches to Diffusion Models\"></a>Deep Equilibrium Approaches to Diffusion Models</h3><p>从不同的角度（深度）平衡（DEQ）定点模型来研究扩散模型。具体来说，我们扩展了最近的去噪扩散隐式模型（DDIM）[68]，并将整个采样链建模为联合多变量定点系统。该设置提供了扩散和平衡模型的优雅统一，并显示出以下优点：1）单图像采样，因为它用并行采样过程取代了全串行典型采样过程； 2）模型反演，我们可以利用 DEQ 设置中的快速梯度来更快地找到生成给定图像的噪声。该方法也是正交的，因此与用于减少采样时间或改进模型反演的其他方法互补。</p>\n<h3 id=\"Improved-Denoising-Diffusion-Probabilistic-Models\"><a href=\"#Improved-Denoising-Diffusion-Probabilistic-Models\" class=\"headerlink\" title=\"Improved Denoising Diffusion Probabilistic Models\"></a>Improved Denoising Diffusion Probabilistic Models</h3><p>在本文中，我们证明 DDPM 可以实现与其他基于似然的模型竞争的对数似然，即使在 ImageNet 等高多样性数据集上也是如此。为了更紧密地优化变分下界（VLB），我们使用简单的重新参数化和混合学习目标来学习逆向过程方差，该混合学习目标将 VLB 与 Ho 等人的简化目标相结合。我们令人惊讶地发现，通过我们的混合目标，我们的模型比直接优化对数似然获得的模型获得了更好的对数似然，并且发现后一个目标在训练过程中具有更多的梯度噪声。我们证明了一种简单的重要性采样技术可以减少这种噪声，并使我们能够比混合目标获得更好的对数似然。在将学习到的方差合并到我们的模型中后，我们惊讶地发现我们可以用很少的时间从我们的模型中<strong>以更少的步骤进行采样</strong>。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>12.27<br>调研AI发展中的表格类数据研究趋势<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231227125526.png\" alt=\"image.png\"></p>\n<p>看扩散模型<a href=\"https://zhuanlan.zhihu.com/p/624221952\">超详细的扩散模型（Diffusion Models）原理+代码 - 知乎</a><br>看TabDDPM<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104173325.png\" alt=\"image.png\"></p>\n<p>看Improved DDPM<br>看Generative models for tabular problems、ddpm diffusion model相关检索。</p>\n<h3 id=\"Accelerating-Diffusion-Models-via-Early-Stop-of-the-Diffusion-Process\"><a href=\"#Accelerating-Diffusion-Models-via-Early-Stop-of-the-Diffusion-Process\" class=\"headerlink\" title=\"Accelerating Diffusion Models via Early Stop of the Diffusion Process\"></a>Accelerating Diffusion Models via Early Stop of the Diffusion Process</h3><h4 id=\"2022-CVPR-2205-12524-pdf\"><a href=\"#2022-CVPR-2205-12524-pdf\" class=\"headerlink\" title=\"2022 CVPR 2205.12524.pdf\"></a>2022 CVPR <a href=\"https://arxiv.org/pdf/2205.12524.pdf\">2205.12524.pdf</a></h4><p>在实际应用中，DDPM往往需要数百甚至数千个去噪步骤才能从高斯噪声中获得高质量的样本，从而导致推理效率极低。<br>在这项工作中，我们提出了一种针对 DDPM 的原则性加速策略，称为 Early-Stopped DDPM (ES-DDPM)。关键思想是尽早停止扩散过程，其中仅考虑少数初始扩散步骤，并且反向降噪过程从非高斯分布开始。 ES-DDPM中进一步<strong>采用强大的预训练生成模型，如GAN和VAE，通过扩散从预训练生成模型获得的样本</strong>，可以有效地实现对目标非高斯分布的采样。通过这种方式，所需的去噪步骤数量显着减少。</p>\n<p>为了最大化数据对数似然 log p(x0)，我们需要最小化两个损失项：LDDPM 和LVAE。我们将证明，这意味着我们可以分别训练 ES-DDPM 和变分自动编码器（VAE），然后将它们组合在一起以获得新的生成模型。<br>分别训练ES-DDPM和VAE后，我们可以将它们组合起来形成新的生成模型。我们按照方程 5 生成样本，如图 1 所示：首先从标准高斯分布中采样 z，然后使用 VAE 的解码器生成图像 fφ(z)。接下来，从公式 11 中描述的分布中采样 xT ‘。最后，使用 ES-DDPM 从 pθ(xt−1|xt) 中采样 xt−1，其中 t &#x3D; T ‘, T ‘ − 1, · · · , 1 ，并输出生成的图像x0。注意，在上面的采样过程中，我们只需要使用解码器fφ(z)，而没有使用编码器qψ(z|x0)。这意味着我们不仅可以将 ES-DDPM 与 VAE 结合起来，还可以与任何可以将潜在代码 z 映射到干净图像的生成模型（例如 GAN）结合起来。由于 GAN 训练完成后，它的生成器非常类似于 VAE 的解码器，只是生成器的编码器是未知的，但我们根本不需要编码器从组合模型生成样本。因此，我们也可以将 fφ(z) 视为经过训练的 GAN 的生成器。</p>\n<h3 id=\"Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality\"><a href=\"#Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality\" class=\"headerlink\" title=\"Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality\"></a>Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality</h3><h3 id=\"2022-ICLR-Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality-OpenReview\"><a href=\"#2022-ICLR-Learning-Fast-Samplers-for-Diffusion-Models-by-Differentiating-Through-Sample-Quality-OpenReview\" class=\"headerlink\" title=\"2022 ICLR Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality | OpenReview\"></a>2022 ICLR <a href=\"https://openreview.net/forum?id=VFBjuF8HEp\">Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality | OpenReview</a></h3><p><strong>通过区分样本质量进行快速采样</strong><br>我们引入可微扩散采样器搜索（DDSS）：一种通过区分样本质量分数来优化任何预训练扩散模型的快速采样器的方法。我们还提出了广义高斯扩散模型（GGDM），这是一系列用于扩散模型的灵活非马尔可夫采样器。我们表明，通过梯度下降最大化样本质量分数来优化 GGDM 采样器的自由度可以提高样本质量。我们的优化过程使用重参数化技巧和梯度重物化通过采样过程进行反向传播。</p>\n<h3 id=\"Learning-to-Efﬁciently-Sample-from-Diffusion-Probabilistic-Models\"><a href=\"#Learning-to-Efﬁciently-Sample-from-Diffusion-Probabilistic-Models\" class=\"headerlink\" title=\"Learning to Efﬁciently Sample from Diffusion Probabilistic Models\"></a>Learning to Efﬁciently Sample from Diffusion Probabilistic Models</h3><p><strong>引入了一种动态规划算法，减少推理步数，进行快速采样</strong><br>在这项工作中，我们将推理调度路径的选择视为一个独立的优化问题，其中我们尝试学习最佳调度。我们的方法依赖于动态规划算法，在给定 K 个细化步骤的固定预算和预先训练的 DDPM 的情况下，我们找到最大化优化目标 (ELBO) 的时间步集。作为优化目标，ELBO 具有关键的可分解性属性：总 ELBO 是各个 KL 项的总和，对于任何两个推理路径，如果时间步 (s, t) 连续出现在两者中，则它们共享一个共同的 KL 项，因此承认记忆化。<br>引入了一种动态规划算法，该算法基于 ELBO 为 K 细化步骤的所有可能计算预算找到最佳推理路径。该算法搜索 T &gt; K 个时间步长，仅需要 O(T ) 神经网络前向传递。它只需要对预训练的DDPM 应用一次，不需要训练或重新训练DDPM，并且适用于时间离散和时间连续的DDPM。</p>\n<h3 id=\"Deep-Equilibrium-Approaches-to-Diffusion-Models\"><a href=\"#Deep-Equilibrium-Approaches-to-Diffusion-Models\" class=\"headerlink\" title=\"Deep Equilibrium Approaches to Diffusion Models\"></a>Deep Equilibrium Approaches to Diffusion Models</h3><p>从不同的角度（深度）平衡（DEQ）定点模型来研究扩散模型。具体来说，我们扩展了最近的去噪扩散隐式模型（DDIM）[68]，并将整个采样链建模为联合多变量定点系统。该设置提供了扩散和平衡模型的优雅统一，并显示出以下优点：1）单图像采样，因为它用并行采样过程取代了全串行典型采样过程； 2）模型反演，我们可以利用 DEQ 设置中的快速梯度来更快地找到生成给定图像的噪声。该方法也是正交的，因此与用于减少采样时间或改进模型反演的其他方法互补。</p>\n<h3 id=\"Improved-Denoising-Diffusion-Probabilistic-Models\"><a href=\"#Improved-Denoising-Diffusion-Probabilistic-Models\" class=\"headerlink\" title=\"Improved Denoising Diffusion Probabilistic Models\"></a>Improved Denoising Diffusion Probabilistic Models</h3><p>在本文中，我们证明 DDPM 可以实现与其他基于似然的模型竞争的对数似然，即使在 ImageNet 等高多样性数据集上也是如此。为了更紧密地优化变分下界（VLB），我们使用简单的重新参数化和混合学习目标来学习逆向过程方差，该混合学习目标将 VLB 与 Ho 等人的简化目标相结合。我们令人惊讶地发现，通过我们的混合目标，我们的模型比直接优化对数似然获得的模型获得了更好的对数似然，并且发现后一个目标在训练过程中具有更多的梯度噪声。我们证明了一种简单的重要性采样技术可以减少这种噪声，并使我们能够比混合目标获得更好的对数似然。在将学习到的方差合并到我们的模型中后，我们惊讶地发现我们可以用很少的时间从我们的模型中<strong>以更少的步骤进行采样</strong>。</p>\n"},{"title":"ChatEdit","date":"2023-11-13T12:43:06.374Z","_content":"\n### ChatEdit\nChatEdit是一个数据集，用于评估在此背景下的图像编辑和对话能力。\nChatEdit是根据CelebA-HQ数据集（30k张1024×1024面部图像数据集，它提供了 40 个面部属性的二进制注释）构建的，包含与图像上的用户编辑请求相对应的带注释的多轮对话，21个可编辑属性\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231113213751.png)\n\n\nTODO：\n数据集是什么类型？一张图像和几个标签？\n图像生成后怎么判断生成是否符合要求\n\n\n### 交互式面部图像编辑\n1. 跟踪用户编辑请求\n2. 图像编辑\n3. 响应生成（对话）\n\n跟踪整个对话历史记录到当前用户编辑请求，直接对原始图像进行修改，而不是调整前一轮的输出。减少错误累计、防止属性遗忘","source":"_posts/Notes/论文/ChatEdit.md","raw":"---\ntitle: ChatEdit\ncategories:\n  - Notes\n  - 论文\ndate:\ntags:\n---\n\n### ChatEdit\nChatEdit是一个数据集，用于评估在此背景下的图像编辑和对话能力。\nChatEdit是根据CelebA-HQ数据集（30k张1024×1024面部图像数据集，它提供了 40 个面部属性的二进制注释）构建的，包含与图像上的用户编辑请求相对应的带注释的多轮对话，21个可编辑属性\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231113213751.png)\n\n\nTODO：\n数据集是什么类型？一张图像和几个标签？\n图像生成后怎么判断生成是否符合要求\n\n\n### 交互式面部图像编辑\n1. 跟踪用户编辑请求\n2. 图像编辑\n3. 响应生成（对话）\n\n跟踪整个对话历史记录到当前用户编辑请求，直接对原始图像进行修改，而不是调整前一轮的输出。减少错误累计、防止属性遗忘","slug":"Notes/论文/ChatEdit","published":1,"updated":"2023-11-13T14:06:20.761Z","_id":"cloww9wlj0000n48c0mfd6ilu","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"ChatEdit\"><a href=\"#ChatEdit\" class=\"headerlink\" title=\"ChatEdit\"></a>ChatEdit</h3><p>ChatEdit是一个数据集，用于评估在此背景下的图像编辑和对话能力。<br>ChatEdit是根据CelebA-HQ数据集（30k张1024×1024面部图像数据集，它提供了 40 个面部属性的二进制注释）构建的，包含与图像上的用户编辑请求相对应的带注释的多轮对话，21个可编辑属性<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231113213751.png\" alt=\"image.png\"></p>\n<p>TODO：<br>数据集是什么类型？一张图像和几个标签？<br>图像生成后怎么判断生成是否符合要求</p>\n<h3 id=\"交互式面部图像编辑\"><a href=\"#交互式面部图像编辑\" class=\"headerlink\" title=\"交互式面部图像编辑\"></a>交互式面部图像编辑</h3><ol>\n<li>跟踪用户编辑请求</li>\n<li>图像编辑</li>\n<li>响应生成（对话）</li>\n</ol>\n<p>跟踪整个对话历史记录到当前用户编辑请求，直接对原始图像进行修改，而不是调整前一轮的输出。减少错误累计、防止属性遗忘</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"ChatEdit\"><a href=\"#ChatEdit\" class=\"headerlink\" title=\"ChatEdit\"></a>ChatEdit</h3><p>ChatEdit是一个数据集，用于评估在此背景下的图像编辑和对话能力。<br>ChatEdit是根据CelebA-HQ数据集（30k张1024×1024面部图像数据集，它提供了 40 个面部属性的二进制注释）构建的，包含与图像上的用户编辑请求相对应的带注释的多轮对话，21个可编辑属性<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231113213751.png\" alt=\"image.png\"></p>\n<p>TODO：<br>数据集是什么类型？一张图像和几个标签？<br>图像生成后怎么判断生成是否符合要求</p>\n<h3 id=\"交互式面部图像编辑\"><a href=\"#交互式面部图像编辑\" class=\"headerlink\" title=\"交互式面部图像编辑\"></a>交互式面部图像编辑</h3><ol>\n<li>跟踪用户编辑请求</li>\n<li>图像编辑</li>\n<li>响应生成（对话）</li>\n</ol>\n<p>跟踪整个对话历史记录到当前用户编辑请求，直接对原始图像进行修改，而不是调整前一轮的输出。减少错误累计、防止属性遗忘</p>\n"},{"title":"论文查阅、写作与投稿的综合指南","date":"2023-11-15T10:32:57.008Z","_content":"\n学习性阅读：获取知识\n批判性阅读：创造知识\n论文中的研究内容得到的检验少，需要更多批判\n正题\n反题\n合题：结合正题和反题\n\n问题是完成任务时出现某种现象的内在原因\n批判性阅读帮助发现、提出问题\n\n如何进行批判性阅读：\n批：比较（求诸于外） 判：剖析（求诸于内）\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192520.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192615.png)\n\n论文写作工具：Overleaf & Grammarly\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193425.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193727.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194316.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194647.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115200808.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204202.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204413.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115205611.png)\n","source":"_posts/Notes/论文/论文查阅、写作与投稿的综合指南.md","raw":"---\ntitle: 论文查阅、写作与投稿的综合指南\ncategories:\n  - Notes\n  - 论文\ndate:\ntags:\n---\n\n学习性阅读：获取知识\n批判性阅读：创造知识\n论文中的研究内容得到的检验少，需要更多批判\n正题\n反题\n合题：结合正题和反题\n\n问题是完成任务时出现某种现象的内在原因\n批判性阅读帮助发现、提出问题\n\n如何进行批判性阅读：\n批：比较（求诸于外） 判：剖析（求诸于内）\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192520.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192615.png)\n\n论文写作工具：Overleaf & Grammarly\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193425.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193727.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194316.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194647.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115200808.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204202.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204413.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115205611.png)\n","slug":"Notes/论文/论文查阅、写作与投稿的综合指南","published":1,"updated":"2023-11-15T12:56:16.272Z","_id":"clozmwiwz0000ug8ceroj0vt8","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>学习性阅读：获取知识<br>批判性阅读：创造知识<br>论文中的研究内容得到的检验少，需要更多批判<br>正题<br>反题<br>合题：结合正题和反题</p>\n<p>问题是完成任务时出现某种现象的内在原因<br>批判性阅读帮助发现、提出问题</p>\n<p>如何进行批判性阅读：<br>批：比较（求诸于外） 判：剖析（求诸于内）</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192520.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192615.png\" alt=\"image.png\"></p>\n<p>论文写作工具：Overleaf &amp; Grammarly</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193425.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193727.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194316.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194647.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115200808.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204202.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204413.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115205611.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>学习性阅读：获取知识<br>批判性阅读：创造知识<br>论文中的研究内容得到的检验少，需要更多批判<br>正题<br>反题<br>合题：结合正题和反题</p>\n<p>问题是完成任务时出现某种现象的内在原因<br>批判性阅读帮助发现、提出问题</p>\n<p>如何进行批判性阅读：<br>批：比较（求诸于外） 判：剖析（求诸于内）</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192520.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115192615.png\" alt=\"image.png\"></p>\n<p>论文写作工具：Overleaf &amp; Grammarly</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193425.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115193727.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194316.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115194647.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115200808.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204202.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115204413.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115205611.png\" alt=\"image.png\"></p>\n"},{"title":"择校","date":"2024-02-02T14:07:52.561Z","_content":"\n- 人大高瓴\n- 南大\n- 北航\n- 上交\n- 自动化所","source":"_posts/Notes/考研/择校.md","raw":"---\ntitle: 择校\ncategories:\n  - Notes\n  - 考研\ndate:\ntags:\n---\n\n- 人大高瓴\n- 南大\n- 北航\n- 上交\n- 自动化所","slug":"Notes/考研/择校","published":1,"updated":"2024-02-02T14:17:38.785Z","_id":"cls4q01yx0000ko8cesu726dg","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><ul>\n<li>人大高瓴</li>\n<li>南大</li>\n<li>北航</li>\n<li>上交</li>\n<li>自动化所</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li>人大高瓴</li>\n<li>南大</li>\n<li>北航</li>\n<li>上交</li>\n<li>自动化所</li>\n</ul>\n"},{"title":"C语言基础","date":"2024-02-25T07:04:07.368Z","_content":"\n### 类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150635.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150754.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150818.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150933.png)\n","source":"_posts/Notes/考研/C语言基础.md","raw":"---\ncategories:\n  - Notes\n  - 考研\ntitle: C语言基础\ndate: \ntags:\n---\n\n### 类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150635.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150754.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150818.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150933.png)\n","slug":"Notes/考研/C语言基础","published":1,"updated":"2024-02-25T07:09:35.036Z","_id":"clt15zuhi00000w8capbb3ggw","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"类型\"><a href=\"#类型\" class=\"headerlink\" title=\"类型\"></a>类型</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150635.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150754.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150818.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150933.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"类型\"><a href=\"#类型\" class=\"headerlink\" title=\"类型\"></a>类型</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150635.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150754.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150818.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240225150933.png\" alt=\"image.png\"></p>\n"},{"title":"Blitz软件","update":null,"_content":"Blitz是一个软件包，内含一个虚拟机，给操作者提供一个建立操作系统内核的功能\n\n学生将在主机上编辑、编译和链接他们的操作系统内核项目。然后，他们将使用仿真器在虚拟机上执行这些项目。当学生的代码出现错误时，仿真器会显示各种错误信息，学生可以使用仿真器工具进行调试\n\n#### 架构\nCPU 采用 RISC 设计，大致仿照 Sun 的 Sparc 架构。 BLITZ 处理器包含 32 个通用整数寄存器，每个寄存器为 32 位。\n\n处理器有两种执行模式，即 \"系统模式 \"和 \"用户模式\"。内核代码在系统模式下运行，而应用程序则在用户模式下运行。\n\nBLITZ 机器包括两个输入输出设备。第一个是磁盘。仿真器通过主机上的文件来模拟磁盘。第二个 I/O 设备是终端，仿真器通常将终端输入/输出直接传递到主机的用户界面，这样学生就可以直接与运行中的 BLITZ 代码交互。\n\n该架构还包括一个 \"trap \"指令，用户程序可以执行该指令。陷阱指令允许用户进程进入内核。\n\n调试器内置于模拟器中，并且是模拟器的组成部分。该模拟器可以在命令行模式下运行，学生可以一次键入一个命令。这些命令可用于调试其 BLITZ 内核代码。\n\nCPU 都在系统模式或用户模式下执行。内核代码在系统模式下执行，而用户级程序在用户模式下执行。任何指令都可以在系统模式下执行，但某些指令是有特权的，因为它们只能在系统模式下执行。特权指令的示例包括更改页表和更改模式本身的指令。\n\n在 BLITZ 架构中，寄存器“r15”指向执行堆栈，该堆栈从较高的内存地址向较低的地址向下增长。\n#### 工具\n- **blitz：** The BLITZ virtual machine emulator and debugger虚拟机模拟器和调试器\n- **asm：** The BLITZ assembler汇编器。\n> 汇编器（Assembler）是将汇编语言翻译为机器语言的程序。一般而言，汇编生成的是目标代码，需要经链接器（Linker）生成可执行代码才可以执行。\n- **lddd：** The BLITZ linker链接器\n- **dumpObj：** A tool to examine BLITZ object and executable files检查blitz对象和可执行文件\n- **diskUtil：** A tool to manipulate the initial file system on the emulated BLITZ disk用于操作模拟 BLITZ 磁盘上的初始文件系统的工具\n- **kpl：** The KPL compiler编译器\n\n#### 模拟器Emulator\n在 Unix 下运行时，您可以通过在 Unix 提示符下键入“blitz”来启动模拟器。模拟器首先读入 BLITZ 程序并将其加载到内存中。通常，BLITZ 可执行文件称为“a.out”，但也可以指定其他名称。模拟器首先从“a.out”读取数据并将其加载到其内部存储器中。实际上，模拟器首先使用“a.out”文件中的字节初始化 BLITZ 机器的主内存\n\nasm 程序是 BLITZ 汇编程序。它以汇编语言程序为输入，生成一个对象文件\n第二个程序（称为 \"ldd\"）是 BLITZ 连接器。它输入一个或多个对象文件，生成一个名为 \"a.out \"的可执行文件。\n\nquit 命令（可缩写为 \"q\"）将终止 BLITZ 模拟器。\n\ngo 指令（可缩写为 \"g\"）用于启动仿真器的执行。一旦开始执行，BLITZ 机器将执行指令，直到检测到错误或 BLITZ 机器执行 \"等待 \"或 \"调试 \"指令为止。\n\n在KPL中，以“.h”结尾的文件称为“头文件”，以“.c”结尾的文件称为“代码文件”。每个包（如HelloWorld）都会有头文件和代码文件。HelloWorld包使用系统包。每当HelloWorld使用的包的头文件发生变化时，HelloWorld必须重新编译。但是，如果系统的代码文件发生变化，则不需要重新编译HelloWorld。\n\n","source":"_posts/Notes/课程/大三（上）/操作系统/Blitz软件.md","raw":"---\ntitle: Blitz软件\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\nupdate:\n---\nBlitz是一个软件包，内含一个虚拟机，给操作者提供一个建立操作系统内核的功能\n\n学生将在主机上编辑、编译和链接他们的操作系统内核项目。然后，他们将使用仿真器在虚拟机上执行这些项目。当学生的代码出现错误时，仿真器会显示各种错误信息，学生可以使用仿真器工具进行调试\n\n#### 架构\nCPU 采用 RISC 设计，大致仿照 Sun 的 Sparc 架构。 BLITZ 处理器包含 32 个通用整数寄存器，每个寄存器为 32 位。\n\n处理器有两种执行模式，即 \"系统模式 \"和 \"用户模式\"。内核代码在系统模式下运行，而应用程序则在用户模式下运行。\n\nBLITZ 机器包括两个输入输出设备。第一个是磁盘。仿真器通过主机上的文件来模拟磁盘。第二个 I/O 设备是终端，仿真器通常将终端输入/输出直接传递到主机的用户界面，这样学生就可以直接与运行中的 BLITZ 代码交互。\n\n该架构还包括一个 \"trap \"指令，用户程序可以执行该指令。陷阱指令允许用户进程进入内核。\n\n调试器内置于模拟器中，并且是模拟器的组成部分。该模拟器可以在命令行模式下运行，学生可以一次键入一个命令。这些命令可用于调试其 BLITZ 内核代码。\n\nCPU 都在系统模式或用户模式下执行。内核代码在系统模式下执行，而用户级程序在用户模式下执行。任何指令都可以在系统模式下执行，但某些指令是有特权的，因为它们只能在系统模式下执行。特权指令的示例包括更改页表和更改模式本身的指令。\n\n在 BLITZ 架构中，寄存器“r15”指向执行堆栈，该堆栈从较高的内存地址向较低的地址向下增长。\n#### 工具\n- **blitz：** The BLITZ virtual machine emulator and debugger虚拟机模拟器和调试器\n- **asm：** The BLITZ assembler汇编器。\n> 汇编器（Assembler）是将汇编语言翻译为机器语言的程序。一般而言，汇编生成的是目标代码，需要经链接器（Linker）生成可执行代码才可以执行。\n- **lddd：** The BLITZ linker链接器\n- **dumpObj：** A tool to examine BLITZ object and executable files检查blitz对象和可执行文件\n- **diskUtil：** A tool to manipulate the initial file system on the emulated BLITZ disk用于操作模拟 BLITZ 磁盘上的初始文件系统的工具\n- **kpl：** The KPL compiler编译器\n\n#### 模拟器Emulator\n在 Unix 下运行时，您可以通过在 Unix 提示符下键入“blitz”来启动模拟器。模拟器首先读入 BLITZ 程序并将其加载到内存中。通常，BLITZ 可执行文件称为“a.out”，但也可以指定其他名称。模拟器首先从“a.out”读取数据并将其加载到其内部存储器中。实际上，模拟器首先使用“a.out”文件中的字节初始化 BLITZ 机器的主内存\n\nasm 程序是 BLITZ 汇编程序。它以汇编语言程序为输入，生成一个对象文件\n第二个程序（称为 \"ldd\"）是 BLITZ 连接器。它输入一个或多个对象文件，生成一个名为 \"a.out \"的可执行文件。\n\nquit 命令（可缩写为 \"q\"）将终止 BLITZ 模拟器。\n\ngo 指令（可缩写为 \"g\"）用于启动仿真器的执行。一旦开始执行，BLITZ 机器将执行指令，直到检测到错误或 BLITZ 机器执行 \"等待 \"或 \"调试 \"指令为止。\n\n在KPL中，以“.h”结尾的文件称为“头文件”，以“.c”结尾的文件称为“代码文件”。每个包（如HelloWorld）都会有头文件和代码文件。HelloWorld包使用系统包。每当HelloWorld使用的包的头文件发生变化时，HelloWorld必须重新编译。但是，如果系统的代码文件发生变化，则不需要重新编译HelloWorld。\n\n","slug":"Notes/课程/大三（上）/操作系统/Blitz软件","published":1,"date":"2023-09-21T16:36:34.336Z","updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr11r0000vw8c7idr7zk9","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>Blitz是一个软件包，内含一个虚拟机，给操作者提供一个建立操作系统内核的功能</p>\n<p>学生将在主机上编辑、编译和链接他们的操作系统内核项目。然后，他们将使用仿真器在虚拟机上执行这些项目。当学生的代码出现错误时，仿真器会显示各种错误信息，学生可以使用仿真器工具进行调试</p>\n<h4 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h4><p>CPU 采用 RISC 设计，大致仿照 Sun 的 Sparc 架构。 BLITZ 处理器包含 32 个通用整数寄存器，每个寄存器为 32 位。</p>\n<p>处理器有两种执行模式，即 “系统模式 “和 “用户模式”。内核代码在系统模式下运行，而应用程序则在用户模式下运行。</p>\n<p>BLITZ 机器包括两个输入输出设备。第一个是磁盘。仿真器通过主机上的文件来模拟磁盘。第二个 I&#x2F;O 设备是终端，仿真器通常将终端输入&#x2F;输出直接传递到主机的用户界面，这样学生就可以直接与运行中的 BLITZ 代码交互。</p>\n<p>该架构还包括一个 “trap “指令，用户程序可以执行该指令。陷阱指令允许用户进程进入内核。</p>\n<p>调试器内置于模拟器中，并且是模拟器的组成部分。该模拟器可以在命令行模式下运行，学生可以一次键入一个命令。这些命令可用于调试其 BLITZ 内核代码。</p>\n<p>CPU 都在系统模式或用户模式下执行。内核代码在系统模式下执行，而用户级程序在用户模式下执行。任何指令都可以在系统模式下执行，但某些指令是有特权的，因为它们只能在系统模式下执行。特权指令的示例包括更改页表和更改模式本身的指令。</p>\n<p>在 BLITZ 架构中，寄存器“r15”指向执行堆栈，该堆栈从较高的内存地址向较低的地址向下增长。</p>\n<h4 id=\"工具\"><a href=\"#工具\" class=\"headerlink\" title=\"工具\"></a>工具</h4><ul>\n<li><strong>blitz：</strong> The BLITZ virtual machine emulator and debugger虚拟机模拟器和调试器</li>\n<li><strong>asm：</strong> The BLITZ assembler汇编器。<blockquote>\n<p>汇编器（Assembler）是将汇编语言翻译为机器语言的程序。一般而言，汇编生成的是目标代码，需要经链接器（Linker）生成可执行代码才可以执行。</p>\n</blockquote>\n</li>\n<li><strong>lddd：</strong> The BLITZ linker链接器</li>\n<li><strong>dumpObj：</strong> A tool to examine BLITZ object and executable files检查blitz对象和可执行文件</li>\n<li><strong>diskUtil：</strong> A tool to manipulate the initial file system on the emulated BLITZ disk用于操作模拟 BLITZ 磁盘上的初始文件系统的工具</li>\n<li><strong>kpl：</strong> The KPL compiler编译器</li>\n</ul>\n<h4 id=\"模拟器Emulator\"><a href=\"#模拟器Emulator\" class=\"headerlink\" title=\"模拟器Emulator\"></a>模拟器Emulator</h4><p>在 Unix 下运行时，您可以通过在 Unix 提示符下键入“blitz”来启动模拟器。模拟器首先读入 BLITZ 程序并将其加载到内存中。通常，BLITZ 可执行文件称为“a.out”，但也可以指定其他名称。模拟器首先从“a.out”读取数据并将其加载到其内部存储器中。实际上，模拟器首先使用“a.out”文件中的字节初始化 BLITZ 机器的主内存</p>\n<p>asm 程序是 BLITZ 汇编程序。它以汇编语言程序为输入，生成一个对象文件<br>第二个程序（称为 “ldd”）是 BLITZ 连接器。它输入一个或多个对象文件，生成一个名为 “a.out “的可执行文件。</p>\n<p>quit 命令（可缩写为 “q”）将终止 BLITZ 模拟器。</p>\n<p>go 指令（可缩写为 “g”）用于启动仿真器的执行。一旦开始执行，BLITZ 机器将执行指令，直到检测到错误或 BLITZ 机器执行 “等待 “或 “调试 “指令为止。</p>\n<p>在KPL中，以“.h”结尾的文件称为“头文件”，以“.c”结尾的文件称为“代码文件”。每个包（如HelloWorld）都会有头文件和代码文件。HelloWorld包使用系统包。每当HelloWorld使用的包的头文件发生变化时，HelloWorld必须重新编译。但是，如果系统的代码文件发生变化，则不需要重新编译HelloWorld。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Blitz是一个软件包，内含一个虚拟机，给操作者提供一个建立操作系统内核的功能</p>\n<p>学生将在主机上编辑、编译和链接他们的操作系统内核项目。然后，他们将使用仿真器在虚拟机上执行这些项目。当学生的代码出现错误时，仿真器会显示各种错误信息，学生可以使用仿真器工具进行调试</p>\n<h4 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h4><p>CPU 采用 RISC 设计，大致仿照 Sun 的 Sparc 架构。 BLITZ 处理器包含 32 个通用整数寄存器，每个寄存器为 32 位。</p>\n<p>处理器有两种执行模式，即 “系统模式 “和 “用户模式”。内核代码在系统模式下运行，而应用程序则在用户模式下运行。</p>\n<p>BLITZ 机器包括两个输入输出设备。第一个是磁盘。仿真器通过主机上的文件来模拟磁盘。第二个 I&#x2F;O 设备是终端，仿真器通常将终端输入&#x2F;输出直接传递到主机的用户界面，这样学生就可以直接与运行中的 BLITZ 代码交互。</p>\n<p>该架构还包括一个 “trap “指令，用户程序可以执行该指令。陷阱指令允许用户进程进入内核。</p>\n<p>调试器内置于模拟器中，并且是模拟器的组成部分。该模拟器可以在命令行模式下运行，学生可以一次键入一个命令。这些命令可用于调试其 BLITZ 内核代码。</p>\n<p>CPU 都在系统模式或用户模式下执行。内核代码在系统模式下执行，而用户级程序在用户模式下执行。任何指令都可以在系统模式下执行，但某些指令是有特权的，因为它们只能在系统模式下执行。特权指令的示例包括更改页表和更改模式本身的指令。</p>\n<p>在 BLITZ 架构中，寄存器“r15”指向执行堆栈，该堆栈从较高的内存地址向较低的地址向下增长。</p>\n<h4 id=\"工具\"><a href=\"#工具\" class=\"headerlink\" title=\"工具\"></a>工具</h4><ul>\n<li><strong>blitz：</strong> The BLITZ virtual machine emulator and debugger虚拟机模拟器和调试器</li>\n<li><strong>asm：</strong> The BLITZ assembler汇编器。<blockquote>\n<p>汇编器（Assembler）是将汇编语言翻译为机器语言的程序。一般而言，汇编生成的是目标代码，需要经链接器（Linker）生成可执行代码才可以执行。</p>\n</blockquote>\n</li>\n<li><strong>lddd：</strong> The BLITZ linker链接器</li>\n<li><strong>dumpObj：</strong> A tool to examine BLITZ object and executable files检查blitz对象和可执行文件</li>\n<li><strong>diskUtil：</strong> A tool to manipulate the initial file system on the emulated BLITZ disk用于操作模拟 BLITZ 磁盘上的初始文件系统的工具</li>\n<li><strong>kpl：</strong> The KPL compiler编译器</li>\n</ul>\n<h4 id=\"模拟器Emulator\"><a href=\"#模拟器Emulator\" class=\"headerlink\" title=\"模拟器Emulator\"></a>模拟器Emulator</h4><p>在 Unix 下运行时，您可以通过在 Unix 提示符下键入“blitz”来启动模拟器。模拟器首先读入 BLITZ 程序并将其加载到内存中。通常，BLITZ 可执行文件称为“a.out”，但也可以指定其他名称。模拟器首先从“a.out”读取数据并将其加载到其内部存储器中。实际上，模拟器首先使用“a.out”文件中的字节初始化 BLITZ 机器的主内存</p>\n<p>asm 程序是 BLITZ 汇编程序。它以汇编语言程序为输入，生成一个对象文件<br>第二个程序（称为 “ldd”）是 BLITZ 连接器。它输入一个或多个对象文件，生成一个名为 “a.out “的可执行文件。</p>\n<p>quit 命令（可缩写为 “q”）将终止 BLITZ 模拟器。</p>\n<p>go 指令（可缩写为 “g”）用于启动仿真器的执行。一旦开始执行，BLITZ 机器将执行指令，直到检测到错误或 BLITZ 机器执行 “等待 “或 “调试 “指令为止。</p>\n<p>在KPL中，以“.h”结尾的文件称为“头文件”，以“.c”结尾的文件称为“代码文件”。每个包（如HelloWorld）都会有头文件和代码文件。HelloWorld包使用系统包。每当HelloWorld使用的包的头文件发生变化时，HelloWorld必须重新编译。但是，如果系统的代码文件发生变化，则不需要重新编译HelloWorld。</p>\n"},{"title":"Monitor管程","date":"2023-10-18T02:13:15.672Z","_content":"### Monitor\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018102847.png)\n每个monitor都有一个互斥锁，monitor中只能有一个线程处于活动状态\n\n### Condition variables\ncondition是一个类，实现了以下函数：\n- condition.wait()\n- condition.signal()\n- condition.broadcast()\n\n\n### 管程\n管程是一种特殊的软件模块，有这些部分组成：\n1. 局部于管程的共享数据结构说明\n2. 对该数据结构进行操作的一组过程\n3. 对局部于管程的共享数据设置初始值的语句\n4. 管程有一个名字\n\n管程的基本特征：\n1. 局部于管程的数据只能被局部于管程的过程所访问\n2. 一个进程只有通过调用管程内的过程才能进入管程访问共享数据\n3. 每次仅允许一个进程在管程内执行某个内部过程","source":"_posts/Notes/课程/大三（上）/操作系统/Monitor管程.md","raw":"---\ntitle: Monitor管程\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\ndate:\n---\n### Monitor\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018102847.png)\n每个monitor都有一个互斥锁，monitor中只能有一个线程处于活动状态\n\n### Condition variables\ncondition是一个类，实现了以下函数：\n- condition.wait()\n- condition.signal()\n- condition.broadcast()\n\n\n### 管程\n管程是一种特殊的软件模块，有这些部分组成：\n1. 局部于管程的共享数据结构说明\n2. 对该数据结构进行操作的一组过程\n3. 对局部于管程的共享数据设置初始值的语句\n4. 管程有一个名字\n\n管程的基本特征：\n1. 局部于管程的数据只能被局部于管程的过程所访问\n2. 一个进程只有通过调用管程内的过程才能进入管程访问共享数据\n3. 每次仅允许一个进程在管程内执行某个内部过程","slug":"Notes/课程/大三（上）/操作系统/Monitor管程","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr11v0001vw8c4uca0evn","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"Monitor\"><a href=\"#Monitor\" class=\"headerlink\" title=\"Monitor\"></a>Monitor</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018102847.png\" alt=\"image.png\"><br>每个monitor都有一个互斥锁，monitor中只能有一个线程处于活动状态</p>\n<h3 id=\"Condition-variables\"><a href=\"#Condition-variables\" class=\"headerlink\" title=\"Condition variables\"></a>Condition variables</h3><p>condition是一个类，实现了以下函数：</p>\n<ul>\n<li>condition.wait()</li>\n<li>condition.signal()</li>\n<li>condition.broadcast()</li>\n</ul>\n<h3 id=\"管程\"><a href=\"#管程\" class=\"headerlink\" title=\"管程\"></a>管程</h3><p>管程是一种特殊的软件模块，有这些部分组成：</p>\n<ol>\n<li>局部于管程的共享数据结构说明</li>\n<li>对该数据结构进行操作的一组过程</li>\n<li>对局部于管程的共享数据设置初始值的语句</li>\n<li>管程有一个名字</li>\n</ol>\n<p>管程的基本特征：</p>\n<ol>\n<li>局部于管程的数据只能被局部于管程的过程所访问</li>\n<li>一个进程只有通过调用管程内的过程才能进入管程访问共享数据</li>\n<li>每次仅允许一个进程在管程内执行某个内部过程</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Monitor\"><a href=\"#Monitor\" class=\"headerlink\" title=\"Monitor\"></a>Monitor</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018102847.png\" alt=\"image.png\"><br>每个monitor都有一个互斥锁，monitor中只能有一个线程处于活动状态</p>\n<h3 id=\"Condition-variables\"><a href=\"#Condition-variables\" class=\"headerlink\" title=\"Condition variables\"></a>Condition variables</h3><p>condition是一个类，实现了以下函数：</p>\n<ul>\n<li>condition.wait()</li>\n<li>condition.signal()</li>\n<li>condition.broadcast()</li>\n</ul>\n<h3 id=\"管程\"><a href=\"#管程\" class=\"headerlink\" title=\"管程\"></a>管程</h3><p>管程是一种特殊的软件模块，有这些部分组成：</p>\n<ol>\n<li>局部于管程的共享数据结构说明</li>\n<li>对该数据结构进行操作的一组过程</li>\n<li>对局部于管程的共享数据设置初始值的语句</li>\n<li>管程有一个名字</li>\n</ol>\n<p>管程的基本特征：</p>\n<ol>\n<li>局部于管程的数据只能被局部于管程的过程所访问</li>\n<li>一个进程只有通过调用管程内的过程才能进入管程访问共享数据</li>\n<li>每次仅允许一个进程在管程内执行某个内部过程</li>\n</ol>\n"},{"title":"Deadlock死锁","date":"2023-10-25T02:00:44.849Z","_content":"Pi为进程，Rj为资源\nPi请求资源Rj：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100217.png)\nPi拥有资源Rj：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100233.png)\n\n不死锁：\nP3执行完后释放R3，P2就可以申请到R3，P2执行，P2执行完之后释放R1，P1就可以申请到R1执行\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100454.png)\n\n死锁：\nP3申请R2，R2被P1持有，P3无法执行，导致P2和P1无法执行\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100550.png)\n\n- 图里面没有环则不会死锁\n- 图里面有环，如果一个资源里只有一个实例，则会死锁；如果一个资源里有多个实例，则不一定会死锁\n\nHold and Wait持有等待\n要么持有全部资源，要么等待\n\n资源类型\n- Available：一个长度为m的向量，表示每种可用资源的数量（m种资源）\n- Allocation：一个n×m的矩阵代表每个进程现在持有的资源数量（n个进程）\n- Request：一个n×m的矩阵代表每个进程需要的资源数量\n\n检测算法：\n1. `Work`和`Finish`为长度为m和n的向量，\n\t(a) Work = Available\n\t(b) For **i = 1,2, …, n**, \n\t\t\tif **Allocationi != 0**, \n\t\t\tthen **Finish[i] = false**; \n\t\t\totherwise, **Finish[i] = true**\n2. 找到一个index `i`，使得\n\t(a) Finish[i] == false \n\t(b) Request_i <= Work\n\n3. Work = Work + Allocation_i \n\tFinish[i] = true \n\tgo to step 2\n\n4. If Finish[i] == false, for some i, 1 <= i <= n, then the system is in deadlock state. Moreover, if Finish[i] == false, then P_i is deadlocked\n\n### 死锁\n在并发环境下，各进程因竞争资源而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象，就是“死锁”。\n#### 死锁产生的必要条件\n1. 互斥条件：只有对互斥使用的资源的争抢才会导致死锁\n2. 不剥夺条件：进程所获得的资源未使用完之前，不能由其他进程强行夺走，只能主动释放\n3. 请求和保持条件：进程已经保持了至少一个资源，又提出新的资源请求，而该资源又被其他进程占用，此时请求进程被阻塞，但又对自己已有的资源保持不放\n4. 循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求\n\n#### 产生情况\n1. 对系统资源的竞争\n2. 进程推进顺序非法\n3. 信号量的使用不当也会造成死锁\n\n#### 预防死锁\n破坏互斥条件：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150332.png)\n破坏不剥夺条件：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150225.png)\n\n破坏请求和保持条件：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150533.png)\n\n破坏循环等待条件：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150733.png)\n\n#### 避免死锁\n>安全序列：如果按照这种序列分配资源，则每个进程都能顺利完成。\n\n如果分配了资源后，系统中找不出任何一个安全序列，系统就进入了不安全状态。如果系统进入不安全状态，就**有可能发生死锁**\n\n银行家算法：在进程提出资源申请时，先预判此次分配是否会导致系统进入不安全状态，如果会进入不安全状态，就暂时不答应这次请求，让该进程先阻塞等待。\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152524.png)\n\n#### 死锁的检测\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152801.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105153928.png)\n\n#### 死锁的解除\n1. 资源剥夺法：挂起某些死锁进程，并抢占它的资源。但是应防止被挂起的进程长时间得不到资源而饥饿\n2. 撤销进程法：强制撤销部分、甚至全部死锁进程。\n3. 进程回退法：让一个或多个死锁进程回退到足以避免死锁的地步。要求记录历史信息，设置还原点","source":"_posts/Notes/课程/大三（上）/操作系统/Deadlock死锁.md","raw":"---\ntitle: Deadlock死锁\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\ndate:\n---\nPi为进程，Rj为资源\nPi请求资源Rj：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100217.png)\nPi拥有资源Rj：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100233.png)\n\n不死锁：\nP3执行完后释放R3，P2就可以申请到R3，P2执行，P2执行完之后释放R1，P1就可以申请到R1执行\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100454.png)\n\n死锁：\nP3申请R2，R2被P1持有，P3无法执行，导致P2和P1无法执行\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100550.png)\n\n- 图里面没有环则不会死锁\n- 图里面有环，如果一个资源里只有一个实例，则会死锁；如果一个资源里有多个实例，则不一定会死锁\n\nHold and Wait持有等待\n要么持有全部资源，要么等待\n\n资源类型\n- Available：一个长度为m的向量，表示每种可用资源的数量（m种资源）\n- Allocation：一个n×m的矩阵代表每个进程现在持有的资源数量（n个进程）\n- Request：一个n×m的矩阵代表每个进程需要的资源数量\n\n检测算法：\n1. `Work`和`Finish`为长度为m和n的向量，\n\t(a) Work = Available\n\t(b) For **i = 1,2, …, n**, \n\t\t\tif **Allocationi != 0**, \n\t\t\tthen **Finish[i] = false**; \n\t\t\totherwise, **Finish[i] = true**\n2. 找到一个index `i`，使得\n\t(a) Finish[i] == false \n\t(b) Request_i <= Work\n\n3. Work = Work + Allocation_i \n\tFinish[i] = true \n\tgo to step 2\n\n4. If Finish[i] == false, for some i, 1 <= i <= n, then the system is in deadlock state. Moreover, if Finish[i] == false, then P_i is deadlocked\n\n### 死锁\n在并发环境下，各进程因竞争资源而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象，就是“死锁”。\n#### 死锁产生的必要条件\n1. 互斥条件：只有对互斥使用的资源的争抢才会导致死锁\n2. 不剥夺条件：进程所获得的资源未使用完之前，不能由其他进程强行夺走，只能主动释放\n3. 请求和保持条件：进程已经保持了至少一个资源，又提出新的资源请求，而该资源又被其他进程占用，此时请求进程被阻塞，但又对自己已有的资源保持不放\n4. 循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求\n\n#### 产生情况\n1. 对系统资源的竞争\n2. 进程推进顺序非法\n3. 信号量的使用不当也会造成死锁\n\n#### 预防死锁\n破坏互斥条件：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150332.png)\n破坏不剥夺条件：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150225.png)\n\n破坏请求和保持条件：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150533.png)\n\n破坏循环等待条件：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150733.png)\n\n#### 避免死锁\n>安全序列：如果按照这种序列分配资源，则每个进程都能顺利完成。\n\n如果分配了资源后，系统中找不出任何一个安全序列，系统就进入了不安全状态。如果系统进入不安全状态，就**有可能发生死锁**\n\n银行家算法：在进程提出资源申请时，先预判此次分配是否会导致系统进入不安全状态，如果会进入不安全状态，就暂时不答应这次请求，让该进程先阻塞等待。\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152524.png)\n\n#### 死锁的检测\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152801.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105153928.png)\n\n#### 死锁的解除\n1. 资源剥夺法：挂起某些死锁进程，并抢占它的资源。但是应防止被挂起的进程长时间得不到资源而饥饿\n2. 撤销进程法：强制撤销部分、甚至全部死锁进程。\n3. 进程回退法：让一个或多个死锁进程回退到足以避免死锁的地步。要求记录历史信息，设置还原点","slug":"Notes/课程/大三（上）/操作系统/Deadlock死锁","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr11w0003vw8cek8cdiee","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>Pi为进程，Rj为资源<br>Pi请求资源Rj：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100217.png\" alt=\"image.png\"><br>Pi拥有资源Rj：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100233.png\" alt=\"image.png\"></p>\n<p>不死锁：<br>P3执行完后释放R3，P2就可以申请到R3，P2执行，P2执行完之后释放R1，P1就可以申请到R1执行<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100454.png\" alt=\"image.png\"></p>\n<p>死锁：<br>P3申请R2，R2被P1持有，P3无法执行，导致P2和P1无法执行<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100550.png\" alt=\"image.png\"></p>\n<ul>\n<li>图里面没有环则不会死锁</li>\n<li>图里面有环，如果一个资源里只有一个实例，则会死锁；如果一个资源里有多个实例，则不一定会死锁</li>\n</ul>\n<p>Hold and Wait持有等待<br>要么持有全部资源，要么等待</p>\n<p>资源类型</p>\n<ul>\n<li>Available：一个长度为m的向量，表示每种可用资源的数量（m种资源）</li>\n<li>Allocation：一个n×m的矩阵代表每个进程现在持有的资源数量（n个进程）</li>\n<li>Request：一个n×m的矩阵代表每个进程需要的资源数量</li>\n</ul>\n<p>检测算法：</p>\n<ol>\n<li><p><code>Work</code>和<code>Finish</code>为长度为m和n的向量，<br> (a) Work &#x3D; Available<br> (b) For <strong>i &#x3D; 1,2, …, n</strong>,<br>     if <strong>Allocationi !&#x3D; 0</strong>,<br>     then <strong>Finish[i] &#x3D; false</strong>;<br>     otherwise, <strong>Finish[i] &#x3D; true</strong></p>\n</li>\n<li><p>找到一个index <code>i</code>，使得<br> (a) Finish[i] &#x3D;&#x3D; false<br> (b) Request_i &lt;&#x3D; Work</p>\n</li>\n<li><p>Work &#x3D; Work + Allocation_i<br> Finish[i] &#x3D; true<br> go to step 2</p>\n</li>\n<li><p>If Finish[i] &#x3D;&#x3D; false, for some i, 1 &lt;&#x3D; i &lt;&#x3D; n, then the system is in deadlock state. Moreover, if Finish[i] &#x3D;&#x3D; false, then P_i is deadlocked</p>\n</li>\n</ol>\n<h3 id=\"死锁\"><a href=\"#死锁\" class=\"headerlink\" title=\"死锁\"></a>死锁</h3><p>在并发环境下，各进程因竞争资源而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象，就是“死锁”。</p>\n<h4 id=\"死锁产生的必要条件\"><a href=\"#死锁产生的必要条件\" class=\"headerlink\" title=\"死锁产生的必要条件\"></a>死锁产生的必要条件</h4><ol>\n<li>互斥条件：只有对互斥使用的资源的争抢才会导致死锁</li>\n<li>不剥夺条件：进程所获得的资源未使用完之前，不能由其他进程强行夺走，只能主动释放</li>\n<li>请求和保持条件：进程已经保持了至少一个资源，又提出新的资源请求，而该资源又被其他进程占用，此时请求进程被阻塞，但又对自己已有的资源保持不放</li>\n<li>循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求</li>\n</ol>\n<h4 id=\"产生情况\"><a href=\"#产生情况\" class=\"headerlink\" title=\"产生情况\"></a>产生情况</h4><ol>\n<li>对系统资源的竞争</li>\n<li>进程推进顺序非法</li>\n<li>信号量的使用不当也会造成死锁</li>\n</ol>\n<h4 id=\"预防死锁\"><a href=\"#预防死锁\" class=\"headerlink\" title=\"预防死锁\"></a>预防死锁</h4><p>破坏互斥条件：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150332.png\" alt=\"image.png\"><br>破坏不剥夺条件：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150225.png\" alt=\"image.png\"></p>\n<p>破坏请求和保持条件：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150533.png\" alt=\"image.png\"></p>\n<p>破坏循环等待条件：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150733.png\" alt=\"image.png\"></p>\n<h4 id=\"避免死锁\"><a href=\"#避免死锁\" class=\"headerlink\" title=\"避免死锁\"></a>避免死锁</h4><blockquote>\n<p>安全序列：如果按照这种序列分配资源，则每个进程都能顺利完成。</p>\n</blockquote>\n<p>如果分配了资源后，系统中找不出任何一个安全序列，系统就进入了不安全状态。如果系统进入不安全状态，就<strong>有可能发生死锁</strong></p>\n<p>银行家算法：在进程提出资源申请时，先预判此次分配是否会导致系统进入不安全状态，如果会进入不安全状态，就暂时不答应这次请求，让该进程先阻塞等待。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152524.png\" alt=\"image.png\"></p>\n<h4 id=\"死锁的检测\"><a href=\"#死锁的检测\" class=\"headerlink\" title=\"死锁的检测\"></a>死锁的检测</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152801.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105153928.png\" alt=\"image.png\"></p>\n<h4 id=\"死锁的解除\"><a href=\"#死锁的解除\" class=\"headerlink\" title=\"死锁的解除\"></a>死锁的解除</h4><ol>\n<li>资源剥夺法：挂起某些死锁进程，并抢占它的资源。但是应防止被挂起的进程长时间得不到资源而饥饿</li>\n<li>撤销进程法：强制撤销部分、甚至全部死锁进程。</li>\n<li>进程回退法：让一个或多个死锁进程回退到足以避免死锁的地步。要求记录历史信息，设置还原点</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>Pi为进程，Rj为资源<br>Pi请求资源Rj：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100217.png\" alt=\"image.png\"><br>Pi拥有资源Rj：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100233.png\" alt=\"image.png\"></p>\n<p>不死锁：<br>P3执行完后释放R3，P2就可以申请到R3，P2执行，P2执行完之后释放R1，P1就可以申请到R1执行<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100454.png\" alt=\"image.png\"></p>\n<p>死锁：<br>P3申请R2，R2被P1持有，P3无法执行，导致P2和P1无法执行<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231025100550.png\" alt=\"image.png\"></p>\n<ul>\n<li>图里面没有环则不会死锁</li>\n<li>图里面有环，如果一个资源里只有一个实例，则会死锁；如果一个资源里有多个实例，则不一定会死锁</li>\n</ul>\n<p>Hold and Wait持有等待<br>要么持有全部资源，要么等待</p>\n<p>资源类型</p>\n<ul>\n<li>Available：一个长度为m的向量，表示每种可用资源的数量（m种资源）</li>\n<li>Allocation：一个n×m的矩阵代表每个进程现在持有的资源数量（n个进程）</li>\n<li>Request：一个n×m的矩阵代表每个进程需要的资源数量</li>\n</ul>\n<p>检测算法：</p>\n<ol>\n<li><p><code>Work</code>和<code>Finish</code>为长度为m和n的向量，<br> (a) Work &#x3D; Available<br> (b) For <strong>i &#x3D; 1,2, …, n</strong>,<br>     if <strong>Allocationi !&#x3D; 0</strong>,<br>     then <strong>Finish[i] &#x3D; false</strong>;<br>     otherwise, <strong>Finish[i] &#x3D; true</strong></p>\n</li>\n<li><p>找到一个index <code>i</code>，使得<br> (a) Finish[i] &#x3D;&#x3D; false<br> (b) Request_i &lt;&#x3D; Work</p>\n</li>\n<li><p>Work &#x3D; Work + Allocation_i<br> Finish[i] &#x3D; true<br> go to step 2</p>\n</li>\n<li><p>If Finish[i] &#x3D;&#x3D; false, for some i, 1 &lt;&#x3D; i &lt;&#x3D; n, then the system is in deadlock state. Moreover, if Finish[i] &#x3D;&#x3D; false, then P_i is deadlocked</p>\n</li>\n</ol>\n<h3 id=\"死锁\"><a href=\"#死锁\" class=\"headerlink\" title=\"死锁\"></a>死锁</h3><p>在并发环境下，各进程因竞争资源而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象，就是“死锁”。</p>\n<h4 id=\"死锁产生的必要条件\"><a href=\"#死锁产生的必要条件\" class=\"headerlink\" title=\"死锁产生的必要条件\"></a>死锁产生的必要条件</h4><ol>\n<li>互斥条件：只有对互斥使用的资源的争抢才会导致死锁</li>\n<li>不剥夺条件：进程所获得的资源未使用完之前，不能由其他进程强行夺走，只能主动释放</li>\n<li>请求和保持条件：进程已经保持了至少一个资源，又提出新的资源请求，而该资源又被其他进程占用，此时请求进程被阻塞，但又对自己已有的资源保持不放</li>\n<li>循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求</li>\n</ol>\n<h4 id=\"产生情况\"><a href=\"#产生情况\" class=\"headerlink\" title=\"产生情况\"></a>产生情况</h4><ol>\n<li>对系统资源的竞争</li>\n<li>进程推进顺序非法</li>\n<li>信号量的使用不当也会造成死锁</li>\n</ol>\n<h4 id=\"预防死锁\"><a href=\"#预防死锁\" class=\"headerlink\" title=\"预防死锁\"></a>预防死锁</h4><p>破坏互斥条件：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150332.png\" alt=\"image.png\"><br>破坏不剥夺条件：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150225.png\" alt=\"image.png\"></p>\n<p>破坏请求和保持条件：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150533.png\" alt=\"image.png\"></p>\n<p>破坏循环等待条件：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105150733.png\" alt=\"image.png\"></p>\n<h4 id=\"避免死锁\"><a href=\"#避免死锁\" class=\"headerlink\" title=\"避免死锁\"></a>避免死锁</h4><blockquote>\n<p>安全序列：如果按照这种序列分配资源，则每个进程都能顺利完成。</p>\n</blockquote>\n<p>如果分配了资源后，系统中找不出任何一个安全序列，系统就进入了不安全状态。如果系统进入不安全状态，就<strong>有可能发生死锁</strong></p>\n<p>银行家算法：在进程提出资源申请时，先预判此次分配是否会导致系统进入不安全状态，如果会进入不安全状态，就暂时不答应这次请求，让该进程先阻塞等待。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152524.png\" alt=\"image.png\"></p>\n<h4 id=\"死锁的检测\"><a href=\"#死锁的检测\" class=\"headerlink\" title=\"死锁的检测\"></a>死锁的检测</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105152801.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105153928.png\" alt=\"image.png\"></p>\n<h4 id=\"死锁的解除\"><a href=\"#死锁的解除\" class=\"headerlink\" title=\"死锁的解除\"></a>死锁的解除</h4><ol>\n<li>资源剥夺法：挂起某些死锁进程，并抢占它的资源。但是应防止被挂起的进程长时间得不到资源而饥饿</li>\n<li>撤销进程法：强制撤销部分、甚至全部死锁进程。</li>\n<li>进程回退法：让一个或多个死锁进程回退到足以避免死锁的地步。要求记录历史信息，设置还原点</li>\n</ol>\n"},{"title":"Scheduling调度","date":"2023-11-03T07:47:49.049Z","_content":"调度有两个方面：\n1）如何从一个进程切换到另一个进程\n2）下一个应该运行什么进程\n机制：上下文切换（如何切换）\n机制：抢占（保持控制）\n策略：调度（切换到哪里）\n\n上下文切换是一种允许操作系统存储当前进程状态并切换到另一个先前存储的上下文的机制。\n上下文切换的原因： \n进程完成/退出 \n进程执行缓慢的硬件操作（例如，从磁盘加载）并且操作系统切换到另一个就绪任务 \n硬件需要操作系统帮助并发出中断 \n操作系统决定抢占该任务并切换到另一个任务（即该进程已用完其时间片）\n\n异步返回的函数调用：进程A开始执行上下文切换，但进程B在函数返回后继续执行。\n该函数将所有寄存器保存在暂存区域（在进程的内核堆栈上或任务结构的预定义区域中）。\n操作系统切换地址空间。\n该函数从暂存区恢复所有寄存器。\n操作系统返回到进程B。\n\n如果任务从未放弃控制权 (yield())、退出或执行 I/O，那么它可以永远运行，并且操作系统无法获得控制权。\n因此，操作系统在调度进程之前设置一个计时器。如果定时器到期，硬件就会中断进程的执行并切换到内核。然后内核决定该进程是否可以继续。\n\n上下文切换机制负责​​内核如何从一个进程切换到另一个进程，即通过存储其上下文并恢复另一个进程的上下文。调度策略决定接下来应该运行哪个进程。如果只有一个“就绪”进程，那么答案很简单。如果有更多进程，则策略决定进程的执行顺序。\n\n在分析调度程序策略时，我们使用以下术语： \n利用率：CPU 执行程序的时间比例（目标：最大化） \n周转时间：完成作业所需的总时间，$T_{completion} − T_{arrival}$（目标：最小化） \n响应时间：从作业到达到第一次调度的时间，$T_{firstrun} - T_{arrival}$（目标：最小化） \n公平性：所有进程随着时间的推移获得相同数量的 CPU（目标：无饥饿） \n进度：允许进程向前推进（目标：最小化内核中断）\n\n让我们逐步了解调度策略。我们从一些简化假设开始 \n每个作业的运行时间相同 \n所有作业在同一时间到达 一旦启动，每个作业都会运行到完成 \n所有作业只使用 CPU（无 I/O） \n作业的运行时间已知 \n\n\n### 调度的三个层次\n#### 高级调度\n按一定原则从外存的作业后备队列中挑选一个作业调入内存，并创建进程。每个作业只调入一次，调出一次。作业调入时会建立PCB，调出时才撤销PCB。\n\n#### 中级调度\n内存中暂时调到外存等待的进程状态为挂起状态。被挂起的进程PCB会被组织成挂起队列\n中级调度按照某种策略决定将哪个处于挂起状态的进程重新调入内存。\n\n#### 低级调度（进程调度）\n按某种策略从就绪队列中选取一个进程，将处理机分配给它。进程调度是操作系统中最基本的一种调度，进程调度的频率很高，一般几十毫秒一次。\n\n### 进程调度的时机\n- 当前进程主动放弃处理机\n- 当前进程被动放弃处理机\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113430.png)\n\n不能进行进程调度与切换的情况：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113457.png)\n### 进程调度的方式\n- 非抢占式\n- 抢占式：优先处理更紧急的进程\n\n进程切换的过程：\n1. 对原来运行进程各种数据的保存\n2. 对新的进程各种数据的恢复\n进程切换是有代价的\n\n### 调度程序\n决定让谁运行，以及运行多长时间\n抢占式调度策略在时钟中断时唤醒调度程序\n\n闲逛进程（idle）：\n- 优先级最低\n- 可以是0地址指令，占一个完整的指令周期\n- 能耗低\n\n### CPU利用率\n利用率=忙碌的时间/总时间\n\n**周转时间：作业被提交给系统开始，到作业完成为止需要的时间**。包括四个部分：\n- 作业在外存后备队列上等待作业调度（高级调度）的时间\n- 进程在就绪队列上等待进程调度的时间\n- 进程在CPU上执行的时间\n- 进程等待I/O操作完成的时间\n\n平均周转时间：各作业周转时间之和/作业数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104115312.png)\n\n等待时间：进程处于等待处理机状态时间之和\n\n响应时间：用户从提出请求到首次产生响应所用的时\n\n### 调度算法\n#### 先来先服务（FCFS）\n按照到达的先后顺序，等待时间越久的越先得到服务。\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104145306.png)\n非抢占式算法\n优点：公平，算法实现简单\n缺点：排在长作业（进程）后面的短作业需要等待很长时间，带权周转时间很大。即FCFS算法**对长作业有利，对短作业不利**。\n\n不会导致饥饿\n\n#### 短作业优先（SJF）\n每次调度时选择当前已到达且运行时间最短的进程。\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150027.png)\n非抢占式算法\n\n##### 抢占式的短作业优先算法：最短剩余时间优先算法（SRTN/STCF）\n每当有进程加入，就绪队列改变时就需要调度，如果新到达进程的剩余时间比当前运行的进程剩余时间更短，则由新进程抢占处理机，当前进程回到就绪队列。\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150908.png)![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150952.png)\n未特别说明，短作业优先算法默认是非抢占式的。\n在所有进程都几乎同时到达时，采用SJF算法的平均等待时间、平均周转时间最少。\n抢占式的短作业优先算法的平均等待时间、平均周转时间最少。在所有进程同时到达时，SJF算法等同于抢占式SJF算法。\n\n缺点：可能产生饥饿现象。**短作业友好，长作业不友好**。\n\n#### 高相应比优先（HRRN）\n在调度时计算每个进程的相应比，选择相应比最高的进程为其服务。\n>![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152027.png)\n\n非抢占式算法\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152220.png)\n综合考虑了等待时间和运行时间\n避免了长作业饥饿的问题\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152435.png)\n\n#### 时间片轮转（RR）\n按各进程到达就绪队列的顺序，轮流让各个进程执行一个**时间片**（如100ms）。若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。\n\n抢占式算法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153525.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153547.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153618.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153635.png)\n\n如果时间片太大，使得每个进程都可以在一个时间片内完成，则时间片轮转调度算法退化为先来先服务算法，并且会增大进程相应时间。\n如果时间片太小，会导致进程切换过于频繁。\n\n#### 优先级调度算法\n每个进程都有各自的优先级，调度时选择优先级最高的进程。\n有抢占式和非抢占式。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104154536.png)\n\n会导致饥饿\n\n#### 多级反馈队列调度算法（MLFQ）\n**设置多级就绪队列，各级队列优先级从高到低，时间片从小到大。**\n**新进程到达时先进入第一级队列，按FCFS原则排队等待被分配时间片**。**若用完时间片进程还没结束，则进程进入下一级队列队尾**。如果此时已经在最下级队列，则重新放回最下级队列队尾。**只有在第k级队列为空时，才会为k+1级队头的进程分配时间片**\n抢占式算法。在k级队列的进程运行过程中，若更上级的队列中进入了一个新进程，则由于新进程处于优先级更高的队列中，因此新进程会抢占处理机，原来运行的进程放回k级队列队尾。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104155638.png)\n\n\n#### 完全公平调度器（CFS）\n对于n个正在运行的任务，当这些任务同时不断地运行时，CPU会尽可能分配给他们1/n的处理时间。CFS是一种基于加权公平排队思想的调度算法\n实现：将所有进程保留在红黑树中，按最大执行时间排序（跟踪其正余额） 调度 调度最左边的进程（余额最高的进程） 如果进程退出，则将其从调度树中删除 中断时（时间片或 I/O 结束），将进程重新插入树中的新位置\n\n>红黑树是一种特殊的二叉搜索树，也就是左边节点都小于根节点都小于右边节点，递归整个树都满足这一点。也就是说最左边的叶子节点是最小的，最右边的叶子节点是最大的。红黑树相比二叉搜索树多了红色黑色两个颜色的宏定义,红黑树有以下5个性质：\n>每个结点要么是红的要么是黑的。  \n  根结点是黑的。  \n  每个叶结点都是黑的。  \n  如果一个结点是红的，那么它的两个儿子都是黑的。  \n  对于任意结点而言，其到叶结点的每条路径都包含相同数目的黑结点。\n\n1. CFS使用红黑树结构，来存储要调度的任务队列。\n2. 每个节点代表了一个要调度的任务，节点的key即为虚拟时间（vruntime），虚拟时间由这个人物的运行时间计算而来。\n3. key越小，也就是vruntime越小的话，红黑树对应的节点就越靠左。\n4. CFS scheduler每次都挑选最左边的节点作为下一个要运行的任务，这个节点是“缓存的”——由一个特殊的指针指向；不需要进行O（logn）遍历来查找。也因此，CFS搜索的时间是O(1)。\n","source":"_posts/Notes/课程/大三（上）/操作系统/Scheduling调度.md","raw":"---\ntitle: Scheduling调度\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\ndate:\n---\n调度有两个方面：\n1）如何从一个进程切换到另一个进程\n2）下一个应该运行什么进程\n机制：上下文切换（如何切换）\n机制：抢占（保持控制）\n策略：调度（切换到哪里）\n\n上下文切换是一种允许操作系统存储当前进程状态并切换到另一个先前存储的上下文的机制。\n上下文切换的原因： \n进程完成/退出 \n进程执行缓慢的硬件操作（例如，从磁盘加载）并且操作系统切换到另一个就绪任务 \n硬件需要操作系统帮助并发出中断 \n操作系统决定抢占该任务并切换到另一个任务（即该进程已用完其时间片）\n\n异步返回的函数调用：进程A开始执行上下文切换，但进程B在函数返回后继续执行。\n该函数将所有寄存器保存在暂存区域（在进程的内核堆栈上或任务结构的预定义区域中）。\n操作系统切换地址空间。\n该函数从暂存区恢复所有寄存器。\n操作系统返回到进程B。\n\n如果任务从未放弃控制权 (yield())、退出或执行 I/O，那么它可以永远运行，并且操作系统无法获得控制权。\n因此，操作系统在调度进程之前设置一个计时器。如果定时器到期，硬件就会中断进程的执行并切换到内核。然后内核决定该进程是否可以继续。\n\n上下文切换机制负责​​内核如何从一个进程切换到另一个进程，即通过存储其上下文并恢复另一个进程的上下文。调度策略决定接下来应该运行哪个进程。如果只有一个“就绪”进程，那么答案很简单。如果有更多进程，则策略决定进程的执行顺序。\n\n在分析调度程序策略时，我们使用以下术语： \n利用率：CPU 执行程序的时间比例（目标：最大化） \n周转时间：完成作业所需的总时间，$T_{completion} − T_{arrival}$（目标：最小化） \n响应时间：从作业到达到第一次调度的时间，$T_{firstrun} - T_{arrival}$（目标：最小化） \n公平性：所有进程随着时间的推移获得相同数量的 CPU（目标：无饥饿） \n进度：允许进程向前推进（目标：最小化内核中断）\n\n让我们逐步了解调度策略。我们从一些简化假设开始 \n每个作业的运行时间相同 \n所有作业在同一时间到达 一旦启动，每个作业都会运行到完成 \n所有作业只使用 CPU（无 I/O） \n作业的运行时间已知 \n\n\n### 调度的三个层次\n#### 高级调度\n按一定原则从外存的作业后备队列中挑选一个作业调入内存，并创建进程。每个作业只调入一次，调出一次。作业调入时会建立PCB，调出时才撤销PCB。\n\n#### 中级调度\n内存中暂时调到外存等待的进程状态为挂起状态。被挂起的进程PCB会被组织成挂起队列\n中级调度按照某种策略决定将哪个处于挂起状态的进程重新调入内存。\n\n#### 低级调度（进程调度）\n按某种策略从就绪队列中选取一个进程，将处理机分配给它。进程调度是操作系统中最基本的一种调度，进程调度的频率很高，一般几十毫秒一次。\n\n### 进程调度的时机\n- 当前进程主动放弃处理机\n- 当前进程被动放弃处理机\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113430.png)\n\n不能进行进程调度与切换的情况：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113457.png)\n### 进程调度的方式\n- 非抢占式\n- 抢占式：优先处理更紧急的进程\n\n进程切换的过程：\n1. 对原来运行进程各种数据的保存\n2. 对新的进程各种数据的恢复\n进程切换是有代价的\n\n### 调度程序\n决定让谁运行，以及运行多长时间\n抢占式调度策略在时钟中断时唤醒调度程序\n\n闲逛进程（idle）：\n- 优先级最低\n- 可以是0地址指令，占一个完整的指令周期\n- 能耗低\n\n### CPU利用率\n利用率=忙碌的时间/总时间\n\n**周转时间：作业被提交给系统开始，到作业完成为止需要的时间**。包括四个部分：\n- 作业在外存后备队列上等待作业调度（高级调度）的时间\n- 进程在就绪队列上等待进程调度的时间\n- 进程在CPU上执行的时间\n- 进程等待I/O操作完成的时间\n\n平均周转时间：各作业周转时间之和/作业数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104115312.png)\n\n等待时间：进程处于等待处理机状态时间之和\n\n响应时间：用户从提出请求到首次产生响应所用的时\n\n### 调度算法\n#### 先来先服务（FCFS）\n按照到达的先后顺序，等待时间越久的越先得到服务。\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104145306.png)\n非抢占式算法\n优点：公平，算法实现简单\n缺点：排在长作业（进程）后面的短作业需要等待很长时间，带权周转时间很大。即FCFS算法**对长作业有利，对短作业不利**。\n\n不会导致饥饿\n\n#### 短作业优先（SJF）\n每次调度时选择当前已到达且运行时间最短的进程。\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150027.png)\n非抢占式算法\n\n##### 抢占式的短作业优先算法：最短剩余时间优先算法（SRTN/STCF）\n每当有进程加入，就绪队列改变时就需要调度，如果新到达进程的剩余时间比当前运行的进程剩余时间更短，则由新进程抢占处理机，当前进程回到就绪队列。\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150908.png)![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150952.png)\n未特别说明，短作业优先算法默认是非抢占式的。\n在所有进程都几乎同时到达时，采用SJF算法的平均等待时间、平均周转时间最少。\n抢占式的短作业优先算法的平均等待时间、平均周转时间最少。在所有进程同时到达时，SJF算法等同于抢占式SJF算法。\n\n缺点：可能产生饥饿现象。**短作业友好，长作业不友好**。\n\n#### 高相应比优先（HRRN）\n在调度时计算每个进程的相应比，选择相应比最高的进程为其服务。\n>![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152027.png)\n\n非抢占式算法\n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152220.png)\n综合考虑了等待时间和运行时间\n避免了长作业饥饿的问题\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152435.png)\n\n#### 时间片轮转（RR）\n按各进程到达就绪队列的顺序，轮流让各个进程执行一个**时间片**（如100ms）。若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。\n\n抢占式算法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153525.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153547.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153618.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153635.png)\n\n如果时间片太大，使得每个进程都可以在一个时间片内完成，则时间片轮转调度算法退化为先来先服务算法，并且会增大进程相应时间。\n如果时间片太小，会导致进程切换过于频繁。\n\n#### 优先级调度算法\n每个进程都有各自的优先级，调度时选择优先级最高的进程。\n有抢占式和非抢占式。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104154536.png)\n\n会导致饥饿\n\n#### 多级反馈队列调度算法（MLFQ）\n**设置多级就绪队列，各级队列优先级从高到低，时间片从小到大。**\n**新进程到达时先进入第一级队列，按FCFS原则排队等待被分配时间片**。**若用完时间片进程还没结束，则进程进入下一级队列队尾**。如果此时已经在最下级队列，则重新放回最下级队列队尾。**只有在第k级队列为空时，才会为k+1级队头的进程分配时间片**\n抢占式算法。在k级队列的进程运行过程中，若更上级的队列中进入了一个新进程，则由于新进程处于优先级更高的队列中，因此新进程会抢占处理机，原来运行的进程放回k级队列队尾。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104155638.png)\n\n\n#### 完全公平调度器（CFS）\n对于n个正在运行的任务，当这些任务同时不断地运行时，CPU会尽可能分配给他们1/n的处理时间。CFS是一种基于加权公平排队思想的调度算法\n实现：将所有进程保留在红黑树中，按最大执行时间排序（跟踪其正余额） 调度 调度最左边的进程（余额最高的进程） 如果进程退出，则将其从调度树中删除 中断时（时间片或 I/O 结束），将进程重新插入树中的新位置\n\n>红黑树是一种特殊的二叉搜索树，也就是左边节点都小于根节点都小于右边节点，递归整个树都满足这一点。也就是说最左边的叶子节点是最小的，最右边的叶子节点是最大的。红黑树相比二叉搜索树多了红色黑色两个颜色的宏定义,红黑树有以下5个性质：\n>每个结点要么是红的要么是黑的。  \n  根结点是黑的。  \n  每个叶结点都是黑的。  \n  如果一个结点是红的，那么它的两个儿子都是黑的。  \n  对于任意结点而言，其到叶结点的每条路径都包含相同数目的黑结点。\n\n1. CFS使用红黑树结构，来存储要调度的任务队列。\n2. 每个节点代表了一个要调度的任务，节点的key即为虚拟时间（vruntime），虚拟时间由这个人物的运行时间计算而来。\n3. key越小，也就是vruntime越小的话，红黑树对应的节点就越靠左。\n4. CFS scheduler每次都挑选最左边的节点作为下一个要运行的任务，这个节点是“缓存的”——由一个特殊的指针指向；不需要进行O（logn）遍历来查找。也因此，CFS搜索的时间是O(1)。\n","slug":"Notes/课程/大三（上）/操作系统/Scheduling调度","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr11x0005vw8c33u725wx","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>调度有两个方面：<br>1）如何从一个进程切换到另一个进程<br>2）下一个应该运行什么进程<br>机制：上下文切换（如何切换）<br>机制：抢占（保持控制）<br>策略：调度（切换到哪里）</p>\n<p>上下文切换是一种允许操作系统存储当前进程状态并切换到另一个先前存储的上下文的机制。<br>上下文切换的原因：<br>进程完成&#x2F;退出<br>进程执行缓慢的硬件操作（例如，从磁盘加载）并且操作系统切换到另一个就绪任务<br>硬件需要操作系统帮助并发出中断<br>操作系统决定抢占该任务并切换到另一个任务（即该进程已用完其时间片）</p>\n<p>异步返回的函数调用：进程A开始执行上下文切换，但进程B在函数返回后继续执行。<br>该函数将所有寄存器保存在暂存区域（在进程的内核堆栈上或任务结构的预定义区域中）。<br>操作系统切换地址空间。<br>该函数从暂存区恢复所有寄存器。<br>操作系统返回到进程B。</p>\n<p>如果任务从未放弃控制权 (yield())、退出或执行 I&#x2F;O，那么它可以永远运行，并且操作系统无法获得控制权。<br>因此，操作系统在调度进程之前设置一个计时器。如果定时器到期，硬件就会中断进程的执行并切换到内核。然后内核决定该进程是否可以继续。</p>\n<p>上下文切换机制负责​​内核如何从一个进程切换到另一个进程，即通过存储其上下文并恢复另一个进程的上下文。调度策略决定接下来应该运行哪个进程。如果只有一个“就绪”进程，那么答案很简单。如果有更多进程，则策略决定进程的执行顺序。</p>\n<p>在分析调度程序策略时，我们使用以下术语：<br>利用率：CPU 执行程序的时间比例（目标：最大化）<br>周转时间：完成作业所需的总时间，$T_{completion} − T_{arrival}$（目标：最小化）<br>响应时间：从作业到达到第一次调度的时间，$T_{firstrun} - T_{arrival}$（目标：最小化）<br>公平性：所有进程随着时间的推移获得相同数量的 CPU（目标：无饥饿）<br>进度：允许进程向前推进（目标：最小化内核中断）</p>\n<p>让我们逐步了解调度策略。我们从一些简化假设开始<br>每个作业的运行时间相同<br>所有作业在同一时间到达 一旦启动，每个作业都会运行到完成<br>所有作业只使用 CPU（无 I&#x2F;O）<br>作业的运行时间已知 </p>\n<h3 id=\"调度的三个层次\"><a href=\"#调度的三个层次\" class=\"headerlink\" title=\"调度的三个层次\"></a>调度的三个层次</h3><h4 id=\"高级调度\"><a href=\"#高级调度\" class=\"headerlink\" title=\"高级调度\"></a>高级调度</h4><p>按一定原则从外存的作业后备队列中挑选一个作业调入内存，并创建进程。每个作业只调入一次，调出一次。作业调入时会建立PCB，调出时才撤销PCB。</p>\n<h4 id=\"中级调度\"><a href=\"#中级调度\" class=\"headerlink\" title=\"中级调度\"></a>中级调度</h4><p>内存中暂时调到外存等待的进程状态为挂起状态。被挂起的进程PCB会被组织成挂起队列<br>中级调度按照某种策略决定将哪个处于挂起状态的进程重新调入内存。</p>\n<h4 id=\"低级调度（进程调度）\"><a href=\"#低级调度（进程调度）\" class=\"headerlink\" title=\"低级调度（进程调度）\"></a>低级调度（进程调度）</h4><p>按某种策略从就绪队列中选取一个进程，将处理机分配给它。进程调度是操作系统中最基本的一种调度，进程调度的频率很高，一般几十毫秒一次。</p>\n<h3 id=\"进程调度的时机\"><a href=\"#进程调度的时机\" class=\"headerlink\" title=\"进程调度的时机\"></a>进程调度的时机</h3><ul>\n<li>当前进程主动放弃处理机</li>\n<li>当前进程被动放弃处理机<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113430.png\" alt=\"image.png\"></li>\n</ul>\n<p>不能进行进程调度与切换的情况：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113457.png\" alt=\"image.png\"></p>\n<h3 id=\"进程调度的方式\"><a href=\"#进程调度的方式\" class=\"headerlink\" title=\"进程调度的方式\"></a>进程调度的方式</h3><ul>\n<li>非抢占式</li>\n<li>抢占式：优先处理更紧急的进程</li>\n</ul>\n<p>进程切换的过程：</p>\n<ol>\n<li>对原来运行进程各种数据的保存</li>\n<li>对新的进程各种数据的恢复<br>进程切换是有代价的</li>\n</ol>\n<h3 id=\"调度程序\"><a href=\"#调度程序\" class=\"headerlink\" title=\"调度程序\"></a>调度程序</h3><p>决定让谁运行，以及运行多长时间<br>抢占式调度策略在时钟中断时唤醒调度程序</p>\n<p>闲逛进程（idle）：</p>\n<ul>\n<li>优先级最低</li>\n<li>可以是0地址指令，占一个完整的指令周期</li>\n<li>能耗低</li>\n</ul>\n<h3 id=\"CPU利用率\"><a href=\"#CPU利用率\" class=\"headerlink\" title=\"CPU利用率\"></a>CPU利用率</h3><p>利用率&#x3D;忙碌的时间&#x2F;总时间</p>\n<p><strong>周转时间：作业被提交给系统开始，到作业完成为止需要的时间</strong>。包括四个部分：</p>\n<ul>\n<li>作业在外存后备队列上等待作业调度（高级调度）的时间</li>\n<li>进程在就绪队列上等待进程调度的时间</li>\n<li>进程在CPU上执行的时间</li>\n<li>进程等待I&#x2F;O操作完成的时间</li>\n</ul>\n<p>平均周转时间：各作业周转时间之和&#x2F;作业数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104115312.png\" alt=\"image.png\"></p>\n<p>等待时间：进程处于等待处理机状态时间之和</p>\n<p>响应时间：用户从提出请求到首次产生响应所用的时</p>\n<h3 id=\"调度算法\"><a href=\"#调度算法\" class=\"headerlink\" title=\"调度算法\"></a>调度算法</h3><h4 id=\"先来先服务（FCFS）\"><a href=\"#先来先服务（FCFS）\" class=\"headerlink\" title=\"先来先服务（FCFS）\"></a>先来先服务（FCFS）</h4><p>按照到达的先后顺序，等待时间越久的越先得到服务。<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104145306.png\" alt=\"image.png\"><br>非抢占式算法<br>优点：公平，算法实现简单<br>缺点：排在长作业（进程）后面的短作业需要等待很长时间，带权周转时间很大。即FCFS算法<strong>对长作业有利，对短作业不利</strong>。</p>\n<p>不会导致饥饿</p>\n<h4 id=\"短作业优先（SJF）\"><a href=\"#短作业优先（SJF）\" class=\"headerlink\" title=\"短作业优先（SJF）\"></a>短作业优先（SJF）</h4><p>每次调度时选择当前已到达且运行时间最短的进程。<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150027.png\" alt=\"image.png\"><br>非抢占式算法</p>\n<h5 id=\"抢占式的短作业优先算法：最短剩余时间优先算法（SRTN-STCF）\"><a href=\"#抢占式的短作业优先算法：最短剩余时间优先算法（SRTN-STCF）\" class=\"headerlink\" title=\"抢占式的短作业优先算法：最短剩余时间优先算法（SRTN&#x2F;STCF）\"></a>抢占式的短作业优先算法：最短剩余时间优先算法（SRTN&#x2F;STCF）</h5><p>每当有进程加入，就绪队列改变时就需要调度，如果新到达进程的剩余时间比当前运行的进程剩余时间更短，则由新进程抢占处理机，当前进程回到就绪队列。<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150908.png\" alt=\"image.png\"><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150952.png\" alt=\"image.png\"><br>未特别说明，短作业优先算法默认是非抢占式的。<br>在所有进程都几乎同时到达时，采用SJF算法的平均等待时间、平均周转时间最少。<br>抢占式的短作业优先算法的平均等待时间、平均周转时间最少。在所有进程同时到达时，SJF算法等同于抢占式SJF算法。</p>\n<p>缺点：可能产生饥饿现象。<strong>短作业友好，长作业不友好</strong>。</p>\n<h4 id=\"高相应比优先（HRRN）\"><a href=\"#高相应比优先（HRRN）\" class=\"headerlink\" title=\"高相应比优先（HRRN）\"></a>高相应比优先（HRRN）</h4><p>在调度时计算每个进程的相应比，选择相应比最高的进程为其服务。</p>\n<blockquote>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152027.png\" alt=\"image.png\"></p>\n</blockquote>\n<p>非抢占式算法<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152220.png\" alt=\"image.png\"><br>综合考虑了等待时间和运行时间<br>避免了长作业饥饿的问题<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152435.png\" alt=\"image.png\"></p>\n<h4 id=\"时间片轮转（RR）\"><a href=\"#时间片轮转（RR）\" class=\"headerlink\" title=\"时间片轮转（RR）\"></a>时间片轮转（RR）</h4><p>按各进程到达就绪队列的顺序，轮流让各个进程执行一个<strong>时间片</strong>（如100ms）。若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。</p>\n<p>抢占式算法<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153525.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153547.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153618.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153635.png\" alt=\"image.png\"></p>\n<p>如果时间片太大，使得每个进程都可以在一个时间片内完成，则时间片轮转调度算法退化为先来先服务算法，并且会增大进程相应时间。<br>如果时间片太小，会导致进程切换过于频繁。</p>\n<h4 id=\"优先级调度算法\"><a href=\"#优先级调度算法\" class=\"headerlink\" title=\"优先级调度算法\"></a>优先级调度算法</h4><p>每个进程都有各自的优先级，调度时选择优先级最高的进程。<br>有抢占式和非抢占式。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104154536.png\" alt=\"image.png\"></p>\n<p>会导致饥饿</p>\n<h4 id=\"多级反馈队列调度算法（MLFQ）\"><a href=\"#多级反馈队列调度算法（MLFQ）\" class=\"headerlink\" title=\"多级反馈队列调度算法（MLFQ）\"></a>多级反馈队列调度算法（MLFQ）</h4><p><strong>设置多级就绪队列，各级队列优先级从高到低，时间片从小到大。</strong><br><strong>新进程到达时先进入第一级队列，按FCFS原则排队等待被分配时间片</strong>。<strong>若用完时间片进程还没结束，则进程进入下一级队列队尾</strong>。如果此时已经在最下级队列，则重新放回最下级队列队尾。<strong>只有在第k级队列为空时，才会为k+1级队头的进程分配时间片</strong><br>抢占式算法。在k级队列的进程运行过程中，若更上级的队列中进入了一个新进程，则由于新进程处于优先级更高的队列中，因此新进程会抢占处理机，原来运行的进程放回k级队列队尾。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104155638.png\" alt=\"image.png\"></p>\n<h4 id=\"完全公平调度器（CFS）\"><a href=\"#完全公平调度器（CFS）\" class=\"headerlink\" title=\"完全公平调度器（CFS）\"></a>完全公平调度器（CFS）</h4><p>对于n个正在运行的任务，当这些任务同时不断地运行时，CPU会尽可能分配给他们1&#x2F;n的处理时间。CFS是一种基于加权公平排队思想的调度算法<br>实现：将所有进程保留在红黑树中，按最大执行时间排序（跟踪其正余额） 调度 调度最左边的进程（余额最高的进程） 如果进程退出，则将其从调度树中删除 中断时（时间片或 I&#x2F;O 结束），将进程重新插入树中的新位置</p>\n<blockquote>\n<p>红黑树是一种特殊的二叉搜索树，也就是左边节点都小于根节点都小于右边节点，递归整个树都满足这一点。也就是说最左边的叶子节点是最小的，最右边的叶子节点是最大的。红黑树相比二叉搜索树多了红色黑色两个颜色的宏定义,红黑树有以下5个性质：<br>每个结点要么是红的要么是黑的。<br>  根结点是黑的。<br>  每个叶结点都是黑的。<br>  如果一个结点是红的，那么它的两个儿子都是黑的。<br>  对于任意结点而言，其到叶结点的每条路径都包含相同数目的黑结点。</p>\n</blockquote>\n<ol>\n<li>CFS使用红黑树结构，来存储要调度的任务队列。</li>\n<li>每个节点代表了一个要调度的任务，节点的key即为虚拟时间（vruntime），虚拟时间由这个人物的运行时间计算而来。</li>\n<li>key越小，也就是vruntime越小的话，红黑树对应的节点就越靠左。<br>4. CFS scheduler每次都挑选最左边的节点作为下一个要运行的任务，这个节点是“缓存的”——由一个特殊的指针指向；不需要进行O（logn）遍历来查找。也因此，CFS搜索的时间是O(1)。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>调度有两个方面：<br>1）如何从一个进程切换到另一个进程<br>2）下一个应该运行什么进程<br>机制：上下文切换（如何切换）<br>机制：抢占（保持控制）<br>策略：调度（切换到哪里）</p>\n<p>上下文切换是一种允许操作系统存储当前进程状态并切换到另一个先前存储的上下文的机制。<br>上下文切换的原因：<br>进程完成&#x2F;退出<br>进程执行缓慢的硬件操作（例如，从磁盘加载）并且操作系统切换到另一个就绪任务<br>硬件需要操作系统帮助并发出中断<br>操作系统决定抢占该任务并切换到另一个任务（即该进程已用完其时间片）</p>\n<p>异步返回的函数调用：进程A开始执行上下文切换，但进程B在函数返回后继续执行。<br>该函数将所有寄存器保存在暂存区域（在进程的内核堆栈上或任务结构的预定义区域中）。<br>操作系统切换地址空间。<br>该函数从暂存区恢复所有寄存器。<br>操作系统返回到进程B。</p>\n<p>如果任务从未放弃控制权 (yield())、退出或执行 I&#x2F;O，那么它可以永远运行，并且操作系统无法获得控制权。<br>因此，操作系统在调度进程之前设置一个计时器。如果定时器到期，硬件就会中断进程的执行并切换到内核。然后内核决定该进程是否可以继续。</p>\n<p>上下文切换机制负责​​内核如何从一个进程切换到另一个进程，即通过存储其上下文并恢复另一个进程的上下文。调度策略决定接下来应该运行哪个进程。如果只有一个“就绪”进程，那么答案很简单。如果有更多进程，则策略决定进程的执行顺序。</p>\n<p>在分析调度程序策略时，我们使用以下术语：<br>利用率：CPU 执行程序的时间比例（目标：最大化）<br>周转时间：完成作业所需的总时间，$T_{completion} − T_{arrival}$（目标：最小化）<br>响应时间：从作业到达到第一次调度的时间，$T_{firstrun} - T_{arrival}$（目标：最小化）<br>公平性：所有进程随着时间的推移获得相同数量的 CPU（目标：无饥饿）<br>进度：允许进程向前推进（目标：最小化内核中断）</p>\n<p>让我们逐步了解调度策略。我们从一些简化假设开始<br>每个作业的运行时间相同<br>所有作业在同一时间到达 一旦启动，每个作业都会运行到完成<br>所有作业只使用 CPU（无 I&#x2F;O）<br>作业的运行时间已知 </p>\n<h3 id=\"调度的三个层次\"><a href=\"#调度的三个层次\" class=\"headerlink\" title=\"调度的三个层次\"></a>调度的三个层次</h3><h4 id=\"高级调度\"><a href=\"#高级调度\" class=\"headerlink\" title=\"高级调度\"></a>高级调度</h4><p>按一定原则从外存的作业后备队列中挑选一个作业调入内存，并创建进程。每个作业只调入一次，调出一次。作业调入时会建立PCB，调出时才撤销PCB。</p>\n<h4 id=\"中级调度\"><a href=\"#中级调度\" class=\"headerlink\" title=\"中级调度\"></a>中级调度</h4><p>内存中暂时调到外存等待的进程状态为挂起状态。被挂起的进程PCB会被组织成挂起队列<br>中级调度按照某种策略决定将哪个处于挂起状态的进程重新调入内存。</p>\n<h4 id=\"低级调度（进程调度）\"><a href=\"#低级调度（进程调度）\" class=\"headerlink\" title=\"低级调度（进程调度）\"></a>低级调度（进程调度）</h4><p>按某种策略从就绪队列中选取一个进程，将处理机分配给它。进程调度是操作系统中最基本的一种调度，进程调度的频率很高，一般几十毫秒一次。</p>\n<h3 id=\"进程调度的时机\"><a href=\"#进程调度的时机\" class=\"headerlink\" title=\"进程调度的时机\"></a>进程调度的时机</h3><ul>\n<li>当前进程主动放弃处理机</li>\n<li>当前进程被动放弃处理机<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113430.png\" alt=\"image.png\"></li>\n</ul>\n<p>不能进行进程调度与切换的情况：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104113457.png\" alt=\"image.png\"></p>\n<h3 id=\"进程调度的方式\"><a href=\"#进程调度的方式\" class=\"headerlink\" title=\"进程调度的方式\"></a>进程调度的方式</h3><ul>\n<li>非抢占式</li>\n<li>抢占式：优先处理更紧急的进程</li>\n</ul>\n<p>进程切换的过程：</p>\n<ol>\n<li>对原来运行进程各种数据的保存</li>\n<li>对新的进程各种数据的恢复<br>进程切换是有代价的</li>\n</ol>\n<h3 id=\"调度程序\"><a href=\"#调度程序\" class=\"headerlink\" title=\"调度程序\"></a>调度程序</h3><p>决定让谁运行，以及运行多长时间<br>抢占式调度策略在时钟中断时唤醒调度程序</p>\n<p>闲逛进程（idle）：</p>\n<ul>\n<li>优先级最低</li>\n<li>可以是0地址指令，占一个完整的指令周期</li>\n<li>能耗低</li>\n</ul>\n<h3 id=\"CPU利用率\"><a href=\"#CPU利用率\" class=\"headerlink\" title=\"CPU利用率\"></a>CPU利用率</h3><p>利用率&#x3D;忙碌的时间&#x2F;总时间</p>\n<p><strong>周转时间：作业被提交给系统开始，到作业完成为止需要的时间</strong>。包括四个部分：</p>\n<ul>\n<li>作业在外存后备队列上等待作业调度（高级调度）的时间</li>\n<li>进程在就绪队列上等待进程调度的时间</li>\n<li>进程在CPU上执行的时间</li>\n<li>进程等待I&#x2F;O操作完成的时间</li>\n</ul>\n<p>平均周转时间：各作业周转时间之和&#x2F;作业数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104115312.png\" alt=\"image.png\"></p>\n<p>等待时间：进程处于等待处理机状态时间之和</p>\n<p>响应时间：用户从提出请求到首次产生响应所用的时</p>\n<h3 id=\"调度算法\"><a href=\"#调度算法\" class=\"headerlink\" title=\"调度算法\"></a>调度算法</h3><h4 id=\"先来先服务（FCFS）\"><a href=\"#先来先服务（FCFS）\" class=\"headerlink\" title=\"先来先服务（FCFS）\"></a>先来先服务（FCFS）</h4><p>按照到达的先后顺序，等待时间越久的越先得到服务。<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104145306.png\" alt=\"image.png\"><br>非抢占式算法<br>优点：公平，算法实现简单<br>缺点：排在长作业（进程）后面的短作业需要等待很长时间，带权周转时间很大。即FCFS算法<strong>对长作业有利，对短作业不利</strong>。</p>\n<p>不会导致饥饿</p>\n<h4 id=\"短作业优先（SJF）\"><a href=\"#短作业优先（SJF）\" class=\"headerlink\" title=\"短作业优先（SJF）\"></a>短作业优先（SJF）</h4><p>每次调度时选择当前已到达且运行时间最短的进程。<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150027.png\" alt=\"image.png\"><br>非抢占式算法</p>\n<h5 id=\"抢占式的短作业优先算法：最短剩余时间优先算法（SRTN-STCF）\"><a href=\"#抢占式的短作业优先算法：最短剩余时间优先算法（SRTN-STCF）\" class=\"headerlink\" title=\"抢占式的短作业优先算法：最短剩余时间优先算法（SRTN&#x2F;STCF）\"></a>抢占式的短作业优先算法：最短剩余时间优先算法（SRTN&#x2F;STCF）</h5><p>每当有进程加入，就绪队列改变时就需要调度，如果新到达进程的剩余时间比当前运行的进程剩余时间更短，则由新进程抢占处理机，当前进程回到就绪队列。<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150908.png\" alt=\"image.png\"><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104150952.png\" alt=\"image.png\"><br>未特别说明，短作业优先算法默认是非抢占式的。<br>在所有进程都几乎同时到达时，采用SJF算法的平均等待时间、平均周转时间最少。<br>抢占式的短作业优先算法的平均等待时间、平均周转时间最少。在所有进程同时到达时，SJF算法等同于抢占式SJF算法。</p>\n<p>缺点：可能产生饥饿现象。<strong>短作业友好，长作业不友好</strong>。</p>\n<h4 id=\"高相应比优先（HRRN）\"><a href=\"#高相应比优先（HRRN）\" class=\"headerlink\" title=\"高相应比优先（HRRN）\"></a>高相应比优先（HRRN）</h4><p>在调度时计算每个进程的相应比，选择相应比最高的进程为其服务。</p>\n<blockquote>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152027.png\" alt=\"image.png\"></p>\n</blockquote>\n<p>非抢占式算法<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152220.png\" alt=\"image.png\"><br>综合考虑了等待时间和运行时间<br>避免了长作业饥饿的问题<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104152435.png\" alt=\"image.png\"></p>\n<h4 id=\"时间片轮转（RR）\"><a href=\"#时间片轮转（RR）\" class=\"headerlink\" title=\"时间片轮转（RR）\"></a>时间片轮转（RR）</h4><p>按各进程到达就绪队列的顺序，轮流让各个进程执行一个<strong>时间片</strong>（如100ms）。若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。</p>\n<p>抢占式算法<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153525.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153547.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153618.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104153635.png\" alt=\"image.png\"></p>\n<p>如果时间片太大，使得每个进程都可以在一个时间片内完成，则时间片轮转调度算法退化为先来先服务算法，并且会增大进程相应时间。<br>如果时间片太小，会导致进程切换过于频繁。</p>\n<h4 id=\"优先级调度算法\"><a href=\"#优先级调度算法\" class=\"headerlink\" title=\"优先级调度算法\"></a>优先级调度算法</h4><p>每个进程都有各自的优先级，调度时选择优先级最高的进程。<br>有抢占式和非抢占式。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104154536.png\" alt=\"image.png\"></p>\n<p>会导致饥饿</p>\n<h4 id=\"多级反馈队列调度算法（MLFQ）\"><a href=\"#多级反馈队列调度算法（MLFQ）\" class=\"headerlink\" title=\"多级反馈队列调度算法（MLFQ）\"></a>多级反馈队列调度算法（MLFQ）</h4><p><strong>设置多级就绪队列，各级队列优先级从高到低，时间片从小到大。</strong><br><strong>新进程到达时先进入第一级队列，按FCFS原则排队等待被分配时间片</strong>。<strong>若用完时间片进程还没结束，则进程进入下一级队列队尾</strong>。如果此时已经在最下级队列，则重新放回最下级队列队尾。<strong>只有在第k级队列为空时，才会为k+1级队头的进程分配时间片</strong><br>抢占式算法。在k级队列的进程运行过程中，若更上级的队列中进入了一个新进程，则由于新进程处于优先级更高的队列中，因此新进程会抢占处理机，原来运行的进程放回k级队列队尾。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104155638.png\" alt=\"image.png\"></p>\n<h4 id=\"完全公平调度器（CFS）\"><a href=\"#完全公平调度器（CFS）\" class=\"headerlink\" title=\"完全公平调度器（CFS）\"></a>完全公平调度器（CFS）</h4><p>对于n个正在运行的任务，当这些任务同时不断地运行时，CPU会尽可能分配给他们1&#x2F;n的处理时间。CFS是一种基于加权公平排队思想的调度算法<br>实现：将所有进程保留在红黑树中，按最大执行时间排序（跟踪其正余额） 调度 调度最左边的进程（余额最高的进程） 如果进程退出，则将其从调度树中删除 中断时（时间片或 I&#x2F;O 结束），将进程重新插入树中的新位置</p>\n<blockquote>\n<p>红黑树是一种特殊的二叉搜索树，也就是左边节点都小于根节点都小于右边节点，递归整个树都满足这一点。也就是说最左边的叶子节点是最小的，最右边的叶子节点是最大的。红黑树相比二叉搜索树多了红色黑色两个颜色的宏定义,红黑树有以下5个性质：<br>每个结点要么是红的要么是黑的。<br>  根结点是黑的。<br>  每个叶结点都是黑的。<br>  如果一个结点是红的，那么它的两个儿子都是黑的。<br>  对于任意结点而言，其到叶结点的每条路径都包含相同数目的黑结点。</p>\n</blockquote>\n<ol>\n<li>CFS使用红黑树结构，来存储要调度的任务队列。</li>\n<li>每个节点代表了一个要调度的任务，节点的key即为虚拟时间（vruntime），虚拟时间由这个人物的运行时间计算而来。</li>\n<li>key越小，也就是vruntime越小的话，红黑树对应的节点就越靠左。<br>4. CFS scheduler每次都挑选最左边的节点作为下一个要运行的任务，这个节点是“缓存的”——由一个特殊的指针指向；不需要进行O（logn）遍历来查找。也因此，CFS搜索的时间是O(1)。</li>\n</ol>\n"},{"title":"Semaphores信号量","date":"2023-10-16T00:42:44.174Z","_content":"sleep(): 暂停线程，把状态改为BLOCKED\nweakup(): 唤醒另一个线程，把其状态改为READY\n\nsemaphore（信号量）：\n一个非负整数，记录过往weakup的次数\n\n或者一个可负整数\n\n### 信号量机制\n信号量是一个变量，用来表示系统中某种资源的数量\n原语是一种特殊的程序段，其执行只能一气呵成，不可被中断。\n一对原语：wait(S)和signal(S)，S为信号量，简称为P(S)和V(S)\n\n#### 整型信号量\n用一个整数型变量作为信号量，用来表示系统中某种资源的数量。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104439.png)\n存在的问题：不满足“让权等待”原则，会发生“忙等”\n\n#### 记录型信号量\n用记录型数据结构表示的信号量\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104723.png)\nS.value的初值表示系统中某资源的数目。遵循“让权等待”原则\n\n### 信号量机制实现进程互斥\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105105903.png)\n设置互斥信号量mutex，初值为1\n\n### 信号量机制实现进程同步\n需要保证“一前一后”执行的两个操作（或两句代码）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110323.png)\n设置同步信号量S，初值为0\n在“前操作”之后执行V(S)\n在“后操作”之前执行P(S) \n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110711.png)\n### 生产者-消费者问题\n系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区，消费者进程每次从缓冲区中取出一个产品使用。生产者共享一个初始为空、大小为n的缓冲区。\n- 只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待\n- 只有缓冲区不空时，消费者才能从中取出产品，否则必须等待\n- 缓冲区是临界资源，各进程必须互斥访问。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112301.png)\n\n实现互斥的P操作一定要在实现同步的P操作之后。先上锁再操作缓冲区会产生阻塞。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112422.png)\n两个V操作顺序可以交换。\n\n### 读者写者问题\n1. 允许多个读者同时对文件执行读操作\n2. 只允许一个写者往文件中写信息\n3. 任一写者在完成写操作前不允许其他读者或者写者工作\n4. 写者执行写操作前，应让已有的读者和写者全部退出\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105130752.png)\n用rw实现互斥访问，用count实现多个读进程访问，用mutex实现count和rw的原子化操作\n潜在问题：只要有读进程还在读，写进程就要一直阻塞等待，可能饿死。这种算法中，读进程是优先的。\n实现写优先：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131045.png)\n### 哲学家进餐问题\n问题描述：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131830.png)\n防止死锁发生：\n1. 最多允许四个哲学家同时进餐\n2. 要求奇数号哲学家先拿左边的筷子，然后再拿右边的筷子，而偶数号哲学家刚好相反\n3. 使各哲学家拿筷子的操作互斥进行。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105132054.png)\n","source":"_posts/Notes/课程/大三（上）/操作系统/Semaphores信号量.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntitle: Semaphores信号量\ntags:\n  - 操作系统\ndate:\n---\nsleep(): 暂停线程，把状态改为BLOCKED\nweakup(): 唤醒另一个线程，把其状态改为READY\n\nsemaphore（信号量）：\n一个非负整数，记录过往weakup的次数\n\n或者一个可负整数\n\n### 信号量机制\n信号量是一个变量，用来表示系统中某种资源的数量\n原语是一种特殊的程序段，其执行只能一气呵成，不可被中断。\n一对原语：wait(S)和signal(S)，S为信号量，简称为P(S)和V(S)\n\n#### 整型信号量\n用一个整数型变量作为信号量，用来表示系统中某种资源的数量。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104439.png)\n存在的问题：不满足“让权等待”原则，会发生“忙等”\n\n#### 记录型信号量\n用记录型数据结构表示的信号量\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104723.png)\nS.value的初值表示系统中某资源的数目。遵循“让权等待”原则\n\n### 信号量机制实现进程互斥\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105105903.png)\n设置互斥信号量mutex，初值为1\n\n### 信号量机制实现进程同步\n需要保证“一前一后”执行的两个操作（或两句代码）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110323.png)\n设置同步信号量S，初值为0\n在“前操作”之后执行V(S)\n在“后操作”之前执行P(S) \n例题：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110711.png)\n### 生产者-消费者问题\n系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区，消费者进程每次从缓冲区中取出一个产品使用。生产者共享一个初始为空、大小为n的缓冲区。\n- 只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待\n- 只有缓冲区不空时，消费者才能从中取出产品，否则必须等待\n- 缓冲区是临界资源，各进程必须互斥访问。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112301.png)\n\n实现互斥的P操作一定要在实现同步的P操作之后。先上锁再操作缓冲区会产生阻塞。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112422.png)\n两个V操作顺序可以交换。\n\n### 读者写者问题\n1. 允许多个读者同时对文件执行读操作\n2. 只允许一个写者往文件中写信息\n3. 任一写者在完成写操作前不允许其他读者或者写者工作\n4. 写者执行写操作前，应让已有的读者和写者全部退出\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105130752.png)\n用rw实现互斥访问，用count实现多个读进程访问，用mutex实现count和rw的原子化操作\n潜在问题：只要有读进程还在读，写进程就要一直阻塞等待，可能饿死。这种算法中，读进程是优先的。\n实现写优先：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131045.png)\n### 哲学家进餐问题\n问题描述：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131830.png)\n防止死锁发生：\n1. 最多允许四个哲学家同时进餐\n2. 要求奇数号哲学家先拿左边的筷子，然后再拿右边的筷子，而偶数号哲学家刚好相反\n3. 使各哲学家拿筷子的操作互斥进行。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105132054.png)\n","slug":"Notes/课程/大三（上）/操作系统/Semaphores信号量","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr11y0008vw8c39iw5btz","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>sleep(): 暂停线程，把状态改为BLOCKED<br>weakup(): 唤醒另一个线程，把其状态改为READY</p>\n<p>semaphore（信号量）：<br>一个非负整数，记录过往weakup的次数</p>\n<p>或者一个可负整数</p>\n<h3 id=\"信号量机制\"><a href=\"#信号量机制\" class=\"headerlink\" title=\"信号量机制\"></a>信号量机制</h3><p>信号量是一个变量，用来表示系统中某种资源的数量<br>原语是一种特殊的程序段，其执行只能一气呵成，不可被中断。<br>一对原语：wait(S)和signal(S)，S为信号量，简称为P(S)和V(S)</p>\n<h4 id=\"整型信号量\"><a href=\"#整型信号量\" class=\"headerlink\" title=\"整型信号量\"></a>整型信号量</h4><p>用一个整数型变量作为信号量，用来表示系统中某种资源的数量。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104439.png\" alt=\"image.png\"><br>存在的问题：不满足“让权等待”原则，会发生“忙等”</p>\n<h4 id=\"记录型信号量\"><a href=\"#记录型信号量\" class=\"headerlink\" title=\"记录型信号量\"></a>记录型信号量</h4><p>用记录型数据结构表示的信号量<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104723.png\" alt=\"image.png\"><br>S.value的初值表示系统中某资源的数目。遵循“让权等待”原则</p>\n<h3 id=\"信号量机制实现进程互斥\"><a href=\"#信号量机制实现进程互斥\" class=\"headerlink\" title=\"信号量机制实现进程互斥\"></a>信号量机制实现进程互斥</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105105903.png\" alt=\"image.png\"><br>设置互斥信号量mutex，初值为1</p>\n<h3 id=\"信号量机制实现进程同步\"><a href=\"#信号量机制实现进程同步\" class=\"headerlink\" title=\"信号量机制实现进程同步\"></a>信号量机制实现进程同步</h3><p>需要保证“一前一后”执行的两个操作（或两句代码）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110323.png\" alt=\"image.png\"><br>设置同步信号量S，初值为0<br>在“前操作”之后执行V(S)<br>在“后操作”之前执行P(S)<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110711.png\" alt=\"image.png\"></p>\n<h3 id=\"生产者-消费者问题\"><a href=\"#生产者-消费者问题\" class=\"headerlink\" title=\"生产者-消费者问题\"></a>生产者-消费者问题</h3><p>系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区，消费者进程每次从缓冲区中取出一个产品使用。生产者共享一个初始为空、大小为n的缓冲区。</p>\n<ul>\n<li>只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待</li>\n<li>只有缓冲区不空时，消费者才能从中取出产品，否则必须等待</li>\n<li>缓冲区是临界资源，各进程必须互斥访问。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112301.png\" alt=\"image.png\"></li>\n</ul>\n<p>实现互斥的P操作一定要在实现同步的P操作之后。先上锁再操作缓冲区会产生阻塞。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112422.png\" alt=\"image.png\"><br>两个V操作顺序可以交换。</p>\n<h3 id=\"读者写者问题\"><a href=\"#读者写者问题\" class=\"headerlink\" title=\"读者写者问题\"></a>读者写者问题</h3><ol>\n<li>允许多个读者同时对文件执行读操作</li>\n<li>只允许一个写者往文件中写信息</li>\n<li>任一写者在完成写操作前不允许其他读者或者写者工作</li>\n<li>写者执行写操作前，应让已有的读者和写者全部退出<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105130752.png\" alt=\"image.png\"><br>用rw实现互斥访问，用count实现多个读进程访问，用mutex实现count和rw的原子化操作<br>潜在问题：只要有读进程还在读，写进程就要一直阻塞等待，可能饿死。这种算法中，读进程是优先的。<br>实现写优先：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131045.png\" alt=\"image.png\"></li>\n</ol>\n<h3 id=\"哲学家进餐问题\"><a href=\"#哲学家进餐问题\" class=\"headerlink\" title=\"哲学家进餐问题\"></a>哲学家进餐问题</h3><p>问题描述：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131830.png\" alt=\"image.png\"><br>防止死锁发生：</p>\n<ol>\n<li>最多允许四个哲学家同时进餐</li>\n<li>要求奇数号哲学家先拿左边的筷子，然后再拿右边的筷子，而偶数号哲学家刚好相反</li>\n<li>使各哲学家拿筷子的操作互斥进行。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105132054.png\" alt=\"image.png\"></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>sleep(): 暂停线程，把状态改为BLOCKED<br>weakup(): 唤醒另一个线程，把其状态改为READY</p>\n<p>semaphore（信号量）：<br>一个非负整数，记录过往weakup的次数</p>\n<p>或者一个可负整数</p>\n<h3 id=\"信号量机制\"><a href=\"#信号量机制\" class=\"headerlink\" title=\"信号量机制\"></a>信号量机制</h3><p>信号量是一个变量，用来表示系统中某种资源的数量<br>原语是一种特殊的程序段，其执行只能一气呵成，不可被中断。<br>一对原语：wait(S)和signal(S)，S为信号量，简称为P(S)和V(S)</p>\n<h4 id=\"整型信号量\"><a href=\"#整型信号量\" class=\"headerlink\" title=\"整型信号量\"></a>整型信号量</h4><p>用一个整数型变量作为信号量，用来表示系统中某种资源的数量。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104439.png\" alt=\"image.png\"><br>存在的问题：不满足“让权等待”原则，会发生“忙等”</p>\n<h4 id=\"记录型信号量\"><a href=\"#记录型信号量\" class=\"headerlink\" title=\"记录型信号量\"></a>记录型信号量</h4><p>用记录型数据结构表示的信号量<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105104723.png\" alt=\"image.png\"><br>S.value的初值表示系统中某资源的数目。遵循“让权等待”原则</p>\n<h3 id=\"信号量机制实现进程互斥\"><a href=\"#信号量机制实现进程互斥\" class=\"headerlink\" title=\"信号量机制实现进程互斥\"></a>信号量机制实现进程互斥</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105105903.png\" alt=\"image.png\"><br>设置互斥信号量mutex，初值为1</p>\n<h3 id=\"信号量机制实现进程同步\"><a href=\"#信号量机制实现进程同步\" class=\"headerlink\" title=\"信号量机制实现进程同步\"></a>信号量机制实现进程同步</h3><p>需要保证“一前一后”执行的两个操作（或两句代码）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110323.png\" alt=\"image.png\"><br>设置同步信号量S，初值为0<br>在“前操作”之后执行V(S)<br>在“后操作”之前执行P(S)<br>例题：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105110711.png\" alt=\"image.png\"></p>\n<h3 id=\"生产者-消费者问题\"><a href=\"#生产者-消费者问题\" class=\"headerlink\" title=\"生产者-消费者问题\"></a>生产者-消费者问题</h3><p>系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区，消费者进程每次从缓冲区中取出一个产品使用。生产者共享一个初始为空、大小为n的缓冲区。</p>\n<ul>\n<li>只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待</li>\n<li>只有缓冲区不空时，消费者才能从中取出产品，否则必须等待</li>\n<li>缓冲区是临界资源，各进程必须互斥访问。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112301.png\" alt=\"image.png\"></li>\n</ul>\n<p>实现互斥的P操作一定要在实现同步的P操作之后。先上锁再操作缓冲区会产生阻塞。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105112422.png\" alt=\"image.png\"><br>两个V操作顺序可以交换。</p>\n<h3 id=\"读者写者问题\"><a href=\"#读者写者问题\" class=\"headerlink\" title=\"读者写者问题\"></a>读者写者问题</h3><ol>\n<li>允许多个读者同时对文件执行读操作</li>\n<li>只允许一个写者往文件中写信息</li>\n<li>任一写者在完成写操作前不允许其他读者或者写者工作</li>\n<li>写者执行写操作前，应让已有的读者和写者全部退出<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105130752.png\" alt=\"image.png\"><br>用rw实现互斥访问，用count实现多个读进程访问，用mutex实现count和rw的原子化操作<br>潜在问题：只要有读进程还在读，写进程就要一直阻塞等待，可能饿死。这种算法中，读进程是优先的。<br>实现写优先：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131045.png\" alt=\"image.png\"></li>\n</ol>\n<h3 id=\"哲学家进餐问题\"><a href=\"#哲学家进餐问题\" class=\"headerlink\" title=\"哲学家进餐问题\"></a>哲学家进餐问题</h3><p>问题描述：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105131830.png\" alt=\"image.png\"><br>防止死锁发生：</p>\n<ol>\n<li>最多允许四个哲学家同时进餐</li>\n<li>要求奇数号哲学家先拿左边的筷子，然后再拿右边的筷子，而偶数号哲学家刚好相反</li>\n<li>使各哲学家拿筷子的操作互斥进行。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105132054.png\" alt=\"image.png\"></li>\n</ol>\n"},{"title":"Processes进程","date":"2023-11-02T12:53:55.380Z","_content":"为什么我们需要同时运行多个程序？称为“多道程序设计” 因为它会提高 CPU 利用率 \nI/O 密集型程序大部分时间都在等待 I/O，因此最好让 CPU 忙于其他任务\n\n多道程序设计：在一个物理地址空间中容纳多个进程 每个进程可以是 I/O 密集型或 CPU 密集型 混合使用 I/O 密集型和 CPU 密集型进程会很好 目标是提高 CPU 利用率 调度程序决定哪个进程执行\n\n分时（或“多任务”）：非常快速地在进程之间来回切换 - 称为“上下文切换” \n目标是减少用户与计算机交互时的延迟\n\n程序由静态代码和数据（如磁盘上的数据）组成。 进程是程序的运行实例。在任何时候，一个程序都可能有 0 个或多个运行实例，例如，一个用户可能同时运行多个 shell\n\n从运行时上下文的角度来看，进程是一个执行流。执行流是​​执行指令的序列（即“控制线程”）。运行时上下文包含执行指令可能影响或受其影响的所有内容（例如，寄存器、地址空间、文件等持久数据）\n\n进程可访问的一组内存部分称为进程的地址空间 \n- 文本 — 程序代码（通常是只读） \n- 数据 — 全局变量和常量 \n- 栈（Stack） — 每个帧包含参数、局部变量和函数的返回地址 \n- 堆（Heap）— 动态分配内存，例如，通过在 C 中调用 malloc()\n\n允许在同一物理地址空间中执行多个程序 \n虚拟化CPU：多个独立进程同时运行在一台物理机上 但实际上，每个CPU上任一时刻最多只能有一个进程处于活动状态。\n\n程序是磁盘上的应用程序，由代码和数据组成；\n**程序在执行时就成为进程**。**进程是程序的运行实例**。进程以单个执行线程和地址空间开始。一个进程可以在同一地址空间中启动多个执行线程。**每个线程都有自己的堆栈**，但它们**共享全局数据、代码和堆**。\n\n当用户执行程序时，操作系统会创建一个进程。操作系统在多个进程之间分时共享 CPU。操作系统调度程序选择要运行的可执行进程之一。\n调度程序必须保留进程列表 \n调度程序必须保留调度策略的元数据\n\n策略和机制之间的区别可以实现模块化。调度策略独立于上下文切换功能。\n\n运行（Running）：此进程当前正在执行 \n就绪（Ready）：此进程已准备好执行（并且将在策略决定时进行调度） \n阻塞（Blocked）：此进程已挂起（例如，等待某些操作；当该操作完成时，操作系统将取消阻止它） \nNew：此进程正在创建（以确保它不会被调度） Dead/termination：此进程已终止（例如，如果父进程尚未读出返回值）\n\n如果所有进程都被阻塞，应该调度什么进程？空闲进程（idle）。现代内核使用低优先级空闲进程，如果没有其他进程准备好，该进程就会被调度并执行。空闲进程从不阻塞或执行任何 I/O。空闲进程是解决挑战性问题的简单方法。如果没有空闲进程，调度程序将必须检查是否没有进程准备好运行，并且必须保守地采取行动。空闲进程保证至少有一个进程可以运行\n\n操作系统维护活动进程的数据结构（数组/列表）。每个进程的信息都存储在进程控制块（在 Linux 上，称为 task_struct）中，其中包含： \n进程标识符 (PID) \n进程状态（例如，就绪） \n指向父进程的指针 (cat /proc/self/status) \nCPU 上下文（如果进程未运行） \n指向地址空间的指针 (cat /proc/self/maps) \n指向打开文件列表的指针（文件描述符，cat /proc/self/fdinfo/\\*）\n\n保存进程的所有状态允许进程暂时挂起并稍后从同一点恢复\n然后可以通过恢复其保存的状态来恢复另一个进程\n执行上下文切换所需的时间是我们希望最小化的开销\n\n程序：由磁盘上的可执行文件组成。包含启动进程的所有信息 \n进程：程序的运行实例；具有数据部分和堆栈初始化\n线程：一个进程可以在同一地址空间中拥有多个线程（计算相同的数据）\n\n读取地址0xc0f3的两个进程可能读取到不同的值。而同一进程中的两个线程会读取相同的值\n\n进程可以通过系统调用API（应用程序编程接口）请求服务\n进程 API 使进程能够通过一组系统调用来控制自身和其他进程： \ngetpid() 检索进程的 ID，每个进程都有唯一的 PID \nfork() 创建一个新的子进程（进程的副本） \nexec () 执行一个新程序 \nexit() 终止当前进程 \nwait() 阻塞父进程，直到子进程终止\n\n操作系统为新进程（子进程）分配数据结构。操作系统复制调用者（父级）的地址空间。子进程已准备就绪并添加到进程列表中。 fork() 为父/子返回不同的值。父级和子级继续在各自的地址空间副本中执行\nexec() 替换地址空间，从磁盘加载新程序。总是执行同一个程序很无聊。程序可以传递命令行参数和环境。旧的地址空间/状态被销毁，除了保留的 STDIN、STDOUT、STDERR 之外，允许父级重定向/重新连接子级的输出！\n假设用户想要启动另一个程序。为此，操作系统需要创建一个新进程并创建一个新的地址空间来加载程序。\n fork() 使用该地址空间的副本创建一个新进程 exec() 为程序创建一个新的地址空间 clone() 将一个（执行的）线程添加到该地址空间\n 子进程与其父进程相关联。 exit(int retval) 接受一个返回值参数。父级可以 wait() 终止子级并读取子级的返回值\nfork() 通过复制调用进程地址空间的内容来创建新进程 新进程有自己的地址空间（内容从父进程复制） 操作系统中的进程控制块\n\n进程直接在CPU上执行指令\n\n进程可能会做一些非法的事情（读/写不属于该进程的内存，直接访问硬件） 进程可能会永远运行（操作系统必须保持控制） 进程可能会做一些缓慢的事情，例如 I/O（操作系统可能想要切换到另一个进程）\n解决方案：操作系统在硬件的帮助下维持一些控制。例如，操作系统维护定时器以定期拦截执行，并且进程可能不会执行直接访问硬件的特权指令\n\n在大多数操作系统上，进程是： 相互隔离 与操作系统隔离 隔离是安全的核心要求： 将错误限制在进程中 启用权限隔离 启用分区（将复杂系统分解为独立的故障域)\n\n\n### 进程的组成--PCB\n进程控制块PCB，记录PID、UID\n进程被创建时创建唯一PCB，进程结束时回收\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232517.png)\n\n### 进程的特征\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232654.png)\n\n### 进程的状态\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102233236.png)\n\n进程的等待队列用指针建立表\n\n### 进程控制\n#### 实现原语的原子性：\n关中断指令和开中断指令。这两个指令属于特权指令，只能CPU调用\nCPU执行了关中断指令后就不再检查中断信号，直到执行开中断指令。\n\n#### 进程控制相关的原语\n创建原语：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234319.png)\n撤销原语：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234559.png)\n\n阻塞原语和唤醒原语：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234718.png)\n\n切换原语：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102235107.png)\n\n\n#### 进程通信\n##### 共享存储\n操作系统在内存中划分一块共享存储区\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103152934.png)\n基于储存区的共享：高级通信方式，灵活性高，速度快\n基于数据结构的共享：低级通信方式，速度慢、限制多\n\n##### 消息传递\n进程间的数据交换以格式化的消息为单位，通过发送和接收消息两个原语进行数据交换。\n直接通信方式\n间接通信方式\n管道通信\n\n","source":"_posts/Notes/课程/大三（上）/操作系统/Processes进程.md","raw":"---\ntitle: Processes进程\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\ndate:\n---\n为什么我们需要同时运行多个程序？称为“多道程序设计” 因为它会提高 CPU 利用率 \nI/O 密集型程序大部分时间都在等待 I/O，因此最好让 CPU 忙于其他任务\n\n多道程序设计：在一个物理地址空间中容纳多个进程 每个进程可以是 I/O 密集型或 CPU 密集型 混合使用 I/O 密集型和 CPU 密集型进程会很好 目标是提高 CPU 利用率 调度程序决定哪个进程执行\n\n分时（或“多任务”）：非常快速地在进程之间来回切换 - 称为“上下文切换” \n目标是减少用户与计算机交互时的延迟\n\n程序由静态代码和数据（如磁盘上的数据）组成。 进程是程序的运行实例。在任何时候，一个程序都可能有 0 个或多个运行实例，例如，一个用户可能同时运行多个 shell\n\n从运行时上下文的角度来看，进程是一个执行流。执行流是​​执行指令的序列（即“控制线程”）。运行时上下文包含执行指令可能影响或受其影响的所有内容（例如，寄存器、地址空间、文件等持久数据）\n\n进程可访问的一组内存部分称为进程的地址空间 \n- 文本 — 程序代码（通常是只读） \n- 数据 — 全局变量和常量 \n- 栈（Stack） — 每个帧包含参数、局部变量和函数的返回地址 \n- 堆（Heap）— 动态分配内存，例如，通过在 C 中调用 malloc()\n\n允许在同一物理地址空间中执行多个程序 \n虚拟化CPU：多个独立进程同时运行在一台物理机上 但实际上，每个CPU上任一时刻最多只能有一个进程处于活动状态。\n\n程序是磁盘上的应用程序，由代码和数据组成；\n**程序在执行时就成为进程**。**进程是程序的运行实例**。进程以单个执行线程和地址空间开始。一个进程可以在同一地址空间中启动多个执行线程。**每个线程都有自己的堆栈**，但它们**共享全局数据、代码和堆**。\n\n当用户执行程序时，操作系统会创建一个进程。操作系统在多个进程之间分时共享 CPU。操作系统调度程序选择要运行的可执行进程之一。\n调度程序必须保留进程列表 \n调度程序必须保留调度策略的元数据\n\n策略和机制之间的区别可以实现模块化。调度策略独立于上下文切换功能。\n\n运行（Running）：此进程当前正在执行 \n就绪（Ready）：此进程已准备好执行（并且将在策略决定时进行调度） \n阻塞（Blocked）：此进程已挂起（例如，等待某些操作；当该操作完成时，操作系统将取消阻止它） \nNew：此进程正在创建（以确保它不会被调度） Dead/termination：此进程已终止（例如，如果父进程尚未读出返回值）\n\n如果所有进程都被阻塞，应该调度什么进程？空闲进程（idle）。现代内核使用低优先级空闲进程，如果没有其他进程准备好，该进程就会被调度并执行。空闲进程从不阻塞或执行任何 I/O。空闲进程是解决挑战性问题的简单方法。如果没有空闲进程，调度程序将必须检查是否没有进程准备好运行，并且必须保守地采取行动。空闲进程保证至少有一个进程可以运行\n\n操作系统维护活动进程的数据结构（数组/列表）。每个进程的信息都存储在进程控制块（在 Linux 上，称为 task_struct）中，其中包含： \n进程标识符 (PID) \n进程状态（例如，就绪） \n指向父进程的指针 (cat /proc/self/status) \nCPU 上下文（如果进程未运行） \n指向地址空间的指针 (cat /proc/self/maps) \n指向打开文件列表的指针（文件描述符，cat /proc/self/fdinfo/\\*）\n\n保存进程的所有状态允许进程暂时挂起并稍后从同一点恢复\n然后可以通过恢复其保存的状态来恢复另一个进程\n执行上下文切换所需的时间是我们希望最小化的开销\n\n程序：由磁盘上的可执行文件组成。包含启动进程的所有信息 \n进程：程序的运行实例；具有数据部分和堆栈初始化\n线程：一个进程可以在同一地址空间中拥有多个线程（计算相同的数据）\n\n读取地址0xc0f3的两个进程可能读取到不同的值。而同一进程中的两个线程会读取相同的值\n\n进程可以通过系统调用API（应用程序编程接口）请求服务\n进程 API 使进程能够通过一组系统调用来控制自身和其他进程： \ngetpid() 检索进程的 ID，每个进程都有唯一的 PID \nfork() 创建一个新的子进程（进程的副本） \nexec () 执行一个新程序 \nexit() 终止当前进程 \nwait() 阻塞父进程，直到子进程终止\n\n操作系统为新进程（子进程）分配数据结构。操作系统复制调用者（父级）的地址空间。子进程已准备就绪并添加到进程列表中。 fork() 为父/子返回不同的值。父级和子级继续在各自的地址空间副本中执行\nexec() 替换地址空间，从磁盘加载新程序。总是执行同一个程序很无聊。程序可以传递命令行参数和环境。旧的地址空间/状态被销毁，除了保留的 STDIN、STDOUT、STDERR 之外，允许父级重定向/重新连接子级的输出！\n假设用户想要启动另一个程序。为此，操作系统需要创建一个新进程并创建一个新的地址空间来加载程序。\n fork() 使用该地址空间的副本创建一个新进程 exec() 为程序创建一个新的地址空间 clone() 将一个（执行的）线程添加到该地址空间\n 子进程与其父进程相关联。 exit(int retval) 接受一个返回值参数。父级可以 wait() 终止子级并读取子级的返回值\nfork() 通过复制调用进程地址空间的内容来创建新进程 新进程有自己的地址空间（内容从父进程复制） 操作系统中的进程控制块\n\n进程直接在CPU上执行指令\n\n进程可能会做一些非法的事情（读/写不属于该进程的内存，直接访问硬件） 进程可能会永远运行（操作系统必须保持控制） 进程可能会做一些缓慢的事情，例如 I/O（操作系统可能想要切换到另一个进程）\n解决方案：操作系统在硬件的帮助下维持一些控制。例如，操作系统维护定时器以定期拦截执行，并且进程可能不会执行直接访问硬件的特权指令\n\n在大多数操作系统上，进程是： 相互隔离 与操作系统隔离 隔离是安全的核心要求： 将错误限制在进程中 启用权限隔离 启用分区（将复杂系统分解为独立的故障域)\n\n\n### 进程的组成--PCB\n进程控制块PCB，记录PID、UID\n进程被创建时创建唯一PCB，进程结束时回收\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232517.png)\n\n### 进程的特征\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232654.png)\n\n### 进程的状态\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102233236.png)\n\n进程的等待队列用指针建立表\n\n### 进程控制\n#### 实现原语的原子性：\n关中断指令和开中断指令。这两个指令属于特权指令，只能CPU调用\nCPU执行了关中断指令后就不再检查中断信号，直到执行开中断指令。\n\n#### 进程控制相关的原语\n创建原语：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234319.png)\n撤销原语：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234559.png)\n\n阻塞原语和唤醒原语：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234718.png)\n\n切换原语：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102235107.png)\n\n\n#### 进程通信\n##### 共享存储\n操作系统在内存中划分一块共享存储区\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103152934.png)\n基于储存区的共享：高级通信方式，灵活性高，速度快\n基于数据结构的共享：低级通信方式，速度慢、限制多\n\n##### 消息传递\n进程间的数据交换以格式化的消息为单位，通过发送和接收消息两个原语进行数据交换。\n直接通信方式\n间接通信方式\n管道通信\n\n","slug":"Notes/课程/大三（上）/操作系统/Processes进程","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr11z000bvw8c6ein5fn7","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>为什么我们需要同时运行多个程序？称为“多道程序设计” 因为它会提高 CPU 利用率<br>I&#x2F;O 密集型程序大部分时间都在等待 I&#x2F;O，因此最好让 CPU 忙于其他任务</p>\n<p>多道程序设计：在一个物理地址空间中容纳多个进程 每个进程可以是 I&#x2F;O 密集型或 CPU 密集型 混合使用 I&#x2F;O 密集型和 CPU 密集型进程会很好 目标是提高 CPU 利用率 调度程序决定哪个进程执行</p>\n<p>分时（或“多任务”）：非常快速地在进程之间来回切换 - 称为“上下文切换”<br>目标是减少用户与计算机交互时的延迟</p>\n<p>程序由静态代码和数据（如磁盘上的数据）组成。 进程是程序的运行实例。在任何时候，一个程序都可能有 0 个或多个运行实例，例如，一个用户可能同时运行多个 shell</p>\n<p>从运行时上下文的角度来看，进程是一个执行流。执行流是​​执行指令的序列（即“控制线程”）。运行时上下文包含执行指令可能影响或受其影响的所有内容（例如，寄存器、地址空间、文件等持久数据）</p>\n<p>进程可访问的一组内存部分称为进程的地址空间 </p>\n<ul>\n<li>文本 — 程序代码（通常是只读） </li>\n<li>数据 — 全局变量和常量 </li>\n<li>栈（Stack） — 每个帧包含参数、局部变量和函数的返回地址 </li>\n<li>堆（Heap）— 动态分配内存，例如，通过在 C 中调用 malloc()</li>\n</ul>\n<p>允许在同一物理地址空间中执行多个程序<br>虚拟化CPU：多个独立进程同时运行在一台物理机上 但实际上，每个CPU上任一时刻最多只能有一个进程处于活动状态。</p>\n<p>程序是磁盘上的应用程序，由代码和数据组成；<br><strong>程序在执行时就成为进程</strong>。<strong>进程是程序的运行实例</strong>。进程以单个执行线程和地址空间开始。一个进程可以在同一地址空间中启动多个执行线程。<strong>每个线程都有自己的堆栈</strong>，但它们<strong>共享全局数据、代码和堆</strong>。</p>\n<p>当用户执行程序时，操作系统会创建一个进程。操作系统在多个进程之间分时共享 CPU。操作系统调度程序选择要运行的可执行进程之一。<br>调度程序必须保留进程列表<br>调度程序必须保留调度策略的元数据</p>\n<p>策略和机制之间的区别可以实现模块化。调度策略独立于上下文切换功能。</p>\n<p>运行（Running）：此进程当前正在执行<br>就绪（Ready）：此进程已准备好执行（并且将在策略决定时进行调度）<br>阻塞（Blocked）：此进程已挂起（例如，等待某些操作；当该操作完成时，操作系统将取消阻止它）<br>New：此进程正在创建（以确保它不会被调度） Dead&#x2F;termination：此进程已终止（例如，如果父进程尚未读出返回值）</p>\n<p>如果所有进程都被阻塞，应该调度什么进程？空闲进程（idle）。现代内核使用低优先级空闲进程，如果没有其他进程准备好，该进程就会被调度并执行。空闲进程从不阻塞或执行任何 I&#x2F;O。空闲进程是解决挑战性问题的简单方法。如果没有空闲进程，调度程序将必须检查是否没有进程准备好运行，并且必须保守地采取行动。空闲进程保证至少有一个进程可以运行</p>\n<p>操作系统维护活动进程的数据结构（数组&#x2F;列表）。每个进程的信息都存储在进程控制块（在 Linux 上，称为 task_struct）中，其中包含：<br>进程标识符 (PID)<br>进程状态（例如，就绪）<br>指向父进程的指针 (cat &#x2F;proc&#x2F;self&#x2F;status)<br>CPU 上下文（如果进程未运行）<br>指向地址空间的指针 (cat &#x2F;proc&#x2F;self&#x2F;maps)<br>指向打开文件列表的指针（文件描述符，cat &#x2F;proc&#x2F;self&#x2F;fdinfo&#x2F;*）</p>\n<p>保存进程的所有状态允许进程暂时挂起并稍后从同一点恢复<br>然后可以通过恢复其保存的状态来恢复另一个进程<br>执行上下文切换所需的时间是我们希望最小化的开销</p>\n<p>程序：由磁盘上的可执行文件组成。包含启动进程的所有信息<br>进程：程序的运行实例；具有数据部分和堆栈初始化<br>线程：一个进程可以在同一地址空间中拥有多个线程（计算相同的数据）</p>\n<p>读取地址0xc0f3的两个进程可能读取到不同的值。而同一进程中的两个线程会读取相同的值</p>\n<p>进程可以通过系统调用API（应用程序编程接口）请求服务<br>进程 API 使进程能够通过一组系统调用来控制自身和其他进程：<br>getpid() 检索进程的 ID，每个进程都有唯一的 PID<br>fork() 创建一个新的子进程（进程的副本）<br>exec () 执行一个新程序<br>exit() 终止当前进程<br>wait() 阻塞父进程，直到子进程终止</p>\n<p>操作系统为新进程（子进程）分配数据结构。操作系统复制调用者（父级）的地址空间。子进程已准备就绪并添加到进程列表中。 fork() 为父&#x2F;子返回不同的值。父级和子级继续在各自的地址空间副本中执行<br>exec() 替换地址空间，从磁盘加载新程序。总是执行同一个程序很无聊。程序可以传递命令行参数和环境。旧的地址空间&#x2F;状态被销毁，除了保留的 STDIN、STDOUT、STDERR 之外，允许父级重定向&#x2F;重新连接子级的输出！<br>假设用户想要启动另一个程序。为此，操作系统需要创建一个新进程并创建一个新的地址空间来加载程序。<br> fork() 使用该地址空间的副本创建一个新进程 exec() 为程序创建一个新的地址空间 clone() 将一个（执行的）线程添加到该地址空间<br> 子进程与其父进程相关联。 exit(int retval) 接受一个返回值参数。父级可以 wait() 终止子级并读取子级的返回值<br>fork() 通过复制调用进程地址空间的内容来创建新进程 新进程有自己的地址空间（内容从父进程复制） 操作系统中的进程控制块</p>\n<p>进程直接在CPU上执行指令</p>\n<p>进程可能会做一些非法的事情（读&#x2F;写不属于该进程的内存，直接访问硬件） 进程可能会永远运行（操作系统必须保持控制） 进程可能会做一些缓慢的事情，例如 I&#x2F;O（操作系统可能想要切换到另一个进程）<br>解决方案：操作系统在硬件的帮助下维持一些控制。例如，操作系统维护定时器以定期拦截执行，并且进程可能不会执行直接访问硬件的特权指令</p>\n<p>在大多数操作系统上，进程是： 相互隔离 与操作系统隔离 隔离是安全的核心要求： 将错误限制在进程中 启用权限隔离 启用分区（将复杂系统分解为独立的故障域)</p>\n<h3 id=\"进程的组成–PCB\"><a href=\"#进程的组成–PCB\" class=\"headerlink\" title=\"进程的组成–PCB\"></a>进程的组成–PCB</h3><p>进程控制块PCB，记录PID、UID<br>进程被创建时创建唯一PCB，进程结束时回收<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232517.png\" alt=\"image.png\"></p>\n<h3 id=\"进程的特征\"><a href=\"#进程的特征\" class=\"headerlink\" title=\"进程的特征\"></a>进程的特征</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232654.png\" alt=\"image.png\"></p>\n<h3 id=\"进程的状态\"><a href=\"#进程的状态\" class=\"headerlink\" title=\"进程的状态\"></a>进程的状态</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102233236.png\" alt=\"image.png\"></p>\n<p>进程的等待队列用指针建立表</p>\n<h3 id=\"进程控制\"><a href=\"#进程控制\" class=\"headerlink\" title=\"进程控制\"></a>进程控制</h3><h4 id=\"实现原语的原子性：\"><a href=\"#实现原语的原子性：\" class=\"headerlink\" title=\"实现原语的原子性：\"></a>实现原语的原子性：</h4><p>关中断指令和开中断指令。这两个指令属于特权指令，只能CPU调用<br>CPU执行了关中断指令后就不再检查中断信号，直到执行开中断指令。</p>\n<h4 id=\"进程控制相关的原语\"><a href=\"#进程控制相关的原语\" class=\"headerlink\" title=\"进程控制相关的原语\"></a>进程控制相关的原语</h4><p>创建原语：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234319.png\" alt=\"image.png\"><br>撤销原语：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234559.png\" alt=\"image.png\"></p>\n<p>阻塞原语和唤醒原语：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234718.png\" alt=\"image.png\"></p>\n<p>切换原语：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102235107.png\" alt=\"image.png\"></p>\n<h4 id=\"进程通信\"><a href=\"#进程通信\" class=\"headerlink\" title=\"进程通信\"></a>进程通信</h4><h5 id=\"共享存储\"><a href=\"#共享存储\" class=\"headerlink\" title=\"共享存储\"></a>共享存储</h5><p>操作系统在内存中划分一块共享存储区<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103152934.png\" alt=\"image.png\"><br>基于储存区的共享：高级通信方式，灵活性高，速度快<br>基于数据结构的共享：低级通信方式，速度慢、限制多</p>\n<h5 id=\"消息传递\"><a href=\"#消息传递\" class=\"headerlink\" title=\"消息传递\"></a>消息传递</h5><p>进程间的数据交换以格式化的消息为单位，通过发送和接收消息两个原语进行数据交换。<br>直接通信方式<br>间接通信方式<br>管道通信</p>\n","site":{"data":{}},"excerpt":"","more":"<p>为什么我们需要同时运行多个程序？称为“多道程序设计” 因为它会提高 CPU 利用率<br>I&#x2F;O 密集型程序大部分时间都在等待 I&#x2F;O，因此最好让 CPU 忙于其他任务</p>\n<p>多道程序设计：在一个物理地址空间中容纳多个进程 每个进程可以是 I&#x2F;O 密集型或 CPU 密集型 混合使用 I&#x2F;O 密集型和 CPU 密集型进程会很好 目标是提高 CPU 利用率 调度程序决定哪个进程执行</p>\n<p>分时（或“多任务”）：非常快速地在进程之间来回切换 - 称为“上下文切换”<br>目标是减少用户与计算机交互时的延迟</p>\n<p>程序由静态代码和数据（如磁盘上的数据）组成。 进程是程序的运行实例。在任何时候，一个程序都可能有 0 个或多个运行实例，例如，一个用户可能同时运行多个 shell</p>\n<p>从运行时上下文的角度来看，进程是一个执行流。执行流是​​执行指令的序列（即“控制线程”）。运行时上下文包含执行指令可能影响或受其影响的所有内容（例如，寄存器、地址空间、文件等持久数据）</p>\n<p>进程可访问的一组内存部分称为进程的地址空间 </p>\n<ul>\n<li>文本 — 程序代码（通常是只读） </li>\n<li>数据 — 全局变量和常量 </li>\n<li>栈（Stack） — 每个帧包含参数、局部变量和函数的返回地址 </li>\n<li>堆（Heap）— 动态分配内存，例如，通过在 C 中调用 malloc()</li>\n</ul>\n<p>允许在同一物理地址空间中执行多个程序<br>虚拟化CPU：多个独立进程同时运行在一台物理机上 但实际上，每个CPU上任一时刻最多只能有一个进程处于活动状态。</p>\n<p>程序是磁盘上的应用程序，由代码和数据组成；<br><strong>程序在执行时就成为进程</strong>。<strong>进程是程序的运行实例</strong>。进程以单个执行线程和地址空间开始。一个进程可以在同一地址空间中启动多个执行线程。<strong>每个线程都有自己的堆栈</strong>，但它们<strong>共享全局数据、代码和堆</strong>。</p>\n<p>当用户执行程序时，操作系统会创建一个进程。操作系统在多个进程之间分时共享 CPU。操作系统调度程序选择要运行的可执行进程之一。<br>调度程序必须保留进程列表<br>调度程序必须保留调度策略的元数据</p>\n<p>策略和机制之间的区别可以实现模块化。调度策略独立于上下文切换功能。</p>\n<p>运行（Running）：此进程当前正在执行<br>就绪（Ready）：此进程已准备好执行（并且将在策略决定时进行调度）<br>阻塞（Blocked）：此进程已挂起（例如，等待某些操作；当该操作完成时，操作系统将取消阻止它）<br>New：此进程正在创建（以确保它不会被调度） Dead&#x2F;termination：此进程已终止（例如，如果父进程尚未读出返回值）</p>\n<p>如果所有进程都被阻塞，应该调度什么进程？空闲进程（idle）。现代内核使用低优先级空闲进程，如果没有其他进程准备好，该进程就会被调度并执行。空闲进程从不阻塞或执行任何 I&#x2F;O。空闲进程是解决挑战性问题的简单方法。如果没有空闲进程，调度程序将必须检查是否没有进程准备好运行，并且必须保守地采取行动。空闲进程保证至少有一个进程可以运行</p>\n<p>操作系统维护活动进程的数据结构（数组&#x2F;列表）。每个进程的信息都存储在进程控制块（在 Linux 上，称为 task_struct）中，其中包含：<br>进程标识符 (PID)<br>进程状态（例如，就绪）<br>指向父进程的指针 (cat &#x2F;proc&#x2F;self&#x2F;status)<br>CPU 上下文（如果进程未运行）<br>指向地址空间的指针 (cat &#x2F;proc&#x2F;self&#x2F;maps)<br>指向打开文件列表的指针（文件描述符，cat &#x2F;proc&#x2F;self&#x2F;fdinfo&#x2F;*）</p>\n<p>保存进程的所有状态允许进程暂时挂起并稍后从同一点恢复<br>然后可以通过恢复其保存的状态来恢复另一个进程<br>执行上下文切换所需的时间是我们希望最小化的开销</p>\n<p>程序：由磁盘上的可执行文件组成。包含启动进程的所有信息<br>进程：程序的运行实例；具有数据部分和堆栈初始化<br>线程：一个进程可以在同一地址空间中拥有多个线程（计算相同的数据）</p>\n<p>读取地址0xc0f3的两个进程可能读取到不同的值。而同一进程中的两个线程会读取相同的值</p>\n<p>进程可以通过系统调用API（应用程序编程接口）请求服务<br>进程 API 使进程能够通过一组系统调用来控制自身和其他进程：<br>getpid() 检索进程的 ID，每个进程都有唯一的 PID<br>fork() 创建一个新的子进程（进程的副本）<br>exec () 执行一个新程序<br>exit() 终止当前进程<br>wait() 阻塞父进程，直到子进程终止</p>\n<p>操作系统为新进程（子进程）分配数据结构。操作系统复制调用者（父级）的地址空间。子进程已准备就绪并添加到进程列表中。 fork() 为父&#x2F;子返回不同的值。父级和子级继续在各自的地址空间副本中执行<br>exec() 替换地址空间，从磁盘加载新程序。总是执行同一个程序很无聊。程序可以传递命令行参数和环境。旧的地址空间&#x2F;状态被销毁，除了保留的 STDIN、STDOUT、STDERR 之外，允许父级重定向&#x2F;重新连接子级的输出！<br>假设用户想要启动另一个程序。为此，操作系统需要创建一个新进程并创建一个新的地址空间来加载程序。<br> fork() 使用该地址空间的副本创建一个新进程 exec() 为程序创建一个新的地址空间 clone() 将一个（执行的）线程添加到该地址空间<br> 子进程与其父进程相关联。 exit(int retval) 接受一个返回值参数。父级可以 wait() 终止子级并读取子级的返回值<br>fork() 通过复制调用进程地址空间的内容来创建新进程 新进程有自己的地址空间（内容从父进程复制） 操作系统中的进程控制块</p>\n<p>进程直接在CPU上执行指令</p>\n<p>进程可能会做一些非法的事情（读&#x2F;写不属于该进程的内存，直接访问硬件） 进程可能会永远运行（操作系统必须保持控制） 进程可能会做一些缓慢的事情，例如 I&#x2F;O（操作系统可能想要切换到另一个进程）<br>解决方案：操作系统在硬件的帮助下维持一些控制。例如，操作系统维护定时器以定期拦截执行，并且进程可能不会执行直接访问硬件的特权指令</p>\n<p>在大多数操作系统上，进程是： 相互隔离 与操作系统隔离 隔离是安全的核心要求： 将错误限制在进程中 启用权限隔离 启用分区（将复杂系统分解为独立的故障域)</p>\n<h3 id=\"进程的组成–PCB\"><a href=\"#进程的组成–PCB\" class=\"headerlink\" title=\"进程的组成–PCB\"></a>进程的组成–PCB</h3><p>进程控制块PCB，记录PID、UID<br>进程被创建时创建唯一PCB，进程结束时回收<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232517.png\" alt=\"image.png\"></p>\n<h3 id=\"进程的特征\"><a href=\"#进程的特征\" class=\"headerlink\" title=\"进程的特征\"></a>进程的特征</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102232654.png\" alt=\"image.png\"></p>\n<h3 id=\"进程的状态\"><a href=\"#进程的状态\" class=\"headerlink\" title=\"进程的状态\"></a>进程的状态</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102233236.png\" alt=\"image.png\"></p>\n<p>进程的等待队列用指针建立表</p>\n<h3 id=\"进程控制\"><a href=\"#进程控制\" class=\"headerlink\" title=\"进程控制\"></a>进程控制</h3><h4 id=\"实现原语的原子性：\"><a href=\"#实现原语的原子性：\" class=\"headerlink\" title=\"实现原语的原子性：\"></a>实现原语的原子性：</h4><p>关中断指令和开中断指令。这两个指令属于特权指令，只能CPU调用<br>CPU执行了关中断指令后就不再检查中断信号，直到执行开中断指令。</p>\n<h4 id=\"进程控制相关的原语\"><a href=\"#进程控制相关的原语\" class=\"headerlink\" title=\"进程控制相关的原语\"></a>进程控制相关的原语</h4><p>创建原语：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234319.png\" alt=\"image.png\"><br>撤销原语：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234559.png\" alt=\"image.png\"></p>\n<p>阻塞原语和唤醒原语：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102234718.png\" alt=\"image.png\"></p>\n<p>切换原语：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102235107.png\" alt=\"image.png\"></p>\n<h4 id=\"进程通信\"><a href=\"#进程通信\" class=\"headerlink\" title=\"进程通信\"></a>进程通信</h4><h5 id=\"共享存储\"><a href=\"#共享存储\" class=\"headerlink\" title=\"共享存储\"></a>共享存储</h5><p>操作系统在内存中划分一块共享存储区<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103152934.png\" alt=\"image.png\"><br>基于储存区的共享：高级通信方式，灵活性高，速度快<br>基于数据结构的共享：低级通信方式，速度慢、限制多</p>\n<h5 id=\"消息传递\"><a href=\"#消息传递\" class=\"headerlink\" title=\"消息传递\"></a>消息传递</h5><p>进程间的数据交换以格式化的消息为单位，通过发送和接收消息两个原语进行数据交换。<br>直接通信方式<br>间接通信方式<br>管道通信</p>\n"},{"title":"课程介绍","date":"2024-03-02T04:18:00.358Z","_content":"![f4e86555239c68c915a76e6294f7366.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f4e86555239c68c915a76e6294f7366.jpg)\n","source":"_posts/Notes/课程/自然语言处理/课程介绍.md","raw":"---\ntitle: 课程介绍\ncategories:\n  - Notes\n  - 课程\n  - 自然语言处理\ndate:\ntags:\n---\n![f4e86555239c68c915a76e6294f7366.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f4e86555239c68c915a76e6294f7366.jpg)\n","slug":"Notes/课程/自然语言处理/课程介绍","published":1,"updated":"2024-03-02T04:22:26.382Z","_id":"clt9kr120000evw8c707pbyho","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f4e86555239c68c915a76e6294f7366.jpg\" alt=\"f4e86555239c68c915a76e6294f7366.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f4e86555239c68c915a76e6294f7366.jpg\" alt=\"f4e86555239c68c915a76e6294f7366.jpg\"></p>\n"},{"title":"Threads线程","date":"2023-10-09T06:02:39.808Z","_content":"进程是最小资源分配单位\n线程是最小执行单元\n\n多线程服务框架\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140512.png)\n\n单线程与多线程进程：\n线程共享进程中的数据，进程为每个线程创建栈和寄存器\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140611.png)\n\nConcurrency并发：在单核系统上\nParallelism并行：在多核系统上\n\n操作系统会为每个内核级线程建立相应的TCB（线程控制块），通过TCB对线程进行管理\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104103658.png)\n\n可将多个TCB组织成一张线程表\n\n### Race Conditions竞争条件\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009143100.png)\n线程分为send和receive\n- send：当buffer中有空间，可以写入\n- receive：如果buffer中有message，返回message，创建线程\n>producer-consumer problem：\n>producer需要把message添加到buffer\n>consumer需要读取buffer中的message来创建线程\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144647.png)\n\n当有多个sender时：\nA和B会产生竞争，导致in的值不正确\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144932.png)\n\n解决方法：acquire和release\n当一个线程进行时，需要先执行acquire申请一个lock，在执行期间持有，结束后释放\n当一个线程持有lock时，其他线程acquire相同的lock会失败\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082019.png)\n\n\n在两个线程共用一个lock时，也会出现竞争条件\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016080611.png)\n\n解决方法：Test and Set Lock（TSL）把参数变为真值，返回它的旧值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016081529.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082444.png)\n\nCompare-and-swap lock\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083314.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083325.png)\n当线程A进行比较时，返回true，lck被置为true，线程A得到lock，此时线程B进行比较，返回false，进入循环，直到A进行release\n\n\n### 进程同步\n进程具有异步性的特征，各并发执行的进程以各自独立的、不可预知的速度向前推进\n\n### 进程互斥\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104223505.png)\n一个时间段内只允许一个进程使用的资源称为临界资源。对临界资源的访问，必须**互斥**地进行。\n进程互斥指当一个进程访问某临界资源时，另一个想要访问该资源的进程必须等待。当前访问临界资源的进程访问结束，释放该资源后，另一个进程才能去访问临界资源。\n\n对临界资源的互斥访问，分为四部分：\n- 进入区：检查是否可进入临界区，可以则上锁\n- 临界区：访问临界资源\n- 退出区：解锁\n- 剩余区：做其他处理\n\n进程互斥的原则：\n1. 空闲让进\n2. 忙则等待\n3. 有限等待\n4. 让权等待\n\n### 进程互斥的软件实现方式\n#### 单标志法\n一个进程在访问完临界区后会把使用临界区的权限转交给另一个进程。也就是说**每个进程进入临界区的权限只能被另一个进程赋予**\n\n举例：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104224824.png)\n\n如果此时允许P0进入临界区，但P0一直不访问临界区，那么虽然此时临界区空闲，但是并不允许P1访问。\n因此单标志法**违背”空闲让进“原则**\n\n#### 双标志先检查法\n设置一个布尔型数组flag[]，数组中各个元素用来标记各进程想进入临界区的意愿\n先检查后上锁\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230557.png)\n如果两个进程同时想要进入临界区，就会出现冲突。\n因此双标志先检查法的问题是**违背”忙则等待“原则**\n\n#### 双标志后检查法\n先上锁后检查\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230953.png)\n如果两个进程同时想要进入临界区，则它们都将无法进入临界区\n**违背了”空闲让进“和”有限等待“原则**，会让进程产生饥饿现象\n\n#### Peterson算法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104231828.png)\n最后一个设置turn的进程会失去优先权\n\n\n### 进程互斥的硬件实现方式\n#### 中断屏蔽方法\n利用“开/关中断指令”实现，即某进程开始访问临界区到结束访问为止都不允许被中断![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102139.png)\n#### TestAndSet指令\n简称TS指令或TSL指令\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102430.png)\n\n### 锁\n#### 互斥锁（mutex lock）\n一个进程在进入临界区时获得锁，在退出临界区时释放锁![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105103037.png)\n函数acquire()获得锁，函数release()释放锁\n每个互斥锁有一个布尔变量available，表示锁是否可用\n需要连续循环忙等的互斥锁，都可称为自旋锁（spin lock），如TSL指令、swap指令、单标志法","source":"_posts/Notes/课程/大三（上）/操作系统/Threads线程.md","raw":"---\ntitle: Threads线程\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\ndate:\n---\n进程是最小资源分配单位\n线程是最小执行单元\n\n多线程服务框架\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140512.png)\n\n单线程与多线程进程：\n线程共享进程中的数据，进程为每个线程创建栈和寄存器\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140611.png)\n\nConcurrency并发：在单核系统上\nParallelism并行：在多核系统上\n\n操作系统会为每个内核级线程建立相应的TCB（线程控制块），通过TCB对线程进行管理\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104103658.png)\n\n可将多个TCB组织成一张线程表\n\n### Race Conditions竞争条件\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009143100.png)\n线程分为send和receive\n- send：当buffer中有空间，可以写入\n- receive：如果buffer中有message，返回message，创建线程\n>producer-consumer problem：\n>producer需要把message添加到buffer\n>consumer需要读取buffer中的message来创建线程\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144647.png)\n\n当有多个sender时：\nA和B会产生竞争，导致in的值不正确\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144932.png)\n\n解决方法：acquire和release\n当一个线程进行时，需要先执行acquire申请一个lock，在执行期间持有，结束后释放\n当一个线程持有lock时，其他线程acquire相同的lock会失败\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082019.png)\n\n\n在两个线程共用一个lock时，也会出现竞争条件\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016080611.png)\n\n解决方法：Test and Set Lock（TSL）把参数变为真值，返回它的旧值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016081529.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082444.png)\n\nCompare-and-swap lock\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083314.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083325.png)\n当线程A进行比较时，返回true，lck被置为true，线程A得到lock，此时线程B进行比较，返回false，进入循环，直到A进行release\n\n\n### 进程同步\n进程具有异步性的特征，各并发执行的进程以各自独立的、不可预知的速度向前推进\n\n### 进程互斥\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104223505.png)\n一个时间段内只允许一个进程使用的资源称为临界资源。对临界资源的访问，必须**互斥**地进行。\n进程互斥指当一个进程访问某临界资源时，另一个想要访问该资源的进程必须等待。当前访问临界资源的进程访问结束，释放该资源后，另一个进程才能去访问临界资源。\n\n对临界资源的互斥访问，分为四部分：\n- 进入区：检查是否可进入临界区，可以则上锁\n- 临界区：访问临界资源\n- 退出区：解锁\n- 剩余区：做其他处理\n\n进程互斥的原则：\n1. 空闲让进\n2. 忙则等待\n3. 有限等待\n4. 让权等待\n\n### 进程互斥的软件实现方式\n#### 单标志法\n一个进程在访问完临界区后会把使用临界区的权限转交给另一个进程。也就是说**每个进程进入临界区的权限只能被另一个进程赋予**\n\n举例：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104224824.png)\n\n如果此时允许P0进入临界区，但P0一直不访问临界区，那么虽然此时临界区空闲，但是并不允许P1访问。\n因此单标志法**违背”空闲让进“原则**\n\n#### 双标志先检查法\n设置一个布尔型数组flag[]，数组中各个元素用来标记各进程想进入临界区的意愿\n先检查后上锁\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230557.png)\n如果两个进程同时想要进入临界区，就会出现冲突。\n因此双标志先检查法的问题是**违背”忙则等待“原则**\n\n#### 双标志后检查法\n先上锁后检查\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230953.png)\n如果两个进程同时想要进入临界区，则它们都将无法进入临界区\n**违背了”空闲让进“和”有限等待“原则**，会让进程产生饥饿现象\n\n#### Peterson算法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104231828.png)\n最后一个设置turn的进程会失去优先权\n\n\n### 进程互斥的硬件实现方式\n#### 中断屏蔽方法\n利用“开/关中断指令”实现，即某进程开始访问临界区到结束访问为止都不允许被中断![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102139.png)\n#### TestAndSet指令\n简称TS指令或TSL指令\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102430.png)\n\n### 锁\n#### 互斥锁（mutex lock）\n一个进程在进入临界区时获得锁，在退出临界区时释放锁![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105103037.png)\n函数acquire()获得锁，函数release()释放锁\n每个互斥锁有一个布尔变量available，表示锁是否可用\n需要连续循环忙等的互斥锁，都可称为自旋锁（spin lock），如TSL指令、swap指令、单标志法","slug":"Notes/课程/大三（上）/操作系统/Threads线程","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr120000hvw8cdha62zj3","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>进程是最小资源分配单位<br>线程是最小执行单元</p>\n<p>多线程服务框架<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140512.png\" alt=\"image.png\"></p>\n<p>单线程与多线程进程：<br>线程共享进程中的数据，进程为每个线程创建栈和寄存器<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140611.png\" alt=\"image.png\"></p>\n<p>Concurrency并发：在单核系统上<br>Parallelism并行：在多核系统上</p>\n<p>操作系统会为每个内核级线程建立相应的TCB（线程控制块），通过TCB对线程进行管理<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104103658.png\" alt=\"image.png\"></p>\n<p>可将多个TCB组织成一张线程表</p>\n<h3 id=\"Race-Conditions竞争条件\"><a href=\"#Race-Conditions竞争条件\" class=\"headerlink\" title=\"Race Conditions竞争条件\"></a>Race Conditions竞争条件</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009143100.png\" alt=\"image.png\"><br>线程分为send和receive</p>\n<ul>\n<li>send：当buffer中有空间，可以写入</li>\n<li>receive：如果buffer中有message，返回message，创建线程<blockquote>\n<p>producer-consumer problem：<br>producer需要把message添加到buffer<br>consumer需要读取buffer中的message来创建线程</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144647.png\" alt=\"image.png\"></p>\n<p>当有多个sender时：<br>A和B会产生竞争，导致in的值不正确<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144932.png\" alt=\"image.png\"></p>\n<p>解决方法：acquire和release<br>当一个线程进行时，需要先执行acquire申请一个lock，在执行期间持有，结束后释放<br>当一个线程持有lock时，其他线程acquire相同的lock会失败<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082019.png\" alt=\"image.png\"></p>\n<p>在两个线程共用一个lock时，也会出现竞争条件<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016080611.png\" alt=\"image.png\"></p>\n<p>解决方法：Test and Set Lock（TSL）把参数变为真值，返回它的旧值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016081529.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082444.png\" alt=\"image.png\"></p>\n<p>Compare-and-swap lock<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083314.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083325.png\" alt=\"image.png\"><br>当线程A进行比较时，返回true，lck被置为true，线程A得到lock，此时线程B进行比较，返回false，进入循环，直到A进行release</p>\n<h3 id=\"进程同步\"><a href=\"#进程同步\" class=\"headerlink\" title=\"进程同步\"></a>进程同步</h3><p>进程具有异步性的特征，各并发执行的进程以各自独立的、不可预知的速度向前推进</p>\n<h3 id=\"进程互斥\"><a href=\"#进程互斥\" class=\"headerlink\" title=\"进程互斥\"></a>进程互斥</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104223505.png\" alt=\"image.png\"><br>一个时间段内只允许一个进程使用的资源称为临界资源。对临界资源的访问，必须<strong>互斥</strong>地进行。<br>进程互斥指当一个进程访问某临界资源时，另一个想要访问该资源的进程必须等待。当前访问临界资源的进程访问结束，释放该资源后，另一个进程才能去访问临界资源。</p>\n<p>对临界资源的互斥访问，分为四部分：</p>\n<ul>\n<li>进入区：检查是否可进入临界区，可以则上锁</li>\n<li>临界区：访问临界资源</li>\n<li>退出区：解锁</li>\n<li>剩余区：做其他处理</li>\n</ul>\n<p>进程互斥的原则：</p>\n<ol>\n<li>空闲让进</li>\n<li>忙则等待</li>\n<li>有限等待</li>\n<li>让权等待</li>\n</ol>\n<h3 id=\"进程互斥的软件实现方式\"><a href=\"#进程互斥的软件实现方式\" class=\"headerlink\" title=\"进程互斥的软件实现方式\"></a>进程互斥的软件实现方式</h3><h4 id=\"单标志法\"><a href=\"#单标志法\" class=\"headerlink\" title=\"单标志法\"></a>单标志法</h4><p>一个进程在访问完临界区后会把使用临界区的权限转交给另一个进程。也就是说<strong>每个进程进入临界区的权限只能被另一个进程赋予</strong></p>\n<p>举例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104224824.png\" alt=\"image.png\"></p>\n<p>如果此时允许P0进入临界区，但P0一直不访问临界区，那么虽然此时临界区空闲，但是并不允许P1访问。<br>因此单标志法<strong>违背”空闲让进“原则</strong></p>\n<h4 id=\"双标志先检查法\"><a href=\"#双标志先检查法\" class=\"headerlink\" title=\"双标志先检查法\"></a>双标志先检查法</h4><p>设置一个布尔型数组flag[]，数组中各个元素用来标记各进程想进入临界区的意愿<br>先检查后上锁<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230557.png\" alt=\"image.png\"><br>如果两个进程同时想要进入临界区，就会出现冲突。<br>因此双标志先检查法的问题是<strong>违背”忙则等待“原则</strong></p>\n<h4 id=\"双标志后检查法\"><a href=\"#双标志后检查法\" class=\"headerlink\" title=\"双标志后检查法\"></a>双标志后检查法</h4><p>先上锁后检查<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230953.png\" alt=\"image.png\"><br>如果两个进程同时想要进入临界区，则它们都将无法进入临界区<br><strong>违背了”空闲让进“和”有限等待“原则</strong>，会让进程产生饥饿现象</p>\n<h4 id=\"Peterson算法\"><a href=\"#Peterson算法\" class=\"headerlink\" title=\"Peterson算法\"></a>Peterson算法</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104231828.png\" alt=\"image.png\"><br>最后一个设置turn的进程会失去优先权</p>\n<h3 id=\"进程互斥的硬件实现方式\"><a href=\"#进程互斥的硬件实现方式\" class=\"headerlink\" title=\"进程互斥的硬件实现方式\"></a>进程互斥的硬件实现方式</h3><h4 id=\"中断屏蔽方法\"><a href=\"#中断屏蔽方法\" class=\"headerlink\" title=\"中断屏蔽方法\"></a>中断屏蔽方法</h4><p>利用“开&#x2F;关中断指令”实现，即某进程开始访问临界区到结束访问为止都不允许被中断<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102139.png\" alt=\"image.png\"></p>\n<h4 id=\"TestAndSet指令\"><a href=\"#TestAndSet指令\" class=\"headerlink\" title=\"TestAndSet指令\"></a>TestAndSet指令</h4><p>简称TS指令或TSL指令<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102430.png\" alt=\"image.png\"></p>\n<h3 id=\"锁\"><a href=\"#锁\" class=\"headerlink\" title=\"锁\"></a>锁</h3><h4 id=\"互斥锁（mutex-lock）\"><a href=\"#互斥锁（mutex-lock）\" class=\"headerlink\" title=\"互斥锁（mutex lock）\"></a>互斥锁（mutex lock）</h4><p>一个进程在进入临界区时获得锁，在退出临界区时释放锁<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105103037.png\" alt=\"image.png\"><br>函数acquire()获得锁，函数release()释放锁<br>每个互斥锁有一个布尔变量available，表示锁是否可用<br>需要连续循环忙等的互斥锁，都可称为自旋锁（spin lock），如TSL指令、swap指令、单标志法</p>\n","site":{"data":{}},"excerpt":"","more":"<p>进程是最小资源分配单位<br>线程是最小执行单元</p>\n<p>多线程服务框架<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140512.png\" alt=\"image.png\"></p>\n<p>单线程与多线程进程：<br>线程共享进程中的数据，进程为每个线程创建栈和寄存器<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009140611.png\" alt=\"image.png\"></p>\n<p>Concurrency并发：在单核系统上<br>Parallelism并行：在多核系统上</p>\n<p>操作系统会为每个内核级线程建立相应的TCB（线程控制块），通过TCB对线程进行管理<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104103658.png\" alt=\"image.png\"></p>\n<p>可将多个TCB组织成一张线程表</p>\n<h3 id=\"Race-Conditions竞争条件\"><a href=\"#Race-Conditions竞争条件\" class=\"headerlink\" title=\"Race Conditions竞争条件\"></a>Race Conditions竞争条件</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009143100.png\" alt=\"image.png\"><br>线程分为send和receive</p>\n<ul>\n<li>send：当buffer中有空间，可以写入</li>\n<li>receive：如果buffer中有message，返回message，创建线程<blockquote>\n<p>producer-consumer problem：<br>producer需要把message添加到buffer<br>consumer需要读取buffer中的message来创建线程</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144647.png\" alt=\"image.png\"></p>\n<p>当有多个sender时：<br>A和B会产生竞争，导致in的值不正确<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009144932.png\" alt=\"image.png\"></p>\n<p>解决方法：acquire和release<br>当一个线程进行时，需要先执行acquire申请一个lock，在执行期间持有，结束后释放<br>当一个线程持有lock时，其他线程acquire相同的lock会失败<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082019.png\" alt=\"image.png\"></p>\n<p>在两个线程共用一个lock时，也会出现竞争条件<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016080611.png\" alt=\"image.png\"></p>\n<p>解决方法：Test and Set Lock（TSL）把参数变为真值，返回它的旧值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016081529.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016082444.png\" alt=\"image.png\"></p>\n<p>Compare-and-swap lock<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083314.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016083325.png\" alt=\"image.png\"><br>当线程A进行比较时，返回true，lck被置为true，线程A得到lock，此时线程B进行比较，返回false，进入循环，直到A进行release</p>\n<h3 id=\"进程同步\"><a href=\"#进程同步\" class=\"headerlink\" title=\"进程同步\"></a>进程同步</h3><p>进程具有异步性的特征，各并发执行的进程以各自独立的、不可预知的速度向前推进</p>\n<h3 id=\"进程互斥\"><a href=\"#进程互斥\" class=\"headerlink\" title=\"进程互斥\"></a>进程互斥</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104223505.png\" alt=\"image.png\"><br>一个时间段内只允许一个进程使用的资源称为临界资源。对临界资源的访问，必须<strong>互斥</strong>地进行。<br>进程互斥指当一个进程访问某临界资源时，另一个想要访问该资源的进程必须等待。当前访问临界资源的进程访问结束，释放该资源后，另一个进程才能去访问临界资源。</p>\n<p>对临界资源的互斥访问，分为四部分：</p>\n<ul>\n<li>进入区：检查是否可进入临界区，可以则上锁</li>\n<li>临界区：访问临界资源</li>\n<li>退出区：解锁</li>\n<li>剩余区：做其他处理</li>\n</ul>\n<p>进程互斥的原则：</p>\n<ol>\n<li>空闲让进</li>\n<li>忙则等待</li>\n<li>有限等待</li>\n<li>让权等待</li>\n</ol>\n<h3 id=\"进程互斥的软件实现方式\"><a href=\"#进程互斥的软件实现方式\" class=\"headerlink\" title=\"进程互斥的软件实现方式\"></a>进程互斥的软件实现方式</h3><h4 id=\"单标志法\"><a href=\"#单标志法\" class=\"headerlink\" title=\"单标志法\"></a>单标志法</h4><p>一个进程在访问完临界区后会把使用临界区的权限转交给另一个进程。也就是说<strong>每个进程进入临界区的权限只能被另一个进程赋予</strong></p>\n<p>举例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104224824.png\" alt=\"image.png\"></p>\n<p>如果此时允许P0进入临界区，但P0一直不访问临界区，那么虽然此时临界区空闲，但是并不允许P1访问。<br>因此单标志法<strong>违背”空闲让进“原则</strong></p>\n<h4 id=\"双标志先检查法\"><a href=\"#双标志先检查法\" class=\"headerlink\" title=\"双标志先检查法\"></a>双标志先检查法</h4><p>设置一个布尔型数组flag[]，数组中各个元素用来标记各进程想进入临界区的意愿<br>先检查后上锁<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230557.png\" alt=\"image.png\"><br>如果两个进程同时想要进入临界区，就会出现冲突。<br>因此双标志先检查法的问题是<strong>违背”忙则等待“原则</strong></p>\n<h4 id=\"双标志后检查法\"><a href=\"#双标志后检查法\" class=\"headerlink\" title=\"双标志后检查法\"></a>双标志后检查法</h4><p>先上锁后检查<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104230953.png\" alt=\"image.png\"><br>如果两个进程同时想要进入临界区，则它们都将无法进入临界区<br><strong>违背了”空闲让进“和”有限等待“原则</strong>，会让进程产生饥饿现象</p>\n<h4 id=\"Peterson算法\"><a href=\"#Peterson算法\" class=\"headerlink\" title=\"Peterson算法\"></a>Peterson算法</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231104231828.png\" alt=\"image.png\"><br>最后一个设置turn的进程会失去优先权</p>\n<h3 id=\"进程互斥的硬件实现方式\"><a href=\"#进程互斥的硬件实现方式\" class=\"headerlink\" title=\"进程互斥的硬件实现方式\"></a>进程互斥的硬件实现方式</h3><h4 id=\"中断屏蔽方法\"><a href=\"#中断屏蔽方法\" class=\"headerlink\" title=\"中断屏蔽方法\"></a>中断屏蔽方法</h4><p>利用“开&#x2F;关中断指令”实现，即某进程开始访问临界区到结束访问为止都不允许被中断<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102139.png\" alt=\"image.png\"></p>\n<h4 id=\"TestAndSet指令\"><a href=\"#TestAndSet指令\" class=\"headerlink\" title=\"TestAndSet指令\"></a>TestAndSet指令</h4><p>简称TS指令或TSL指令<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105102430.png\" alt=\"image.png\"></p>\n<h3 id=\"锁\"><a href=\"#锁\" class=\"headerlink\" title=\"锁\"></a>锁</h3><h4 id=\"互斥锁（mutex-lock）\"><a href=\"#互斥锁（mutex-lock）\" class=\"headerlink\" title=\"互斥锁（mutex lock）\"></a>互斥锁（mutex lock）</h4><p>一个进程在进入临界区时获得锁，在退出临界区时释放锁<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231105103037.png\" alt=\"image.png\"><br>函数acquire()获得锁，函数release()释放锁<br>每个互斥锁有一个布尔变量available，表示锁是否可用<br>需要连续循环忙等的互斥锁，都可称为自旋锁（spin lock），如TSL指令、swap指令、单标志法</p>\n"},{"title":"SQL中级","date":"2023-10-09T03:36:13.314Z","_content":"![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016171429.png)\n\n### 4.1 Joined Expressions\n`join`是笛卡尔积，它要求两个表中的元组匹配，用于`from`子句中的子查询表达式。\n使用 `join ... on`子句可以指定任意的连接条件。`on`条件允许在参与连接的关系上设置通用的谓词。该谓词的写法与where子句谓词类似。\n\n#### Natural join operations自然连接\n将表中具有相同名称的列进行匹配\n\nNatural join特征：\n- 关联的表具有一对或多对同名的列\n- 连接时候不需要使用on或者using关键字\n\n在自然连接中，会产生数据的丢失\n#### Outer join外连接\n- Left Outer Join左外连接：把左边表的数据全部取出来，而右边表的数据有相等的，显示出来，如果没有，显示NULL\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172709.png)\n\n- Right Outer Join右外连接：把右边表的数据全部取出来，而左边表的数据有相等的，显示出来，如果没有，显示NULL\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172943.png)\n\n\n- Full Outer Join全外连接：兼顾左外连接和右外连接\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009173144.png)\n\n\n#### Inner join内连接\n为了把常规连接和外连接区分开来，在SQL中把常规连接称作内连接。关键字inner是可选的，当join子句中没有使用outer前缀时，缺省的连接是内连接。内连接以**笛卡尔积**的形式表现出来。\n\n自然连接只显示一列相同名称的列，而内连接显示两列\n>自然连接和内连接的区别：\n>[自然连接和内连接的区别|极客教程](https://geek-docs.com/sql/sql-ask-answer/the-difference-between-natural-join-and-inner-join.html)\n\n### 4.2 Views视图\n\n在 SQL 中，视图是基于 SQL 语句的结果集的可视化的表。\n视图包含行和列，就像一个真实的表。视图中的字段就是来自一个或多个数据库中的真实的表中的字段。\n视图是虚关系，在数据库中不存在，根据用户需求临时生成，数据库只储存视图定义。\n\n#### Create view创建视图\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009174818.png)\n其中\\<query expression>为SQL表达式，v是视图的名称\n\n#### Views Defined Using Other Views\nv1可以直接依赖（depend directly on）v2，即 v2 ->  v1\nv1也可以依赖（depend on）v2，即 v2 -> v3 -> v2\n如果视图依赖于自身，则称为递归关系\n\n#### Materialized Views实例化视图\n创建一个物理表，其中包含查询定义视图的结果中的所有元组\n如果更新查询中使用的关系，则实例化视图结果将过期\n### 4.3 Transactions事务\n一个transaction包括一系列的查询和更新\n特性：原子性、一致性、隔离性、耐用性\n事务必须以下列语句之一结束：\n- Commit work\n- Rollback work\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016100923.png)\n\n### 4.4 Integrity Constraints完整性约束\n通过对数据的约束防止数据的意外损坏\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016101652.png)\n- E.g. 1   the salary of manager should not be lower than $1000 in Employee\n- E.g. 2   table T (x, y, z ), z =x+y, z is a derived attributes from x and y.\n- E.g. 3   the student# for table student should not be null\n- E.g. 4   the age  of students should only be added\n- E.g. 5   when  employee tuples is modified,  new.sal > old.sal + 0.5\\*age\n- E.g. 6   statistical  constraints\n\n#### Constraints on Single Relation\n完整性约束包括：\n- primary key\n- not null\n- unique\n- check(P), where P is a predicate（谓词）\n\n#### Referential Integrity参照完整性\n确保在一个relation中attribute的值也出现在另一个relation的attribute中\n参照完整性约束也称为子集依赖关系（subset dependency）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016104636.png)\ntable $r_2$的外键$\\alpha$参考table $r_1$的主键K1\n\nCascading Actions级联操作：对$r_1$的主键K1的删除/更新操作将导致$r_2$的外键$\\alpha$的删除/更新\n\n### 4.5 SQL Data Types and Schemas\n内置数据类型：\n- date: 年月日\n- time: 时分秒\n- timestamp: date+time\n- interval: 时间间隔\n\n#### Type Conversion类型转换\n`cast` \\<e> `as` \\<t>\n将字符串e转换为类型t\n\n`extract` value d `from` day or time\n对于日期或时间d，提取其时间或日期字段\n\n#### Formatting Functions格式化函数\n数据可能需要以不同的类型显示：\n- 以特定位数显示数字\n- 以特定格式显示数据\n\nData Type Transition: `CAST` 和 `CONVERT`\n\n#### Default Values\n在创建table时设置属性的默认值，可以在插入tuple时不设置该属性的值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164651.png)\n\n#### Large-Object Types\nblob：二进制大对象\nclob：字符大对象\nXML数据类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164842.png)\n\n#### User-defined Types/Domains\n子句`create type`可以用于创建用户自定义类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165050.png)\n\n`create domain`可以创建用户定义域类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165114.png)\n\n#### Schemas, Catalogs, and Environments\n现代数据库系统的三级层次结构\n- cactalogs（全文目录）\n- schemas（架构）\n- SQL objects\n\n#### Check Conditions and Assertions检查和评估\n`check`和`assertion`可以定义复杂的完整性约束\nassertion是一个谓词，表示我们希望数据库始终满足的条件\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165326.png)\n\n### 4.6 Index Definition in SQL\n许多查询只需参考表中的一小部分记录，读取每条记录以查找具有特定值的记录效率低下\n建立索引，在查找时数据库可以只遍历索引而非遍历所有的值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016111831.png)\n\n\n### 4.7 Authorization\n对数据库各部分的授权形式包括：\n- Read：允许读取数据，但不允许修改数据\n- Insert：允许插入新数据，但不允许修改现有数据\n- Update：允许修改，但不允许删除数据\n- Delete：允许删除数据\n对schema的授权：\n- resources：允许创建新关系表\n- alteration：允许在关系表中添加或删除属性\n- drop：允许删除关系表\n- index：允许创建索引\n\n#### Authorization Specification授权规范\n`grant`语句用于授予权限\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170537.png)\n\n授予视图特权并不意味着授予对基础关系的任何特权\n特权的授予者必须已经拥有指定项的权限（或者是数据库管理员）\n\n#### Privileges特权\n- select：允许对关系表进行读取访问，或使用视图进行查询的能力\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170318.png)\n\n- insert：插入元组的能力\n- update：使用 SQL 更新语句进行更新的能力\n- delete：删除元组的功能\n- all privileges：所有可被允许的特权\n- references：创建外键的权限\n\n\n#### Revoking Authorization撤销授权\n`revoke`用于撤销授权\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170604.png)\n\n#### Roles\n`create role` \\<role name>\n可以向角色授予权限\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170845.png)\n\n可以向用户以及其他角色授予角色\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170853.png)\n\n\n\n\n\n","source":"_posts/Notes/课程/大三（上）/数据库/SQL中级.md","raw":"---\ntitle: SQL中级\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 数据库\ntags:\n  - SQL\n  - 数据库\ndate:\n---\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016171429.png)\n\n### 4.1 Joined Expressions\n`join`是笛卡尔积，它要求两个表中的元组匹配，用于`from`子句中的子查询表达式。\n使用 `join ... on`子句可以指定任意的连接条件。`on`条件允许在参与连接的关系上设置通用的谓词。该谓词的写法与where子句谓词类似。\n\n#### Natural join operations自然连接\n将表中具有相同名称的列进行匹配\n\nNatural join特征：\n- 关联的表具有一对或多对同名的列\n- 连接时候不需要使用on或者using关键字\n\n在自然连接中，会产生数据的丢失\n#### Outer join外连接\n- Left Outer Join左外连接：把左边表的数据全部取出来，而右边表的数据有相等的，显示出来，如果没有，显示NULL\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172709.png)\n\n- Right Outer Join右外连接：把右边表的数据全部取出来，而左边表的数据有相等的，显示出来，如果没有，显示NULL\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172943.png)\n\n\n- Full Outer Join全外连接：兼顾左外连接和右外连接\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009173144.png)\n\n\n#### Inner join内连接\n为了把常规连接和外连接区分开来，在SQL中把常规连接称作内连接。关键字inner是可选的，当join子句中没有使用outer前缀时，缺省的连接是内连接。内连接以**笛卡尔积**的形式表现出来。\n\n自然连接只显示一列相同名称的列，而内连接显示两列\n>自然连接和内连接的区别：\n>[自然连接和内连接的区别|极客教程](https://geek-docs.com/sql/sql-ask-answer/the-difference-between-natural-join-and-inner-join.html)\n\n### 4.2 Views视图\n\n在 SQL 中，视图是基于 SQL 语句的结果集的可视化的表。\n视图包含行和列，就像一个真实的表。视图中的字段就是来自一个或多个数据库中的真实的表中的字段。\n视图是虚关系，在数据库中不存在，根据用户需求临时生成，数据库只储存视图定义。\n\n#### Create view创建视图\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009174818.png)\n其中\\<query expression>为SQL表达式，v是视图的名称\n\n#### Views Defined Using Other Views\nv1可以直接依赖（depend directly on）v2，即 v2 ->  v1\nv1也可以依赖（depend on）v2，即 v2 -> v3 -> v2\n如果视图依赖于自身，则称为递归关系\n\n#### Materialized Views实例化视图\n创建一个物理表，其中包含查询定义视图的结果中的所有元组\n如果更新查询中使用的关系，则实例化视图结果将过期\n### 4.3 Transactions事务\n一个transaction包括一系列的查询和更新\n特性：原子性、一致性、隔离性、耐用性\n事务必须以下列语句之一结束：\n- Commit work\n- Rollback work\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016100923.png)\n\n### 4.4 Integrity Constraints完整性约束\n通过对数据的约束防止数据的意外损坏\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016101652.png)\n- E.g. 1   the salary of manager should not be lower than $1000 in Employee\n- E.g. 2   table T (x, y, z ), z =x+y, z is a derived attributes from x and y.\n- E.g. 3   the student# for table student should not be null\n- E.g. 4   the age  of students should only be added\n- E.g. 5   when  employee tuples is modified,  new.sal > old.sal + 0.5\\*age\n- E.g. 6   statistical  constraints\n\n#### Constraints on Single Relation\n完整性约束包括：\n- primary key\n- not null\n- unique\n- check(P), where P is a predicate（谓词）\n\n#### Referential Integrity参照完整性\n确保在一个relation中attribute的值也出现在另一个relation的attribute中\n参照完整性约束也称为子集依赖关系（subset dependency）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016104636.png)\ntable $r_2$的外键$\\alpha$参考table $r_1$的主键K1\n\nCascading Actions级联操作：对$r_1$的主键K1的删除/更新操作将导致$r_2$的外键$\\alpha$的删除/更新\n\n### 4.5 SQL Data Types and Schemas\n内置数据类型：\n- date: 年月日\n- time: 时分秒\n- timestamp: date+time\n- interval: 时间间隔\n\n#### Type Conversion类型转换\n`cast` \\<e> `as` \\<t>\n将字符串e转换为类型t\n\n`extract` value d `from` day or time\n对于日期或时间d，提取其时间或日期字段\n\n#### Formatting Functions格式化函数\n数据可能需要以不同的类型显示：\n- 以特定位数显示数字\n- 以特定格式显示数据\n\nData Type Transition: `CAST` 和 `CONVERT`\n\n#### Default Values\n在创建table时设置属性的默认值，可以在插入tuple时不设置该属性的值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164651.png)\n\n#### Large-Object Types\nblob：二进制大对象\nclob：字符大对象\nXML数据类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164842.png)\n\n#### User-defined Types/Domains\n子句`create type`可以用于创建用户自定义类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165050.png)\n\n`create domain`可以创建用户定义域类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165114.png)\n\n#### Schemas, Catalogs, and Environments\n现代数据库系统的三级层次结构\n- cactalogs（全文目录）\n- schemas（架构）\n- SQL objects\n\n#### Check Conditions and Assertions检查和评估\n`check`和`assertion`可以定义复杂的完整性约束\nassertion是一个谓词，表示我们希望数据库始终满足的条件\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165326.png)\n\n### 4.6 Index Definition in SQL\n许多查询只需参考表中的一小部分记录，读取每条记录以查找具有特定值的记录效率低下\n建立索引，在查找时数据库可以只遍历索引而非遍历所有的值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016111831.png)\n\n\n### 4.7 Authorization\n对数据库各部分的授权形式包括：\n- Read：允许读取数据，但不允许修改数据\n- Insert：允许插入新数据，但不允许修改现有数据\n- Update：允许修改，但不允许删除数据\n- Delete：允许删除数据\n对schema的授权：\n- resources：允许创建新关系表\n- alteration：允许在关系表中添加或删除属性\n- drop：允许删除关系表\n- index：允许创建索引\n\n#### Authorization Specification授权规范\n`grant`语句用于授予权限\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170537.png)\n\n授予视图特权并不意味着授予对基础关系的任何特权\n特权的授予者必须已经拥有指定项的权限（或者是数据库管理员）\n\n#### Privileges特权\n- select：允许对关系表进行读取访问，或使用视图进行查询的能力\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170318.png)\n\n- insert：插入元组的能力\n- update：使用 SQL 更新语句进行更新的能力\n- delete：删除元组的功能\n- all privileges：所有可被允许的特权\n- references：创建外键的权限\n\n\n#### Revoking Authorization撤销授权\n`revoke`用于撤销授权\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170604.png)\n\n#### Roles\n`create role` \\<role name>\n可以向角色授予权限\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170845.png)\n\n可以向用户以及其他角色授予角色\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170853.png)\n\n\n\n\n\n","slug":"Notes/课程/大三（上）/数据库/SQL中级","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr121000jvw8c614ueu3h","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016171429.png\" alt=\"image.png\"></p>\n<h3 id=\"4-1-Joined-Expressions\"><a href=\"#4-1-Joined-Expressions\" class=\"headerlink\" title=\"4.1 Joined Expressions\"></a>4.1 Joined Expressions</h3><p><code>join</code>是笛卡尔积，它要求两个表中的元组匹配，用于<code>from</code>子句中的子查询表达式。<br>使用 <code>join ... on</code>子句可以指定任意的连接条件。<code>on</code>条件允许在参与连接的关系上设置通用的谓词。该谓词的写法与where子句谓词类似。</p>\n<h4 id=\"Natural-join-operations自然连接\"><a href=\"#Natural-join-operations自然连接\" class=\"headerlink\" title=\"Natural join operations自然连接\"></a>Natural join operations自然连接</h4><p>将表中具有相同名称的列进行匹配</p>\n<p>Natural join特征：</p>\n<ul>\n<li>关联的表具有一对或多对同名的列</li>\n<li>连接时候不需要使用on或者using关键字</li>\n</ul>\n<p>在自然连接中，会产生数据的丢失</p>\n<h4 id=\"Outer-join外连接\"><a href=\"#Outer-join外连接\" class=\"headerlink\" title=\"Outer join外连接\"></a>Outer join外连接</h4><ul>\n<li><p>Left Outer Join左外连接：把左边表的数据全部取出来，而右边表的数据有相等的，显示出来，如果没有，显示NULL<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172709.png\" alt=\"image.png\"></p>\n</li>\n<li><p>Right Outer Join右外连接：把右边表的数据全部取出来，而左边表的数据有相等的，显示出来，如果没有，显示NULL<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172943.png\" alt=\"image.png\"></p>\n</li>\n<li><p>Full Outer Join全外连接：兼顾左外连接和右外连接<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009173144.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<h4 id=\"Inner-join内连接\"><a href=\"#Inner-join内连接\" class=\"headerlink\" title=\"Inner join内连接\"></a>Inner join内连接</h4><p>为了把常规连接和外连接区分开来，在SQL中把常规连接称作内连接。关键字inner是可选的，当join子句中没有使用outer前缀时，缺省的连接是内连接。内连接以<strong>笛卡尔积</strong>的形式表现出来。</p>\n<p>自然连接只显示一列相同名称的列，而内连接显示两列</p>\n<blockquote>\n<p>自然连接和内连接的区别：<br><a href=\"https://geek-docs.com/sql/sql-ask-answer/the-difference-between-natural-join-and-inner-join.html\">自然连接和内连接的区别|极客教程</a></p>\n</blockquote>\n<h3 id=\"4-2-Views视图\"><a href=\"#4-2-Views视图\" class=\"headerlink\" title=\"4.2 Views视图\"></a>4.2 Views视图</h3><p>在 SQL 中，视图是基于 SQL 语句的结果集的可视化的表。<br>视图包含行和列，就像一个真实的表。视图中的字段就是来自一个或多个数据库中的真实的表中的字段。<br>视图是虚关系，在数据库中不存在，根据用户需求临时生成，数据库只储存视图定义。</p>\n<h4 id=\"Create-view创建视图\"><a href=\"#Create-view创建视图\" class=\"headerlink\" title=\"Create view创建视图\"></a>Create view创建视图</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009174818.png\" alt=\"image.png\"><br>其中&lt;query expression&gt;为SQL表达式，v是视图的名称</p>\n<h4 id=\"Views-Defined-Using-Other-Views\"><a href=\"#Views-Defined-Using-Other-Views\" class=\"headerlink\" title=\"Views Defined Using Other Views\"></a>Views Defined Using Other Views</h4><p>v1可以直接依赖（depend directly on）v2，即 v2 -&gt;  v1<br>v1也可以依赖（depend on）v2，即 v2 -&gt; v3 -&gt; v2<br>如果视图依赖于自身，则称为递归关系</p>\n<h4 id=\"Materialized-Views实例化视图\"><a href=\"#Materialized-Views实例化视图\" class=\"headerlink\" title=\"Materialized Views实例化视图\"></a>Materialized Views实例化视图</h4><p>创建一个物理表，其中包含查询定义视图的结果中的所有元组<br>如果更新查询中使用的关系，则实例化视图结果将过期</p>\n<h3 id=\"4-3-Transactions事务\"><a href=\"#4-3-Transactions事务\" class=\"headerlink\" title=\"4.3 Transactions事务\"></a>4.3 Transactions事务</h3><p>一个transaction包括一系列的查询和更新<br>特性：原子性、一致性、隔离性、耐用性<br>事务必须以下列语句之一结束：</p>\n<ul>\n<li>Commit work</li>\n<li>Rollback work<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016100923.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"4-4-Integrity-Constraints完整性约束\"><a href=\"#4-4-Integrity-Constraints完整性约束\" class=\"headerlink\" title=\"4.4 Integrity Constraints完整性约束\"></a>4.4 Integrity Constraints完整性约束</h3><p>通过对数据的约束防止数据的意外损坏<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016101652.png\" alt=\"image.png\"></p>\n<ul>\n<li>E.g. 1   the salary of manager should not be lower than $1000 in Employee</li>\n<li>E.g. 2   table T (x, y, z ), z &#x3D;x+y, z is a derived attributes from x and y.</li>\n<li>E.g. 3   the student# for table student should not be null</li>\n<li>E.g. 4   the age  of students should only be added</li>\n<li>E.g. 5   when  employee tuples is modified,  new.sal &gt; old.sal + 0.5*age</li>\n<li>E.g. 6   statistical  constraints</li>\n</ul>\n<h4 id=\"Constraints-on-Single-Relation\"><a href=\"#Constraints-on-Single-Relation\" class=\"headerlink\" title=\"Constraints on Single Relation\"></a>Constraints on Single Relation</h4><p>完整性约束包括：</p>\n<ul>\n<li>primary key</li>\n<li>not null</li>\n<li>unique</li>\n<li>check(P), where P is a predicate（谓词）</li>\n</ul>\n<h4 id=\"Referential-Integrity参照完整性\"><a href=\"#Referential-Integrity参照完整性\" class=\"headerlink\" title=\"Referential Integrity参照完整性\"></a>Referential Integrity参照完整性</h4><p>确保在一个relation中attribute的值也出现在另一个relation的attribute中<br>参照完整性约束也称为子集依赖关系（subset dependency）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016104636.png\" alt=\"image.png\"><br>table $r_2$的外键$\\alpha$参考table $r_1$的主键K1</p>\n<p>Cascading Actions级联操作：对$r_1$的主键K1的删除&#x2F;更新操作将导致$r_2$的外键$\\alpha$的删除&#x2F;更新</p>\n<h3 id=\"4-5-SQL-Data-Types-and-Schemas\"><a href=\"#4-5-SQL-Data-Types-and-Schemas\" class=\"headerlink\" title=\"4.5 SQL Data Types and Schemas\"></a>4.5 SQL Data Types and Schemas</h3><p>内置数据类型：</p>\n<ul>\n<li>date: 年月日</li>\n<li>time: 时分秒</li>\n<li>timestamp: date+time</li>\n<li>interval: 时间间隔</li>\n</ul>\n<h4 id=\"Type-Conversion类型转换\"><a href=\"#Type-Conversion类型转换\" class=\"headerlink\" title=\"Type Conversion类型转换\"></a>Type Conversion类型转换</h4><p><code>cast</code> &lt;e&gt; <code>as</code> &lt;t&gt;<br>将字符串e转换为类型t</p>\n<p><code>extract</code> value d <code>from</code> day or time<br>对于日期或时间d，提取其时间或日期字段</p>\n<h4 id=\"Formatting-Functions格式化函数\"><a href=\"#Formatting-Functions格式化函数\" class=\"headerlink\" title=\"Formatting Functions格式化函数\"></a>Formatting Functions格式化函数</h4><p>数据可能需要以不同的类型显示：</p>\n<ul>\n<li>以特定位数显示数字</li>\n<li>以特定格式显示数据</li>\n</ul>\n<p>Data Type Transition: <code>CAST</code> 和 <code>CONVERT</code></p>\n<h4 id=\"Default-Values\"><a href=\"#Default-Values\" class=\"headerlink\" title=\"Default Values\"></a>Default Values</h4><p>在创建table时设置属性的默认值，可以在插入tuple时不设置该属性的值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164651.png\" alt=\"image.png\"></p>\n<h4 id=\"Large-Object-Types\"><a href=\"#Large-Object-Types\" class=\"headerlink\" title=\"Large-Object Types\"></a>Large-Object Types</h4><p>blob：二进制大对象<br>clob：字符大对象<br>XML数据类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164842.png\" alt=\"image.png\"></p>\n<h4 id=\"User-defined-Types-Domains\"><a href=\"#User-defined-Types-Domains\" class=\"headerlink\" title=\"User-defined Types&#x2F;Domains\"></a>User-defined Types&#x2F;Domains</h4><p>子句<code>create type</code>可以用于创建用户自定义类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165050.png\" alt=\"image.png\"></p>\n<p><code>create domain</code>可以创建用户定义域类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165114.png\" alt=\"image.png\"></p>\n<h4 id=\"Schemas-Catalogs-and-Environments\"><a href=\"#Schemas-Catalogs-and-Environments\" class=\"headerlink\" title=\"Schemas, Catalogs, and Environments\"></a>Schemas, Catalogs, and Environments</h4><p>现代数据库系统的三级层次结构</p>\n<ul>\n<li>cactalogs（全文目录）</li>\n<li>schemas（架构）</li>\n<li>SQL objects</li>\n</ul>\n<h4 id=\"Check-Conditions-and-Assertions检查和评估\"><a href=\"#Check-Conditions-and-Assertions检查和评估\" class=\"headerlink\" title=\"Check Conditions and Assertions检查和评估\"></a>Check Conditions and Assertions检查和评估</h4><p><code>check</code>和<code>assertion</code>可以定义复杂的完整性约束<br>assertion是一个谓词，表示我们希望数据库始终满足的条件<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165326.png\" alt=\"image.png\"></p>\n<h3 id=\"4-6-Index-Definition-in-SQL\"><a href=\"#4-6-Index-Definition-in-SQL\" class=\"headerlink\" title=\"4.6 Index Definition in SQL\"></a>4.6 Index Definition in SQL</h3><p>许多查询只需参考表中的一小部分记录，读取每条记录以查找具有特定值的记录效率低下<br>建立索引，在查找时数据库可以只遍历索引而非遍历所有的值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016111831.png\" alt=\"image.png\"></p>\n<h3 id=\"4-7-Authorization\"><a href=\"#4-7-Authorization\" class=\"headerlink\" title=\"4.7 Authorization\"></a>4.7 Authorization</h3><p>对数据库各部分的授权形式包括：</p>\n<ul>\n<li>Read：允许读取数据，但不允许修改数据</li>\n<li>Insert：允许插入新数据，但不允许修改现有数据</li>\n<li>Update：允许修改，但不允许删除数据</li>\n<li>Delete：允许删除数据<br>对schema的授权：</li>\n<li>resources：允许创建新关系表</li>\n<li>alteration：允许在关系表中添加或删除属性</li>\n<li>drop：允许删除关系表</li>\n<li>index：允许创建索引</li>\n</ul>\n<h4 id=\"Authorization-Specification授权规范\"><a href=\"#Authorization-Specification授权规范\" class=\"headerlink\" title=\"Authorization Specification授权规范\"></a>Authorization Specification授权规范</h4><p><code>grant</code>语句用于授予权限<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170537.png\" alt=\"image.png\"></p>\n<p>授予视图特权并不意味着授予对基础关系的任何特权<br>特权的授予者必须已经拥有指定项的权限（或者是数据库管理员）</p>\n<h4 id=\"Privileges特权\"><a href=\"#Privileges特权\" class=\"headerlink\" title=\"Privileges特权\"></a>Privileges特权</h4><ul>\n<li><p>select：允许对关系表进行读取访问，或使用视图进行查询的能力<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170318.png\" alt=\"image.png\"></p>\n</li>\n<li><p>insert：插入元组的能力</p>\n</li>\n<li><p>update：使用 SQL 更新语句进行更新的能力</p>\n</li>\n<li><p>delete：删除元组的功能</p>\n</li>\n<li><p>all privileges：所有可被允许的特权</p>\n</li>\n<li><p>references：创建外键的权限</p>\n</li>\n</ul>\n<h4 id=\"Revoking-Authorization撤销授权\"><a href=\"#Revoking-Authorization撤销授权\" class=\"headerlink\" title=\"Revoking Authorization撤销授权\"></a>Revoking Authorization撤销授权</h4><p><code>revoke</code>用于撤销授权<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170604.png\" alt=\"image.png\"></p>\n<h4 id=\"Roles\"><a href=\"#Roles\" class=\"headerlink\" title=\"Roles\"></a>Roles</h4><p><code>create role</code> &lt;role name&gt;<br>可以向角色授予权限<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170845.png\" alt=\"image.png\"></p>\n<p>可以向用户以及其他角色授予角色<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170853.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016171429.png\" alt=\"image.png\"></p>\n<h3 id=\"4-1-Joined-Expressions\"><a href=\"#4-1-Joined-Expressions\" class=\"headerlink\" title=\"4.1 Joined Expressions\"></a>4.1 Joined Expressions</h3><p><code>join</code>是笛卡尔积，它要求两个表中的元组匹配，用于<code>from</code>子句中的子查询表达式。<br>使用 <code>join ... on</code>子句可以指定任意的连接条件。<code>on</code>条件允许在参与连接的关系上设置通用的谓词。该谓词的写法与where子句谓词类似。</p>\n<h4 id=\"Natural-join-operations自然连接\"><a href=\"#Natural-join-operations自然连接\" class=\"headerlink\" title=\"Natural join operations自然连接\"></a>Natural join operations自然连接</h4><p>将表中具有相同名称的列进行匹配</p>\n<p>Natural join特征：</p>\n<ul>\n<li>关联的表具有一对或多对同名的列</li>\n<li>连接时候不需要使用on或者using关键字</li>\n</ul>\n<p>在自然连接中，会产生数据的丢失</p>\n<h4 id=\"Outer-join外连接\"><a href=\"#Outer-join外连接\" class=\"headerlink\" title=\"Outer join外连接\"></a>Outer join外连接</h4><ul>\n<li><p>Left Outer Join左外连接：把左边表的数据全部取出来，而右边表的数据有相等的，显示出来，如果没有，显示NULL<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172709.png\" alt=\"image.png\"></p>\n</li>\n<li><p>Right Outer Join右外连接：把右边表的数据全部取出来，而左边表的数据有相等的，显示出来，如果没有，显示NULL<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009172943.png\" alt=\"image.png\"></p>\n</li>\n<li><p>Full Outer Join全外连接：兼顾左外连接和右外连接<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009173144.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<h4 id=\"Inner-join内连接\"><a href=\"#Inner-join内连接\" class=\"headerlink\" title=\"Inner join内连接\"></a>Inner join内连接</h4><p>为了把常规连接和外连接区分开来，在SQL中把常规连接称作内连接。关键字inner是可选的，当join子句中没有使用outer前缀时，缺省的连接是内连接。内连接以<strong>笛卡尔积</strong>的形式表现出来。</p>\n<p>自然连接只显示一列相同名称的列，而内连接显示两列</p>\n<blockquote>\n<p>自然连接和内连接的区别：<br><a href=\"https://geek-docs.com/sql/sql-ask-answer/the-difference-between-natural-join-and-inner-join.html\">自然连接和内连接的区别|极客教程</a></p>\n</blockquote>\n<h3 id=\"4-2-Views视图\"><a href=\"#4-2-Views视图\" class=\"headerlink\" title=\"4.2 Views视图\"></a>4.2 Views视图</h3><p>在 SQL 中，视图是基于 SQL 语句的结果集的可视化的表。<br>视图包含行和列，就像一个真实的表。视图中的字段就是来自一个或多个数据库中的真实的表中的字段。<br>视图是虚关系，在数据库中不存在，根据用户需求临时生成，数据库只储存视图定义。</p>\n<h4 id=\"Create-view创建视图\"><a href=\"#Create-view创建视图\" class=\"headerlink\" title=\"Create view创建视图\"></a>Create view创建视图</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009174818.png\" alt=\"image.png\"><br>其中&lt;query expression&gt;为SQL表达式，v是视图的名称</p>\n<h4 id=\"Views-Defined-Using-Other-Views\"><a href=\"#Views-Defined-Using-Other-Views\" class=\"headerlink\" title=\"Views Defined Using Other Views\"></a>Views Defined Using Other Views</h4><p>v1可以直接依赖（depend directly on）v2，即 v2 -&gt;  v1<br>v1也可以依赖（depend on）v2，即 v2 -&gt; v3 -&gt; v2<br>如果视图依赖于自身，则称为递归关系</p>\n<h4 id=\"Materialized-Views实例化视图\"><a href=\"#Materialized-Views实例化视图\" class=\"headerlink\" title=\"Materialized Views实例化视图\"></a>Materialized Views实例化视图</h4><p>创建一个物理表，其中包含查询定义视图的结果中的所有元组<br>如果更新查询中使用的关系，则实例化视图结果将过期</p>\n<h3 id=\"4-3-Transactions事务\"><a href=\"#4-3-Transactions事务\" class=\"headerlink\" title=\"4.3 Transactions事务\"></a>4.3 Transactions事务</h3><p>一个transaction包括一系列的查询和更新<br>特性：原子性、一致性、隔离性、耐用性<br>事务必须以下列语句之一结束：</p>\n<ul>\n<li>Commit work</li>\n<li>Rollback work<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016100923.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"4-4-Integrity-Constraints完整性约束\"><a href=\"#4-4-Integrity-Constraints完整性约束\" class=\"headerlink\" title=\"4.4 Integrity Constraints完整性约束\"></a>4.4 Integrity Constraints完整性约束</h3><p>通过对数据的约束防止数据的意外损坏<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016101652.png\" alt=\"image.png\"></p>\n<ul>\n<li>E.g. 1   the salary of manager should not be lower than $1000 in Employee</li>\n<li>E.g. 2   table T (x, y, z ), z &#x3D;x+y, z is a derived attributes from x and y.</li>\n<li>E.g. 3   the student# for table student should not be null</li>\n<li>E.g. 4   the age  of students should only be added</li>\n<li>E.g. 5   when  employee tuples is modified,  new.sal &gt; old.sal + 0.5*age</li>\n<li>E.g. 6   statistical  constraints</li>\n</ul>\n<h4 id=\"Constraints-on-Single-Relation\"><a href=\"#Constraints-on-Single-Relation\" class=\"headerlink\" title=\"Constraints on Single Relation\"></a>Constraints on Single Relation</h4><p>完整性约束包括：</p>\n<ul>\n<li>primary key</li>\n<li>not null</li>\n<li>unique</li>\n<li>check(P), where P is a predicate（谓词）</li>\n</ul>\n<h4 id=\"Referential-Integrity参照完整性\"><a href=\"#Referential-Integrity参照完整性\" class=\"headerlink\" title=\"Referential Integrity参照完整性\"></a>Referential Integrity参照完整性</h4><p>确保在一个relation中attribute的值也出现在另一个relation的attribute中<br>参照完整性约束也称为子集依赖关系（subset dependency）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016104636.png\" alt=\"image.png\"><br>table $r_2$的外键$\\alpha$参考table $r_1$的主键K1</p>\n<p>Cascading Actions级联操作：对$r_1$的主键K1的删除&#x2F;更新操作将导致$r_2$的外键$\\alpha$的删除&#x2F;更新</p>\n<h3 id=\"4-5-SQL-Data-Types-and-Schemas\"><a href=\"#4-5-SQL-Data-Types-and-Schemas\" class=\"headerlink\" title=\"4.5 SQL Data Types and Schemas\"></a>4.5 SQL Data Types and Schemas</h3><p>内置数据类型：</p>\n<ul>\n<li>date: 年月日</li>\n<li>time: 时分秒</li>\n<li>timestamp: date+time</li>\n<li>interval: 时间间隔</li>\n</ul>\n<h4 id=\"Type-Conversion类型转换\"><a href=\"#Type-Conversion类型转换\" class=\"headerlink\" title=\"Type Conversion类型转换\"></a>Type Conversion类型转换</h4><p><code>cast</code> &lt;e&gt; <code>as</code> &lt;t&gt;<br>将字符串e转换为类型t</p>\n<p><code>extract</code> value d <code>from</code> day or time<br>对于日期或时间d，提取其时间或日期字段</p>\n<h4 id=\"Formatting-Functions格式化函数\"><a href=\"#Formatting-Functions格式化函数\" class=\"headerlink\" title=\"Formatting Functions格式化函数\"></a>Formatting Functions格式化函数</h4><p>数据可能需要以不同的类型显示：</p>\n<ul>\n<li>以特定位数显示数字</li>\n<li>以特定格式显示数据</li>\n</ul>\n<p>Data Type Transition: <code>CAST</code> 和 <code>CONVERT</code></p>\n<h4 id=\"Default-Values\"><a href=\"#Default-Values\" class=\"headerlink\" title=\"Default Values\"></a>Default Values</h4><p>在创建table时设置属性的默认值，可以在插入tuple时不设置该属性的值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164651.png\" alt=\"image.png\"></p>\n<h4 id=\"Large-Object-Types\"><a href=\"#Large-Object-Types\" class=\"headerlink\" title=\"Large-Object Types\"></a>Large-Object Types</h4><p>blob：二进制大对象<br>clob：字符大对象<br>XML数据类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016164842.png\" alt=\"image.png\"></p>\n<h4 id=\"User-defined-Types-Domains\"><a href=\"#User-defined-Types-Domains\" class=\"headerlink\" title=\"User-defined Types&#x2F;Domains\"></a>User-defined Types&#x2F;Domains</h4><p>子句<code>create type</code>可以用于创建用户自定义类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165050.png\" alt=\"image.png\"></p>\n<p><code>create domain</code>可以创建用户定义域类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165114.png\" alt=\"image.png\"></p>\n<h4 id=\"Schemas-Catalogs-and-Environments\"><a href=\"#Schemas-Catalogs-and-Environments\" class=\"headerlink\" title=\"Schemas, Catalogs, and Environments\"></a>Schemas, Catalogs, and Environments</h4><p>现代数据库系统的三级层次结构</p>\n<ul>\n<li>cactalogs（全文目录）</li>\n<li>schemas（架构）</li>\n<li>SQL objects</li>\n</ul>\n<h4 id=\"Check-Conditions-and-Assertions检查和评估\"><a href=\"#Check-Conditions-and-Assertions检查和评估\" class=\"headerlink\" title=\"Check Conditions and Assertions检查和评估\"></a>Check Conditions and Assertions检查和评估</h4><p><code>check</code>和<code>assertion</code>可以定义复杂的完整性约束<br>assertion是一个谓词，表示我们希望数据库始终满足的条件<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016165326.png\" alt=\"image.png\"></p>\n<h3 id=\"4-6-Index-Definition-in-SQL\"><a href=\"#4-6-Index-Definition-in-SQL\" class=\"headerlink\" title=\"4.6 Index Definition in SQL\"></a>4.6 Index Definition in SQL</h3><p>许多查询只需参考表中的一小部分记录，读取每条记录以查找具有特定值的记录效率低下<br>建立索引，在查找时数据库可以只遍历索引而非遍历所有的值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016111831.png\" alt=\"image.png\"></p>\n<h3 id=\"4-7-Authorization\"><a href=\"#4-7-Authorization\" class=\"headerlink\" title=\"4.7 Authorization\"></a>4.7 Authorization</h3><p>对数据库各部分的授权形式包括：</p>\n<ul>\n<li>Read：允许读取数据，但不允许修改数据</li>\n<li>Insert：允许插入新数据，但不允许修改现有数据</li>\n<li>Update：允许修改，但不允许删除数据</li>\n<li>Delete：允许删除数据<br>对schema的授权：</li>\n<li>resources：允许创建新关系表</li>\n<li>alteration：允许在关系表中添加或删除属性</li>\n<li>drop：允许删除关系表</li>\n<li>index：允许创建索引</li>\n</ul>\n<h4 id=\"Authorization-Specification授权规范\"><a href=\"#Authorization-Specification授权规范\" class=\"headerlink\" title=\"Authorization Specification授权规范\"></a>Authorization Specification授权规范</h4><p><code>grant</code>语句用于授予权限<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170537.png\" alt=\"image.png\"></p>\n<p>授予视图特权并不意味着授予对基础关系的任何特权<br>特权的授予者必须已经拥有指定项的权限（或者是数据库管理员）</p>\n<h4 id=\"Privileges特权\"><a href=\"#Privileges特权\" class=\"headerlink\" title=\"Privileges特权\"></a>Privileges特权</h4><ul>\n<li><p>select：允许对关系表进行读取访问，或使用视图进行查询的能力<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170318.png\" alt=\"image.png\"></p>\n</li>\n<li><p>insert：插入元组的能力</p>\n</li>\n<li><p>update：使用 SQL 更新语句进行更新的能力</p>\n</li>\n<li><p>delete：删除元组的功能</p>\n</li>\n<li><p>all privileges：所有可被允许的特权</p>\n</li>\n<li><p>references：创建外键的权限</p>\n</li>\n</ul>\n<h4 id=\"Revoking-Authorization撤销授权\"><a href=\"#Revoking-Authorization撤销授权\" class=\"headerlink\" title=\"Revoking Authorization撤销授权\"></a>Revoking Authorization撤销授权</h4><p><code>revoke</code>用于撤销授权<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170604.png\" alt=\"image.png\"></p>\n<h4 id=\"Roles\"><a href=\"#Roles\" class=\"headerlink\" title=\"Roles\"></a>Roles</h4><p><code>create role</code> &lt;role name&gt;<br>可以向角色授予权限<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170845.png\" alt=\"image.png\"></p>\n<p>可以向用户以及其他角色授予角色<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231016170853.png\" alt=\"image.png\"></p>\n"},{"title":"SQL基础","_content":"![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009112654.png)\n### 3.1 Overview of SQL\nStructured Query Language\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100326.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100343.png)\n\n| data query             | Select                                           |\n| ---------------------- | ------------------------------------------------ |\n| data manipulation      | Insert,  Delete,  Update                         |\n| data definition        | Create,  Drop, Alter   (on schema)               |\n| data control           | Grant,  Revoke                                   |\n| transaction processing | begin transaction, commit, rollback              |\n| 指针/游标控制语言(CCL) | DECLARE CURSOR，FETCH INTO和UPDATE WHERE CURRENT |\n\n### 3.2 SQL Data Definition\n\n#### Domain Types 定义域类型\n- char(n).固定长度字符串，用户定义\n- varchar(n).可变长度字符串，用户定义\n- int.整数\n- smallint.小整数\n- numeric(p, d):固定小数，用户定义\n- float(n).浮点数，用户定义\n\n- date:日期包含年月日\n- time:时分秒\n- timestamp:日期加上时间\n- Interval:一段时间\n- Null：空集\n- create domain:自定义类型，不允许是Null\n\n>关系表属性名最好取英文名，便于应用程序的可移植性\n\n#### Create Table\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101602.png)\n- r是关系表的名称\n- A_i是变量名称\n- D_i是数据类型（定义域）\n\n约束：指定主键，外键，非空等。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101909.png)\n\n例：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925102014.png)\n\n#### Updates to Schemas\n删除表：\n\tdrop table r  # 删除表r\nAlter:\n\talter table r add A D  # 在表r中添加A属性，D是A的定义域 \n\talter table r drop A  # 把表r中的A属性删去\n\ntuples逐行存储，添加或删除表产生的数据移动花费很大，大数据表通常逐列存储\n\n### 3.3 Basic Structure of SQL Queries\nSQL query\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103324.png)\n- A_i为属性\n- r_i为关系表\n- P为predicate(谓词)\n等效于：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103521.png)\n\n#### The select Clause\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103844.png)\n\nSQL允许冗余，加入关键字Distinct消除冗余\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103947.png)\n关键字all可以使冗余不被移除\n\n属性可以是没有`from`的文本\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104758.png)\n结果为一行一列的表, 内容为437，可以为它加上名字\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104903.png)\n\nselect可以包含数学表达式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105014.png)\n\n#### The where Clause\n\nwhere为查询添加条件，对应于关系代数的选择代数\n\n可以包含 and，or，not，比较运算符：<,>,<=,>=,=,<>（不等于）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105530.png)\n\nbetween运算符\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110022.png)\n\n元组比较\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110122.png)\n\n#### natural join\n`natural` join in `from` subclause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111025.png)\n等于\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111128.png)\n自然连接默认把主键相同作为条件\n\n### 3.4 Additional Basic Operation\n#### The Rename Opration\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111544.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111558.png)\n利用T和S区分不同的instructor, 实现对同一属性的不同值比较\n>字符要加单引号\n\n#### String Operations\n运算符`like` :\n- percent(%):与任何子字符串匹配\n- underscore(\\_):匹配任意字符\n\n注意转义字符的使用:匹配\"100%\"\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113015.png)\n\n#### Ordering the Display of Tuples\n`order by`对指定属性进行排序,降序desc,升序asc(默认值)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113410.png)\n\n### 3.5 Set Operations\n`union`(并),`intersect`(交),`except`(差)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113554.png)\n自动消除重复项,`union all`可以保留重复项\n\n### 3.6 Null Values\nNull表示:\n- 一个未知的值\n- 一个不存在的值\n任何涉及Null的算术表达式的结果都是Null\n\n检查空值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114200.png)\n\nSQL将涉及Null的比较结果视为unknown\nwhere中的谓词可以涉及布尔运算（and，or，not）,因此需要扩展布尔运算来处理unknown\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114508.png)\n如果只有unknown,则视为false\n\n### 3.7 Aggregate Functions\n聚合函数:输入一列,输出一个值\n- avg:平均值\n- min:最小值\n- max:最大值\n- sum:求和\n- count:数量\n\n分组后聚合:\ngroup by:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925115629.png)\n先按dept_name分组,然后对每个组内的salary求平均\n\n- 聚合函数之外的 select 中的属性必须出现在group by中\n- 除count之外的所有聚合操作都忽略有Null的tuples\n\n#### Having Clause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925120755.png)\n\n对分组后的数据进行条件筛选\n`having`用于group形成以后,where用于group形成之前\n\n### 3.8 Nested Subqueries嵌套子查询\n子查询是嵌套在另一个查询中的“select-from-where”表达式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925121043.png)\n- r_i可以替换为任意有效的子查询\n- P可以替换为B\\<operation>(subquery)\n- A_i可以替换为生成单个值的子查询\n\n#### some Clause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102217.png)\n`some`中只要有一个满足条件即为true\n\n#### all  Clause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102433.png)\n需要`some`中所有都满足条件才为true\n\n#### Empty Relations空关系（exist，except）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102903.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102921.png)\n\n#### Duplicate Tuples重复元组（unique）\n`unique`检查子查询中是否有重复元组，如果没有重复项返回true\n\n#### With Clause\n`with`子句提供了一种定义临时关系的方法，将一个复杂查询分解为若干步，每个视图定义一个各部的中间计算结果，逻辑清晰。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009105244.png)\n\n### 3.9 Modification of Database\n#### Deletion删除\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110615.png)\n\n#### Insertion插入\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110654.png)\n\n#### Update更新\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009111812.png)\n\n","source":"_posts/Notes/课程/大三（上）/数据库/SQL基础.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 数据库\ntitle: SQL基础\ntags:\n  - 数据库\n  - SQL\n---\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009112654.png)\n### 3.1 Overview of SQL\nStructured Query Language\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100326.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100343.png)\n\n| data query             | Select                                           |\n| ---------------------- | ------------------------------------------------ |\n| data manipulation      | Insert,  Delete,  Update                         |\n| data definition        | Create,  Drop, Alter   (on schema)               |\n| data control           | Grant,  Revoke                                   |\n| transaction processing | begin transaction, commit, rollback              |\n| 指针/游标控制语言(CCL) | DECLARE CURSOR，FETCH INTO和UPDATE WHERE CURRENT |\n\n### 3.2 SQL Data Definition\n\n#### Domain Types 定义域类型\n- char(n).固定长度字符串，用户定义\n- varchar(n).可变长度字符串，用户定义\n- int.整数\n- smallint.小整数\n- numeric(p, d):固定小数，用户定义\n- float(n).浮点数，用户定义\n\n- date:日期包含年月日\n- time:时分秒\n- timestamp:日期加上时间\n- Interval:一段时间\n- Null：空集\n- create domain:自定义类型，不允许是Null\n\n>关系表属性名最好取英文名，便于应用程序的可移植性\n\n#### Create Table\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101602.png)\n- r是关系表的名称\n- A_i是变量名称\n- D_i是数据类型（定义域）\n\n约束：指定主键，外键，非空等。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101909.png)\n\n例：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925102014.png)\n\n#### Updates to Schemas\n删除表：\n\tdrop table r  # 删除表r\nAlter:\n\talter table r add A D  # 在表r中添加A属性，D是A的定义域 \n\talter table r drop A  # 把表r中的A属性删去\n\ntuples逐行存储，添加或删除表产生的数据移动花费很大，大数据表通常逐列存储\n\n### 3.3 Basic Structure of SQL Queries\nSQL query\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103324.png)\n- A_i为属性\n- r_i为关系表\n- P为predicate(谓词)\n等效于：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103521.png)\n\n#### The select Clause\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103844.png)\n\nSQL允许冗余，加入关键字Distinct消除冗余\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103947.png)\n关键字all可以使冗余不被移除\n\n属性可以是没有`from`的文本\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104758.png)\n结果为一行一列的表, 内容为437，可以为它加上名字\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104903.png)\n\nselect可以包含数学表达式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105014.png)\n\n#### The where Clause\n\nwhere为查询添加条件，对应于关系代数的选择代数\n\n可以包含 and，or，not，比较运算符：<,>,<=,>=,=,<>（不等于）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105530.png)\n\nbetween运算符\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110022.png)\n\n元组比较\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110122.png)\n\n#### natural join\n`natural` join in `from` subclause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111025.png)\n等于\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111128.png)\n自然连接默认把主键相同作为条件\n\n### 3.4 Additional Basic Operation\n#### The Rename Opration\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111544.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111558.png)\n利用T和S区分不同的instructor, 实现对同一属性的不同值比较\n>字符要加单引号\n\n#### String Operations\n运算符`like` :\n- percent(%):与任何子字符串匹配\n- underscore(\\_):匹配任意字符\n\n注意转义字符的使用:匹配\"100%\"\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113015.png)\n\n#### Ordering the Display of Tuples\n`order by`对指定属性进行排序,降序desc,升序asc(默认值)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113410.png)\n\n### 3.5 Set Operations\n`union`(并),`intersect`(交),`except`(差)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113554.png)\n自动消除重复项,`union all`可以保留重复项\n\n### 3.6 Null Values\nNull表示:\n- 一个未知的值\n- 一个不存在的值\n任何涉及Null的算术表达式的结果都是Null\n\n检查空值\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114200.png)\n\nSQL将涉及Null的比较结果视为unknown\nwhere中的谓词可以涉及布尔运算（and，or，not）,因此需要扩展布尔运算来处理unknown\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114508.png)\n如果只有unknown,则视为false\n\n### 3.7 Aggregate Functions\n聚合函数:输入一列,输出一个值\n- avg:平均值\n- min:最小值\n- max:最大值\n- sum:求和\n- count:数量\n\n分组后聚合:\ngroup by:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925115629.png)\n先按dept_name分组,然后对每个组内的salary求平均\n\n- 聚合函数之外的 select 中的属性必须出现在group by中\n- 除count之外的所有聚合操作都忽略有Null的tuples\n\n#### Having Clause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925120755.png)\n\n对分组后的数据进行条件筛选\n`having`用于group形成以后,where用于group形成之前\n\n### 3.8 Nested Subqueries嵌套子查询\n子查询是嵌套在另一个查询中的“select-from-where”表达式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925121043.png)\n- r_i可以替换为任意有效的子查询\n- P可以替换为B\\<operation>(subquery)\n- A_i可以替换为生成单个值的子查询\n\n#### some Clause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102217.png)\n`some`中只要有一个满足条件即为true\n\n#### all  Clause\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102433.png)\n需要`some`中所有都满足条件才为true\n\n#### Empty Relations空关系（exist，except）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102903.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102921.png)\n\n#### Duplicate Tuples重复元组（unique）\n`unique`检查子查询中是否有重复元组，如果没有重复项返回true\n\n#### With Clause\n`with`子句提供了一种定义临时关系的方法，将一个复杂查询分解为若干步，每个视图定义一个各部的中间计算结果，逻辑清晰。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009105244.png)\n\n### 3.9 Modification of Database\n#### Deletion删除\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110615.png)\n\n#### Insertion插入\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110654.png)\n\n#### Update更新\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009111812.png)\n\n","slug":"Notes/课程/大三（上）/数据库/SQL基础","published":1,"date":"2023-09-25T02:00:25.131Z","updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr122000mvw8c8llb54rr","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009112654.png\" alt=\"image.png\"></p>\n<h3 id=\"3-1-Overview-of-SQL\"><a href=\"#3-1-Overview-of-SQL\" class=\"headerlink\" title=\"3.1 Overview of SQL\"></a>3.1 Overview of SQL</h3><p>Structured Query Language<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100326.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100343.png\" alt=\"image.png\"></p>\n<table>\n<thead>\n<tr>\n<th>data query</th>\n<th>Select</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>data manipulation</td>\n<td>Insert,  Delete,  Update</td>\n</tr>\n<tr>\n<td>data definition</td>\n<td>Create,  Drop, Alter   (on schema)</td>\n</tr>\n<tr>\n<td>data control</td>\n<td>Grant,  Revoke</td>\n</tr>\n<tr>\n<td>transaction processing</td>\n<td>begin transaction, commit, rollback</td>\n</tr>\n<tr>\n<td>指针&#x2F;游标控制语言(CCL)</td>\n<td>DECLARE CURSOR，FETCH INTO和UPDATE WHERE CURRENT</td>\n</tr>\n</tbody></table>\n<h3 id=\"3-2-SQL-Data-Definition\"><a href=\"#3-2-SQL-Data-Definition\" class=\"headerlink\" title=\"3.2 SQL Data Definition\"></a>3.2 SQL Data Definition</h3><h4 id=\"Domain-Types-定义域类型\"><a href=\"#Domain-Types-定义域类型\" class=\"headerlink\" title=\"Domain Types 定义域类型\"></a>Domain Types 定义域类型</h4><ul>\n<li><p>char(n).固定长度字符串，用户定义</p>\n</li>\n<li><p>varchar(n).可变长度字符串，用户定义</p>\n</li>\n<li><p>int.整数</p>\n</li>\n<li><p>smallint.小整数</p>\n</li>\n<li><p>numeric(p, d):固定小数，用户定义</p>\n</li>\n<li><p>float(n).浮点数，用户定义</p>\n</li>\n<li><p>date:日期包含年月日</p>\n</li>\n<li><p>time:时分秒</p>\n</li>\n<li><p>timestamp:日期加上时间</p>\n</li>\n<li><p>Interval:一段时间</p>\n</li>\n<li><p>Null：空集</p>\n</li>\n<li><p>create domain:自定义类型，不允许是Null</p>\n</li>\n</ul>\n<blockquote>\n<p>关系表属性名最好取英文名，便于应用程序的可移植性</p>\n</blockquote>\n<h4 id=\"Create-Table\"><a href=\"#Create-Table\" class=\"headerlink\" title=\"Create Table\"></a>Create Table</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101602.png\" alt=\"image.png\"></p>\n<ul>\n<li>r是关系表的名称</li>\n<li>A_i是变量名称</li>\n<li>D_i是数据类型（定义域）</li>\n</ul>\n<p>约束：指定主键，外键，非空等。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101909.png\" alt=\"image.png\"></p>\n<p>例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925102014.png\" alt=\"image.png\"></p>\n<h4 id=\"Updates-to-Schemas\"><a href=\"#Updates-to-Schemas\" class=\"headerlink\" title=\"Updates to Schemas\"></a>Updates to Schemas</h4><p>删除表：<br>    drop table r  # 删除表r<br>Alter:<br>    alter table r add A D  # 在表r中添加A属性，D是A的定义域<br>    alter table r drop A  # 把表r中的A属性删去</p>\n<p>tuples逐行存储，添加或删除表产生的数据移动花费很大，大数据表通常逐列存储</p>\n<h3 id=\"3-3-Basic-Structure-of-SQL-Queries\"><a href=\"#3-3-Basic-Structure-of-SQL-Queries\" class=\"headerlink\" title=\"3.3 Basic Structure of SQL Queries\"></a>3.3 Basic Structure of SQL Queries</h3><p>SQL query<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103324.png\" alt=\"image.png\"></p>\n<ul>\n<li>A_i为属性</li>\n<li>r_i为关系表</li>\n<li>P为predicate(谓词)<br>等效于：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103521.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"The-select-Clause\"><a href=\"#The-select-Clause\" class=\"headerlink\" title=\"The select Clause\"></a>The select Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103844.png\" alt=\"image.png\"></p>\n<p>SQL允许冗余，加入关键字Distinct消除冗余<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103947.png\" alt=\"image.png\"><br>关键字all可以使冗余不被移除</p>\n<p>属性可以是没有<code>from</code>的文本<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104758.png\" alt=\"image.png\"><br>结果为一行一列的表, 内容为437，可以为它加上名字<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104903.png\" alt=\"image.png\"></p>\n<p>select可以包含数学表达式<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105014.png\" alt=\"image.png\"></p>\n<h4 id=\"The-where-Clause\"><a href=\"#The-where-Clause\" class=\"headerlink\" title=\"The where Clause\"></a>The where Clause</h4><p>where为查询添加条件，对应于关系代数的选择代数</p>\n<p>可以包含 and，or，not，比较运算符：&lt;,&gt;,&lt;&#x3D;,&gt;&#x3D;,&#x3D;,&lt;&gt;（不等于）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105530.png\" alt=\"image.png\"></p>\n<p>between运算符<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110022.png\" alt=\"image.png\"></p>\n<p>元组比较<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110122.png\" alt=\"image.png\"></p>\n<h4 id=\"natural-join\"><a href=\"#natural-join\" class=\"headerlink\" title=\"natural join\"></a>natural join</h4><p><code>natural</code> join in <code>from</code> subclause<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111025.png\" alt=\"image.png\"><br>等于<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111128.png\" alt=\"image.png\"><br>自然连接默认把主键相同作为条件</p>\n<h3 id=\"3-4-Additional-Basic-Operation\"><a href=\"#3-4-Additional-Basic-Operation\" class=\"headerlink\" title=\"3.4 Additional Basic Operation\"></a>3.4 Additional Basic Operation</h3><h4 id=\"The-Rename-Opration\"><a href=\"#The-Rename-Opration\" class=\"headerlink\" title=\"The Rename Opration\"></a>The Rename Opration</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111544.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111558.png\" alt=\"image.png\"><br>利用T和S区分不同的instructor, 实现对同一属性的不同值比较</p>\n<blockquote>\n<p>字符要加单引号</p>\n</blockquote>\n<h4 id=\"String-Operations\"><a href=\"#String-Operations\" class=\"headerlink\" title=\"String Operations\"></a>String Operations</h4><p>运算符<code>like</code> :</p>\n<ul>\n<li>percent(%):与任何子字符串匹配</li>\n<li>underscore(_):匹配任意字符</li>\n</ul>\n<p>注意转义字符的使用:匹配”100%”<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113015.png\" alt=\"image.png\"></p>\n<h4 id=\"Ordering-the-Display-of-Tuples\"><a href=\"#Ordering-the-Display-of-Tuples\" class=\"headerlink\" title=\"Ordering the Display of Tuples\"></a>Ordering the Display of Tuples</h4><p><code>order by</code>对指定属性进行排序,降序desc,升序asc(默认值)<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113410.png\" alt=\"image.png\"></p>\n<h3 id=\"3-5-Set-Operations\"><a href=\"#3-5-Set-Operations\" class=\"headerlink\" title=\"3.5 Set Operations\"></a>3.5 Set Operations</h3><p><code>union</code>(并),<code>intersect</code>(交),<code>except</code>(差)<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113554.png\" alt=\"image.png\"><br>自动消除重复项,<code>union all</code>可以保留重复项</p>\n<h3 id=\"3-6-Null-Values\"><a href=\"#3-6-Null-Values\" class=\"headerlink\" title=\"3.6 Null Values\"></a>3.6 Null Values</h3><p>Null表示:</p>\n<ul>\n<li>一个未知的值</li>\n<li>一个不存在的值<br>任何涉及Null的算术表达式的结果都是Null</li>\n</ul>\n<p>检查空值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114200.png\" alt=\"image.png\"></p>\n<p>SQL将涉及Null的比较结果视为unknown<br>where中的谓词可以涉及布尔运算（and，or，not）,因此需要扩展布尔运算来处理unknown<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114508.png\" alt=\"image.png\"><br>如果只有unknown,则视为false</p>\n<h3 id=\"3-7-Aggregate-Functions\"><a href=\"#3-7-Aggregate-Functions\" class=\"headerlink\" title=\"3.7 Aggregate Functions\"></a>3.7 Aggregate Functions</h3><p>聚合函数:输入一列,输出一个值</p>\n<ul>\n<li>avg:平均值</li>\n<li>min:最小值</li>\n<li>max:最大值</li>\n<li>sum:求和</li>\n<li>count:数量</li>\n</ul>\n<p>分组后聚合:<br>group by:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925115629.png\" alt=\"image.png\"><br>先按dept_name分组,然后对每个组内的salary求平均</p>\n<ul>\n<li>聚合函数之外的 select 中的属性必须出现在group by中</li>\n<li>除count之外的所有聚合操作都忽略有Null的tuples</li>\n</ul>\n<h4 id=\"Having-Clause\"><a href=\"#Having-Clause\" class=\"headerlink\" title=\"Having Clause\"></a>Having Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925120755.png\" alt=\"image.png\"></p>\n<p>对分组后的数据进行条件筛选<br><code>having</code>用于group形成以后,where用于group形成之前</p>\n<h3 id=\"3-8-Nested-Subqueries嵌套子查询\"><a href=\"#3-8-Nested-Subqueries嵌套子查询\" class=\"headerlink\" title=\"3.8 Nested Subqueries嵌套子查询\"></a>3.8 Nested Subqueries嵌套子查询</h3><p>子查询是嵌套在另一个查询中的“select-from-where”表达式<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925121043.png\" alt=\"image.png\"></p>\n<ul>\n<li>r_i可以替换为任意有效的子查询</li>\n<li>P可以替换为B&lt;operation&gt;(subquery)</li>\n<li>A_i可以替换为生成单个值的子查询</li>\n</ul>\n<h4 id=\"some-Clause\"><a href=\"#some-Clause\" class=\"headerlink\" title=\"some Clause\"></a>some Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102217.png\" alt=\"image.png\"><br><code>some</code>中只要有一个满足条件即为true</p>\n<h4 id=\"all-Clause\"><a href=\"#all-Clause\" class=\"headerlink\" title=\"all  Clause\"></a>all  Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102433.png\" alt=\"image.png\"><br>需要<code>some</code>中所有都满足条件才为true</p>\n<h4 id=\"Empty-Relations空关系（exist，except）\"><a href=\"#Empty-Relations空关系（exist，except）\" class=\"headerlink\" title=\"Empty Relations空关系（exist，except）\"></a>Empty Relations空关系（exist，except）</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102903.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102921.png\" alt=\"image.png\"></p>\n<h4 id=\"Duplicate-Tuples重复元组（unique）\"><a href=\"#Duplicate-Tuples重复元组（unique）\" class=\"headerlink\" title=\"Duplicate Tuples重复元组（unique）\"></a>Duplicate Tuples重复元组（unique）</h4><p><code>unique</code>检查子查询中是否有重复元组，如果没有重复项返回true</p>\n<h4 id=\"With-Clause\"><a href=\"#With-Clause\" class=\"headerlink\" title=\"With Clause\"></a>With Clause</h4><p><code>with</code>子句提供了一种定义临时关系的方法，将一个复杂查询分解为若干步，每个视图定义一个各部的中间计算结果，逻辑清晰。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009105244.png\" alt=\"image.png\"></p>\n<h3 id=\"3-9-Modification-of-Database\"><a href=\"#3-9-Modification-of-Database\" class=\"headerlink\" title=\"3.9 Modification of Database\"></a>3.9 Modification of Database</h3><h4 id=\"Deletion删除\"><a href=\"#Deletion删除\" class=\"headerlink\" title=\"Deletion删除\"></a>Deletion删除</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110615.png\" alt=\"image.png\"></p>\n<h4 id=\"Insertion插入\"><a href=\"#Insertion插入\" class=\"headerlink\" title=\"Insertion插入\"></a>Insertion插入</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110654.png\" alt=\"image.png\"></p>\n<h4 id=\"Update更新\"><a href=\"#Update更新\" class=\"headerlink\" title=\"Update更新\"></a>Update更新</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009111812.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009112654.png\" alt=\"image.png\"></p>\n<h3 id=\"3-1-Overview-of-SQL\"><a href=\"#3-1-Overview-of-SQL\" class=\"headerlink\" title=\"3.1 Overview of SQL\"></a>3.1 Overview of SQL</h3><p>Structured Query Language<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100326.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925100343.png\" alt=\"image.png\"></p>\n<table>\n<thead>\n<tr>\n<th>data query</th>\n<th>Select</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>data manipulation</td>\n<td>Insert,  Delete,  Update</td>\n</tr>\n<tr>\n<td>data definition</td>\n<td>Create,  Drop, Alter   (on schema)</td>\n</tr>\n<tr>\n<td>data control</td>\n<td>Grant,  Revoke</td>\n</tr>\n<tr>\n<td>transaction processing</td>\n<td>begin transaction, commit, rollback</td>\n</tr>\n<tr>\n<td>指针&#x2F;游标控制语言(CCL)</td>\n<td>DECLARE CURSOR，FETCH INTO和UPDATE WHERE CURRENT</td>\n</tr>\n</tbody></table>\n<h3 id=\"3-2-SQL-Data-Definition\"><a href=\"#3-2-SQL-Data-Definition\" class=\"headerlink\" title=\"3.2 SQL Data Definition\"></a>3.2 SQL Data Definition</h3><h4 id=\"Domain-Types-定义域类型\"><a href=\"#Domain-Types-定义域类型\" class=\"headerlink\" title=\"Domain Types 定义域类型\"></a>Domain Types 定义域类型</h4><ul>\n<li><p>char(n).固定长度字符串，用户定义</p>\n</li>\n<li><p>varchar(n).可变长度字符串，用户定义</p>\n</li>\n<li><p>int.整数</p>\n</li>\n<li><p>smallint.小整数</p>\n</li>\n<li><p>numeric(p, d):固定小数，用户定义</p>\n</li>\n<li><p>float(n).浮点数，用户定义</p>\n</li>\n<li><p>date:日期包含年月日</p>\n</li>\n<li><p>time:时分秒</p>\n</li>\n<li><p>timestamp:日期加上时间</p>\n</li>\n<li><p>Interval:一段时间</p>\n</li>\n<li><p>Null：空集</p>\n</li>\n<li><p>create domain:自定义类型，不允许是Null</p>\n</li>\n</ul>\n<blockquote>\n<p>关系表属性名最好取英文名，便于应用程序的可移植性</p>\n</blockquote>\n<h4 id=\"Create-Table\"><a href=\"#Create-Table\" class=\"headerlink\" title=\"Create Table\"></a>Create Table</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101602.png\" alt=\"image.png\"></p>\n<ul>\n<li>r是关系表的名称</li>\n<li>A_i是变量名称</li>\n<li>D_i是数据类型（定义域）</li>\n</ul>\n<p>约束：指定主键，外键，非空等。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925101909.png\" alt=\"image.png\"></p>\n<p>例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925102014.png\" alt=\"image.png\"></p>\n<h4 id=\"Updates-to-Schemas\"><a href=\"#Updates-to-Schemas\" class=\"headerlink\" title=\"Updates to Schemas\"></a>Updates to Schemas</h4><p>删除表：<br>    drop table r  # 删除表r<br>Alter:<br>    alter table r add A D  # 在表r中添加A属性，D是A的定义域<br>    alter table r drop A  # 把表r中的A属性删去</p>\n<p>tuples逐行存储，添加或删除表产生的数据移动花费很大，大数据表通常逐列存储</p>\n<h3 id=\"3-3-Basic-Structure-of-SQL-Queries\"><a href=\"#3-3-Basic-Structure-of-SQL-Queries\" class=\"headerlink\" title=\"3.3 Basic Structure of SQL Queries\"></a>3.3 Basic Structure of SQL Queries</h3><p>SQL query<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103324.png\" alt=\"image.png\"></p>\n<ul>\n<li>A_i为属性</li>\n<li>r_i为关系表</li>\n<li>P为predicate(谓词)<br>等效于：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103521.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"The-select-Clause\"><a href=\"#The-select-Clause\" class=\"headerlink\" title=\"The select Clause\"></a>The select Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103844.png\" alt=\"image.png\"></p>\n<p>SQL允许冗余，加入关键字Distinct消除冗余<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925103947.png\" alt=\"image.png\"><br>关键字all可以使冗余不被移除</p>\n<p>属性可以是没有<code>from</code>的文本<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104758.png\" alt=\"image.png\"><br>结果为一行一列的表, 内容为437，可以为它加上名字<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925104903.png\" alt=\"image.png\"></p>\n<p>select可以包含数学表达式<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105014.png\" alt=\"image.png\"></p>\n<h4 id=\"The-where-Clause\"><a href=\"#The-where-Clause\" class=\"headerlink\" title=\"The where Clause\"></a>The where Clause</h4><p>where为查询添加条件，对应于关系代数的选择代数</p>\n<p>可以包含 and，or，not，比较运算符：&lt;,&gt;,&lt;&#x3D;,&gt;&#x3D;,&#x3D;,&lt;&gt;（不等于）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925105530.png\" alt=\"image.png\"></p>\n<p>between运算符<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110022.png\" alt=\"image.png\"></p>\n<p>元组比较<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925110122.png\" alt=\"image.png\"></p>\n<h4 id=\"natural-join\"><a href=\"#natural-join\" class=\"headerlink\" title=\"natural join\"></a>natural join</h4><p><code>natural</code> join in <code>from</code> subclause<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111025.png\" alt=\"image.png\"><br>等于<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111128.png\" alt=\"image.png\"><br>自然连接默认把主键相同作为条件</p>\n<h3 id=\"3-4-Additional-Basic-Operation\"><a href=\"#3-4-Additional-Basic-Operation\" class=\"headerlink\" title=\"3.4 Additional Basic Operation\"></a>3.4 Additional Basic Operation</h3><h4 id=\"The-Rename-Opration\"><a href=\"#The-Rename-Opration\" class=\"headerlink\" title=\"The Rename Opration\"></a>The Rename Opration</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111544.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925111558.png\" alt=\"image.png\"><br>利用T和S区分不同的instructor, 实现对同一属性的不同值比较</p>\n<blockquote>\n<p>字符要加单引号</p>\n</blockquote>\n<h4 id=\"String-Operations\"><a href=\"#String-Operations\" class=\"headerlink\" title=\"String Operations\"></a>String Operations</h4><p>运算符<code>like</code> :</p>\n<ul>\n<li>percent(%):与任何子字符串匹配</li>\n<li>underscore(_):匹配任意字符</li>\n</ul>\n<p>注意转义字符的使用:匹配”100%”<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113015.png\" alt=\"image.png\"></p>\n<h4 id=\"Ordering-the-Display-of-Tuples\"><a href=\"#Ordering-the-Display-of-Tuples\" class=\"headerlink\" title=\"Ordering the Display of Tuples\"></a>Ordering the Display of Tuples</h4><p><code>order by</code>对指定属性进行排序,降序desc,升序asc(默认值)<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113410.png\" alt=\"image.png\"></p>\n<h3 id=\"3-5-Set-Operations\"><a href=\"#3-5-Set-Operations\" class=\"headerlink\" title=\"3.5 Set Operations\"></a>3.5 Set Operations</h3><p><code>union</code>(并),<code>intersect</code>(交),<code>except</code>(差)<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925113554.png\" alt=\"image.png\"><br>自动消除重复项,<code>union all</code>可以保留重复项</p>\n<h3 id=\"3-6-Null-Values\"><a href=\"#3-6-Null-Values\" class=\"headerlink\" title=\"3.6 Null Values\"></a>3.6 Null Values</h3><p>Null表示:</p>\n<ul>\n<li>一个未知的值</li>\n<li>一个不存在的值<br>任何涉及Null的算术表达式的结果都是Null</li>\n</ul>\n<p>检查空值<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114200.png\" alt=\"image.png\"></p>\n<p>SQL将涉及Null的比较结果视为unknown<br>where中的谓词可以涉及布尔运算（and，or，not）,因此需要扩展布尔运算来处理unknown<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925114508.png\" alt=\"image.png\"><br>如果只有unknown,则视为false</p>\n<h3 id=\"3-7-Aggregate-Functions\"><a href=\"#3-7-Aggregate-Functions\" class=\"headerlink\" title=\"3.7 Aggregate Functions\"></a>3.7 Aggregate Functions</h3><p>聚合函数:输入一列,输出一个值</p>\n<ul>\n<li>avg:平均值</li>\n<li>min:最小值</li>\n<li>max:最大值</li>\n<li>sum:求和</li>\n<li>count:数量</li>\n</ul>\n<p>分组后聚合:<br>group by:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925115629.png\" alt=\"image.png\"><br>先按dept_name分组,然后对每个组内的salary求平均</p>\n<ul>\n<li>聚合函数之外的 select 中的属性必须出现在group by中</li>\n<li>除count之外的所有聚合操作都忽略有Null的tuples</li>\n</ul>\n<h4 id=\"Having-Clause\"><a href=\"#Having-Clause\" class=\"headerlink\" title=\"Having Clause\"></a>Having Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925120755.png\" alt=\"image.png\"></p>\n<p>对分组后的数据进行条件筛选<br><code>having</code>用于group形成以后,where用于group形成之前</p>\n<h3 id=\"3-8-Nested-Subqueries嵌套子查询\"><a href=\"#3-8-Nested-Subqueries嵌套子查询\" class=\"headerlink\" title=\"3.8 Nested Subqueries嵌套子查询\"></a>3.8 Nested Subqueries嵌套子查询</h3><p>子查询是嵌套在另一个查询中的“select-from-where”表达式<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230925121043.png\" alt=\"image.png\"></p>\n<ul>\n<li>r_i可以替换为任意有效的子查询</li>\n<li>P可以替换为B&lt;operation&gt;(subquery)</li>\n<li>A_i可以替换为生成单个值的子查询</li>\n</ul>\n<h4 id=\"some-Clause\"><a href=\"#some-Clause\" class=\"headerlink\" title=\"some Clause\"></a>some Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102217.png\" alt=\"image.png\"><br><code>some</code>中只要有一个满足条件即为true</p>\n<h4 id=\"all-Clause\"><a href=\"#all-Clause\" class=\"headerlink\" title=\"all  Clause\"></a>all  Clause</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102433.png\" alt=\"image.png\"><br>需要<code>some</code>中所有都满足条件才为true</p>\n<h4 id=\"Empty-Relations空关系（exist，except）\"><a href=\"#Empty-Relations空关系（exist，except）\" class=\"headerlink\" title=\"Empty Relations空关系（exist，except）\"></a>Empty Relations空关系（exist，except）</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102903.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009102921.png\" alt=\"image.png\"></p>\n<h4 id=\"Duplicate-Tuples重复元组（unique）\"><a href=\"#Duplicate-Tuples重复元组（unique）\" class=\"headerlink\" title=\"Duplicate Tuples重复元组（unique）\"></a>Duplicate Tuples重复元组（unique）</h4><p><code>unique</code>检查子查询中是否有重复元组，如果没有重复项返回true</p>\n<h4 id=\"With-Clause\"><a href=\"#With-Clause\" class=\"headerlink\" title=\"With Clause\"></a>With Clause</h4><p><code>with</code>子句提供了一种定义临时关系的方法，将一个复杂查询分解为若干步，每个视图定义一个各部的中间计算结果，逻辑清晰。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009105244.png\" alt=\"image.png\"></p>\n<h3 id=\"3-9-Modification-of-Database\"><a href=\"#3-9-Modification-of-Database\" class=\"headerlink\" title=\"3.9 Modification of Database\"></a>3.9 Modification of Database</h3><h4 id=\"Deletion删除\"><a href=\"#Deletion删除\" class=\"headerlink\" title=\"Deletion删除\"></a>Deletion删除</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110615.png\" alt=\"image.png\"></p>\n<h4 id=\"Insertion插入\"><a href=\"#Insertion插入\" class=\"headerlink\" title=\"Insertion插入\"></a>Insertion插入</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009110654.png\" alt=\"image.png\"></p>\n<h4 id=\"Update更新\"><a href=\"#Update更新\" class=\"headerlink\" title=\"Update更新\"></a>Update更新</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231009111812.png\" alt=\"image.png\"></p>\n"},{"title":"内存","date":"2024-01-07T03:20:33.749Z","_content":"![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112118.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112152.png)\n\n\n大题：\n逻辑地址转物理地址\n\n动态分区分配算法：\n1. 首次适应算法\n2. 临近适应算法\n3. 最佳适应算法\n4. 最坏适应算法\n\n页面置换算法\n1. 最佳置换算法\n2. 先进先出页面置换算法\n3. 最近最久未使用置换算法\n4. 简单时钟置换算法\n5. 改进型时钟置换算法\n\n求fat表大小\n\n磁盘调度算法\n1. 先来先服务算法\n2. 最短寻找时间优先算法\n3. 扫描算法（电梯调度算法）\n4. 循环扫描算法","source":"_posts/Notes/课程/大三（上）/操作系统/内存.md","raw":"---\ntitle: 内存\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\ndate:\n---\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112118.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112152.png)\n\n\n大题：\n逻辑地址转物理地址\n\n动态分区分配算法：\n1. 首次适应算法\n2. 临近适应算法\n3. 最佳适应算法\n4. 最坏适应算法\n\n页面置换算法\n1. 最佳置换算法\n2. 先进先出页面置换算法\n3. 最近最久未使用置换算法\n4. 简单时钟置换算法\n5. 改进型时钟置换算法\n\n求fat表大小\n\n磁盘调度算法\n1. 先来先服务算法\n2. 最短寻找时间优先算法\n3. 扫描算法（电梯调度算法）\n4. 循环扫描算法","slug":"Notes/课程/大三（上）/操作系统/内存","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr123000pvw8c2ko81xt0","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112118.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112152.png\" alt=\"image.png\"></p>\n<p>大题：<br>逻辑地址转物理地址</p>\n<p>动态分区分配算法：</p>\n<ol>\n<li>首次适应算法</li>\n<li>临近适应算法</li>\n<li>最佳适应算法</li>\n<li>最坏适应算法</li>\n</ol>\n<p>页面置换算法</p>\n<ol>\n<li>最佳置换算法</li>\n<li>先进先出页面置换算法</li>\n<li>最近最久未使用置换算法</li>\n<li>简单时钟置换算法</li>\n<li>改进型时钟置换算法</li>\n</ol>\n<p>求fat表大小</p>\n<p>磁盘调度算法</p>\n<ol>\n<li>先来先服务算法</li>\n<li>最短寻找时间优先算法</li>\n<li>扫描算法（电梯调度算法）</li>\n<li>循环扫描算法</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112118.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240107112152.png\" alt=\"image.png\"></p>\n<p>大题：<br>逻辑地址转物理地址</p>\n<p>动态分区分配算法：</p>\n<ol>\n<li>首次适应算法</li>\n<li>临近适应算法</li>\n<li>最佳适应算法</li>\n<li>最坏适应算法</li>\n</ol>\n<p>页面置换算法</p>\n<ol>\n<li>最佳置换算法</li>\n<li>先进先出页面置换算法</li>\n<li>最近最久未使用置换算法</li>\n<li>简单时钟置换算法</li>\n<li>改进型时钟置换算法</li>\n</ol>\n<p>求fat表大小</p>\n<p>磁盘调度算法</p>\n<ol>\n<li>先来先服务算法</li>\n<li>最短寻找时间优先算法</li>\n<li>扫描算法（电梯调度算法）</li>\n<li>循环扫描算法</li>\n</ol>\n"},{"title":"操作系统","update":null,"_content":"老师声音小，需要坐前排\n\n40分平时作业（gitee），60分期末\n\nTODO：\n- [x] 加入gitee仓库\n\n\n\n---\n\n虚拟换CPU：\n操作系统需要以某种方式在看似同时运行的许多程序之间共享物理CPU 基本思想：运行一个程序一小会儿，然后切换到运行另一个程序，依此类推 分时CPU - 虚拟化就实现了！\n\n都具有用于在内存和寄存器之间移动项目的加载和存储指令 \n- 将位于内存中某个地址的字加载到寄存器中 \n- 将寄存器的内容存储到位于内存中某个地址的字 \n- 许多用于比较和的指令组合寄存器中的值并将结果放入寄存器\n\nCPU的结构：\n- 程序计数器（Program Counter，PC）：保存下一条指令的内存地址\n- 指令寄存器（Instruction Register，IR）：保存当前正在执行的指令\n- 通用寄存器（General Registers，R1...Rn）：保存执行环境：临时结果\n- 算术逻辑单元（Arithmetic Logic Unit，ALU）：执行算术函数和逻辑运行\n- 堆栈指针（The Stack Pointer，SP）：保存堆栈的内存地址，以及每个活动函数的参数和局部变量的帧（frame）\n- 程序状态字（The Program Status Word，PSW）：包含一些重要的控制位\n\nCPU 所做的就是获取/解码/执行获取 PC 指向的下一条指令，对其进行解码以查找其类型，然后操作数重复执行它\n\n寄存器r15指向执行栈的顶部\n堆栈从较高的内存地址向较低的内存地址向下增长\n\nCPU在PSW中有一个模式位，用于定义程序的执行能力\n\n内核模式：执行任何指令\n用户模式： 执行指令的子集\n仅在内核模式下执行的指令称为特权指令\n\n在用户模式下运行的应用程序要执行系统调用，用户程序需要执行一条称为陷阱`trap`的指令\n它所做的只是跳转到内核，同时将特权级别提升到内核模式\n我们需要返回到进行系统调用的程序，但同时，需要清除模式位（将特权级别降低回用户模式）同样，操作系统依赖于CPU的一些帮助，通过使用另一条指令，我们称之为从陷阱返回`return-from-trap`\n\n内核在内核模式下启动时会设置一个陷阱表，然后让硬件知道它在哪里。陷阱表也称为中断表/向量，因为它也有硬件中断和异常的条目！事实证明，trap 只是陷阱表中的条目之一。系统调用的实际类型称为系统调用号，它可以存储在内核堆栈中的众所周知的位置\n\n中断是异步的（在任意时间出现） ：\n- 由硬件事件引起 \n- 定时器中断 属于外中断\n- I/O（如键盘或磁盘）中断 属于外中断\n- 由用户模式程序中的编程错误引起。属于内终端。示例：算术异常（除以零）尝试访问程序无法控制的内存尝试执行特权指令 \n\n中断的示例：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102223922.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224053.png)\n\n内中断和外中断：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224829.png)\n","source":"_posts/Notes/课程/大三（上）/操作系统/操作系统.md","raw":"---\ntitle: 操作系统\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\nupdate:\n---\n老师声音小，需要坐前排\n\n40分平时作业（gitee），60分期末\n\nTODO：\n- [x] 加入gitee仓库\n\n\n\n---\n\n虚拟换CPU：\n操作系统需要以某种方式在看似同时运行的许多程序之间共享物理CPU 基本思想：运行一个程序一小会儿，然后切换到运行另一个程序，依此类推 分时CPU - 虚拟化就实现了！\n\n都具有用于在内存和寄存器之间移动项目的加载和存储指令 \n- 将位于内存中某个地址的字加载到寄存器中 \n- 将寄存器的内容存储到位于内存中某个地址的字 \n- 许多用于比较和的指令组合寄存器中的值并将结果放入寄存器\n\nCPU的结构：\n- 程序计数器（Program Counter，PC）：保存下一条指令的内存地址\n- 指令寄存器（Instruction Register，IR）：保存当前正在执行的指令\n- 通用寄存器（General Registers，R1...Rn）：保存执行环境：临时结果\n- 算术逻辑单元（Arithmetic Logic Unit，ALU）：执行算术函数和逻辑运行\n- 堆栈指针（The Stack Pointer，SP）：保存堆栈的内存地址，以及每个活动函数的参数和局部变量的帧（frame）\n- 程序状态字（The Program Status Word，PSW）：包含一些重要的控制位\n\nCPU 所做的就是获取/解码/执行获取 PC 指向的下一条指令，对其进行解码以查找其类型，然后操作数重复执行它\n\n寄存器r15指向执行栈的顶部\n堆栈从较高的内存地址向较低的内存地址向下增长\n\nCPU在PSW中有一个模式位，用于定义程序的执行能力\n\n内核模式：执行任何指令\n用户模式： 执行指令的子集\n仅在内核模式下执行的指令称为特权指令\n\n在用户模式下运行的应用程序要执行系统调用，用户程序需要执行一条称为陷阱`trap`的指令\n它所做的只是跳转到内核，同时将特权级别提升到内核模式\n我们需要返回到进行系统调用的程序，但同时，需要清除模式位（将特权级别降低回用户模式）同样，操作系统依赖于CPU的一些帮助，通过使用另一条指令，我们称之为从陷阱返回`return-from-trap`\n\n内核在内核模式下启动时会设置一个陷阱表，然后让硬件知道它在哪里。陷阱表也称为中断表/向量，因为它也有硬件中断和异常的条目！事实证明，trap 只是陷阱表中的条目之一。系统调用的实际类型称为系统调用号，它可以存储在内核堆栈中的众所周知的位置\n\n中断是异步的（在任意时间出现） ：\n- 由硬件事件引起 \n- 定时器中断 属于外中断\n- I/O（如键盘或磁盘）中断 属于外中断\n- 由用户模式程序中的编程错误引起。属于内终端。示例：算术异常（除以零）尝试访问程序无法控制的内存尝试执行特权指令 \n\n中断的示例：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102223922.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224053.png)\n\n内中断和外中断：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224829.png)\n","slug":"Notes/课程/大三（上）/操作系统/操作系统","published":1,"date":"2023-09-21T16:36:34.336Z","updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr124000svw8c7nfla28m","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>老师声音小，需要坐前排</p>\n<p>40分平时作业（gitee），60分期末</p>\n<p>TODO：</p>\n<ul>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 加入gitee仓库</li>\n</ul>\n<hr>\n<p>虚拟换CPU：<br>操作系统需要以某种方式在看似同时运行的许多程序之间共享物理CPU 基本思想：运行一个程序一小会儿，然后切换到运行另一个程序，依此类推 分时CPU - 虚拟化就实现了！</p>\n<p>都具有用于在内存和寄存器之间移动项目的加载和存储指令 </p>\n<ul>\n<li>将位于内存中某个地址的字加载到寄存器中 </li>\n<li>将寄存器的内容存储到位于内存中某个地址的字 </li>\n<li>许多用于比较和的指令组合寄存器中的值并将结果放入寄存器</li>\n</ul>\n<p>CPU的结构：</p>\n<ul>\n<li>程序计数器（Program Counter，PC）：保存下一条指令的内存地址</li>\n<li>指令寄存器（Instruction Register，IR）：保存当前正在执行的指令</li>\n<li>通用寄存器（General Registers，R1…Rn）：保存执行环境：临时结果</li>\n<li>算术逻辑单元（Arithmetic Logic Unit，ALU）：执行算术函数和逻辑运行</li>\n<li>堆栈指针（The Stack Pointer，SP）：保存堆栈的内存地址，以及每个活动函数的参数和局部变量的帧（frame）</li>\n<li>程序状态字（The Program Status Word，PSW）：包含一些重要的控制位</li>\n</ul>\n<p>CPU 所做的就是获取&#x2F;解码&#x2F;执行获取 PC 指向的下一条指令，对其进行解码以查找其类型，然后操作数重复执行它</p>\n<p>寄存器r15指向执行栈的顶部<br>堆栈从较高的内存地址向较低的内存地址向下增长</p>\n<p>CPU在PSW中有一个模式位，用于定义程序的执行能力</p>\n<p>内核模式：执行任何指令<br>用户模式： 执行指令的子集<br>仅在内核模式下执行的指令称为特权指令</p>\n<p>在用户模式下运行的应用程序要执行系统调用，用户程序需要执行一条称为陷阱<code>trap</code>的指令<br>它所做的只是跳转到内核，同时将特权级别提升到内核模式<br>我们需要返回到进行系统调用的程序，但同时，需要清除模式位（将特权级别降低回用户模式）同样，操作系统依赖于CPU的一些帮助，通过使用另一条指令，我们称之为从陷阱返回<code>return-from-trap</code></p>\n<p>内核在内核模式下启动时会设置一个陷阱表，然后让硬件知道它在哪里。陷阱表也称为中断表&#x2F;向量，因为它也有硬件中断和异常的条目！事实证明，trap 只是陷阱表中的条目之一。系统调用的实际类型称为系统调用号，它可以存储在内核堆栈中的众所周知的位置</p>\n<p>中断是异步的（在任意时间出现） ：</p>\n<ul>\n<li>由硬件事件引起 </li>\n<li>定时器中断 属于外中断</li>\n<li>I&#x2F;O（如键盘或磁盘）中断 属于外中断</li>\n<li>由用户模式程序中的编程错误引起。属于内终端。示例：算术异常（除以零）尝试访问程序无法控制的内存尝试执行特权指令</li>\n</ul>\n<p>中断的示例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102223922.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224053.png\" alt=\"image.png\"></p>\n<p>内中断和外中断：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224829.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>老师声音小，需要坐前排</p>\n<p>40分平时作业（gitee），60分期末</p>\n<p>TODO：</p>\n<ul>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 加入gitee仓库</li>\n</ul>\n<hr>\n<p>虚拟换CPU：<br>操作系统需要以某种方式在看似同时运行的许多程序之间共享物理CPU 基本思想：运行一个程序一小会儿，然后切换到运行另一个程序，依此类推 分时CPU - 虚拟化就实现了！</p>\n<p>都具有用于在内存和寄存器之间移动项目的加载和存储指令 </p>\n<ul>\n<li>将位于内存中某个地址的字加载到寄存器中 </li>\n<li>将寄存器的内容存储到位于内存中某个地址的字 </li>\n<li>许多用于比较和的指令组合寄存器中的值并将结果放入寄存器</li>\n</ul>\n<p>CPU的结构：</p>\n<ul>\n<li>程序计数器（Program Counter，PC）：保存下一条指令的内存地址</li>\n<li>指令寄存器（Instruction Register，IR）：保存当前正在执行的指令</li>\n<li>通用寄存器（General Registers，R1…Rn）：保存执行环境：临时结果</li>\n<li>算术逻辑单元（Arithmetic Logic Unit，ALU）：执行算术函数和逻辑运行</li>\n<li>堆栈指针（The Stack Pointer，SP）：保存堆栈的内存地址，以及每个活动函数的参数和局部变量的帧（frame）</li>\n<li>程序状态字（The Program Status Word，PSW）：包含一些重要的控制位</li>\n</ul>\n<p>CPU 所做的就是获取&#x2F;解码&#x2F;执行获取 PC 指向的下一条指令，对其进行解码以查找其类型，然后操作数重复执行它</p>\n<p>寄存器r15指向执行栈的顶部<br>堆栈从较高的内存地址向较低的内存地址向下增长</p>\n<p>CPU在PSW中有一个模式位，用于定义程序的执行能力</p>\n<p>内核模式：执行任何指令<br>用户模式： 执行指令的子集<br>仅在内核模式下执行的指令称为特权指令</p>\n<p>在用户模式下运行的应用程序要执行系统调用，用户程序需要执行一条称为陷阱<code>trap</code>的指令<br>它所做的只是跳转到内核，同时将特权级别提升到内核模式<br>我们需要返回到进行系统调用的程序，但同时，需要清除模式位（将特权级别降低回用户模式）同样，操作系统依赖于CPU的一些帮助，通过使用另一条指令，我们称之为从陷阱返回<code>return-from-trap</code></p>\n<p>内核在内核模式下启动时会设置一个陷阱表，然后让硬件知道它在哪里。陷阱表也称为中断表&#x2F;向量，因为它也有硬件中断和异常的条目！事实证明，trap 只是陷阱表中的条目之一。系统调用的实际类型称为系统调用号，它可以存储在内核堆栈中的众所周知的位置</p>\n<p>中断是异步的（在任意时间出现） ：</p>\n<ul>\n<li>由硬件事件引起 </li>\n<li>定时器中断 属于外中断</li>\n<li>I&#x2F;O（如键盘或磁盘）中断 属于外中断</li>\n<li>由用户模式程序中的编程错误引起。属于内终端。示例：算术异常（除以零）尝试访问程序无法控制的内存尝试执行特权指令</li>\n</ul>\n<p>中断的示例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102223922.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224053.png\" alt=\"image.png\"></p>\n<p>内中断和外中断：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102224829.png\" alt=\"image.png\"></p>\n"},{"title":"使用E-R模型的数据库设计","date":"2023-10-30T02:48:52.676Z","_content":"### 6.1 Overview of Design Process\n分析用户需求：\n- 什么数据需要被存储\n- 什么操作和事务（transaction）被执行，例如插入、删除和更新\n\n数据库设计的三种模式：\n- 概念设计\n- 逻辑设计，在逻辑级别和视图级别\n- 物理设计，在物理层面\n\n数据库设计的阶段：\n- 需求分析\n- 概念设计\n- 逻辑模型设计\n- 物理模型设计\n\nDBAS设计过程：\n生命周期包括五个阶段：\n- 项目规划、需求分析、系统设计、实现与部署、运行管理与维护\n根据软件组成和各自功能，分为三条设计主线：\n数据组织与存储设计、数据访问与处理设计、应用设计\n分别用于设计数据库、数据库事务和应用程序\n\n### 6.2 The Entity-Relationship Model\n建立一个图来表示整个数据库的特征\nER模型包括三个概念：\n- entity sets（实体集）\n- relationship sets（关系集）\n- attributes（属性）\nER模型使用有关联的图表表示，即ER图，它以图形的方式表示数据库的整体逻辑结构\n\n\n#### 6.2.1 Entity Sets（实体集）\n实体是一个存在的对象，区别于其他对象。实体通过一组属性表示。\n实体集是一些相同类型实体的集合。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101224017.png)\n\n#### 6.2.2 Relationship Sets（关系集）\n关系是多个实体之间的关联\n实体集 E1， E2， ...， En 参与关系集 R\n##### Attributes with Relationship Sets\n\n关系集在E-R图中用菱形表示，菱形通过线条连接到多个不同的实体集（矩形）。属性也可以与关系集合联系。\n在教师和学生之间设置的顾问关系可能具有属性日期，该属性跟踪学生何时开始与顾问关联\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101225949.png)\n\n##### Role in Relationship Sets\n实体在关系中扮演的功能称为该实体的角色\n在E-R图中角色一般标注在菱形和矩形之间的连线上\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101230806.png)\n\n##### Degree of a Relationship Set（关系集的度） vs Non-binary Relationship Sets（非二元关系集）\n二元关系涉及两个实体集（或二度）\n在某些情况下，我们将关系表示为非二元关系。 具有三元关系的 E-R 图示例：学生在教师的指导下从事研究项目，教师、学生和项目之间的关系proj_guide是三元的\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231144.png)\n\n\n### 6.3 Complex Attributes\n每个属性都有一个可取值的集合，称为该属性的域（domain），或者值集（value set）\n属性类型： \n- 简单属性和复合属性 \n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231709.png)\n\n- 单值和多值属性\n- 派生属性: 可以从其他属性计算\n\n属性的 Null 值表示:\n- 该属性不适用于实体，不存在\n- 该属性的值存在，但为“未知”\n\n冗余属性：假设我们有实体集： 讲师：ID、姓名、**dept_name**、薪水 ；部门：**dept_name**、建筑、预算 \n模型：每个教师都有一个使用关系集的关联部门inst_dept 属性dept_name出现在两个实体集中。 由于它是实体集部门的主键，因此它复制关系中存在的信息，因此在实体集讲师中是多余的。 但是：当转换回表时，在某些情况下会重新引入该属性\n\n### 6.4 Mapping  Cardinalities Constrants（映射基数约束）\n语义/约束：作为完整性约束，以保持 DBS 的一致性\n##### Mapping Cardinalities（映射基数）\n表示另一个实体可以通过关系集关联到的实体数\n对于二元关系集 R，从 A 到 B 映射基数必须如下所示：\n- 一对一\n- 一对多\n- 多对一\n- 多对多\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233304.png)\n\n有些元素可能不被映射到其他集合\n\n在 ER 图中表示基数约束：\n- 有→的代表一，没有箭头为多\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233549.png)\n- 用双线表示至少有一个实体相关联，单线则表示可以有零个或多个\n\n##### Participate Constraints（参与约束）\n实体 E 在关系 R 中的参与是**完全的（total）**，如果 E 中的每个实体都参与 R 中的至少一个关系：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233933.png)\n实体 E 在关系 R 中的参与是**部分的（partial）**，如果 E 中的某些实体可能不参与 R 中的任何关系：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234007.png)\n##### Cardinality Limits for Participation（参与的基数限制）\n基数限制（参与的基数界限）用于表示对参与的定量约束\ne.g.\n- 每个instructor最多可以指导多个student, 最少可以指导0个student\n- 每个student最少有1个指导instructor，最多也只有1个指导instructor\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234344.png)\n\\*表示无限制\nA参与R的基数下界$l_{A}$和上界$h_{A}$\n- A中的每个实体a通过R关联了最少$l_{A}$个、最多$h_{A}$个B中实体b\n- $l_{A}$ ：对A中的每个实体a, B中至少有$l_{A}$个实体b通过R与其对应/关联\n- $h_{A}$：对A中的每个实体a, B中至多有$h_{A}$个实体b通过R与其对应/关联\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234446.png)\n最小值 lA = 0：A 是 R 的部分参与 \n最小值 lA > 0：A 为 R 的总参与，相当于双线\n最大值 * 表示无限制\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101235347.png)\n\n我们最多允许一个箭头从三元（或更大程度）关系中表示基数约束  如果有多个箭头，则有两种定义含义的方法。 \n- A、B 和 C 之间的三元关系 R，带有指向 B 和 C 的箭头可能意味着 每个 A 实体都与 B 和 C 中的唯一实体相关联，或者\n- （A， B） 中的每对实体都与一个唯一的 C 实体相关联，并且每一对（A、C）都与一个唯一的 B 实体相关联\n\n### 6.5 Primary Keys\n键是一组属性\n键包括 superkey （超键）， candidate key （候选键）， primary key （主键）\n\n#### Keys For Entity Sets\n实体集的superkey是一个或多个属性的集合，其值唯一地确定实体集中的每个实体\nsuperkey可能包含无关属性，候选键是最小的超键\n\n主键是被选为标识实体集中实体的主要方法的候选键\n尽管可能存在多个候选键，但选择其中一个候选键作为主键\n\n#### Weak Entity Sets\n没有主键的实体集称为弱实体集\n弱实体集是其存在依赖于另一个实体（称为其标识实体，identifying entity）的实体集\n不是将主键与弱实体相关联，而是使用标识实体以及称为鉴别器(discriminator)的额外属性来唯一标识弱实体\n每个弱实体都必须与一个标识实体相关联，弱实体集是依赖于标识实体集的存在。\n标识实体集是拥有弱实体集。 将弱实体集与标识实体集相关联的关系称为标识关系(identifying relationship)\n在 E-R 图中，弱实体集通过双矩形进行描述。 我们用虚线强调弱实体集的判别器。 将弱实体集连接到标识强实体集的关系集由双菱形表示。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106103032.png)\n弱实体集的鉴别器（或部分键）是一组属性，用于区分弱实体集中依赖于特定强实体的所有实体 \n例如sec_id、学期、年份 弱实体集的鉴别器用虚线下划线\n弱实体集的主键由弱实体集存在依赖的强实体集的主键加上弱实体集的判别器组成 弱实体集 E1 的主键 = 鉴别器 ∪ primary_key（E2）\n弱实体集 E1 具有多个标识实体集 E2、E3、..、En，弱实体集 E1 的主键 = 鉴别器 ∪ primary_key（E2） ∪...primary_key（En） .\n\n### 6.6 Removing Redundant Attributes\n两个实体集存在共同属性构成关系，该属性不作为主键时的实体集中属性是冗余的需要删除。转换成表时，属性需要重新引入\n\n### 6.7 Reducing E-R Diagrams to Relation Schemas\n将 E-R 图转换为表格式是从 E-R 图派生关系数据库设计的基础 \n符合 E-R 图的数据库可以由架构/表的集合表示 \n实体集和关系集可以统一表示为表示数据库内容的关系架构。 对于每个实体集或关系集，都有一个与实体集或关系集对应的唯一架构。 每个表都有许多列，对应于它们的属性\n\n强实体集简化为具有相同属性的架构\n弱实体集将变成一个表，其中包含标识强实体集的主键的列\n#### Representation of Entity Sets with Multivalued Attributes\n实体 E 的多值属性 M 由单独的架构 EM 表示 \n架构 EM 具有与 E 的主键对应的属性和对应于多值属性 M 的属性\n\n#### Representing Relationship Sets\n将关系集简化为表在很大程度上依赖于映射基数约束和总/部分约束 \n多对多关系集表示为一个表，其中包含两个参与实体集的主键的列，以及关系集的任何描述性属性\n可以通过向多端添加一个额外的属性来表示在多端上合计的多对一和一对多关系集，其中包含一端的主键\n\n该关系被简化为一个独立的表，以避免表中的空值\n\n对于一对一关系集，可以选择任何一方作为多方 可以将额外的属性添加到与两个实体集对应的任一表中\n\n### 6.8 Extended E-R Features\n面向对象 （OO） E-R ：\nspecialization （特化，特殊化，例化） generalization （概括化，泛化，普遍化） attributes inheritance （属性继承）\n\n#### Specialization\n自上而下的设计过程： 我们在实体集中指定有别于其他实体的子组 这些子分组成为较低级别的实体集，这些实体集具有属性或参与适用于较高级别实体集的关系\n属性继承：较低级别的实体集继承它所链接到的较高级别实体集的所有属性和关系参与，较高级别实体的属性和关系可以应用于其所有较低级别的实体\n\n#### Generalization\n自下而上的设计过程\n将多个共享相同功能的实体集组合成更高级别的实体集\n超类和子类：对于实体集合 A 和 B，如果 A 是 B 的泛化，即 B 是 A 的特化， 则 A 是 B 的超类，B 是 A 的子类\n\n#### Completeness Constraint\n完整性约束\n指定高级实体集中的实体是否必须至少属于泛化中的一个较低级别的实体集 \n- total：实体必须属于较低级别的实体集之一 例如，学生泛化，所有实体必须是研究生或本科生 \n- partial：实体不必属于较低级别的实体集之一 例如，人员专业化 部分泛化是默认设置。 \n我们可以通过在图中添加关键字 total 并在关键字到相应的空心箭头（用于全面泛化）或空心箭头集（用于重叠泛化）来指定总泛化。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114347.png)\n\n关于一个高级实体是否可能属于单个专业化中集的多个较低级别实体的约束,\n- disjoint：一个高级实体只能属于一个较低级别的实体集，即 L-entity-set-1 ∩ L-entity-set-2 = Φ \n- overlapping：一个高级实体可以属于多个较低级别的实体集 L-实体集-1∩L-实体集-2 ≠ Φ\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114410.png)\n\n#### Representing Specialization via Schemas\n1. 形成更高级别实体的架构 为每个较低级别的实体集形成一个架构，包括较高级别实体集的主键和本地属性![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114529.png)\n2. 为每个实体集形成一个架构，其中包含所有本地属性和继承属性![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114548.png)\n3. 如果泛化/特殊化是不相交的且完整的，则创建两个表\n\n#### Aggregation\n通过聚合消除冗余 \n将关系视为抽象实体 \n允许关系之间的关系 \n将关系抽象为新实体\n\n### 6.9 Entity-relationship Design Issues\nE-R 图中的常见错误 \n- 实体与属性的使用 \n- 实体集与关系集的使用 \n- 二元关系与非二元关系\n\n规则1.不要将一个实体集的主键用作另一个实体集的属性（为了表示这两个实体集之间的隐式关联），最好使用关系集来显式显示这种关联\n规则2.如果 E 参与 R，则不要将主键 （E） 指定为 R 的属性，以避免信息冗余\n规则3.可能的准则是指定一个关系集来描述实体之间发生的操作\n规则4.首选使用二元关系集，用多个二元关系集替换非二元关系集 尽管可以用许多不同的二元关系集替换任何非二元关系集（n-ary，对于 n > 2）关系集，但 n 元关系集显示得更清楚。 一些看似非二元关系的关系可能使用二元关系来更好地表示。但有些关系天生是非二元的\n\n通常，任何非二元关系都可以通过创建人工实体集 E 来使用二元关系来表示\n\n#### Placement of Relationship Attributes\n关系集 R 的属性可能与其参与实体集之一相关联 \n对于关系集 R 之间的属性 attr-A （R ⊆ E1 ╳ E2 ），如果 R 的基数为 \n一对一：attr-A 可以指定为 E1 或 E2 的属性，而不是关系集 R 从 E1 到 E2 的\n一对多：attr-A 只能指定为实体集 E2（多边）的属性，而不是关系集 R 的属性","source":"_posts/Notes/课程/大三（上）/数据库/使用E-R模型的数据库设计.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 数据库\ntags:\n  - 数据库\ntitle: 使用E-R模型的数据库设计\ndate:\n---\n### 6.1 Overview of Design Process\n分析用户需求：\n- 什么数据需要被存储\n- 什么操作和事务（transaction）被执行，例如插入、删除和更新\n\n数据库设计的三种模式：\n- 概念设计\n- 逻辑设计，在逻辑级别和视图级别\n- 物理设计，在物理层面\n\n数据库设计的阶段：\n- 需求分析\n- 概念设计\n- 逻辑模型设计\n- 物理模型设计\n\nDBAS设计过程：\n生命周期包括五个阶段：\n- 项目规划、需求分析、系统设计、实现与部署、运行管理与维护\n根据软件组成和各自功能，分为三条设计主线：\n数据组织与存储设计、数据访问与处理设计、应用设计\n分别用于设计数据库、数据库事务和应用程序\n\n### 6.2 The Entity-Relationship Model\n建立一个图来表示整个数据库的特征\nER模型包括三个概念：\n- entity sets（实体集）\n- relationship sets（关系集）\n- attributes（属性）\nER模型使用有关联的图表表示，即ER图，它以图形的方式表示数据库的整体逻辑结构\n\n\n#### 6.2.1 Entity Sets（实体集）\n实体是一个存在的对象，区别于其他对象。实体通过一组属性表示。\n实体集是一些相同类型实体的集合。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101224017.png)\n\n#### 6.2.2 Relationship Sets（关系集）\n关系是多个实体之间的关联\n实体集 E1， E2， ...， En 参与关系集 R\n##### Attributes with Relationship Sets\n\n关系集在E-R图中用菱形表示，菱形通过线条连接到多个不同的实体集（矩形）。属性也可以与关系集合联系。\n在教师和学生之间设置的顾问关系可能具有属性日期，该属性跟踪学生何时开始与顾问关联\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101225949.png)\n\n##### Role in Relationship Sets\n实体在关系中扮演的功能称为该实体的角色\n在E-R图中角色一般标注在菱形和矩形之间的连线上\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101230806.png)\n\n##### Degree of a Relationship Set（关系集的度） vs Non-binary Relationship Sets（非二元关系集）\n二元关系涉及两个实体集（或二度）\n在某些情况下，我们将关系表示为非二元关系。 具有三元关系的 E-R 图示例：学生在教师的指导下从事研究项目，教师、学生和项目之间的关系proj_guide是三元的\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231144.png)\n\n\n### 6.3 Complex Attributes\n每个属性都有一个可取值的集合，称为该属性的域（domain），或者值集（value set）\n属性类型： \n- 简单属性和复合属性 \n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231709.png)\n\n- 单值和多值属性\n- 派生属性: 可以从其他属性计算\n\n属性的 Null 值表示:\n- 该属性不适用于实体，不存在\n- 该属性的值存在，但为“未知”\n\n冗余属性：假设我们有实体集： 讲师：ID、姓名、**dept_name**、薪水 ；部门：**dept_name**、建筑、预算 \n模型：每个教师都有一个使用关系集的关联部门inst_dept 属性dept_name出现在两个实体集中。 由于它是实体集部门的主键，因此它复制关系中存在的信息，因此在实体集讲师中是多余的。 但是：当转换回表时，在某些情况下会重新引入该属性\n\n### 6.4 Mapping  Cardinalities Constrants（映射基数约束）\n语义/约束：作为完整性约束，以保持 DBS 的一致性\n##### Mapping Cardinalities（映射基数）\n表示另一个实体可以通过关系集关联到的实体数\n对于二元关系集 R，从 A 到 B 映射基数必须如下所示：\n- 一对一\n- 一对多\n- 多对一\n- 多对多\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233304.png)\n\n有些元素可能不被映射到其他集合\n\n在 ER 图中表示基数约束：\n- 有→的代表一，没有箭头为多\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233549.png)\n- 用双线表示至少有一个实体相关联，单线则表示可以有零个或多个\n\n##### Participate Constraints（参与约束）\n实体 E 在关系 R 中的参与是**完全的（total）**，如果 E 中的每个实体都参与 R 中的至少一个关系：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233933.png)\n实体 E 在关系 R 中的参与是**部分的（partial）**，如果 E 中的某些实体可能不参与 R 中的任何关系：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234007.png)\n##### Cardinality Limits for Participation（参与的基数限制）\n基数限制（参与的基数界限）用于表示对参与的定量约束\ne.g.\n- 每个instructor最多可以指导多个student, 最少可以指导0个student\n- 每个student最少有1个指导instructor，最多也只有1个指导instructor\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234344.png)\n\\*表示无限制\nA参与R的基数下界$l_{A}$和上界$h_{A}$\n- A中的每个实体a通过R关联了最少$l_{A}$个、最多$h_{A}$个B中实体b\n- $l_{A}$ ：对A中的每个实体a, B中至少有$l_{A}$个实体b通过R与其对应/关联\n- $h_{A}$：对A中的每个实体a, B中至多有$h_{A}$个实体b通过R与其对应/关联\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234446.png)\n最小值 lA = 0：A 是 R 的部分参与 \n最小值 lA > 0：A 为 R 的总参与，相当于双线\n最大值 * 表示无限制\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101235347.png)\n\n我们最多允许一个箭头从三元（或更大程度）关系中表示基数约束  如果有多个箭头，则有两种定义含义的方法。 \n- A、B 和 C 之间的三元关系 R，带有指向 B 和 C 的箭头可能意味着 每个 A 实体都与 B 和 C 中的唯一实体相关联，或者\n- （A， B） 中的每对实体都与一个唯一的 C 实体相关联，并且每一对（A、C）都与一个唯一的 B 实体相关联\n\n### 6.5 Primary Keys\n键是一组属性\n键包括 superkey （超键）， candidate key （候选键）， primary key （主键）\n\n#### Keys For Entity Sets\n实体集的superkey是一个或多个属性的集合，其值唯一地确定实体集中的每个实体\nsuperkey可能包含无关属性，候选键是最小的超键\n\n主键是被选为标识实体集中实体的主要方法的候选键\n尽管可能存在多个候选键，但选择其中一个候选键作为主键\n\n#### Weak Entity Sets\n没有主键的实体集称为弱实体集\n弱实体集是其存在依赖于另一个实体（称为其标识实体，identifying entity）的实体集\n不是将主键与弱实体相关联，而是使用标识实体以及称为鉴别器(discriminator)的额外属性来唯一标识弱实体\n每个弱实体都必须与一个标识实体相关联，弱实体集是依赖于标识实体集的存在。\n标识实体集是拥有弱实体集。 将弱实体集与标识实体集相关联的关系称为标识关系(identifying relationship)\n在 E-R 图中，弱实体集通过双矩形进行描述。 我们用虚线强调弱实体集的判别器。 将弱实体集连接到标识强实体集的关系集由双菱形表示。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106103032.png)\n弱实体集的鉴别器（或部分键）是一组属性，用于区分弱实体集中依赖于特定强实体的所有实体 \n例如sec_id、学期、年份 弱实体集的鉴别器用虚线下划线\n弱实体集的主键由弱实体集存在依赖的强实体集的主键加上弱实体集的判别器组成 弱实体集 E1 的主键 = 鉴别器 ∪ primary_key（E2）\n弱实体集 E1 具有多个标识实体集 E2、E3、..、En，弱实体集 E1 的主键 = 鉴别器 ∪ primary_key（E2） ∪...primary_key（En） .\n\n### 6.6 Removing Redundant Attributes\n两个实体集存在共同属性构成关系，该属性不作为主键时的实体集中属性是冗余的需要删除。转换成表时，属性需要重新引入\n\n### 6.7 Reducing E-R Diagrams to Relation Schemas\n将 E-R 图转换为表格式是从 E-R 图派生关系数据库设计的基础 \n符合 E-R 图的数据库可以由架构/表的集合表示 \n实体集和关系集可以统一表示为表示数据库内容的关系架构。 对于每个实体集或关系集，都有一个与实体集或关系集对应的唯一架构。 每个表都有许多列，对应于它们的属性\n\n强实体集简化为具有相同属性的架构\n弱实体集将变成一个表，其中包含标识强实体集的主键的列\n#### Representation of Entity Sets with Multivalued Attributes\n实体 E 的多值属性 M 由单独的架构 EM 表示 \n架构 EM 具有与 E 的主键对应的属性和对应于多值属性 M 的属性\n\n#### Representing Relationship Sets\n将关系集简化为表在很大程度上依赖于映射基数约束和总/部分约束 \n多对多关系集表示为一个表，其中包含两个参与实体集的主键的列，以及关系集的任何描述性属性\n可以通过向多端添加一个额外的属性来表示在多端上合计的多对一和一对多关系集，其中包含一端的主键\n\n该关系被简化为一个独立的表，以避免表中的空值\n\n对于一对一关系集，可以选择任何一方作为多方 可以将额外的属性添加到与两个实体集对应的任一表中\n\n### 6.8 Extended E-R Features\n面向对象 （OO） E-R ：\nspecialization （特化，特殊化，例化） generalization （概括化，泛化，普遍化） attributes inheritance （属性继承）\n\n#### Specialization\n自上而下的设计过程： 我们在实体集中指定有别于其他实体的子组 这些子分组成为较低级别的实体集，这些实体集具有属性或参与适用于较高级别实体集的关系\n属性继承：较低级别的实体集继承它所链接到的较高级别实体集的所有属性和关系参与，较高级别实体的属性和关系可以应用于其所有较低级别的实体\n\n#### Generalization\n自下而上的设计过程\n将多个共享相同功能的实体集组合成更高级别的实体集\n超类和子类：对于实体集合 A 和 B，如果 A 是 B 的泛化，即 B 是 A 的特化， 则 A 是 B 的超类，B 是 A 的子类\n\n#### Completeness Constraint\n完整性约束\n指定高级实体集中的实体是否必须至少属于泛化中的一个较低级别的实体集 \n- total：实体必须属于较低级别的实体集之一 例如，学生泛化，所有实体必须是研究生或本科生 \n- partial：实体不必属于较低级别的实体集之一 例如，人员专业化 部分泛化是默认设置。 \n我们可以通过在图中添加关键字 total 并在关键字到相应的空心箭头（用于全面泛化）或空心箭头集（用于重叠泛化）来指定总泛化。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114347.png)\n\n关于一个高级实体是否可能属于单个专业化中集的多个较低级别实体的约束,\n- disjoint：一个高级实体只能属于一个较低级别的实体集，即 L-entity-set-1 ∩ L-entity-set-2 = Φ \n- overlapping：一个高级实体可以属于多个较低级别的实体集 L-实体集-1∩L-实体集-2 ≠ Φ\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114410.png)\n\n#### Representing Specialization via Schemas\n1. 形成更高级别实体的架构 为每个较低级别的实体集形成一个架构，包括较高级别实体集的主键和本地属性![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114529.png)\n2. 为每个实体集形成一个架构，其中包含所有本地属性和继承属性![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114548.png)\n3. 如果泛化/特殊化是不相交的且完整的，则创建两个表\n\n#### Aggregation\n通过聚合消除冗余 \n将关系视为抽象实体 \n允许关系之间的关系 \n将关系抽象为新实体\n\n### 6.9 Entity-relationship Design Issues\nE-R 图中的常见错误 \n- 实体与属性的使用 \n- 实体集与关系集的使用 \n- 二元关系与非二元关系\n\n规则1.不要将一个实体集的主键用作另一个实体集的属性（为了表示这两个实体集之间的隐式关联），最好使用关系集来显式显示这种关联\n规则2.如果 E 参与 R，则不要将主键 （E） 指定为 R 的属性，以避免信息冗余\n规则3.可能的准则是指定一个关系集来描述实体之间发生的操作\n规则4.首选使用二元关系集，用多个二元关系集替换非二元关系集 尽管可以用许多不同的二元关系集替换任何非二元关系集（n-ary，对于 n > 2）关系集，但 n 元关系集显示得更清楚。 一些看似非二元关系的关系可能使用二元关系来更好地表示。但有些关系天生是非二元的\n\n通常，任何非二元关系都可以通过创建人工实体集 E 来使用二元关系来表示\n\n#### Placement of Relationship Attributes\n关系集 R 的属性可能与其参与实体集之一相关联 \n对于关系集 R 之间的属性 attr-A （R ⊆ E1 ╳ E2 ），如果 R 的基数为 \n一对一：attr-A 可以指定为 E1 或 E2 的属性，而不是关系集 R 从 E1 到 E2 的\n一对多：attr-A 只能指定为实体集 E2（多边）的属性，而不是关系集 R 的属性","slug":"Notes/课程/大三（上）/数据库/使用E-R模型的数据库设计","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr124000vvw8c9kmt6st8","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"6-1-Overview-of-Design-Process\"><a href=\"#6-1-Overview-of-Design-Process\" class=\"headerlink\" title=\"6.1 Overview of Design Process\"></a>6.1 Overview of Design Process</h3><p>分析用户需求：</p>\n<ul>\n<li>什么数据需要被存储</li>\n<li>什么操作和事务（transaction）被执行，例如插入、删除和更新</li>\n</ul>\n<p>数据库设计的三种模式：</p>\n<ul>\n<li>概念设计</li>\n<li>逻辑设计，在逻辑级别和视图级别</li>\n<li>物理设计，在物理层面</li>\n</ul>\n<p>数据库设计的阶段：</p>\n<ul>\n<li>需求分析</li>\n<li>概念设计</li>\n<li>逻辑模型设计</li>\n<li>物理模型设计</li>\n</ul>\n<p>DBAS设计过程：<br>生命周期包括五个阶段：</p>\n<ul>\n<li>项目规划、需求分析、系统设计、实现与部署、运行管理与维护<br>根据软件组成和各自功能，分为三条设计主线：<br>数据组织与存储设计、数据访问与处理设计、应用设计<br>分别用于设计数据库、数据库事务和应用程序</li>\n</ul>\n<h3 id=\"6-2-The-Entity-Relationship-Model\"><a href=\"#6-2-The-Entity-Relationship-Model\" class=\"headerlink\" title=\"6.2 The Entity-Relationship Model\"></a>6.2 The Entity-Relationship Model</h3><p>建立一个图来表示整个数据库的特征<br>ER模型包括三个概念：</p>\n<ul>\n<li>entity sets（实体集）</li>\n<li>relationship sets（关系集）</li>\n<li>attributes（属性）<br>ER模型使用有关联的图表表示，即ER图，它以图形的方式表示数据库的整体逻辑结构</li>\n</ul>\n<h4 id=\"6-2-1-Entity-Sets（实体集）\"><a href=\"#6-2-1-Entity-Sets（实体集）\" class=\"headerlink\" title=\"6.2.1 Entity Sets（实体集）\"></a>6.2.1 Entity Sets（实体集）</h4><p>实体是一个存在的对象，区别于其他对象。实体通过一组属性表示。<br>实体集是一些相同类型实体的集合。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101224017.png\" alt=\"image.png\"></p>\n<h4 id=\"6-2-2-Relationship-Sets（关系集）\"><a href=\"#6-2-2-Relationship-Sets（关系集）\" class=\"headerlink\" title=\"6.2.2 Relationship Sets（关系集）\"></a>6.2.2 Relationship Sets（关系集）</h4><p>关系是多个实体之间的关联<br>实体集 E1， E2， …， En 参与关系集 R</p>\n<h5 id=\"Attributes-with-Relationship-Sets\"><a href=\"#Attributes-with-Relationship-Sets\" class=\"headerlink\" title=\"Attributes with Relationship Sets\"></a>Attributes with Relationship Sets</h5><p>关系集在E-R图中用菱形表示，菱形通过线条连接到多个不同的实体集（矩形）。属性也可以与关系集合联系。<br>在教师和学生之间设置的顾问关系可能具有属性日期，该属性跟踪学生何时开始与顾问关联<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101225949.png\" alt=\"image.png\"></p>\n<h5 id=\"Role-in-Relationship-Sets\"><a href=\"#Role-in-Relationship-Sets\" class=\"headerlink\" title=\"Role in Relationship Sets\"></a>Role in Relationship Sets</h5><p>实体在关系中扮演的功能称为该实体的角色<br>在E-R图中角色一般标注在菱形和矩形之间的连线上<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101230806.png\" alt=\"image.png\"></p>\n<h5 id=\"Degree-of-a-Relationship-Set（关系集的度）-vs-Non-binary-Relationship-Sets（非二元关系集）\"><a href=\"#Degree-of-a-Relationship-Set（关系集的度）-vs-Non-binary-Relationship-Sets（非二元关系集）\" class=\"headerlink\" title=\"Degree of a Relationship Set（关系集的度） vs Non-binary Relationship Sets（非二元关系集）\"></a>Degree of a Relationship Set（关系集的度） vs Non-binary Relationship Sets（非二元关系集）</h5><p>二元关系涉及两个实体集（或二度）<br>在某些情况下，我们将关系表示为非二元关系。 具有三元关系的 E-R 图示例：学生在教师的指导下从事研究项目，教师、学生和项目之间的关系proj_guide是三元的<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231144.png\" alt=\"image.png\"></p>\n<h3 id=\"6-3-Complex-Attributes\"><a href=\"#6-3-Complex-Attributes\" class=\"headerlink\" title=\"6.3 Complex Attributes\"></a>6.3 Complex Attributes</h3><p>每个属性都有一个可取值的集合，称为该属性的域（domain），或者值集（value set）<br>属性类型： </p>\n<ul>\n<li><p>简单属性和复合属性<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231709.png\" alt=\"image.png\"></p>\n</li>\n<li><p>单值和多值属性</p>\n</li>\n<li><p>派生属性: 可以从其他属性计算</p>\n</li>\n</ul>\n<p>属性的 Null 值表示:</p>\n<ul>\n<li>该属性不适用于实体，不存在</li>\n<li>该属性的值存在，但为“未知”</li>\n</ul>\n<p>冗余属性：假设我们有实体集： 讲师：ID、姓名、<strong>dept_name</strong>、薪水 ；部门：<strong>dept_name</strong>、建筑、预算<br>模型：每个教师都有一个使用关系集的关联部门inst_dept 属性dept_name出现在两个实体集中。 由于它是实体集部门的主键，因此它复制关系中存在的信息，因此在实体集讲师中是多余的。 但是：当转换回表时，在某些情况下会重新引入该属性</p>\n<h3 id=\"6-4-Mapping-Cardinalities-Constrants（映射基数约束）\"><a href=\"#6-4-Mapping-Cardinalities-Constrants（映射基数约束）\" class=\"headerlink\" title=\"6.4 Mapping  Cardinalities Constrants（映射基数约束）\"></a>6.4 Mapping  Cardinalities Constrants（映射基数约束）</h3><p>语义&#x2F;约束：作为完整性约束，以保持 DBS 的一致性</p>\n<h5 id=\"Mapping-Cardinalities（映射基数）\"><a href=\"#Mapping-Cardinalities（映射基数）\" class=\"headerlink\" title=\"Mapping Cardinalities（映射基数）\"></a>Mapping Cardinalities（映射基数）</h5><p>表示另一个实体可以通过关系集关联到的实体数<br>对于二元关系集 R，从 A 到 B 映射基数必须如下所示：</p>\n<ul>\n<li>一对一</li>\n<li>一对多</li>\n<li>多对一</li>\n<li>多对多<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233304.png\" alt=\"image.png\"></li>\n</ul>\n<p>有些元素可能不被映射到其他集合</p>\n<p>在 ER 图中表示基数约束：</p>\n<ul>\n<li>有→的代表一，没有箭头为多<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233549.png\" alt=\"image.png\"></li>\n<li>用双线表示至少有一个实体相关联，单线则表示可以有零个或多个</li>\n</ul>\n<h5 id=\"Participate-Constraints（参与约束）\"><a href=\"#Participate-Constraints（参与约束）\" class=\"headerlink\" title=\"Participate Constraints（参与约束）\"></a>Participate Constraints（参与约束）</h5><p>实体 E 在关系 R 中的参与是<strong>完全的（total）</strong>，如果 E 中的每个实体都参与 R 中的至少一个关系：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233933.png\" alt=\"image.png\"><br>实体 E 在关系 R 中的参与是<strong>部分的（partial）</strong>，如果 E 中的某些实体可能不参与 R 中的任何关系：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234007.png\" alt=\"image.png\"></p>\n<h5 id=\"Cardinality-Limits-for-Participation（参与的基数限制）\"><a href=\"#Cardinality-Limits-for-Participation（参与的基数限制）\" class=\"headerlink\" title=\"Cardinality Limits for Participation（参与的基数限制）\"></a>Cardinality Limits for Participation（参与的基数限制）</h5><p>基数限制（参与的基数界限）用于表示对参与的定量约束<br>e.g.</p>\n<ul>\n<li>每个instructor最多可以指导多个student, 最少可以指导0个student</li>\n<li>每个student最少有1个指导instructor，最多也只有1个指导instructor<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234344.png\" alt=\"image.png\"><br>*表示无限制<br>A参与R的基数下界$l_{A}$和上界$h_{A}$</li>\n<li>A中的每个实体a通过R关联了最少$l_{A}$个、最多$h_{A}$个B中实体b</li>\n<li>$l_{A}$ ：对A中的每个实体a, B中至少有$l_{A}$个实体b通过R与其对应&#x2F;关联</li>\n<li>$h_{A}$：对A中的每个实体a, B中至多有$h_{A}$个实体b通过R与其对应&#x2F;关联<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234446.png\" alt=\"image.png\"><br>最小值 lA &#x3D; 0：A 是 R 的部分参与<br>最小值 lA &gt; 0：A 为 R 的总参与，相当于双线<br>最大值 * 表示无限制</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101235347.png\" alt=\"image.png\"></p>\n<p>我们最多允许一个箭头从三元（或更大程度）关系中表示基数约束  如果有多个箭头，则有两种定义含义的方法。 </p>\n<ul>\n<li>A、B 和 C 之间的三元关系 R，带有指向 B 和 C 的箭头可能意味着 每个 A 实体都与 B 和 C 中的唯一实体相关联，或者</li>\n<li>（A， B） 中的每对实体都与一个唯一的 C 实体相关联，并且每一对（A、C）都与一个唯一的 B 实体相关联</li>\n</ul>\n<h3 id=\"6-5-Primary-Keys\"><a href=\"#6-5-Primary-Keys\" class=\"headerlink\" title=\"6.5 Primary Keys\"></a>6.5 Primary Keys</h3><p>键是一组属性<br>键包括 superkey （超键）， candidate key （候选键）， primary key （主键）</p>\n<h4 id=\"Keys-For-Entity-Sets\"><a href=\"#Keys-For-Entity-Sets\" class=\"headerlink\" title=\"Keys For Entity Sets\"></a>Keys For Entity Sets</h4><p>实体集的superkey是一个或多个属性的集合，其值唯一地确定实体集中的每个实体<br>superkey可能包含无关属性，候选键是最小的超键</p>\n<p>主键是被选为标识实体集中实体的主要方法的候选键<br>尽管可能存在多个候选键，但选择其中一个候选键作为主键</p>\n<h4 id=\"Weak-Entity-Sets\"><a href=\"#Weak-Entity-Sets\" class=\"headerlink\" title=\"Weak Entity Sets\"></a>Weak Entity Sets</h4><p>没有主键的实体集称为弱实体集<br>弱实体集是其存在依赖于另一个实体（称为其标识实体，identifying entity）的实体集<br>不是将主键与弱实体相关联，而是使用标识实体以及称为鉴别器(discriminator)的额外属性来唯一标识弱实体<br>每个弱实体都必须与一个标识实体相关联，弱实体集是依赖于标识实体集的存在。<br>标识实体集是拥有弱实体集。 将弱实体集与标识实体集相关联的关系称为标识关系(identifying relationship)<br>在 E-R 图中，弱实体集通过双矩形进行描述。 我们用虚线强调弱实体集的判别器。 将弱实体集连接到标识强实体集的关系集由双菱形表示。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106103032.png\" alt=\"image.png\"><br>弱实体集的鉴别器（或部分键）是一组属性，用于区分弱实体集中依赖于特定强实体的所有实体<br>例如sec_id、学期、年份 弱实体集的鉴别器用虚线下划线<br>弱实体集的主键由弱实体集存在依赖的强实体集的主键加上弱实体集的判别器组成 弱实体集 E1 的主键 &#x3D; 鉴别器 ∪ primary_key（E2）<br>弱实体集 E1 具有多个标识实体集 E2、E3、..、En，弱实体集 E1 的主键 &#x3D; 鉴别器 ∪ primary_key（E2） ∪…primary_key（En） .</p>\n<h3 id=\"6-6-Removing-Redundant-Attributes\"><a href=\"#6-6-Removing-Redundant-Attributes\" class=\"headerlink\" title=\"6.6 Removing Redundant Attributes\"></a>6.6 Removing Redundant Attributes</h3><p>两个实体集存在共同属性构成关系，该属性不作为主键时的实体集中属性是冗余的需要删除。转换成表时，属性需要重新引入</p>\n<h3 id=\"6-7-Reducing-E-R-Diagrams-to-Relation-Schemas\"><a href=\"#6-7-Reducing-E-R-Diagrams-to-Relation-Schemas\" class=\"headerlink\" title=\"6.7 Reducing E-R Diagrams to Relation Schemas\"></a>6.7 Reducing E-R Diagrams to Relation Schemas</h3><p>将 E-R 图转换为表格式是从 E-R 图派生关系数据库设计的基础<br>符合 E-R 图的数据库可以由架构&#x2F;表的集合表示<br>实体集和关系集可以统一表示为表示数据库内容的关系架构。 对于每个实体集或关系集，都有一个与实体集或关系集对应的唯一架构。 每个表都有许多列，对应于它们的属性</p>\n<p>强实体集简化为具有相同属性的架构<br>弱实体集将变成一个表，其中包含标识强实体集的主键的列</p>\n<h4 id=\"Representation-of-Entity-Sets-with-Multivalued-Attributes\"><a href=\"#Representation-of-Entity-Sets-with-Multivalued-Attributes\" class=\"headerlink\" title=\"Representation of Entity Sets with Multivalued Attributes\"></a>Representation of Entity Sets with Multivalued Attributes</h4><p>实体 E 的多值属性 M 由单独的架构 EM 表示<br>架构 EM 具有与 E 的主键对应的属性和对应于多值属性 M 的属性</p>\n<h4 id=\"Representing-Relationship-Sets\"><a href=\"#Representing-Relationship-Sets\" class=\"headerlink\" title=\"Representing Relationship Sets\"></a>Representing Relationship Sets</h4><p>将关系集简化为表在很大程度上依赖于映射基数约束和总&#x2F;部分约束<br>多对多关系集表示为一个表，其中包含两个参与实体集的主键的列，以及关系集的任何描述性属性<br>可以通过向多端添加一个额外的属性来表示在多端上合计的多对一和一对多关系集，其中包含一端的主键</p>\n<p>该关系被简化为一个独立的表，以避免表中的空值</p>\n<p>对于一对一关系集，可以选择任何一方作为多方 可以将额外的属性添加到与两个实体集对应的任一表中</p>\n<h3 id=\"6-8-Extended-E-R-Features\"><a href=\"#6-8-Extended-E-R-Features\" class=\"headerlink\" title=\"6.8 Extended E-R Features\"></a>6.8 Extended E-R Features</h3><p>面向对象 （OO） E-R ：<br>specialization （特化，特殊化，例化） generalization （概括化，泛化，普遍化） attributes inheritance （属性继承）</p>\n<h4 id=\"Specialization\"><a href=\"#Specialization\" class=\"headerlink\" title=\"Specialization\"></a>Specialization</h4><p>自上而下的设计过程： 我们在实体集中指定有别于其他实体的子组 这些子分组成为较低级别的实体集，这些实体集具有属性或参与适用于较高级别实体集的关系<br>属性继承：较低级别的实体集继承它所链接到的较高级别实体集的所有属性和关系参与，较高级别实体的属性和关系可以应用于其所有较低级别的实体</p>\n<h4 id=\"Generalization\"><a href=\"#Generalization\" class=\"headerlink\" title=\"Generalization\"></a>Generalization</h4><p>自下而上的设计过程<br>将多个共享相同功能的实体集组合成更高级别的实体集<br>超类和子类：对于实体集合 A 和 B，如果 A 是 B 的泛化，即 B 是 A 的特化， 则 A 是 B 的超类，B 是 A 的子类</p>\n<h4 id=\"Completeness-Constraint\"><a href=\"#Completeness-Constraint\" class=\"headerlink\" title=\"Completeness Constraint\"></a>Completeness Constraint</h4><p>完整性约束<br>指定高级实体集中的实体是否必须至少属于泛化中的一个较低级别的实体集 </p>\n<ul>\n<li>total：实体必须属于较低级别的实体集之一 例如，学生泛化，所有实体必须是研究生或本科生 </li>\n<li>partial：实体不必属于较低级别的实体集之一 例如，人员专业化 部分泛化是默认设置。<br>我们可以通过在图中添加关键字 total 并在关键字到相应的空心箭头（用于全面泛化）或空心箭头集（用于重叠泛化）来指定总泛化。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114347.png\" alt=\"image.png\"></li>\n</ul>\n<p>关于一个高级实体是否可能属于单个专业化中集的多个较低级别实体的约束,</p>\n<ul>\n<li>disjoint：一个高级实体只能属于一个较低级别的实体集，即 L-entity-set-1 ∩ L-entity-set-2 &#x3D; Φ </li>\n<li>overlapping：一个高级实体可以属于多个较低级别的实体集 L-实体集-1∩L-实体集-2 ≠ Φ<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114410.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"Representing-Specialization-via-Schemas\"><a href=\"#Representing-Specialization-via-Schemas\" class=\"headerlink\" title=\"Representing Specialization via Schemas\"></a>Representing Specialization via Schemas</h4><ol>\n<li>形成更高级别实体的架构 为每个较低级别的实体集形成一个架构，包括较高级别实体集的主键和本地属性<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114529.png\" alt=\"image.png\"></li>\n<li>为每个实体集形成一个架构，其中包含所有本地属性和继承属性<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114548.png\" alt=\"image.png\"></li>\n<li>如果泛化&#x2F;特殊化是不相交的且完整的，则创建两个表</li>\n</ol>\n<h4 id=\"Aggregation\"><a href=\"#Aggregation\" class=\"headerlink\" title=\"Aggregation\"></a>Aggregation</h4><p>通过聚合消除冗余<br>将关系视为抽象实体<br>允许关系之间的关系<br>将关系抽象为新实体</p>\n<h3 id=\"6-9-Entity-relationship-Design-Issues\"><a href=\"#6-9-Entity-relationship-Design-Issues\" class=\"headerlink\" title=\"6.9 Entity-relationship Design Issues\"></a>6.9 Entity-relationship Design Issues</h3><p>E-R 图中的常见错误 </p>\n<ul>\n<li>实体与属性的使用 </li>\n<li>实体集与关系集的使用 </li>\n<li>二元关系与非二元关系</li>\n</ul>\n<p>规则1.不要将一个实体集的主键用作另一个实体集的属性（为了表示这两个实体集之间的隐式关联），最好使用关系集来显式显示这种关联<br>规则2.如果 E 参与 R，则不要将主键 （E） 指定为 R 的属性，以避免信息冗余<br>规则3.可能的准则是指定一个关系集来描述实体之间发生的操作<br>规则4.首选使用二元关系集，用多个二元关系集替换非二元关系集 尽管可以用许多不同的二元关系集替换任何非二元关系集（n-ary，对于 n &gt; 2）关系集，但 n 元关系集显示得更清楚。 一些看似非二元关系的关系可能使用二元关系来更好地表示。但有些关系天生是非二元的</p>\n<p>通常，任何非二元关系都可以通过创建人工实体集 E 来使用二元关系来表示</p>\n<h4 id=\"Placement-of-Relationship-Attributes\"><a href=\"#Placement-of-Relationship-Attributes\" class=\"headerlink\" title=\"Placement of Relationship Attributes\"></a>Placement of Relationship Attributes</h4><p>关系集 R 的属性可能与其参与实体集之一相关联<br>对于关系集 R 之间的属性 attr-A （R ⊆ E1 ╳ E2 ），如果 R 的基数为<br>一对一：attr-A 可以指定为 E1 或 E2 的属性，而不是关系集 R 从 E1 到 E2 的<br>一对多：attr-A 只能指定为实体集 E2（多边）的属性，而不是关系集 R 的属性</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"6-1-Overview-of-Design-Process\"><a href=\"#6-1-Overview-of-Design-Process\" class=\"headerlink\" title=\"6.1 Overview of Design Process\"></a>6.1 Overview of Design Process</h3><p>分析用户需求：</p>\n<ul>\n<li>什么数据需要被存储</li>\n<li>什么操作和事务（transaction）被执行，例如插入、删除和更新</li>\n</ul>\n<p>数据库设计的三种模式：</p>\n<ul>\n<li>概念设计</li>\n<li>逻辑设计，在逻辑级别和视图级别</li>\n<li>物理设计，在物理层面</li>\n</ul>\n<p>数据库设计的阶段：</p>\n<ul>\n<li>需求分析</li>\n<li>概念设计</li>\n<li>逻辑模型设计</li>\n<li>物理模型设计</li>\n</ul>\n<p>DBAS设计过程：<br>生命周期包括五个阶段：</p>\n<ul>\n<li>项目规划、需求分析、系统设计、实现与部署、运行管理与维护<br>根据软件组成和各自功能，分为三条设计主线：<br>数据组织与存储设计、数据访问与处理设计、应用设计<br>分别用于设计数据库、数据库事务和应用程序</li>\n</ul>\n<h3 id=\"6-2-The-Entity-Relationship-Model\"><a href=\"#6-2-The-Entity-Relationship-Model\" class=\"headerlink\" title=\"6.2 The Entity-Relationship Model\"></a>6.2 The Entity-Relationship Model</h3><p>建立一个图来表示整个数据库的特征<br>ER模型包括三个概念：</p>\n<ul>\n<li>entity sets（实体集）</li>\n<li>relationship sets（关系集）</li>\n<li>attributes（属性）<br>ER模型使用有关联的图表表示，即ER图，它以图形的方式表示数据库的整体逻辑结构</li>\n</ul>\n<h4 id=\"6-2-1-Entity-Sets（实体集）\"><a href=\"#6-2-1-Entity-Sets（实体集）\" class=\"headerlink\" title=\"6.2.1 Entity Sets（实体集）\"></a>6.2.1 Entity Sets（实体集）</h4><p>实体是一个存在的对象，区别于其他对象。实体通过一组属性表示。<br>实体集是一些相同类型实体的集合。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101224017.png\" alt=\"image.png\"></p>\n<h4 id=\"6-2-2-Relationship-Sets（关系集）\"><a href=\"#6-2-2-Relationship-Sets（关系集）\" class=\"headerlink\" title=\"6.2.2 Relationship Sets（关系集）\"></a>6.2.2 Relationship Sets（关系集）</h4><p>关系是多个实体之间的关联<br>实体集 E1， E2， …， En 参与关系集 R</p>\n<h5 id=\"Attributes-with-Relationship-Sets\"><a href=\"#Attributes-with-Relationship-Sets\" class=\"headerlink\" title=\"Attributes with Relationship Sets\"></a>Attributes with Relationship Sets</h5><p>关系集在E-R图中用菱形表示，菱形通过线条连接到多个不同的实体集（矩形）。属性也可以与关系集合联系。<br>在教师和学生之间设置的顾问关系可能具有属性日期，该属性跟踪学生何时开始与顾问关联<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101225949.png\" alt=\"image.png\"></p>\n<h5 id=\"Role-in-Relationship-Sets\"><a href=\"#Role-in-Relationship-Sets\" class=\"headerlink\" title=\"Role in Relationship Sets\"></a>Role in Relationship Sets</h5><p>实体在关系中扮演的功能称为该实体的角色<br>在E-R图中角色一般标注在菱形和矩形之间的连线上<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101230806.png\" alt=\"image.png\"></p>\n<h5 id=\"Degree-of-a-Relationship-Set（关系集的度）-vs-Non-binary-Relationship-Sets（非二元关系集）\"><a href=\"#Degree-of-a-Relationship-Set（关系集的度）-vs-Non-binary-Relationship-Sets（非二元关系集）\" class=\"headerlink\" title=\"Degree of a Relationship Set（关系集的度） vs Non-binary Relationship Sets（非二元关系集）\"></a>Degree of a Relationship Set（关系集的度） vs Non-binary Relationship Sets（非二元关系集）</h5><p>二元关系涉及两个实体集（或二度）<br>在某些情况下，我们将关系表示为非二元关系。 具有三元关系的 E-R 图示例：学生在教师的指导下从事研究项目，教师、学生和项目之间的关系proj_guide是三元的<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231144.png\" alt=\"image.png\"></p>\n<h3 id=\"6-3-Complex-Attributes\"><a href=\"#6-3-Complex-Attributes\" class=\"headerlink\" title=\"6.3 Complex Attributes\"></a>6.3 Complex Attributes</h3><p>每个属性都有一个可取值的集合，称为该属性的域（domain），或者值集（value set）<br>属性类型： </p>\n<ul>\n<li><p>简单属性和复合属性<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101231709.png\" alt=\"image.png\"></p>\n</li>\n<li><p>单值和多值属性</p>\n</li>\n<li><p>派生属性: 可以从其他属性计算</p>\n</li>\n</ul>\n<p>属性的 Null 值表示:</p>\n<ul>\n<li>该属性不适用于实体，不存在</li>\n<li>该属性的值存在，但为“未知”</li>\n</ul>\n<p>冗余属性：假设我们有实体集： 讲师：ID、姓名、<strong>dept_name</strong>、薪水 ；部门：<strong>dept_name</strong>、建筑、预算<br>模型：每个教师都有一个使用关系集的关联部门inst_dept 属性dept_name出现在两个实体集中。 由于它是实体集部门的主键，因此它复制关系中存在的信息，因此在实体集讲师中是多余的。 但是：当转换回表时，在某些情况下会重新引入该属性</p>\n<h3 id=\"6-4-Mapping-Cardinalities-Constrants（映射基数约束）\"><a href=\"#6-4-Mapping-Cardinalities-Constrants（映射基数约束）\" class=\"headerlink\" title=\"6.4 Mapping  Cardinalities Constrants（映射基数约束）\"></a>6.4 Mapping  Cardinalities Constrants（映射基数约束）</h3><p>语义&#x2F;约束：作为完整性约束，以保持 DBS 的一致性</p>\n<h5 id=\"Mapping-Cardinalities（映射基数）\"><a href=\"#Mapping-Cardinalities（映射基数）\" class=\"headerlink\" title=\"Mapping Cardinalities（映射基数）\"></a>Mapping Cardinalities（映射基数）</h5><p>表示另一个实体可以通过关系集关联到的实体数<br>对于二元关系集 R，从 A 到 B 映射基数必须如下所示：</p>\n<ul>\n<li>一对一</li>\n<li>一对多</li>\n<li>多对一</li>\n<li>多对多<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233304.png\" alt=\"image.png\"></li>\n</ul>\n<p>有些元素可能不被映射到其他集合</p>\n<p>在 ER 图中表示基数约束：</p>\n<ul>\n<li>有→的代表一，没有箭头为多<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233549.png\" alt=\"image.png\"></li>\n<li>用双线表示至少有一个实体相关联，单线则表示可以有零个或多个</li>\n</ul>\n<h5 id=\"Participate-Constraints（参与约束）\"><a href=\"#Participate-Constraints（参与约束）\" class=\"headerlink\" title=\"Participate Constraints（参与约束）\"></a>Participate Constraints（参与约束）</h5><p>实体 E 在关系 R 中的参与是<strong>完全的（total）</strong>，如果 E 中的每个实体都参与 R 中的至少一个关系：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101233933.png\" alt=\"image.png\"><br>实体 E 在关系 R 中的参与是<strong>部分的（partial）</strong>，如果 E 中的某些实体可能不参与 R 中的任何关系：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234007.png\" alt=\"image.png\"></p>\n<h5 id=\"Cardinality-Limits-for-Participation（参与的基数限制）\"><a href=\"#Cardinality-Limits-for-Participation（参与的基数限制）\" class=\"headerlink\" title=\"Cardinality Limits for Participation（参与的基数限制）\"></a>Cardinality Limits for Participation（参与的基数限制）</h5><p>基数限制（参与的基数界限）用于表示对参与的定量约束<br>e.g.</p>\n<ul>\n<li>每个instructor最多可以指导多个student, 最少可以指导0个student</li>\n<li>每个student最少有1个指导instructor，最多也只有1个指导instructor<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234344.png\" alt=\"image.png\"><br>*表示无限制<br>A参与R的基数下界$l_{A}$和上界$h_{A}$</li>\n<li>A中的每个实体a通过R关联了最少$l_{A}$个、最多$h_{A}$个B中实体b</li>\n<li>$l_{A}$ ：对A中的每个实体a, B中至少有$l_{A}$个实体b通过R与其对应&#x2F;关联</li>\n<li>$h_{A}$：对A中的每个实体a, B中至多有$h_{A}$个实体b通过R与其对应&#x2F;关联<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101234446.png\" alt=\"image.png\"><br>最小值 lA &#x3D; 0：A 是 R 的部分参与<br>最小值 lA &gt; 0：A 为 R 的总参与，相当于双线<br>最大值 * 表示无限制</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101235347.png\" alt=\"image.png\"></p>\n<p>我们最多允许一个箭头从三元（或更大程度）关系中表示基数约束  如果有多个箭头，则有两种定义含义的方法。 </p>\n<ul>\n<li>A、B 和 C 之间的三元关系 R，带有指向 B 和 C 的箭头可能意味着 每个 A 实体都与 B 和 C 中的唯一实体相关联，或者</li>\n<li>（A， B） 中的每对实体都与一个唯一的 C 实体相关联，并且每一对（A、C）都与一个唯一的 B 实体相关联</li>\n</ul>\n<h3 id=\"6-5-Primary-Keys\"><a href=\"#6-5-Primary-Keys\" class=\"headerlink\" title=\"6.5 Primary Keys\"></a>6.5 Primary Keys</h3><p>键是一组属性<br>键包括 superkey （超键）， candidate key （候选键）， primary key （主键）</p>\n<h4 id=\"Keys-For-Entity-Sets\"><a href=\"#Keys-For-Entity-Sets\" class=\"headerlink\" title=\"Keys For Entity Sets\"></a>Keys For Entity Sets</h4><p>实体集的superkey是一个或多个属性的集合，其值唯一地确定实体集中的每个实体<br>superkey可能包含无关属性，候选键是最小的超键</p>\n<p>主键是被选为标识实体集中实体的主要方法的候选键<br>尽管可能存在多个候选键，但选择其中一个候选键作为主键</p>\n<h4 id=\"Weak-Entity-Sets\"><a href=\"#Weak-Entity-Sets\" class=\"headerlink\" title=\"Weak Entity Sets\"></a>Weak Entity Sets</h4><p>没有主键的实体集称为弱实体集<br>弱实体集是其存在依赖于另一个实体（称为其标识实体，identifying entity）的实体集<br>不是将主键与弱实体相关联，而是使用标识实体以及称为鉴别器(discriminator)的额外属性来唯一标识弱实体<br>每个弱实体都必须与一个标识实体相关联，弱实体集是依赖于标识实体集的存在。<br>标识实体集是拥有弱实体集。 将弱实体集与标识实体集相关联的关系称为标识关系(identifying relationship)<br>在 E-R 图中，弱实体集通过双矩形进行描述。 我们用虚线强调弱实体集的判别器。 将弱实体集连接到标识强实体集的关系集由双菱形表示。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106103032.png\" alt=\"image.png\"><br>弱实体集的鉴别器（或部分键）是一组属性，用于区分弱实体集中依赖于特定强实体的所有实体<br>例如sec_id、学期、年份 弱实体集的鉴别器用虚线下划线<br>弱实体集的主键由弱实体集存在依赖的强实体集的主键加上弱实体集的判别器组成 弱实体集 E1 的主键 &#x3D; 鉴别器 ∪ primary_key（E2）<br>弱实体集 E1 具有多个标识实体集 E2、E3、..、En，弱实体集 E1 的主键 &#x3D; 鉴别器 ∪ primary_key（E2） ∪…primary_key（En） .</p>\n<h3 id=\"6-6-Removing-Redundant-Attributes\"><a href=\"#6-6-Removing-Redundant-Attributes\" class=\"headerlink\" title=\"6.6 Removing Redundant Attributes\"></a>6.6 Removing Redundant Attributes</h3><p>两个实体集存在共同属性构成关系，该属性不作为主键时的实体集中属性是冗余的需要删除。转换成表时，属性需要重新引入</p>\n<h3 id=\"6-7-Reducing-E-R-Diagrams-to-Relation-Schemas\"><a href=\"#6-7-Reducing-E-R-Diagrams-to-Relation-Schemas\" class=\"headerlink\" title=\"6.7 Reducing E-R Diagrams to Relation Schemas\"></a>6.7 Reducing E-R Diagrams to Relation Schemas</h3><p>将 E-R 图转换为表格式是从 E-R 图派生关系数据库设计的基础<br>符合 E-R 图的数据库可以由架构&#x2F;表的集合表示<br>实体集和关系集可以统一表示为表示数据库内容的关系架构。 对于每个实体集或关系集，都有一个与实体集或关系集对应的唯一架构。 每个表都有许多列，对应于它们的属性</p>\n<p>强实体集简化为具有相同属性的架构<br>弱实体集将变成一个表，其中包含标识强实体集的主键的列</p>\n<h4 id=\"Representation-of-Entity-Sets-with-Multivalued-Attributes\"><a href=\"#Representation-of-Entity-Sets-with-Multivalued-Attributes\" class=\"headerlink\" title=\"Representation of Entity Sets with Multivalued Attributes\"></a>Representation of Entity Sets with Multivalued Attributes</h4><p>实体 E 的多值属性 M 由单独的架构 EM 表示<br>架构 EM 具有与 E 的主键对应的属性和对应于多值属性 M 的属性</p>\n<h4 id=\"Representing-Relationship-Sets\"><a href=\"#Representing-Relationship-Sets\" class=\"headerlink\" title=\"Representing Relationship Sets\"></a>Representing Relationship Sets</h4><p>将关系集简化为表在很大程度上依赖于映射基数约束和总&#x2F;部分约束<br>多对多关系集表示为一个表，其中包含两个参与实体集的主键的列，以及关系集的任何描述性属性<br>可以通过向多端添加一个额外的属性来表示在多端上合计的多对一和一对多关系集，其中包含一端的主键</p>\n<p>该关系被简化为一个独立的表，以避免表中的空值</p>\n<p>对于一对一关系集，可以选择任何一方作为多方 可以将额外的属性添加到与两个实体集对应的任一表中</p>\n<h3 id=\"6-8-Extended-E-R-Features\"><a href=\"#6-8-Extended-E-R-Features\" class=\"headerlink\" title=\"6.8 Extended E-R Features\"></a>6.8 Extended E-R Features</h3><p>面向对象 （OO） E-R ：<br>specialization （特化，特殊化，例化） generalization （概括化，泛化，普遍化） attributes inheritance （属性继承）</p>\n<h4 id=\"Specialization\"><a href=\"#Specialization\" class=\"headerlink\" title=\"Specialization\"></a>Specialization</h4><p>自上而下的设计过程： 我们在实体集中指定有别于其他实体的子组 这些子分组成为较低级别的实体集，这些实体集具有属性或参与适用于较高级别实体集的关系<br>属性继承：较低级别的实体集继承它所链接到的较高级别实体集的所有属性和关系参与，较高级别实体的属性和关系可以应用于其所有较低级别的实体</p>\n<h4 id=\"Generalization\"><a href=\"#Generalization\" class=\"headerlink\" title=\"Generalization\"></a>Generalization</h4><p>自下而上的设计过程<br>将多个共享相同功能的实体集组合成更高级别的实体集<br>超类和子类：对于实体集合 A 和 B，如果 A 是 B 的泛化，即 B 是 A 的特化， 则 A 是 B 的超类，B 是 A 的子类</p>\n<h4 id=\"Completeness-Constraint\"><a href=\"#Completeness-Constraint\" class=\"headerlink\" title=\"Completeness Constraint\"></a>Completeness Constraint</h4><p>完整性约束<br>指定高级实体集中的实体是否必须至少属于泛化中的一个较低级别的实体集 </p>\n<ul>\n<li>total：实体必须属于较低级别的实体集之一 例如，学生泛化，所有实体必须是研究生或本科生 </li>\n<li>partial：实体不必属于较低级别的实体集之一 例如，人员专业化 部分泛化是默认设置。<br>我们可以通过在图中添加关键字 total 并在关键字到相应的空心箭头（用于全面泛化）或空心箭头集（用于重叠泛化）来指定总泛化。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114347.png\" alt=\"image.png\"></li>\n</ul>\n<p>关于一个高级实体是否可能属于单个专业化中集的多个较低级别实体的约束,</p>\n<ul>\n<li>disjoint：一个高级实体只能属于一个较低级别的实体集，即 L-entity-set-1 ∩ L-entity-set-2 &#x3D; Φ </li>\n<li>overlapping：一个高级实体可以属于多个较低级别的实体集 L-实体集-1∩L-实体集-2 ≠ Φ<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114410.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"Representing-Specialization-via-Schemas\"><a href=\"#Representing-Specialization-via-Schemas\" class=\"headerlink\" title=\"Representing Specialization via Schemas\"></a>Representing Specialization via Schemas</h4><ol>\n<li>形成更高级别实体的架构 为每个较低级别的实体集形成一个架构，包括较高级别实体集的主键和本地属性<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114529.png\" alt=\"image.png\"></li>\n<li>为每个实体集形成一个架构，其中包含所有本地属性和继承属性<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231106114548.png\" alt=\"image.png\"></li>\n<li>如果泛化&#x2F;特殊化是不相交的且完整的，则创建两个表</li>\n</ol>\n<h4 id=\"Aggregation\"><a href=\"#Aggregation\" class=\"headerlink\" title=\"Aggregation\"></a>Aggregation</h4><p>通过聚合消除冗余<br>将关系视为抽象实体<br>允许关系之间的关系<br>将关系抽象为新实体</p>\n<h3 id=\"6-9-Entity-relationship-Design-Issues\"><a href=\"#6-9-Entity-relationship-Design-Issues\" class=\"headerlink\" title=\"6.9 Entity-relationship Design Issues\"></a>6.9 Entity-relationship Design Issues</h3><p>E-R 图中的常见错误 </p>\n<ul>\n<li>实体与属性的使用 </li>\n<li>实体集与关系集的使用 </li>\n<li>二元关系与非二元关系</li>\n</ul>\n<p>规则1.不要将一个实体集的主键用作另一个实体集的属性（为了表示这两个实体集之间的隐式关联），最好使用关系集来显式显示这种关联<br>规则2.如果 E 参与 R，则不要将主键 （E） 指定为 R 的属性，以避免信息冗余<br>规则3.可能的准则是指定一个关系集来描述实体之间发生的操作<br>规则4.首选使用二元关系集，用多个二元关系集替换非二元关系集 尽管可以用许多不同的二元关系集替换任何非二元关系集（n-ary，对于 n &gt; 2）关系集，但 n 元关系集显示得更清楚。 一些看似非二元关系的关系可能使用二元关系来更好地表示。但有些关系天生是非二元的</p>\n<p>通常，任何非二元关系都可以通过创建人工实体集 E 来使用二元关系来表示</p>\n<h4 id=\"Placement-of-Relationship-Attributes\"><a href=\"#Placement-of-Relationship-Attributes\" class=\"headerlink\" title=\"Placement of Relationship Attributes\"></a>Placement of Relationship Attributes</h4><p>关系集 R 的属性可能与其参与实体集之一相关联<br>对于关系集 R 之间的属性 attr-A （R ⊆ E1 ╳ E2 ），如果 R 的基数为<br>一对一：attr-A 可以指定为 E1 或 E2 的属性，而不是关系集 R 从 E1 到 E2 的<br>一对多：attr-A 只能指定为实体集 E2（多边）的属性，而不是关系集 R 的属性</p>\n"},{"title":"数据储存结构","date":"2023-12-03T09:33:10.486Z","_content":"\n### 13.1 Introduction\n物理数据库中解决的两个问题：\n- 数据组织，即数据的物理存储结构 — Ch13 \n- 数据访问，例如索引 — Ch14\n\n目标\n- 1. 根据DBMS机制，选择合适的数据库物理结构\n- 2. 在数据库表上合理设置索引，提高数据查询速度\n\n### 13.2 File Organization\n文件逻辑/物理组织 in Operating System Concepts\n- 逻辑结构\n- 基于记录的文件\n- 基于索引的文件（按内容访问）\n\n物理组织/结构\n- contiguous, linked, indexed\n- 以block为单位，存储 在secondary storage, i.e. disk, SSD\n\n\nDB 存储为 DB 文件的集合 \n- 例如.SQL服务器，主文件，辅文件，日志文件 \n- 每个文件都是一系列记录 \n- 每条记录都位于一系列字段中 \n一种方法\n\t记录大小是固定的 \n\t每个文件仅包含一种特定类型的记录 \n\t不同的文件用于不同的关系\n\n每个文件都是记录序列，每个关系表是一组元组，以元组为单位的DB逻辑文件\n在以记录为单位的物理文件 \n元组作为记录存储在数据库中 \n将元组表示为文件中的记录\n\t固定长度记录 \n\t可变长度记录 \n\t例如，名称 varchar（20）\n#### Fixed-Length Records 固定长度记录\n一个块包含多条记录 \n存储记录 i 从字节 n （i – 1） 开始，其中 n 是每条记录的大小。 \n记录访问很简单，但记录可能会跨块 \n修改： 不允许记录跨越块边界\n\n删除记录 i： 备选方案： \n\t压缩：将记录 i + 1， . . .， n 移动到 i， . . . . ， n – 1 \n\t将记录 N 移动到 I \n\t不要移动记录，而是将所有空闲记录链接在空表\n\n#### Free Lists\n将第一条已删除记录的地址存储在文件头中 \n使用此第一个删除的记录来存储第二个已删除记录的地址，依此类推 \n将这些存储的地址视为指针，因为它们指向记录的位置。 \n更节省空间的表示形式 \n将空间重新用于可用记录的正常属性以存储指针。（使用中的记录中没有存储指针。）\n\n#### Variable-Length Records 可变长度记录\n可变长度记录以多种方式出现 \n记录类型允许可变长度，例如字符串 （varchar） \n一个文件中有多种记录类型，例如多表聚类文件 \n记录类型允许重复字段，例如多值属性。\n\n#### 如何在块中存储可变长度记录\n记录中的属性按顺序存储 \n首先是固定长度属性，然后是可变长度属性 \n可变长度属性由固定大小（偏移量、长度）表示，实际数据存储在所有固定长度属性之后 \n由 null 值位图表示的 Null 值\n\n#### Variable-Length Records: Slotted Page Structure可变长度记录：开槽页面结构\n存储空间分为固定大小的开槽页，4KB或8KB，记录分配给开槽页![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174823.png)\n记录可以在页面中移动以保持连续 \nRI 被删除，RI+3、RI+2、RI+1 被移动到删除创建的可用空间 \n标头中的条目必须更新，例如设置为 -1 指针不应直接指向记录，而应指向标头中记录的条目\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174849.png)\n\n### 13.3 Organization of Records in Files\nDB文件可以看作是逻辑级别的一组记录，这些记录在逻辑上组织如下 \n\t堆、序列、哈希、聚类 \n\t记录的逻辑组织决定了如何访问记录，即访问方法\n文件包括多个记录，记录在磁盘上的存放属于文件（记录）结构问题\n对于某种结构的文件如何去查找、插入、删除记录，属于文件的存取方法\n文件记录的组织结构决定了文件的存取方法\n\n堆 – 放置在任何有空间的地方的记录 \n\t没有主键和索引的表中的数据， \n\t例如，学生（ID、姓名、总学分） \n序列 – 根据每条记录的搜索键按顺序存储的记录 \n\t例如学生（ID、姓名、总学分）， 或学生（ID、姓名、总学分），并附有姓名索引 \n\t每个关系的记录可以存储在单独的文件中 \n多表聚类 – 存储在同一文件中的不同关系的记录 \n\t动机：将相关记录存储在同一个块上，以最大程度地减少 I/O， \n\t例如 讲师加入部门，讲师和部门的记录放在一个文件中\n\n#### Heap File Organization堆文件组织\n任何记录都可以放置在文件中有记录空间的任何位置 \n没有记录的顺序 \n记录在分配后通常不会移动 \n关系只有一个文件\n\n以纪录的输入顺序为序，决定了文件中记录顺序\n    纪录的存储顺序与记录中的主键无关\ne.g. 创建新关系表student (ID, name, total-credits)，    但不在student上定义主键、候选键、各类索引，student被组织为heap file\n通过insert，将记录/元组加到堆文件中\n\n自由空间地图\n\t每个区块一个条目。 \n\t每个条目都是几位到一个字节，记录空闲块的部分 \n\t例如，每个块 3 位 （0-7），值除以 8 表示可用块的分数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203175205.png)\n二级自由空间地图 \n\t例如，每个条目最多存储 4 个第一级自由空间映射条目\n定期写入磁盘的可用空间映射，可以对某些条目使用错误（旧）值（将被检测并修复）","source":"_posts/Notes/课程/大三（上）/数据库/数据储存结构.md","raw":"---\ntitle: 数据储存结构\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 数据库\ntags:\n  - 数据库\ndate:\n---\n\n### 13.1 Introduction\n物理数据库中解决的两个问题：\n- 数据组织，即数据的物理存储结构 — Ch13 \n- 数据访问，例如索引 — Ch14\n\n目标\n- 1. 根据DBMS机制，选择合适的数据库物理结构\n- 2. 在数据库表上合理设置索引，提高数据查询速度\n\n### 13.2 File Organization\n文件逻辑/物理组织 in Operating System Concepts\n- 逻辑结构\n- 基于记录的文件\n- 基于索引的文件（按内容访问）\n\n物理组织/结构\n- contiguous, linked, indexed\n- 以block为单位，存储 在secondary storage, i.e. disk, SSD\n\n\nDB 存储为 DB 文件的集合 \n- 例如.SQL服务器，主文件，辅文件，日志文件 \n- 每个文件都是一系列记录 \n- 每条记录都位于一系列字段中 \n一种方法\n\t记录大小是固定的 \n\t每个文件仅包含一种特定类型的记录 \n\t不同的文件用于不同的关系\n\n每个文件都是记录序列，每个关系表是一组元组，以元组为单位的DB逻辑文件\n在以记录为单位的物理文件 \n元组作为记录存储在数据库中 \n将元组表示为文件中的记录\n\t固定长度记录 \n\t可变长度记录 \n\t例如，名称 varchar（20）\n#### Fixed-Length Records 固定长度记录\n一个块包含多条记录 \n存储记录 i 从字节 n （i – 1） 开始，其中 n 是每条记录的大小。 \n记录访问很简单，但记录可能会跨块 \n修改： 不允许记录跨越块边界\n\n删除记录 i： 备选方案： \n\t压缩：将记录 i + 1， . . .， n 移动到 i， . . . . ， n – 1 \n\t将记录 N 移动到 I \n\t不要移动记录，而是将所有空闲记录链接在空表\n\n#### Free Lists\n将第一条已删除记录的地址存储在文件头中 \n使用此第一个删除的记录来存储第二个已删除记录的地址，依此类推 \n将这些存储的地址视为指针，因为它们指向记录的位置。 \n更节省空间的表示形式 \n将空间重新用于可用记录的正常属性以存储指针。（使用中的记录中没有存储指针。）\n\n#### Variable-Length Records 可变长度记录\n可变长度记录以多种方式出现 \n记录类型允许可变长度，例如字符串 （varchar） \n一个文件中有多种记录类型，例如多表聚类文件 \n记录类型允许重复字段，例如多值属性。\n\n#### 如何在块中存储可变长度记录\n记录中的属性按顺序存储 \n首先是固定长度属性，然后是可变长度属性 \n可变长度属性由固定大小（偏移量、长度）表示，实际数据存储在所有固定长度属性之后 \n由 null 值位图表示的 Null 值\n\n#### Variable-Length Records: Slotted Page Structure可变长度记录：开槽页面结构\n存储空间分为固定大小的开槽页，4KB或8KB，记录分配给开槽页![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174823.png)\n记录可以在页面中移动以保持连续 \nRI 被删除，RI+3、RI+2、RI+1 被移动到删除创建的可用空间 \n标头中的条目必须更新，例如设置为 -1 指针不应直接指向记录，而应指向标头中记录的条目\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174849.png)\n\n### 13.3 Organization of Records in Files\nDB文件可以看作是逻辑级别的一组记录，这些记录在逻辑上组织如下 \n\t堆、序列、哈希、聚类 \n\t记录的逻辑组织决定了如何访问记录，即访问方法\n文件包括多个记录，记录在磁盘上的存放属于文件（记录）结构问题\n对于某种结构的文件如何去查找、插入、删除记录，属于文件的存取方法\n文件记录的组织结构决定了文件的存取方法\n\n堆 – 放置在任何有空间的地方的记录 \n\t没有主键和索引的表中的数据， \n\t例如，学生（ID、姓名、总学分） \n序列 – 根据每条记录的搜索键按顺序存储的记录 \n\t例如学生（ID、姓名、总学分）， 或学生（ID、姓名、总学分），并附有姓名索引 \n\t每个关系的记录可以存储在单独的文件中 \n多表聚类 – 存储在同一文件中的不同关系的记录 \n\t动机：将相关记录存储在同一个块上，以最大程度地减少 I/O， \n\t例如 讲师加入部门，讲师和部门的记录放在一个文件中\n\n#### Heap File Organization堆文件组织\n任何记录都可以放置在文件中有记录空间的任何位置 \n没有记录的顺序 \n记录在分配后通常不会移动 \n关系只有一个文件\n\n以纪录的输入顺序为序，决定了文件中记录顺序\n    纪录的存储顺序与记录中的主键无关\ne.g. 创建新关系表student (ID, name, total-credits)，    但不在student上定义主键、候选键、各类索引，student被组织为heap file\n通过insert，将记录/元组加到堆文件中\n\n自由空间地图\n\t每个区块一个条目。 \n\t每个条目都是几位到一个字节，记录空闲块的部分 \n\t例如，每个块 3 位 （0-7），值除以 8 表示可用块的分数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203175205.png)\n二级自由空间地图 \n\t例如，每个条目最多存储 4 个第一级自由空间映射条目\n定期写入磁盘的可用空间映射，可以对某些条目使用错误（旧）值（将被检测并修复）","slug":"Notes/课程/大三（上）/数据库/数据储存结构","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr125000yvw8cd8b0e20x","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"13-1-Introduction\"><a href=\"#13-1-Introduction\" class=\"headerlink\" title=\"13.1 Introduction\"></a>13.1 Introduction</h3><p>物理数据库中解决的两个问题：</p>\n<ul>\n<li>数据组织，即数据的物理存储结构 — Ch13 </li>\n<li>数据访问，例如索引 — Ch14</li>\n</ul>\n<p>目标</p>\n<ul>\n<li><ol>\n<li>根据DBMS机制，选择合适的数据库物理结构</li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>在数据库表上合理设置索引，提高数据查询速度</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"13-2-File-Organization\"><a href=\"#13-2-File-Organization\" class=\"headerlink\" title=\"13.2 File Organization\"></a>13.2 File Organization</h3><p>文件逻辑&#x2F;物理组织 in Operating System Concepts</p>\n<ul>\n<li>逻辑结构</li>\n<li>基于记录的文件</li>\n<li>基于索引的文件（按内容访问）</li>\n</ul>\n<p>物理组织&#x2F;结构</p>\n<ul>\n<li>contiguous, linked, indexed</li>\n<li>以block为单位，存储 在secondary storage, i.e. disk, SSD</li>\n</ul>\n<p>DB 存储为 DB 文件的集合 </p>\n<ul>\n<li>例如.SQL服务器，主文件，辅文件，日志文件 </li>\n<li>每个文件都是一系列记录 </li>\n<li>每条记录都位于一系列字段中<br>一种方法<br>  记录大小是固定的<br>  每个文件仅包含一种特定类型的记录<br>  不同的文件用于不同的关系</li>\n</ul>\n<p>每个文件都是记录序列，每个关系表是一组元组，以元组为单位的DB逻辑文件<br>在以记录为单位的物理文件<br>元组作为记录存储在数据库中<br>将元组表示为文件中的记录<br>    固定长度记录<br>    可变长度记录<br>    例如，名称 varchar（20）</p>\n<h4 id=\"Fixed-Length-Records-固定长度记录\"><a href=\"#Fixed-Length-Records-固定长度记录\" class=\"headerlink\" title=\"Fixed-Length Records 固定长度记录\"></a>Fixed-Length Records 固定长度记录</h4><p>一个块包含多条记录<br>存储记录 i 从字节 n （i – 1） 开始，其中 n 是每条记录的大小。<br>记录访问很简单，但记录可能会跨块<br>修改： 不允许记录跨越块边界</p>\n<p>删除记录 i： 备选方案：<br>    压缩：将记录 i + 1， . . .， n 移动到 i， . . . . ， n – 1<br>    将记录 N 移动到 I<br>    不要移动记录，而是将所有空闲记录链接在空表</p>\n<h4 id=\"Free-Lists\"><a href=\"#Free-Lists\" class=\"headerlink\" title=\"Free Lists\"></a>Free Lists</h4><p>将第一条已删除记录的地址存储在文件头中<br>使用此第一个删除的记录来存储第二个已删除记录的地址，依此类推<br>将这些存储的地址视为指针，因为它们指向记录的位置。<br>更节省空间的表示形式<br>将空间重新用于可用记录的正常属性以存储指针。（使用中的记录中没有存储指针。）</p>\n<h4 id=\"Variable-Length-Records-可变长度记录\"><a href=\"#Variable-Length-Records-可变长度记录\" class=\"headerlink\" title=\"Variable-Length Records 可变长度记录\"></a>Variable-Length Records 可变长度记录</h4><p>可变长度记录以多种方式出现<br>记录类型允许可变长度，例如字符串 （varchar）<br>一个文件中有多种记录类型，例如多表聚类文件<br>记录类型允许重复字段，例如多值属性。</p>\n<h4 id=\"如何在块中存储可变长度记录\"><a href=\"#如何在块中存储可变长度记录\" class=\"headerlink\" title=\"如何在块中存储可变长度记录\"></a>如何在块中存储可变长度记录</h4><p>记录中的属性按顺序存储<br>首先是固定长度属性，然后是可变长度属性<br>可变长度属性由固定大小（偏移量、长度）表示，实际数据存储在所有固定长度属性之后<br>由 null 值位图表示的 Null 值</p>\n<h4 id=\"Variable-Length-Records-Slotted-Page-Structure可变长度记录：开槽页面结构\"><a href=\"#Variable-Length-Records-Slotted-Page-Structure可变长度记录：开槽页面结构\" class=\"headerlink\" title=\"Variable-Length Records: Slotted Page Structure可变长度记录：开槽页面结构\"></a>Variable-Length Records: Slotted Page Structure可变长度记录：开槽页面结构</h4><p>存储空间分为固定大小的开槽页，4KB或8KB，记录分配给开槽页<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174823.png\" alt=\"image.png\"><br>记录可以在页面中移动以保持连续<br>RI 被删除，RI+3、RI+2、RI+1 被移动到删除创建的可用空间<br>标头中的条目必须更新，例如设置为 -1 指针不应直接指向记录，而应指向标头中记录的条目<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174849.png\" alt=\"image.png\"></p>\n<h3 id=\"13-3-Organization-of-Records-in-Files\"><a href=\"#13-3-Organization-of-Records-in-Files\" class=\"headerlink\" title=\"13.3 Organization of Records in Files\"></a>13.3 Organization of Records in Files</h3><p>DB文件可以看作是逻辑级别的一组记录，这些记录在逻辑上组织如下<br>    堆、序列、哈希、聚类<br>    记录的逻辑组织决定了如何访问记录，即访问方法<br>文件包括多个记录，记录在磁盘上的存放属于文件（记录）结构问题<br>对于某种结构的文件如何去查找、插入、删除记录，属于文件的存取方法<br>文件记录的组织结构决定了文件的存取方法</p>\n<p>堆 – 放置在任何有空间的地方的记录<br>    没有主键和索引的表中的数据，<br>    例如，学生（ID、姓名、总学分）<br>序列 – 根据每条记录的搜索键按顺序存储的记录<br>    例如学生（ID、姓名、总学分）， 或学生（ID、姓名、总学分），并附有姓名索引<br>    每个关系的记录可以存储在单独的文件中<br>多表聚类 – 存储在同一文件中的不同关系的记录<br>    动机：将相关记录存储在同一个块上，以最大程度地减少 I&#x2F;O，<br>    例如 讲师加入部门，讲师和部门的记录放在一个文件中</p>\n<h4 id=\"Heap-File-Organization堆文件组织\"><a href=\"#Heap-File-Organization堆文件组织\" class=\"headerlink\" title=\"Heap File Organization堆文件组织\"></a>Heap File Organization堆文件组织</h4><p>任何记录都可以放置在文件中有记录空间的任何位置<br>没有记录的顺序<br>记录在分配后通常不会移动<br>关系只有一个文件</p>\n<p>以纪录的输入顺序为序，决定了文件中记录顺序<br>    纪录的存储顺序与记录中的主键无关<br>e.g. 创建新关系表student (ID, name, total-credits)，    但不在student上定义主键、候选键、各类索引，student被组织为heap file<br>通过insert，将记录&#x2F;元组加到堆文件中</p>\n<p>自由空间地图<br>    每个区块一个条目。<br>    每个条目都是几位到一个字节，记录空闲块的部分<br>    例如，每个块 3 位 （0-7），值除以 8 表示可用块的分数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203175205.png\" alt=\"image.png\"><br>二级自由空间地图<br>    例如，每个条目最多存储 4 个第一级自由空间映射条目<br>定期写入磁盘的可用空间映射，可以对某些条目使用错误（旧）值（将被检测并修复）</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"13-1-Introduction\"><a href=\"#13-1-Introduction\" class=\"headerlink\" title=\"13.1 Introduction\"></a>13.1 Introduction</h3><p>物理数据库中解决的两个问题：</p>\n<ul>\n<li>数据组织，即数据的物理存储结构 — Ch13 </li>\n<li>数据访问，例如索引 — Ch14</li>\n</ul>\n<p>目标</p>\n<ul>\n<li><ol>\n<li>根据DBMS机制，选择合适的数据库物理结构</li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>在数据库表上合理设置索引，提高数据查询速度</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"13-2-File-Organization\"><a href=\"#13-2-File-Organization\" class=\"headerlink\" title=\"13.2 File Organization\"></a>13.2 File Organization</h3><p>文件逻辑&#x2F;物理组织 in Operating System Concepts</p>\n<ul>\n<li>逻辑结构</li>\n<li>基于记录的文件</li>\n<li>基于索引的文件（按内容访问）</li>\n</ul>\n<p>物理组织&#x2F;结构</p>\n<ul>\n<li>contiguous, linked, indexed</li>\n<li>以block为单位，存储 在secondary storage, i.e. disk, SSD</li>\n</ul>\n<p>DB 存储为 DB 文件的集合 </p>\n<ul>\n<li>例如.SQL服务器，主文件，辅文件，日志文件 </li>\n<li>每个文件都是一系列记录 </li>\n<li>每条记录都位于一系列字段中<br>一种方法<br>  记录大小是固定的<br>  每个文件仅包含一种特定类型的记录<br>  不同的文件用于不同的关系</li>\n</ul>\n<p>每个文件都是记录序列，每个关系表是一组元组，以元组为单位的DB逻辑文件<br>在以记录为单位的物理文件<br>元组作为记录存储在数据库中<br>将元组表示为文件中的记录<br>    固定长度记录<br>    可变长度记录<br>    例如，名称 varchar（20）</p>\n<h4 id=\"Fixed-Length-Records-固定长度记录\"><a href=\"#Fixed-Length-Records-固定长度记录\" class=\"headerlink\" title=\"Fixed-Length Records 固定长度记录\"></a>Fixed-Length Records 固定长度记录</h4><p>一个块包含多条记录<br>存储记录 i 从字节 n （i – 1） 开始，其中 n 是每条记录的大小。<br>记录访问很简单，但记录可能会跨块<br>修改： 不允许记录跨越块边界</p>\n<p>删除记录 i： 备选方案：<br>    压缩：将记录 i + 1， . . .， n 移动到 i， . . . . ， n – 1<br>    将记录 N 移动到 I<br>    不要移动记录，而是将所有空闲记录链接在空表</p>\n<h4 id=\"Free-Lists\"><a href=\"#Free-Lists\" class=\"headerlink\" title=\"Free Lists\"></a>Free Lists</h4><p>将第一条已删除记录的地址存储在文件头中<br>使用此第一个删除的记录来存储第二个已删除记录的地址，依此类推<br>将这些存储的地址视为指针，因为它们指向记录的位置。<br>更节省空间的表示形式<br>将空间重新用于可用记录的正常属性以存储指针。（使用中的记录中没有存储指针。）</p>\n<h4 id=\"Variable-Length-Records-可变长度记录\"><a href=\"#Variable-Length-Records-可变长度记录\" class=\"headerlink\" title=\"Variable-Length Records 可变长度记录\"></a>Variable-Length Records 可变长度记录</h4><p>可变长度记录以多种方式出现<br>记录类型允许可变长度，例如字符串 （varchar）<br>一个文件中有多种记录类型，例如多表聚类文件<br>记录类型允许重复字段，例如多值属性。</p>\n<h4 id=\"如何在块中存储可变长度记录\"><a href=\"#如何在块中存储可变长度记录\" class=\"headerlink\" title=\"如何在块中存储可变长度记录\"></a>如何在块中存储可变长度记录</h4><p>记录中的属性按顺序存储<br>首先是固定长度属性，然后是可变长度属性<br>可变长度属性由固定大小（偏移量、长度）表示，实际数据存储在所有固定长度属性之后<br>由 null 值位图表示的 Null 值</p>\n<h4 id=\"Variable-Length-Records-Slotted-Page-Structure可变长度记录：开槽页面结构\"><a href=\"#Variable-Length-Records-Slotted-Page-Structure可变长度记录：开槽页面结构\" class=\"headerlink\" title=\"Variable-Length Records: Slotted Page Structure可变长度记录：开槽页面结构\"></a>Variable-Length Records: Slotted Page Structure可变长度记录：开槽页面结构</h4><p>存储空间分为固定大小的开槽页，4KB或8KB，记录分配给开槽页<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174823.png\" alt=\"image.png\"><br>记录可以在页面中移动以保持连续<br>RI 被删除，RI+3、RI+2、RI+1 被移动到删除创建的可用空间<br>标头中的条目必须更新，例如设置为 -1 指针不应直接指向记录，而应指向标头中记录的条目<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203174849.png\" alt=\"image.png\"></p>\n<h3 id=\"13-3-Organization-of-Records-in-Files\"><a href=\"#13-3-Organization-of-Records-in-Files\" class=\"headerlink\" title=\"13.3 Organization of Records in Files\"></a>13.3 Organization of Records in Files</h3><p>DB文件可以看作是逻辑级别的一组记录，这些记录在逻辑上组织如下<br>    堆、序列、哈希、聚类<br>    记录的逻辑组织决定了如何访问记录，即访问方法<br>文件包括多个记录，记录在磁盘上的存放属于文件（记录）结构问题<br>对于某种结构的文件如何去查找、插入、删除记录，属于文件的存取方法<br>文件记录的组织结构决定了文件的存取方法</p>\n<p>堆 – 放置在任何有空间的地方的记录<br>    没有主键和索引的表中的数据，<br>    例如，学生（ID、姓名、总学分）<br>序列 – 根据每条记录的搜索键按顺序存储的记录<br>    例如学生（ID、姓名、总学分）， 或学生（ID、姓名、总学分），并附有姓名索引<br>    每个关系的记录可以存储在单独的文件中<br>多表聚类 – 存储在同一文件中的不同关系的记录<br>    动机：将相关记录存储在同一个块上，以最大程度地减少 I&#x2F;O，<br>    例如 讲师加入部门，讲师和部门的记录放在一个文件中</p>\n<h4 id=\"Heap-File-Organization堆文件组织\"><a href=\"#Heap-File-Organization堆文件组织\" class=\"headerlink\" title=\"Heap File Organization堆文件组织\"></a>Heap File Organization堆文件组织</h4><p>任何记录都可以放置在文件中有记录空间的任何位置<br>没有记录的顺序<br>记录在分配后通常不会移动<br>关系只有一个文件</p>\n<p>以纪录的输入顺序为序，决定了文件中记录顺序<br>    纪录的存储顺序与记录中的主键无关<br>e.g. 创建新关系表student (ID, name, total-credits)，    但不在student上定义主键、候选键、各类索引，student被组织为heap file<br>通过insert，将记录&#x2F;元组加到堆文件中</p>\n<p>自由空间地图<br>    每个区块一个条目。<br>    每个条目都是几位到一个字节，记录空闲块的部分<br>    例如，每个块 3 位 （0-7），值除以 8 表示可用块的分数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231203175205.png\" alt=\"image.png\"><br>二级自由空间地图<br>    例如，每个条目最多存储 4 个第一级自由空间映射条目<br>定期写入磁盘的可用空间映射，可以对某些条目使用错误（旧）值（将被检测并修复）</p>\n"},{"title":"SQL高级","date":"2023-10-23T01:53:47.998Z","_content":"### 5.1 Accessing SQL from Programming Language\n应用程序执行数据处理，并调用\n- 与数据库服务器连接\n- 将SQL命令发送到数据库服务器\n- 将结果的元组逐个提取到程序变量中\n\n两种方法访问SQL：\n**dynamic SQL**：\n程序用function连接数据库服务器并与之通信\n程序将 SQL 查询构造为字符串，提交查询，然后将结果检索到程序变量中\n- JDBC (Java DB Connectivity) with Java\n- ODBC (Open DB Connectivity) with C, C++, and Visual Basic\n**embedded SQL**：\nSQL 语句在编译时在高级程序中转换为函数调用\n这些函数调用使用提供动态 SQL 功能的 API 连接到数据库\n\n#### 5.1.1 JDBC\nJDBC 是一个基于 Java 的 API\n支持查询、更新和检索\n与数据库通信的模型：\n1. 创建连接\n2. 创建SQL statement（语句）对象\n3. 用statement对象执行查询并获取结果\n4. 处理错误的异常机制\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023103147.png)\n\n使用statement对象 stmt 执行查询以发送查询并获取结果\n使用 `execute.query` 或 `execute.update`，例如 insert/delete/update/createtable\n参数：要执行的SQL语句，表示为字符串\n获取查询结果，使用 `try{...}/catch{...}` 构造\n将结果中的元组集检索到 ResultSet 对象 rset 中，并一次获取一个元组\n`next()`方法测试结果集是否至少有一个元组，如果是，则获取它\n获取结果：如果 dept_name 是 select result 的第一个参数，则`rs.getString（“dept_name”）` 和 `rs.getString(1)`是等价的\n\n##### Prepared Statement\n创建一个prepared statement，其中某些值将替换为`?`，在使用时指定实际值\n`setString()`方法和其他方法，如`setInt()`指定参数的值\n创建：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105724.png)\n设置值：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105834.png)\n第一个参数指定我们要为其赋值的`?`参数，第二个参数指定要分配的值\n\n##### Metadata Features\n获取查询结果集中列的数目（结果关系的属性总数），输出各列的列名、数据类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023110241.png)\n`DatabaseMetaData dbmd=conn.getMetaData()`\n参数：catalog目录、schemas架构模式、table表模式和column列模式\n`null`表示所有目录/schemas\n`\"\"`表示当前目录/schemas\n`%`与SQL子句`like`含义相同\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023111208.png)\n\n##### Transaction Control\n每个 SQL 语句都被视为一个单独的transaction，默认情况下自动提交\n关闭连接上的自动提交:`conn.setAutoCommit (false);`\n在连接上启用自动提交:`conn.setAutoCommit (ture);`\nTransaction必须最终提交或回滚：`conn.commit();` or `conn.rollback();`\n\n##### Other Features\n- 调用函数和过程\n`CallableStatement cStmt1 = conn.prepareCall(\"{? = call some function(?)}\");`\n`CallableStatement cStmt2 = conn.prepareCall(\"{call some procedure(?,?)}\");`\n- 处理大型对象类型\n`getBlob（）` 和 `getClob（）` 类似于 `getString（）` 方法，但分别返回 Blob 和 Clob 类型的对象\n通过 `getBytes（）` 从这些对象获取数据\n\n##### Embedded SQL: SQLJ in Java\nSQLJ：Java 中的嵌入式 SQL\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112106.png)\n\n\n#### 5.1.2 从python访问数据库\n#### 5.1.3 ODBC\n开放式数据库连接 （ODBC） 标准，用于应用程序（作为客户端）与数据库服务器通信\n应用程序接口 （API） 到：\n- 打开与数据库的连接\n- 发送查询和更新\n- 取回结果\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112333.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023113307.png)\n\n#### 5.1.4 Embedded SQL\n将 SQL 用作数据库查询工具的方法\n1. **interactive SQL交互式 SQL**: 通过DBS人机界面直接用作DML和DDL\n2. **dynamic SQL动态 SQL**: e.g JDBC, ODBC\n3. **embedded SQL嵌入式 SQL**: 嵌入在通用编程语言中，例如 C 语言\n交互式SQL只能进行DB的访问操作，不能对DB访问结果进行进一步的数据处理，Embedded SQL将SQL的数据库访问功能与C语言等宿主语言的数据处理能力相结合，提高了数据应用系统的能力\nSQL标准定义了C，C++，Pascal，Fortran和Cobol等语言中的SQL嵌入, 嵌入 SQL 查询的语言称为宿主语言（host language）\n\n在执行任何 SQL 语句之前，程序必须首先连接到数据库![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114744.png)\n`EXEC SQL`语句用于标识对预处理器的嵌入式 SQL 请求![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114708.png)\n可以在嵌入式 SQL 语句中使用主语言的变量。 它们前面带有冒号`:`以区别于 SQL 变量。如上所述使用的变量必须在 DECLARE 部分中声明，用于声明变量的语法遵循通常的主语言语法。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115411.png)\n\n##### Cursor in Embedded SQL 游标\n要编写嵌入式 SQL 查询，我们使用语句![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115748.png)\n变量 c 用于标识查询\n利用Embedded SQL进行查询时，查询结果有可能包括多个元组，此时无法直接将多个元组通过共享变量赋值传递给宿主程序\n系统开辟专门working区域存放SQL查询的结果关系，并利用查询游标c指向此区域。宿主程序根据c指向的查询结果关系集合，使用open, fetch, close依次获取结果关系中的各元组\n游标c相当于一个临时的table\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023120312.png)\n\n##### Updates Through Embedded SQL\n用于修改（更新、插入和删除）的嵌入式 SQL 表达式\n可以通过更新游标来更新游标fetch的tuples\n\n### 5.2 Functions and Procedures\n定义一个函数：\ne.g. 给定一个部门的名称，返回教师人数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095637.png)\n定义函数后可以在查找时直接使用：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095946.png)\n\n过程：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100409.png)\n可以使用 `call` 语句从 SQL 程序或嵌入式 SQL 调用过程。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100501.png)\n\n过程和函数也可以从动态 SQL 调用。函数、存储过程预先生成对应的查询执行计划（类似于目标代码），存储在DBMS中，应用程序直接调用，不需再进行查询处理和优化\n\n### 5.3 Trigger（触发器）\n触发器是一种基于事件-条件-操作模型的机制，作用：完整性定义、检查和补救措施\n触发器执行将在完整性约束检查时进行：指定触发器执行时要执行的操作，如果违反约束，则采取补救措施\n\n指定导致触发器执行的事件（插入、删除、更新）\n更新：更新时的触发器可以限制为特定属性\n`before`和`after`属性值:\n- `referencing old row as`：用于删除和更新\n- `referencing new row as`：用于插入和更新\n\n将空白成绩转换为 null:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030104701.png)\n\n可以对受事务影响的所有行执行单个操作，而不是对每个受影响的行执行单独的操作\n\n","source":"_posts/Notes/课程/大三（上）/数据库/SQL高级.md","raw":"---\ntitle: SQL高级\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 数据库\ntags:\n  - 数据库\n  - SQL\ndate:\n---\n### 5.1 Accessing SQL from Programming Language\n应用程序执行数据处理，并调用\n- 与数据库服务器连接\n- 将SQL命令发送到数据库服务器\n- 将结果的元组逐个提取到程序变量中\n\n两种方法访问SQL：\n**dynamic SQL**：\n程序用function连接数据库服务器并与之通信\n程序将 SQL 查询构造为字符串，提交查询，然后将结果检索到程序变量中\n- JDBC (Java DB Connectivity) with Java\n- ODBC (Open DB Connectivity) with C, C++, and Visual Basic\n**embedded SQL**：\nSQL 语句在编译时在高级程序中转换为函数调用\n这些函数调用使用提供动态 SQL 功能的 API 连接到数据库\n\n#### 5.1.1 JDBC\nJDBC 是一个基于 Java 的 API\n支持查询、更新和检索\n与数据库通信的模型：\n1. 创建连接\n2. 创建SQL statement（语句）对象\n3. 用statement对象执行查询并获取结果\n4. 处理错误的异常机制\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023103147.png)\n\n使用statement对象 stmt 执行查询以发送查询并获取结果\n使用 `execute.query` 或 `execute.update`，例如 insert/delete/update/createtable\n参数：要执行的SQL语句，表示为字符串\n获取查询结果，使用 `try{...}/catch{...}` 构造\n将结果中的元组集检索到 ResultSet 对象 rset 中，并一次获取一个元组\n`next()`方法测试结果集是否至少有一个元组，如果是，则获取它\n获取结果：如果 dept_name 是 select result 的第一个参数，则`rs.getString（“dept_name”）` 和 `rs.getString(1)`是等价的\n\n##### Prepared Statement\n创建一个prepared statement，其中某些值将替换为`?`，在使用时指定实际值\n`setString()`方法和其他方法，如`setInt()`指定参数的值\n创建：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105724.png)\n设置值：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105834.png)\n第一个参数指定我们要为其赋值的`?`参数，第二个参数指定要分配的值\n\n##### Metadata Features\n获取查询结果集中列的数目（结果关系的属性总数），输出各列的列名、数据类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023110241.png)\n`DatabaseMetaData dbmd=conn.getMetaData()`\n参数：catalog目录、schemas架构模式、table表模式和column列模式\n`null`表示所有目录/schemas\n`\"\"`表示当前目录/schemas\n`%`与SQL子句`like`含义相同\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023111208.png)\n\n##### Transaction Control\n每个 SQL 语句都被视为一个单独的transaction，默认情况下自动提交\n关闭连接上的自动提交:`conn.setAutoCommit (false);`\n在连接上启用自动提交:`conn.setAutoCommit (ture);`\nTransaction必须最终提交或回滚：`conn.commit();` or `conn.rollback();`\n\n##### Other Features\n- 调用函数和过程\n`CallableStatement cStmt1 = conn.prepareCall(\"{? = call some function(?)}\");`\n`CallableStatement cStmt2 = conn.prepareCall(\"{call some procedure(?,?)}\");`\n- 处理大型对象类型\n`getBlob（）` 和 `getClob（）` 类似于 `getString（）` 方法，但分别返回 Blob 和 Clob 类型的对象\n通过 `getBytes（）` 从这些对象获取数据\n\n##### Embedded SQL: SQLJ in Java\nSQLJ：Java 中的嵌入式 SQL\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112106.png)\n\n\n#### 5.1.2 从python访问数据库\n#### 5.1.3 ODBC\n开放式数据库连接 （ODBC） 标准，用于应用程序（作为客户端）与数据库服务器通信\n应用程序接口 （API） 到：\n- 打开与数据库的连接\n- 发送查询和更新\n- 取回结果\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112333.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023113307.png)\n\n#### 5.1.4 Embedded SQL\n将 SQL 用作数据库查询工具的方法\n1. **interactive SQL交互式 SQL**: 通过DBS人机界面直接用作DML和DDL\n2. **dynamic SQL动态 SQL**: e.g JDBC, ODBC\n3. **embedded SQL嵌入式 SQL**: 嵌入在通用编程语言中，例如 C 语言\n交互式SQL只能进行DB的访问操作，不能对DB访问结果进行进一步的数据处理，Embedded SQL将SQL的数据库访问功能与C语言等宿主语言的数据处理能力相结合，提高了数据应用系统的能力\nSQL标准定义了C，C++，Pascal，Fortran和Cobol等语言中的SQL嵌入, 嵌入 SQL 查询的语言称为宿主语言（host language）\n\n在执行任何 SQL 语句之前，程序必须首先连接到数据库![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114744.png)\n`EXEC SQL`语句用于标识对预处理器的嵌入式 SQL 请求![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114708.png)\n可以在嵌入式 SQL 语句中使用主语言的变量。 它们前面带有冒号`:`以区别于 SQL 变量。如上所述使用的变量必须在 DECLARE 部分中声明，用于声明变量的语法遵循通常的主语言语法。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115411.png)\n\n##### Cursor in Embedded SQL 游标\n要编写嵌入式 SQL 查询，我们使用语句![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115748.png)\n变量 c 用于标识查询\n利用Embedded SQL进行查询时，查询结果有可能包括多个元组，此时无法直接将多个元组通过共享变量赋值传递给宿主程序\n系统开辟专门working区域存放SQL查询的结果关系，并利用查询游标c指向此区域。宿主程序根据c指向的查询结果关系集合，使用open, fetch, close依次获取结果关系中的各元组\n游标c相当于一个临时的table\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023120312.png)\n\n##### Updates Through Embedded SQL\n用于修改（更新、插入和删除）的嵌入式 SQL 表达式\n可以通过更新游标来更新游标fetch的tuples\n\n### 5.2 Functions and Procedures\n定义一个函数：\ne.g. 给定一个部门的名称，返回教师人数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095637.png)\n定义函数后可以在查找时直接使用：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095946.png)\n\n过程：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100409.png)\n可以使用 `call` 语句从 SQL 程序或嵌入式 SQL 调用过程。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100501.png)\n\n过程和函数也可以从动态 SQL 调用。函数、存储过程预先生成对应的查询执行计划（类似于目标代码），存储在DBMS中，应用程序直接调用，不需再进行查询处理和优化\n\n### 5.3 Trigger（触发器）\n触发器是一种基于事件-条件-操作模型的机制，作用：完整性定义、检查和补救措施\n触发器执行将在完整性约束检查时进行：指定触发器执行时要执行的操作，如果违反约束，则采取补救措施\n\n指定导致触发器执行的事件（插入、删除、更新）\n更新：更新时的触发器可以限制为特定属性\n`before`和`after`属性值:\n- `referencing old row as`：用于删除和更新\n- `referencing new row as`：用于插入和更新\n\n将空白成绩转换为 null:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030104701.png)\n\n可以对受事务影响的所有行执行单个操作，而不是对每个受影响的行执行单独的操作\n\n","slug":"Notes/课程/大三（上）/数据库/SQL高级","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr1260011vw8c39ji8f3l","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"5-1-Accessing-SQL-from-Programming-Language\"><a href=\"#5-1-Accessing-SQL-from-Programming-Language\" class=\"headerlink\" title=\"5.1 Accessing SQL from Programming Language\"></a>5.1 Accessing SQL from Programming Language</h3><p>应用程序执行数据处理，并调用</p>\n<ul>\n<li>与数据库服务器连接</li>\n<li>将SQL命令发送到数据库服务器</li>\n<li>将结果的元组逐个提取到程序变量中</li>\n</ul>\n<p>两种方法访问SQL：<br><strong>dynamic SQL</strong>：<br>程序用function连接数据库服务器并与之通信<br>程序将 SQL 查询构造为字符串，提交查询，然后将结果检索到程序变量中</p>\n<ul>\n<li>JDBC (Java DB Connectivity) with Java</li>\n<li>ODBC (Open DB Connectivity) with C, C++, and Visual Basic<br><strong>embedded SQL</strong>：<br>SQL 语句在编译时在高级程序中转换为函数调用<br>这些函数调用使用提供动态 SQL 功能的 API 连接到数据库</li>\n</ul>\n<h4 id=\"5-1-1-JDBC\"><a href=\"#5-1-1-JDBC\" class=\"headerlink\" title=\"5.1.1 JDBC\"></a>5.1.1 JDBC</h4><p>JDBC 是一个基于 Java 的 API<br>支持查询、更新和检索<br>与数据库通信的模型：</p>\n<ol>\n<li>创建连接</li>\n<li>创建SQL statement（语句）对象</li>\n<li>用statement对象执行查询并获取结果</li>\n<li>处理错误的异常机制</li>\n</ol>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023103147.png\" alt=\"image.png\"></p>\n<p>使用statement对象 stmt 执行查询以发送查询并获取结果<br>使用 <code>execute.query</code> 或 <code>execute.update</code>，例如 insert&#x2F;delete&#x2F;update&#x2F;createtable<br>参数：要执行的SQL语句，表示为字符串<br>获取查询结果，使用 <code>try&#123;...&#125;/catch&#123;...&#125;</code> 构造<br>将结果中的元组集检索到 ResultSet 对象 rset 中，并一次获取一个元组<br><code>next()</code>方法测试结果集是否至少有一个元组，如果是，则获取它<br>获取结果：如果 dept_name 是 select result 的第一个参数，则<code>rs.getString（“dept_name”）</code> 和 <code>rs.getString(1)</code>是等价的</p>\n<h5 id=\"Prepared-Statement\"><a href=\"#Prepared-Statement\" class=\"headerlink\" title=\"Prepared Statement\"></a>Prepared Statement</h5><p>创建一个prepared statement，其中某些值将替换为<code>?</code>，在使用时指定实际值<br><code>setString()</code>方法和其他方法，如<code>setInt()</code>指定参数的值<br>创建：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105724.png\" alt=\"image.png\"><br>设置值：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105834.png\" alt=\"image.png\"><br>第一个参数指定我们要为其赋值的<code>?</code>参数，第二个参数指定要分配的值</p>\n<h5 id=\"Metadata-Features\"><a href=\"#Metadata-Features\" class=\"headerlink\" title=\"Metadata Features\"></a>Metadata Features</h5><p>获取查询结果集中列的数目（结果关系的属性总数），输出各列的列名、数据类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023110241.png\" alt=\"image.png\"><br><code>DatabaseMetaData dbmd=conn.getMetaData()</code><br>参数：catalog目录、schemas架构模式、table表模式和column列模式<br><code>null</code>表示所有目录&#x2F;schemas<br><code>&quot;&quot;</code>表示当前目录&#x2F;schemas<br><code>%</code>与SQL子句<code>like</code>含义相同<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023111208.png\" alt=\"image.png\"></p>\n<h5 id=\"Transaction-Control\"><a href=\"#Transaction-Control\" class=\"headerlink\" title=\"Transaction Control\"></a>Transaction Control</h5><p>每个 SQL 语句都被视为一个单独的transaction，默认情况下自动提交<br>关闭连接上的自动提交:<code>conn.setAutoCommit (false);</code><br>在连接上启用自动提交:<code>conn.setAutoCommit (ture);</code><br>Transaction必须最终提交或回滚：<code>conn.commit();</code> or <code>conn.rollback();</code></p>\n<h5 id=\"Other-Features\"><a href=\"#Other-Features\" class=\"headerlink\" title=\"Other Features\"></a>Other Features</h5><ul>\n<li>调用函数和过程<br><code>CallableStatement cStmt1 = conn.prepareCall(&quot;&#123;? = call some function(?)&#125;&quot;);</code><br><code>CallableStatement cStmt2 = conn.prepareCall(&quot;&#123;call some procedure(?,?)&#125;&quot;);</code></li>\n<li>处理大型对象类型<br><code>getBlob（）</code> 和 <code>getClob（）</code> 类似于 <code>getString（）</code> 方法，但分别返回 Blob 和 Clob 类型的对象<br>通过 <code>getBytes（）</code> 从这些对象获取数据</li>\n</ul>\n<h5 id=\"Embedded-SQL-SQLJ-in-Java\"><a href=\"#Embedded-SQL-SQLJ-in-Java\" class=\"headerlink\" title=\"Embedded SQL: SQLJ in Java\"></a>Embedded SQL: SQLJ in Java</h5><p>SQLJ：Java 中的嵌入式 SQL<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112106.png\" alt=\"image.png\"></p>\n<h4 id=\"5-1-2-从python访问数据库\"><a href=\"#5-1-2-从python访问数据库\" class=\"headerlink\" title=\"5.1.2 从python访问数据库\"></a>5.1.2 从python访问数据库</h4><h4 id=\"5-1-3-ODBC\"><a href=\"#5-1-3-ODBC\" class=\"headerlink\" title=\"5.1.3 ODBC\"></a>5.1.3 ODBC</h4><p>开放式数据库连接 （ODBC） 标准，用于应用程序（作为客户端）与数据库服务器通信<br>应用程序接口 （API） 到：</p>\n<ul>\n<li>打开与数据库的连接</li>\n<li>发送查询和更新</li>\n<li>取回结果<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112333.png\" alt=\"image.png\"></li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023113307.png\" alt=\"image.png\"></p>\n<h4 id=\"5-1-4-Embedded-SQL\"><a href=\"#5-1-4-Embedded-SQL\" class=\"headerlink\" title=\"5.1.4 Embedded SQL\"></a>5.1.4 Embedded SQL</h4><p>将 SQL 用作数据库查询工具的方法</p>\n<ol>\n<li><strong>interactive SQL交互式 SQL</strong>: 通过DBS人机界面直接用作DML和DDL</li>\n<li><strong>dynamic SQL动态 SQL</strong>: e.g JDBC, ODBC</li>\n<li><strong>embedded SQL嵌入式 SQL</strong>: 嵌入在通用编程语言中，例如 C 语言<br>交互式SQL只能进行DB的访问操作，不能对DB访问结果进行进一步的数据处理，Embedded SQL将SQL的数据库访问功能与C语言等宿主语言的数据处理能力相结合，提高了数据应用系统的能力<br>SQL标准定义了C，C++，Pascal，Fortran和Cobol等语言中的SQL嵌入, 嵌入 SQL 查询的语言称为宿主语言（host language）</li>\n</ol>\n<p>在执行任何 SQL 语句之前，程序必须首先连接到数据库<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114744.png\" alt=\"image.png\"><br><code>EXEC SQL</code>语句用于标识对预处理器的嵌入式 SQL 请求<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114708.png\" alt=\"image.png\"><br>可以在嵌入式 SQL 语句中使用主语言的变量。 它们前面带有冒号<code>:</code>以区别于 SQL 变量。如上所述使用的变量必须在 DECLARE 部分中声明，用于声明变量的语法遵循通常的主语言语法。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115411.png\" alt=\"image.png\"></p>\n<h5 id=\"Cursor-in-Embedded-SQL-游标\"><a href=\"#Cursor-in-Embedded-SQL-游标\" class=\"headerlink\" title=\"Cursor in Embedded SQL 游标\"></a>Cursor in Embedded SQL 游标</h5><p>要编写嵌入式 SQL 查询，我们使用语句<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115748.png\" alt=\"image.png\"><br>变量 c 用于标识查询<br>利用Embedded SQL进行查询时，查询结果有可能包括多个元组，此时无法直接将多个元组通过共享变量赋值传递给宿主程序<br>系统开辟专门working区域存放SQL查询的结果关系，并利用查询游标c指向此区域。宿主程序根据c指向的查询结果关系集合，使用open, fetch, close依次获取结果关系中的各元组<br>游标c相当于一个临时的table<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023120312.png\" alt=\"image.png\"></p>\n<h5 id=\"Updates-Through-Embedded-SQL\"><a href=\"#Updates-Through-Embedded-SQL\" class=\"headerlink\" title=\"Updates Through Embedded SQL\"></a>Updates Through Embedded SQL</h5><p>用于修改（更新、插入和删除）的嵌入式 SQL 表达式<br>可以通过更新游标来更新游标fetch的tuples</p>\n<h3 id=\"5-2-Functions-and-Procedures\"><a href=\"#5-2-Functions-and-Procedures\" class=\"headerlink\" title=\"5.2 Functions and Procedures\"></a>5.2 Functions and Procedures</h3><p>定义一个函数：<br>e.g. 给定一个部门的名称，返回教师人数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095637.png\" alt=\"image.png\"><br>定义函数后可以在查找时直接使用：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095946.png\" alt=\"image.png\"></p>\n<p>过程：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100409.png\" alt=\"image.png\"><br>可以使用 <code>call</code> 语句从 SQL 程序或嵌入式 SQL 调用过程。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100501.png\" alt=\"image.png\"></p>\n<p>过程和函数也可以从动态 SQL 调用。函数、存储过程预先生成对应的查询执行计划（类似于目标代码），存储在DBMS中，应用程序直接调用，不需再进行查询处理和优化</p>\n<h3 id=\"5-3-Trigger（触发器）\"><a href=\"#5-3-Trigger（触发器）\" class=\"headerlink\" title=\"5.3 Trigger（触发器）\"></a>5.3 Trigger（触发器）</h3><p>触发器是一种基于事件-条件-操作模型的机制，作用：完整性定义、检查和补救措施<br>触发器执行将在完整性约束检查时进行：指定触发器执行时要执行的操作，如果违反约束，则采取补救措施</p>\n<p>指定导致触发器执行的事件（插入、删除、更新）<br>更新：更新时的触发器可以限制为特定属性<br><code>before</code>和<code>after</code>属性值:</p>\n<ul>\n<li><code>referencing old row as</code>：用于删除和更新</li>\n<li><code>referencing new row as</code>：用于插入和更新</li>\n</ul>\n<p>将空白成绩转换为 null:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030104701.png\" alt=\"image.png\"></p>\n<p>可以对受事务影响的所有行执行单个操作，而不是对每个受影响的行执行单独的操作</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"5-1-Accessing-SQL-from-Programming-Language\"><a href=\"#5-1-Accessing-SQL-from-Programming-Language\" class=\"headerlink\" title=\"5.1 Accessing SQL from Programming Language\"></a>5.1 Accessing SQL from Programming Language</h3><p>应用程序执行数据处理，并调用</p>\n<ul>\n<li>与数据库服务器连接</li>\n<li>将SQL命令发送到数据库服务器</li>\n<li>将结果的元组逐个提取到程序变量中</li>\n</ul>\n<p>两种方法访问SQL：<br><strong>dynamic SQL</strong>：<br>程序用function连接数据库服务器并与之通信<br>程序将 SQL 查询构造为字符串，提交查询，然后将结果检索到程序变量中</p>\n<ul>\n<li>JDBC (Java DB Connectivity) with Java</li>\n<li>ODBC (Open DB Connectivity) with C, C++, and Visual Basic<br><strong>embedded SQL</strong>：<br>SQL 语句在编译时在高级程序中转换为函数调用<br>这些函数调用使用提供动态 SQL 功能的 API 连接到数据库</li>\n</ul>\n<h4 id=\"5-1-1-JDBC\"><a href=\"#5-1-1-JDBC\" class=\"headerlink\" title=\"5.1.1 JDBC\"></a>5.1.1 JDBC</h4><p>JDBC 是一个基于 Java 的 API<br>支持查询、更新和检索<br>与数据库通信的模型：</p>\n<ol>\n<li>创建连接</li>\n<li>创建SQL statement（语句）对象</li>\n<li>用statement对象执行查询并获取结果</li>\n<li>处理错误的异常机制</li>\n</ol>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023103147.png\" alt=\"image.png\"></p>\n<p>使用statement对象 stmt 执行查询以发送查询并获取结果<br>使用 <code>execute.query</code> 或 <code>execute.update</code>，例如 insert&#x2F;delete&#x2F;update&#x2F;createtable<br>参数：要执行的SQL语句，表示为字符串<br>获取查询结果，使用 <code>try&#123;...&#125;/catch&#123;...&#125;</code> 构造<br>将结果中的元组集检索到 ResultSet 对象 rset 中，并一次获取一个元组<br><code>next()</code>方法测试结果集是否至少有一个元组，如果是，则获取它<br>获取结果：如果 dept_name 是 select result 的第一个参数，则<code>rs.getString（“dept_name”）</code> 和 <code>rs.getString(1)</code>是等价的</p>\n<h5 id=\"Prepared-Statement\"><a href=\"#Prepared-Statement\" class=\"headerlink\" title=\"Prepared Statement\"></a>Prepared Statement</h5><p>创建一个prepared statement，其中某些值将替换为<code>?</code>，在使用时指定实际值<br><code>setString()</code>方法和其他方法，如<code>setInt()</code>指定参数的值<br>创建：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105724.png\" alt=\"image.png\"><br>设置值：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023105834.png\" alt=\"image.png\"><br>第一个参数指定我们要为其赋值的<code>?</code>参数，第二个参数指定要分配的值</p>\n<h5 id=\"Metadata-Features\"><a href=\"#Metadata-Features\" class=\"headerlink\" title=\"Metadata Features\"></a>Metadata Features</h5><p>获取查询结果集中列的数目（结果关系的属性总数），输出各列的列名、数据类型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023110241.png\" alt=\"image.png\"><br><code>DatabaseMetaData dbmd=conn.getMetaData()</code><br>参数：catalog目录、schemas架构模式、table表模式和column列模式<br><code>null</code>表示所有目录&#x2F;schemas<br><code>&quot;&quot;</code>表示当前目录&#x2F;schemas<br><code>%</code>与SQL子句<code>like</code>含义相同<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023111208.png\" alt=\"image.png\"></p>\n<h5 id=\"Transaction-Control\"><a href=\"#Transaction-Control\" class=\"headerlink\" title=\"Transaction Control\"></a>Transaction Control</h5><p>每个 SQL 语句都被视为一个单独的transaction，默认情况下自动提交<br>关闭连接上的自动提交:<code>conn.setAutoCommit (false);</code><br>在连接上启用自动提交:<code>conn.setAutoCommit (ture);</code><br>Transaction必须最终提交或回滚：<code>conn.commit();</code> or <code>conn.rollback();</code></p>\n<h5 id=\"Other-Features\"><a href=\"#Other-Features\" class=\"headerlink\" title=\"Other Features\"></a>Other Features</h5><ul>\n<li>调用函数和过程<br><code>CallableStatement cStmt1 = conn.prepareCall(&quot;&#123;? = call some function(?)&#125;&quot;);</code><br><code>CallableStatement cStmt2 = conn.prepareCall(&quot;&#123;call some procedure(?,?)&#125;&quot;);</code></li>\n<li>处理大型对象类型<br><code>getBlob（）</code> 和 <code>getClob（）</code> 类似于 <code>getString（）</code> 方法，但分别返回 Blob 和 Clob 类型的对象<br>通过 <code>getBytes（）</code> 从这些对象获取数据</li>\n</ul>\n<h5 id=\"Embedded-SQL-SQLJ-in-Java\"><a href=\"#Embedded-SQL-SQLJ-in-Java\" class=\"headerlink\" title=\"Embedded SQL: SQLJ in Java\"></a>Embedded SQL: SQLJ in Java</h5><p>SQLJ：Java 中的嵌入式 SQL<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112106.png\" alt=\"image.png\"></p>\n<h4 id=\"5-1-2-从python访问数据库\"><a href=\"#5-1-2-从python访问数据库\" class=\"headerlink\" title=\"5.1.2 从python访问数据库\"></a>5.1.2 从python访问数据库</h4><h4 id=\"5-1-3-ODBC\"><a href=\"#5-1-3-ODBC\" class=\"headerlink\" title=\"5.1.3 ODBC\"></a>5.1.3 ODBC</h4><p>开放式数据库连接 （ODBC） 标准，用于应用程序（作为客户端）与数据库服务器通信<br>应用程序接口 （API） 到：</p>\n<ul>\n<li>打开与数据库的连接</li>\n<li>发送查询和更新</li>\n<li>取回结果<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023112333.png\" alt=\"image.png\"></li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023113307.png\" alt=\"image.png\"></p>\n<h4 id=\"5-1-4-Embedded-SQL\"><a href=\"#5-1-4-Embedded-SQL\" class=\"headerlink\" title=\"5.1.4 Embedded SQL\"></a>5.1.4 Embedded SQL</h4><p>将 SQL 用作数据库查询工具的方法</p>\n<ol>\n<li><strong>interactive SQL交互式 SQL</strong>: 通过DBS人机界面直接用作DML和DDL</li>\n<li><strong>dynamic SQL动态 SQL</strong>: e.g JDBC, ODBC</li>\n<li><strong>embedded SQL嵌入式 SQL</strong>: 嵌入在通用编程语言中，例如 C 语言<br>交互式SQL只能进行DB的访问操作，不能对DB访问结果进行进一步的数据处理，Embedded SQL将SQL的数据库访问功能与C语言等宿主语言的数据处理能力相结合，提高了数据应用系统的能力<br>SQL标准定义了C，C++，Pascal，Fortran和Cobol等语言中的SQL嵌入, 嵌入 SQL 查询的语言称为宿主语言（host language）</li>\n</ol>\n<p>在执行任何 SQL 语句之前，程序必须首先连接到数据库<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114744.png\" alt=\"image.png\"><br><code>EXEC SQL</code>语句用于标识对预处理器的嵌入式 SQL 请求<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023114708.png\" alt=\"image.png\"><br>可以在嵌入式 SQL 语句中使用主语言的变量。 它们前面带有冒号<code>:</code>以区别于 SQL 变量。如上所述使用的变量必须在 DECLARE 部分中声明，用于声明变量的语法遵循通常的主语言语法。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115411.png\" alt=\"image.png\"></p>\n<h5 id=\"Cursor-in-Embedded-SQL-游标\"><a href=\"#Cursor-in-Embedded-SQL-游标\" class=\"headerlink\" title=\"Cursor in Embedded SQL 游标\"></a>Cursor in Embedded SQL 游标</h5><p>要编写嵌入式 SQL 查询，我们使用语句<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023115748.png\" alt=\"image.png\"><br>变量 c 用于标识查询<br>利用Embedded SQL进行查询时，查询结果有可能包括多个元组，此时无法直接将多个元组通过共享变量赋值传递给宿主程序<br>系统开辟专门working区域存放SQL查询的结果关系，并利用查询游标c指向此区域。宿主程序根据c指向的查询结果关系集合，使用open, fetch, close依次获取结果关系中的各元组<br>游标c相当于一个临时的table<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231023120312.png\" alt=\"image.png\"></p>\n<h5 id=\"Updates-Through-Embedded-SQL\"><a href=\"#Updates-Through-Embedded-SQL\" class=\"headerlink\" title=\"Updates Through Embedded SQL\"></a>Updates Through Embedded SQL</h5><p>用于修改（更新、插入和删除）的嵌入式 SQL 表达式<br>可以通过更新游标来更新游标fetch的tuples</p>\n<h3 id=\"5-2-Functions-and-Procedures\"><a href=\"#5-2-Functions-and-Procedures\" class=\"headerlink\" title=\"5.2 Functions and Procedures\"></a>5.2 Functions and Procedures</h3><p>定义一个函数：<br>e.g. 给定一个部门的名称，返回教师人数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095637.png\" alt=\"image.png\"><br>定义函数后可以在查找时直接使用：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030095946.png\" alt=\"image.png\"></p>\n<p>过程：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100409.png\" alt=\"image.png\"><br>可以使用 <code>call</code> 语句从 SQL 程序或嵌入式 SQL 调用过程。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030100501.png\" alt=\"image.png\"></p>\n<p>过程和函数也可以从动态 SQL 调用。函数、存储过程预先生成对应的查询执行计划（类似于目标代码），存储在DBMS中，应用程序直接调用，不需再进行查询处理和优化</p>\n<h3 id=\"5-3-Trigger（触发器）\"><a href=\"#5-3-Trigger（触发器）\" class=\"headerlink\" title=\"5.3 Trigger（触发器）\"></a>5.3 Trigger（触发器）</h3><p>触发器是一种基于事件-条件-操作模型的机制，作用：完整性定义、检查和补救措施<br>触发器执行将在完整性约束检查时进行：指定触发器执行时要执行的操作，如果违反约束，则采取补救措施</p>\n<p>指定导致触发器执行的事件（插入、删除、更新）<br>更新：更新时的触发器可以限制为特定属性<br><code>before</code>和<code>after</code>属性值:</p>\n<ul>\n<li><code>referencing old row as</code>：用于删除和更新</li>\n<li><code>referencing new row as</code>：用于插入和更新</li>\n</ul>\n<p>将空白成绩转换为 null:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231030104701.png\" alt=\"image.png\"></p>\n<p>可以对受事务影响的所有行执行单个操作，而不是对每个受影响的行执行单独的操作</p>\n"},{"title":"智能计算系统","update":null,"_content":"50期末+30课程编程作业+10大作业+10平时\n\n---\n\n智能计算系统：智能的物质载体\n- 集成CPU和智能芯片的**异构系统**\n- 面向开发者的智能计算**编程环境**\n","source":"_posts/Notes/课程/大三（上）/智能计算系统/智能计算系统.md","raw":"---\ntitle: 智能计算系统\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 智能计算系统\ntags:\n  - 智能计算系统\nupdate:\n---\n50期末+30课程编程作业+10大作业+10平时\n\n---\n\n智能计算系统：智能的物质载体\n- 集成CPU和智能芯片的**异构系统**\n- 面向开发者的智能计算**编程环境**\n","slug":"Notes/课程/大三（上）/智能计算系统/智能计算系统","published":1,"date":"2023-09-21T16:36:34.338Z","updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr1260014vw8c6ildb8kk","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>50期末+30课程编程作业+10大作业+10平时</p>\n<hr>\n<p>智能计算系统：智能的物质载体</p>\n<ul>\n<li>集成CPU和智能芯片的<strong>异构系统</strong></li>\n<li>面向开发者的智能计算<strong>编程环境</strong></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>50期末+30课程编程作业+10大作业+10平时</p>\n<hr>\n<p>智能计算系统：智能的物质载体</p>\n<ul>\n<li>集成CPU和智能芯片的<strong>异构系统</strong></li>\n<li>面向开发者的智能计算<strong>编程环境</strong></li>\n</ul>\n"},{"title":"深度学习","date":"2023-10-19T00:11:43.144Z","_content":"### 卷积神经网络（CNN）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019081247.png)\n- 局部连接：一个像素与它周围的几个像素有很强的联系，但是与离它很远的像素联系可能很弱\n- 权重共享：卷积神经网络中，权重又称为卷积模板，用于表达一种图像的特征。在图像的不同位置找特征，可以使用一样的卷积模板\n\n### CNN组成：\n- 卷积层\n- 池化层\n- 全连接层\n- Softmax\n#### 卷积层\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019082709.png)\n\n边界扩充（padding）\n- 扩大图像的尺寸并填充像素\n- 防止深度网络中图像被动持续减小\n- 强化图像边缘信息\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084544.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084557.png)\n\n#### 池化层\nMax Pooling / Avg Pooling / L2 Pooling\n提取卷积核中的最大值/平均值/L2范数\n\n- 主动减小图片尺寸，从而减少参数的数量和计算量，控制过拟合\n- 不引入额外参数\n\n#### 全连接层\n- 卷积层和池化层构成特征提取器，全连接层则为分类器\n- 将特征提取得到的高维特征图映射成一维特征向量，该特征向量包含 所有特征信息，可转化为各个类别的概率。\n\n#### softmax\n- 通常作为网络的最后一层，对输出进行归一化，输出分类概率\n- 凸显其中最大的值并抑制远低于最大值的其他分量\n- Softmax层输入、输出数据规模相同\n\n### 序列模型：循环神经网络\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031164605.png)\n\n- 时序sequence：RNN能建模序列数据，序列指的是前、后输入数据( 𝒙 (𝑡) ， 𝒙 (𝑡+1) )不独立，相互影响； \n- 循环recurrent：对每个输入的操作都是一样的，循环往复地重复这些相 同操作，每时刻有相同参数W和U（参数共享）；\n- 记忆memory： 隐藏层𝒉 (𝑡)中捕捉了所有时刻t之前的信息，理论上𝒉 (𝑡)记 忆的内容可以无限长，然而实际上记忆还是有限的；\n\n#### 正向计算过程\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165458.png)\n\n#### 反向传播\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165539.png)\n\n#### RNN的梯度爆炸和梯度消失\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165725.png)\n由于梯度爆炸或消失的存在，循环神经网络实际上只能学习到短期的依赖关系，无法处理长期依赖关系\n\n改进梯度爆炸：梯度截断\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170209.png)\n\n### 长短期记忆模型（LSTM）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170726.png)\n\n- 隐藏状态：作为神经网络的记忆，保存着网络先前观察到的数据信息。\n- 单元状态：类似信息传送带，它贯穿整个链条，只有一些小的线性相互作用；这很容 易让信息以不变的方式向下流动；LSTM有能力向单元状态中移除或添加信息，这种管理结构称为门限\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170856.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170929.png)\n\n#### 变体\n##### 窥视孔连接\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171128.png)\n\n##### 耦合输入门和遗忘门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171150.png)\n\n##### GRU\n在LSTM的基础上，将单元状态和隐藏状态合并，将遗忘门和输入门合并为更新门，无输出门。更新门决定历史信息和当前信息如何相加；重置门决定保留多少历史信息进入当前信息\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171221.png)\n\n##### LSTM与GRU\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171330.png)\nGRU参数量更少，训练速度快；在训练数据足够的情况下，LSTM的表征能力更强。\n\n### 生成对抗网络（GAN）\n损失函数通过学习得到，网络学习到的是数据的分布\n模型由两部分组成：\n- 生成器（伪装者）：找出观测数据内部的统计规律，尽可能生成 能够以假乱真的样本，使判别网络输出接近0.5，难以区分真假。\n- 判别器（警察）：判断输入数据是来自真实样本集还是生成样本 集。如果输入是真样本，输出接近1；如果输入是生成样本，输出接近0。\n\n#### 训练过程\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031173437.png)\n\n更新k次判别器后更新一次生成器\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174016.png)\n\n解决梯度消失问题：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174326.png)\n\n#### 模式崩溃（model collapse）\n产生原因： GAN的损失函数使判别器假样本的惩罚是巨大的，一旦生成的某一类 假样本成功骗过判别器，生成器就趋向于生成相似的样本，导致生成样本缺乏多样性\n应对方法（WGAN）：采用更加平滑的损失函数，参见Wasserstein GAN\n\n#### GAN结构变种\n卷积GAN\n- DCGAN：将GAN中全连接神经网络扩展到卷积神经网络\n- ResGAN：图像恢复，ResNet\n- SRGAN：超分辨率，ResNet\n- CycleGAN：图像转换\n条件GAN\n- CGAN\n- InfoGAN\n集成推断模型的GAN\n- BiGAN\n对抗自编码器\n- VAE-GAN\n\n### 图像风格迁移\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109080759.png)\n给定一张风格图像 a 和一张内容图像 p:\n- 风格图像 a 经过 CNN 生成的 feature maps 组成风格特征集A\n- 内容图像 p 通过 CNN 生成的 feature maps 组成内容特征集P\n- 输入一张随机噪声图像 x，随机噪声图像 x 通过 CNN 生成的 feature maps 构 成内容特征和风格特征集合 F和 G ，目标损失函数由 A, P, F ,G 计算得到\n- 优化函数是希望调整图像 x，使其最后看起来既保持内容图像 p的内容, 又有风 格图像 a 的风格。\n- CNN网络为在imageNet上训练好的VGG19， 去除了最后的全连接层和softmax\n- 损失函数：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081123.png)![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081133.png)\n\n","source":"_posts/Notes/课程/大三（上）/智能计算系统/深度学习.md","raw":"---\ntitle: 深度学习\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 智能计算系统\ntags:\n  - 深度学习\ndate:\n---\n### 卷积神经网络（CNN）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019081247.png)\n- 局部连接：一个像素与它周围的几个像素有很强的联系，但是与离它很远的像素联系可能很弱\n- 权重共享：卷积神经网络中，权重又称为卷积模板，用于表达一种图像的特征。在图像的不同位置找特征，可以使用一样的卷积模板\n\n### CNN组成：\n- 卷积层\n- 池化层\n- 全连接层\n- Softmax\n#### 卷积层\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019082709.png)\n\n边界扩充（padding）\n- 扩大图像的尺寸并填充像素\n- 防止深度网络中图像被动持续减小\n- 强化图像边缘信息\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084544.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084557.png)\n\n#### 池化层\nMax Pooling / Avg Pooling / L2 Pooling\n提取卷积核中的最大值/平均值/L2范数\n\n- 主动减小图片尺寸，从而减少参数的数量和计算量，控制过拟合\n- 不引入额外参数\n\n#### 全连接层\n- 卷积层和池化层构成特征提取器，全连接层则为分类器\n- 将特征提取得到的高维特征图映射成一维特征向量，该特征向量包含 所有特征信息，可转化为各个类别的概率。\n\n#### softmax\n- 通常作为网络的最后一层，对输出进行归一化，输出分类概率\n- 凸显其中最大的值并抑制远低于最大值的其他分量\n- Softmax层输入、输出数据规模相同\n\n### 序列模型：循环神经网络\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031164605.png)\n\n- 时序sequence：RNN能建模序列数据，序列指的是前、后输入数据( 𝒙 (𝑡) ， 𝒙 (𝑡+1) )不独立，相互影响； \n- 循环recurrent：对每个输入的操作都是一样的，循环往复地重复这些相 同操作，每时刻有相同参数W和U（参数共享）；\n- 记忆memory： 隐藏层𝒉 (𝑡)中捕捉了所有时刻t之前的信息，理论上𝒉 (𝑡)记 忆的内容可以无限长，然而实际上记忆还是有限的；\n\n#### 正向计算过程\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165458.png)\n\n#### 反向传播\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165539.png)\n\n#### RNN的梯度爆炸和梯度消失\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165725.png)\n由于梯度爆炸或消失的存在，循环神经网络实际上只能学习到短期的依赖关系，无法处理长期依赖关系\n\n改进梯度爆炸：梯度截断\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170209.png)\n\n### 长短期记忆模型（LSTM）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170726.png)\n\n- 隐藏状态：作为神经网络的记忆，保存着网络先前观察到的数据信息。\n- 单元状态：类似信息传送带，它贯穿整个链条，只有一些小的线性相互作用；这很容 易让信息以不变的方式向下流动；LSTM有能力向单元状态中移除或添加信息，这种管理结构称为门限\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170856.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170929.png)\n\n#### 变体\n##### 窥视孔连接\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171128.png)\n\n##### 耦合输入门和遗忘门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171150.png)\n\n##### GRU\n在LSTM的基础上，将单元状态和隐藏状态合并，将遗忘门和输入门合并为更新门，无输出门。更新门决定历史信息和当前信息如何相加；重置门决定保留多少历史信息进入当前信息\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171221.png)\n\n##### LSTM与GRU\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171330.png)\nGRU参数量更少，训练速度快；在训练数据足够的情况下，LSTM的表征能力更强。\n\n### 生成对抗网络（GAN）\n损失函数通过学习得到，网络学习到的是数据的分布\n模型由两部分组成：\n- 生成器（伪装者）：找出观测数据内部的统计规律，尽可能生成 能够以假乱真的样本，使判别网络输出接近0.5，难以区分真假。\n- 判别器（警察）：判断输入数据是来自真实样本集还是生成样本 集。如果输入是真样本，输出接近1；如果输入是生成样本，输出接近0。\n\n#### 训练过程\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031173437.png)\n\n更新k次判别器后更新一次生成器\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174016.png)\n\n解决梯度消失问题：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174326.png)\n\n#### 模式崩溃（model collapse）\n产生原因： GAN的损失函数使判别器假样本的惩罚是巨大的，一旦生成的某一类 假样本成功骗过判别器，生成器就趋向于生成相似的样本，导致生成样本缺乏多样性\n应对方法（WGAN）：采用更加平滑的损失函数，参见Wasserstein GAN\n\n#### GAN结构变种\n卷积GAN\n- DCGAN：将GAN中全连接神经网络扩展到卷积神经网络\n- ResGAN：图像恢复，ResNet\n- SRGAN：超分辨率，ResNet\n- CycleGAN：图像转换\n条件GAN\n- CGAN\n- InfoGAN\n集成推断模型的GAN\n- BiGAN\n对抗自编码器\n- VAE-GAN\n\n### 图像风格迁移\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109080759.png)\n给定一张风格图像 a 和一张内容图像 p:\n- 风格图像 a 经过 CNN 生成的 feature maps 组成风格特征集A\n- 内容图像 p 通过 CNN 生成的 feature maps 组成内容特征集P\n- 输入一张随机噪声图像 x，随机噪声图像 x 通过 CNN 生成的 feature maps 构 成内容特征和风格特征集合 F和 G ，目标损失函数由 A, P, F ,G 计算得到\n- 优化函数是希望调整图像 x，使其最后看起来既保持内容图像 p的内容, 又有风 格图像 a 的风格。\n- CNN网络为在imageNet上训练好的VGG19， 去除了最后的全连接层和softmax\n- 损失函数：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081123.png)![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081133.png)\n\n","slug":"Notes/课程/大三（上）/智能计算系统/深度学习","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr1270017vw8cdn33gn9p","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"卷积神经网络（CNN）\"><a href=\"#卷积神经网络（CNN）\" class=\"headerlink\" title=\"卷积神经网络（CNN）\"></a>卷积神经网络（CNN）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019081247.png\" alt=\"image.png\"></p>\n<ul>\n<li>局部连接：一个像素与它周围的几个像素有很强的联系，但是与离它很远的像素联系可能很弱</li>\n<li>权重共享：卷积神经网络中，权重又称为卷积模板，用于表达一种图像的特征。在图像的不同位置找特征，可以使用一样的卷积模板</li>\n</ul>\n<h3 id=\"CNN组成：\"><a href=\"#CNN组成：\" class=\"headerlink\" title=\"CNN组成：\"></a>CNN组成：</h3><ul>\n<li>卷积层</li>\n<li>池化层</li>\n<li>全连接层</li>\n<li>Softmax</li>\n</ul>\n<h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019082709.png\" alt=\"image.png\"></p>\n<p>边界扩充（padding）</p>\n<ul>\n<li>扩大图像的尺寸并填充像素</li>\n<li>防止深度网络中图像被动持续减小</li>\n<li>强化图像边缘信息<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084544.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084557.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h4><p>Max Pooling &#x2F; Avg Pooling &#x2F; L2 Pooling<br>提取卷积核中的最大值&#x2F;平均值&#x2F;L2范数</p>\n<ul>\n<li>主动减小图片尺寸，从而减少参数的数量和计算量，控制过拟合</li>\n<li>不引入额外参数</li>\n</ul>\n<h4 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h4><ul>\n<li>卷积层和池化层构成特征提取器，全连接层则为分类器</li>\n<li>将特征提取得到的高维特征图映射成一维特征向量，该特征向量包含 所有特征信息，可转化为各个类别的概率。</li>\n</ul>\n<h4 id=\"softmax\"><a href=\"#softmax\" class=\"headerlink\" title=\"softmax\"></a>softmax</h4><ul>\n<li>通常作为网络的最后一层，对输出进行归一化，输出分类概率</li>\n<li>凸显其中最大的值并抑制远低于最大值的其他分量</li>\n<li>Softmax层输入、输出数据规模相同</li>\n</ul>\n<h3 id=\"序列模型：循环神经网络\"><a href=\"#序列模型：循环神经网络\" class=\"headerlink\" title=\"序列模型：循环神经网络\"></a>序列模型：循环神经网络</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031164605.png\" alt=\"image.png\"></p>\n<ul>\n<li>时序sequence：RNN能建模序列数据，序列指的是前、后输入数据( 𝒙 (𝑡) ， 𝒙 (𝑡+1) )不独立，相互影响； </li>\n<li>循环recurrent：对每个输入的操作都是一样的，循环往复地重复这些相 同操作，每时刻有相同参数W和U（参数共享）；</li>\n<li>记忆memory： 隐藏层𝒉 (𝑡)中捕捉了所有时刻t之前的信息，理论上𝒉 (𝑡)记 忆的内容可以无限长，然而实际上记忆还是有限的；</li>\n</ul>\n<h4 id=\"正向计算过程\"><a href=\"#正向计算过程\" class=\"headerlink\" title=\"正向计算过程\"></a>正向计算过程</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165458.png\" alt=\"image.png\"></p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165539.png\" alt=\"image.png\"></p>\n<h4 id=\"RNN的梯度爆炸和梯度消失\"><a href=\"#RNN的梯度爆炸和梯度消失\" class=\"headerlink\" title=\"RNN的梯度爆炸和梯度消失\"></a>RNN的梯度爆炸和梯度消失</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165725.png\" alt=\"image.png\"><br>由于梯度爆炸或消失的存在，循环神经网络实际上只能学习到短期的依赖关系，无法处理长期依赖关系</p>\n<p>改进梯度爆炸：梯度截断<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170209.png\" alt=\"image.png\"></p>\n<h3 id=\"长短期记忆模型（LSTM）\"><a href=\"#长短期记忆模型（LSTM）\" class=\"headerlink\" title=\"长短期记忆模型（LSTM）\"></a>长短期记忆模型（LSTM）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170726.png\" alt=\"image.png\"></p>\n<ul>\n<li>隐藏状态：作为神经网络的记忆，保存着网络先前观察到的数据信息。</li>\n<li>单元状态：类似信息传送带，它贯穿整个链条，只有一些小的线性相互作用；这很容 易让信息以不变的方式向下流动；LSTM有能力向单元状态中移除或添加信息，这种管理结构称为门限</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170856.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170929.png\" alt=\"image.png\"></p>\n<h4 id=\"变体\"><a href=\"#变体\" class=\"headerlink\" title=\"变体\"></a>变体</h4><h5 id=\"窥视孔连接\"><a href=\"#窥视孔连接\" class=\"headerlink\" title=\"窥视孔连接\"></a>窥视孔连接</h5><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171128.png\" alt=\"image.png\"></p>\n<h5 id=\"耦合输入门和遗忘门\"><a href=\"#耦合输入门和遗忘门\" class=\"headerlink\" title=\"耦合输入门和遗忘门\"></a>耦合输入门和遗忘门</h5><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171150.png\" alt=\"image.png\"></p>\n<h5 id=\"GRU\"><a href=\"#GRU\" class=\"headerlink\" title=\"GRU\"></a>GRU</h5><p>在LSTM的基础上，将单元状态和隐藏状态合并，将遗忘门和输入门合并为更新门，无输出门。更新门决定历史信息和当前信息如何相加；重置门决定保留多少历史信息进入当前信息<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171221.png\" alt=\"image.png\"></p>\n<h5 id=\"LSTM与GRU\"><a href=\"#LSTM与GRU\" class=\"headerlink\" title=\"LSTM与GRU\"></a>LSTM与GRU</h5><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171330.png\" alt=\"image.png\"><br>GRU参数量更少，训练速度快；在训练数据足够的情况下，LSTM的表征能力更强。</p>\n<h3 id=\"生成对抗网络（GAN）\"><a href=\"#生成对抗网络（GAN）\" class=\"headerlink\" title=\"生成对抗网络（GAN）\"></a>生成对抗网络（GAN）</h3><p>损失函数通过学习得到，网络学习到的是数据的分布<br>模型由两部分组成：</p>\n<ul>\n<li>生成器（伪装者）：找出观测数据内部的统计规律，尽可能生成 能够以假乱真的样本，使判别网络输出接近0.5，难以区分真假。</li>\n<li>判别器（警察）：判断输入数据是来自真实样本集还是生成样本 集。如果输入是真样本，输出接近1；如果输入是生成样本，输出接近0。</li>\n</ul>\n<h4 id=\"训练过程\"><a href=\"#训练过程\" class=\"headerlink\" title=\"训练过程\"></a>训练过程</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031173437.png\" alt=\"image.png\"></p>\n<p>更新k次判别器后更新一次生成器<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174016.png\" alt=\"image.png\"></p>\n<p>解决梯度消失问题：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174326.png\" alt=\"image.png\"></p>\n<h4 id=\"模式崩溃（model-collapse）\"><a href=\"#模式崩溃（model-collapse）\" class=\"headerlink\" title=\"模式崩溃（model collapse）\"></a>模式崩溃（model collapse）</h4><p>产生原因： GAN的损失函数使判别器假样本的惩罚是巨大的，一旦生成的某一类 假样本成功骗过判别器，生成器就趋向于生成相似的样本，导致生成样本缺乏多样性<br>应对方法（WGAN）：采用更加平滑的损失函数，参见Wasserstein GAN</p>\n<h4 id=\"GAN结构变种\"><a href=\"#GAN结构变种\" class=\"headerlink\" title=\"GAN结构变种\"></a>GAN结构变种</h4><p>卷积GAN</p>\n<ul>\n<li>DCGAN：将GAN中全连接神经网络扩展到卷积神经网络</li>\n<li>ResGAN：图像恢复，ResNet</li>\n<li>SRGAN：超分辨率，ResNet</li>\n<li>CycleGAN：图像转换<br>条件GAN</li>\n<li>CGAN</li>\n<li>InfoGAN<br>集成推断模型的GAN</li>\n<li>BiGAN<br>对抗自编码器</li>\n<li>VAE-GAN</li>\n</ul>\n<h3 id=\"图像风格迁移\"><a href=\"#图像风格迁移\" class=\"headerlink\" title=\"图像风格迁移\"></a>图像风格迁移</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109080759.png\" alt=\"image.png\"><br>给定一张风格图像 a 和一张内容图像 p:</p>\n<ul>\n<li>风格图像 a 经过 CNN 生成的 feature maps 组成风格特征集A</li>\n<li>内容图像 p 通过 CNN 生成的 feature maps 组成内容特征集P</li>\n<li>输入一张随机噪声图像 x，随机噪声图像 x 通过 CNN 生成的 feature maps 构 成内容特征和风格特征集合 F和 G ，目标损失函数由 A, P, F ,G 计算得到</li>\n<li>优化函数是希望调整图像 x，使其最后看起来既保持内容图像 p的内容, 又有风 格图像 a 的风格。</li>\n<li>CNN网络为在imageNet上训练好的VGG19， 去除了最后的全连接层和softmax</li>\n<li>损失函数：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081123.png\" alt=\"image.png\"><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081133.png\" alt=\"image.png\"></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"卷积神经网络（CNN）\"><a href=\"#卷积神经网络（CNN）\" class=\"headerlink\" title=\"卷积神经网络（CNN）\"></a>卷积神经网络（CNN）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019081247.png\" alt=\"image.png\"></p>\n<ul>\n<li>局部连接：一个像素与它周围的几个像素有很强的联系，但是与离它很远的像素联系可能很弱</li>\n<li>权重共享：卷积神经网络中，权重又称为卷积模板，用于表达一种图像的特征。在图像的不同位置找特征，可以使用一样的卷积模板</li>\n</ul>\n<h3 id=\"CNN组成：\"><a href=\"#CNN组成：\" class=\"headerlink\" title=\"CNN组成：\"></a>CNN组成：</h3><ul>\n<li>卷积层</li>\n<li>池化层</li>\n<li>全连接层</li>\n<li>Softmax</li>\n</ul>\n<h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019082709.png\" alt=\"image.png\"></p>\n<p>边界扩充（padding）</p>\n<ul>\n<li>扩大图像的尺寸并填充像素</li>\n<li>防止深度网络中图像被动持续减小</li>\n<li>强化图像边缘信息<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084544.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019084557.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h4><p>Max Pooling &#x2F; Avg Pooling &#x2F; L2 Pooling<br>提取卷积核中的最大值&#x2F;平均值&#x2F;L2范数</p>\n<ul>\n<li>主动减小图片尺寸，从而减少参数的数量和计算量，控制过拟合</li>\n<li>不引入额外参数</li>\n</ul>\n<h4 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h4><ul>\n<li>卷积层和池化层构成特征提取器，全连接层则为分类器</li>\n<li>将特征提取得到的高维特征图映射成一维特征向量，该特征向量包含 所有特征信息，可转化为各个类别的概率。</li>\n</ul>\n<h4 id=\"softmax\"><a href=\"#softmax\" class=\"headerlink\" title=\"softmax\"></a>softmax</h4><ul>\n<li>通常作为网络的最后一层，对输出进行归一化，输出分类概率</li>\n<li>凸显其中最大的值并抑制远低于最大值的其他分量</li>\n<li>Softmax层输入、输出数据规模相同</li>\n</ul>\n<h3 id=\"序列模型：循环神经网络\"><a href=\"#序列模型：循环神经网络\" class=\"headerlink\" title=\"序列模型：循环神经网络\"></a>序列模型：循环神经网络</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031164605.png\" alt=\"image.png\"></p>\n<ul>\n<li>时序sequence：RNN能建模序列数据，序列指的是前、后输入数据( 𝒙 (𝑡) ， 𝒙 (𝑡+1) )不独立，相互影响； </li>\n<li>循环recurrent：对每个输入的操作都是一样的，循环往复地重复这些相 同操作，每时刻有相同参数W和U（参数共享）；</li>\n<li>记忆memory： 隐藏层𝒉 (𝑡)中捕捉了所有时刻t之前的信息，理论上𝒉 (𝑡)记 忆的内容可以无限长，然而实际上记忆还是有限的；</li>\n</ul>\n<h4 id=\"正向计算过程\"><a href=\"#正向计算过程\" class=\"headerlink\" title=\"正向计算过程\"></a>正向计算过程</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165458.png\" alt=\"image.png\"></p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165539.png\" alt=\"image.png\"></p>\n<h4 id=\"RNN的梯度爆炸和梯度消失\"><a href=\"#RNN的梯度爆炸和梯度消失\" class=\"headerlink\" title=\"RNN的梯度爆炸和梯度消失\"></a>RNN的梯度爆炸和梯度消失</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031165725.png\" alt=\"image.png\"><br>由于梯度爆炸或消失的存在，循环神经网络实际上只能学习到短期的依赖关系，无法处理长期依赖关系</p>\n<p>改进梯度爆炸：梯度截断<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170209.png\" alt=\"image.png\"></p>\n<h3 id=\"长短期记忆模型（LSTM）\"><a href=\"#长短期记忆模型（LSTM）\" class=\"headerlink\" title=\"长短期记忆模型（LSTM）\"></a>长短期记忆模型（LSTM）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170726.png\" alt=\"image.png\"></p>\n<ul>\n<li>隐藏状态：作为神经网络的记忆，保存着网络先前观察到的数据信息。</li>\n<li>单元状态：类似信息传送带，它贯穿整个链条，只有一些小的线性相互作用；这很容 易让信息以不变的方式向下流动；LSTM有能力向单元状态中移除或添加信息，这种管理结构称为门限</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170856.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031170929.png\" alt=\"image.png\"></p>\n<h4 id=\"变体\"><a href=\"#变体\" class=\"headerlink\" title=\"变体\"></a>变体</h4><h5 id=\"窥视孔连接\"><a href=\"#窥视孔连接\" class=\"headerlink\" title=\"窥视孔连接\"></a>窥视孔连接</h5><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171128.png\" alt=\"image.png\"></p>\n<h5 id=\"耦合输入门和遗忘门\"><a href=\"#耦合输入门和遗忘门\" class=\"headerlink\" title=\"耦合输入门和遗忘门\"></a>耦合输入门和遗忘门</h5><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171150.png\" alt=\"image.png\"></p>\n<h5 id=\"GRU\"><a href=\"#GRU\" class=\"headerlink\" title=\"GRU\"></a>GRU</h5><p>在LSTM的基础上，将单元状态和隐藏状态合并，将遗忘门和输入门合并为更新门，无输出门。更新门决定历史信息和当前信息如何相加；重置门决定保留多少历史信息进入当前信息<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171221.png\" alt=\"image.png\"></p>\n<h5 id=\"LSTM与GRU\"><a href=\"#LSTM与GRU\" class=\"headerlink\" title=\"LSTM与GRU\"></a>LSTM与GRU</h5><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031171330.png\" alt=\"image.png\"><br>GRU参数量更少，训练速度快；在训练数据足够的情况下，LSTM的表征能力更强。</p>\n<h3 id=\"生成对抗网络（GAN）\"><a href=\"#生成对抗网络（GAN）\" class=\"headerlink\" title=\"生成对抗网络（GAN）\"></a>生成对抗网络（GAN）</h3><p>损失函数通过学习得到，网络学习到的是数据的分布<br>模型由两部分组成：</p>\n<ul>\n<li>生成器（伪装者）：找出观测数据内部的统计规律，尽可能生成 能够以假乱真的样本，使判别网络输出接近0.5，难以区分真假。</li>\n<li>判别器（警察）：判断输入数据是来自真实样本集还是生成样本 集。如果输入是真样本，输出接近1；如果输入是生成样本，输出接近0。</li>\n</ul>\n<h4 id=\"训练过程\"><a href=\"#训练过程\" class=\"headerlink\" title=\"训练过程\"></a>训练过程</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031173437.png\" alt=\"image.png\"></p>\n<p>更新k次判别器后更新一次生成器<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174016.png\" alt=\"image.png\"></p>\n<p>解决梯度消失问题：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031174326.png\" alt=\"image.png\"></p>\n<h4 id=\"模式崩溃（model-collapse）\"><a href=\"#模式崩溃（model-collapse）\" class=\"headerlink\" title=\"模式崩溃（model collapse）\"></a>模式崩溃（model collapse）</h4><p>产生原因： GAN的损失函数使判别器假样本的惩罚是巨大的，一旦生成的某一类 假样本成功骗过判别器，生成器就趋向于生成相似的样本，导致生成样本缺乏多样性<br>应对方法（WGAN）：采用更加平滑的损失函数，参见Wasserstein GAN</p>\n<h4 id=\"GAN结构变种\"><a href=\"#GAN结构变种\" class=\"headerlink\" title=\"GAN结构变种\"></a>GAN结构变种</h4><p>卷积GAN</p>\n<ul>\n<li>DCGAN：将GAN中全连接神经网络扩展到卷积神经网络</li>\n<li>ResGAN：图像恢复，ResNet</li>\n<li>SRGAN：超分辨率，ResNet</li>\n<li>CycleGAN：图像转换<br>条件GAN</li>\n<li>CGAN</li>\n<li>InfoGAN<br>集成推断模型的GAN</li>\n<li>BiGAN<br>对抗自编码器</li>\n<li>VAE-GAN</li>\n</ul>\n<h3 id=\"图像风格迁移\"><a href=\"#图像风格迁移\" class=\"headerlink\" title=\"图像风格迁移\"></a>图像风格迁移</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109080759.png\" alt=\"image.png\"><br>给定一张风格图像 a 和一张内容图像 p:</p>\n<ul>\n<li>风格图像 a 经过 CNN 生成的 feature maps 组成风格特征集A</li>\n<li>内容图像 p 通过 CNN 生成的 feature maps 组成内容特征集P</li>\n<li>输入一张随机噪声图像 x，随机噪声图像 x 通过 CNN 生成的 feature maps 构 成内容特征和风格特征集合 F和 G ，目标损失函数由 A, P, F ,G 计算得到</li>\n<li>优化函数是希望调整图像 x，使其最后看起来既保持内容图像 p的内容, 又有风 格图像 a 的风格。</li>\n<li>CNN网络为在imageNet上训练好的VGG19， 去除了最后的全连接层和softmax</li>\n<li>损失函数：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081123.png\" alt=\"image.png\"><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109081133.png\" alt=\"image.png\"></li>\n</ul>\n"},{"title":"数据库","update":null,"_content":"成绩：20期中+60期末+10笔记+10作业\n\n笔记本：一周交一次\n\n\n---\n\n## 9.11\n**concept:**\n- database(DB):数据库，是一个文件，有关联的数据的集合\n- database management system(DBMS):数据库管理系统，包含程序处理DB中的数据\n- database system(DBS):=DB+DBMS+User\n- database application system(DBAS):DB+DBMS+Application programs+User\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225191039.png)\n第一章：选择题和判断题\n\n第二章关系代数：选择和判断，一道大题\n\n第三章：选择、判断、大题\n\n第五章：动态和嵌入式sql\n\n第七章：看例题，掌握各种范式的分解\n\n十六章：画优化树\n\n十七章：选择和判断\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225205235.png)\n\n三个概念：\n1. 完整性约束和视图\n2. 索引的基本概念\n3. 事务的基本概念\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212431.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212432.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103225448.png)\n","source":"_posts/Notes/课程/大三（上）/数据库/数据库.md","raw":"---\ntitle: 数据库\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 数据库\ntags:\n  - 数据库\nupdate:\n---\n成绩：20期中+60期末+10笔记+10作业\n\n笔记本：一周交一次\n\n\n---\n\n## 9.11\n**concept:**\n- database(DB):数据库，是一个文件，有关联的数据的集合\n- database management system(DBMS):数据库管理系统，包含程序处理DB中的数据\n- database system(DBS):=DB+DBMS+User\n- database application system(DBAS):DB+DBMS+Application programs+User\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225191039.png)\n第一章：选择题和判断题\n\n第二章关系代数：选择和判断，一道大题\n\n第三章：选择、判断、大题\n\n第五章：动态和嵌入式sql\n\n第七章：看例题，掌握各种范式的分解\n\n十六章：画优化树\n\n十七章：选择和判断\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225205235.png)\n\n三个概念：\n1. 完整性约束和视图\n2. 索引的基本概念\n3. 事务的基本概念\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212431.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212432.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103225448.png)\n","slug":"Notes/课程/大三（上）/数据库/数据库","published":1,"date":"2023-09-21T16:36:34.337Z","updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr128001avw8c97lmc54b","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>成绩：20期中+60期末+10笔记+10作业</p>\n<p>笔记本：一周交一次</p>\n<hr>\n<h2 id=\"9-11\"><a href=\"#9-11\" class=\"headerlink\" title=\"9.11\"></a>9.11</h2><p><strong>concept:</strong></p>\n<ul>\n<li>database(DB):数据库，是一个文件，有关联的数据的集合</li>\n<li>database management system(DBMS):数据库管理系统，包含程序处理DB中的数据</li>\n<li>database system(DBS):&#x3D;DB+DBMS+User</li>\n<li>database application system(DBAS):DB+DBMS+Application programs+User</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225191039.png\" alt=\"image.png\"><br>第一章：选择题和判断题</p>\n<p>第二章关系代数：选择和判断，一道大题</p>\n<p>第三章：选择、判断、大题</p>\n<p>第五章：动态和嵌入式sql</p>\n<p>第七章：看例题，掌握各种范式的分解</p>\n<p>十六章：画优化树</p>\n<p>十七章：选择和判断</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225205235.png\" alt=\"image.png\"></p>\n<p>三个概念：</p>\n<ol>\n<li>完整性约束和视图</li>\n<li>索引的基本概念</li>\n<li>事务的基本概念</li>\n</ol>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212431.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212432.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103225448.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>成绩：20期中+60期末+10笔记+10作业</p>\n<p>笔记本：一周交一次</p>\n<hr>\n<h2 id=\"9-11\"><a href=\"#9-11\" class=\"headerlink\" title=\"9.11\"></a>9.11</h2><p><strong>concept:</strong></p>\n<ul>\n<li>database(DB):数据库，是一个文件，有关联的数据的集合</li>\n<li>database management system(DBMS):数据库管理系统，包含程序处理DB中的数据</li>\n<li>database system(DBS):&#x3D;DB+DBMS+User</li>\n<li>database application system(DBAS):DB+DBMS+Application programs+User</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225191039.png\" alt=\"image.png\"><br>第一章：选择题和判断题</p>\n<p>第二章关系代数：选择和判断，一道大题</p>\n<p>第三章：选择、判断、大题</p>\n<p>第五章：动态和嵌入式sql</p>\n<p>第七章：看例题，掌握各种范式的分解</p>\n<p>十六章：画优化树</p>\n<p>十七章：选择和判断</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231225205235.png\" alt=\"image.png\"></p>\n<p>三个概念：</p>\n<ol>\n<li>完整性约束和视图</li>\n<li>索引的基本概念</li>\n<li>事务的基本概念</li>\n</ol>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212431.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103212432.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240103225448.png\" alt=\"image.png\"></p>\n"},{"title":"神经网络基础","date":"2023-09-28T00:15:48.192Z","_content":"### 线性函数拟合\n线性回归可以找到一些点的集合背后的规律：一个点集可以用一条直线来拟合，这条拟合出来的直线的参数特征，就是线性回归找到的点集背后的规律\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082548.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082934.png)\n\n### 感知机\n感知机模型𝐻 𝒙 = sign(𝒘_𝑇 𝒙 + 𝑏)对应一个超平面𝒘_𝑇 𝒙 + 𝑏 = 0，模型参数是(𝒘, 𝑏)。感知机的目标是找到一个(𝒘, 𝑏)，将线性可分的数据集T中的所有的样本点正确地分为两类。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084435.png)\n损失函数：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084551.png)\n\n\n### 神经网络设计原则\n神经网络的结构一般为：输入×隐层×输出层\n隐层的设计：\n- 隐层节点的作用是提取输入特征中的隐藏规律，每个节点都赋予一定权重\n- 隐层节点数太少，则网络从样本中获取信息的能力就越差，无法反映数据集的规律；隐层节点数太多，则网络的拟合能力过强，可能拟合数据集中的噪声部分，导致模型泛化能力变差。","source":"_posts/Notes/课程/大三（上）/智能计算系统/神经网络基础.md","raw":"---\ntitle: 神经网络基础\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 智能计算系统\ntags:\n  - 神经网络\ndate:\n---\n### 线性函数拟合\n线性回归可以找到一些点的集合背后的规律：一个点集可以用一条直线来拟合，这条拟合出来的直线的参数特征，就是线性回归找到的点集背后的规律\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082548.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082934.png)\n\n### 感知机\n感知机模型𝐻 𝒙 = sign(𝒘_𝑇 𝒙 + 𝑏)对应一个超平面𝒘_𝑇 𝒙 + 𝑏 = 0，模型参数是(𝒘, 𝑏)。感知机的目标是找到一个(𝒘, 𝑏)，将线性可分的数据集T中的所有的样本点正确地分为两类。![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084435.png)\n损失函数：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084551.png)\n\n\n### 神经网络设计原则\n神经网络的结构一般为：输入×隐层×输出层\n隐层的设计：\n- 隐层节点的作用是提取输入特征中的隐藏规律，每个节点都赋予一定权重\n- 隐层节点数太少，则网络从样本中获取信息的能力就越差，无法反映数据集的规律；隐层节点数太多，则网络的拟合能力过强，可能拟合数据集中的噪声部分，导致模型泛化能力变差。","slug":"Notes/课程/大三（上）/智能计算系统/神经网络基础","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr128001dvw8c4xk51xbc","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"线性函数拟合\"><a href=\"#线性函数拟合\" class=\"headerlink\" title=\"线性函数拟合\"></a>线性函数拟合</h3><p>线性回归可以找到一些点的集合背后的规律：一个点集可以用一条直线来拟合，这条拟合出来的直线的参数特征，就是线性回归找到的点集背后的规律<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082548.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082934.png\" alt=\"image.png\"></p>\n<h3 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h3><p>感知机模型𝐻 𝒙 &#x3D; sign(𝒘_𝑇 𝒙 + 𝑏)对应一个超平面𝒘_𝑇 𝒙 + 𝑏 &#x3D; 0，模型参数是(𝒘, 𝑏)。感知机的目标是找到一个(𝒘, 𝑏)，将线性可分的数据集T中的所有的样本点正确地分为两类。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084435.png\" alt=\"image.png\"><br>损失函数：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084551.png\" alt=\"image.png\"></p>\n<h3 id=\"神经网络设计原则\"><a href=\"#神经网络设计原则\" class=\"headerlink\" title=\"神经网络设计原则\"></a>神经网络设计原则</h3><p>神经网络的结构一般为：输入×隐层×输出层<br>隐层的设计：</p>\n<ul>\n<li>隐层节点的作用是提取输入特征中的隐藏规律，每个节点都赋予一定权重</li>\n<li>隐层节点数太少，则网络从样本中获取信息的能力就越差，无法反映数据集的规律；隐层节点数太多，则网络的拟合能力过强，可能拟合数据集中的噪声部分，导致模型泛化能力变差。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"线性函数拟合\"><a href=\"#线性函数拟合\" class=\"headerlink\" title=\"线性函数拟合\"></a>线性函数拟合</h3><p>线性回归可以找到一些点的集合背后的规律：一个点集可以用一条直线来拟合，这条拟合出来的直线的参数特征，就是线性回归找到的点集背后的规律<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082548.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928082934.png\" alt=\"image.png\"></p>\n<h3 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h3><p>感知机模型𝐻 𝒙 &#x3D; sign(𝒘_𝑇 𝒙 + 𝑏)对应一个超平面𝒘_𝑇 𝒙 + 𝑏 &#x3D; 0，模型参数是(𝒘, 𝑏)。感知机的目标是找到一个(𝒘, 𝑏)，将线性可分的数据集T中的所有的样本点正确地分为两类。<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084435.png\" alt=\"image.png\"><br>损失函数：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230928084551.png\" alt=\"image.png\"></p>\n<h3 id=\"神经网络设计原则\"><a href=\"#神经网络设计原则\" class=\"headerlink\" title=\"神经网络设计原则\"></a>神经网络设计原则</h3><p>神经网络的结构一般为：输入×隐层×输出层<br>隐层的设计：</p>\n<ul>\n<li>隐层节点的作用是提取输入特征中的隐藏规律，每个节点都赋予一定权重</li>\n<li>隐层节点数太少，则网络从样本中获取信息的能力就越差，无法反映数据集的规律；隐层节点数太多，则网络的拟合能力过强，可能拟合数据集中的噪声部分，导致模型泛化能力变差。</li>\n</ul>\n"},{"title":"关系型数据库设计：模式规范化","date":"2023-11-15T02:59:47.886Z","_content":"### 7.1 Good Relational DB Design\n逻辑数据库设计包括:\n- 初始关系架构生成\n- 关系架构规范化\n\n#### Lossless Decomposition无损分解\nDef：设 R 为关系模式，R1 和 R2 构成 R 的分解。 即 R = R1 U R2 \nDef：分解是**无损分解**，如果将模式 R 替换为两个关系模式 R1 U R2 没有丢失信息\n\n#### Normalization Principles归一化原则\n在DBS逻辑设计过程中，将E-R图转换，得到面向应用领域的初始关系模式集\n**初始关系模式集**存在关系模式属性间的**数据依赖 (Data Dependence) 关系**\n- **函数依赖** (functional  dependencies,  FD)\n- **多值依赖** (Mutivalued Dependencies,  MVD)\n- **连接依赖** (Join Dependencies, JD)\n\n直接根据初始关系模式构造DBS，由于初始关系模式中数据依赖关系的存在，  可能会违反DB的完整性约束，导致DBS使用的正确性、性能、效率受到影响\n- 数据冗余问题 pitfalls\n- 插入问题 pitfalls\n- 更新问题 pitfalls\n- 删除问题 pitfalls\n\n等价变换/模式分解: 对初始关系模式集，保证关系模式的：\n- 函数无损连接性（lossless join)，\n- 函数依赖保持性  (dependency preservation)\n\n关系模式集需要规范化处理——等价变换/模式分解\n\n关系模式规范化主要步骤为：\n- 根据函数依赖的Armstrong’s 公理系统和多值依赖的公理系统，从初始关系模式集中已知的函数依赖和多值依赖出发，推导出初始关系模式集中所有的函数依赖和多值依赖\n- 对具有函数依赖和多值依赖的初始关系模式集，采用**模式分解算法**，对其进行（等价）分解和变换，将其转换为各种范式形式，包括：1NF、 2NF、 BCNF、 3NF、 4NF、5NF，以消除函数依赖和多值依赖的负面影响, 保证数据库完整性\n[第一范式、第二范式、第三范式 - 知乎](https://zhuanlan.zhihu.com/p/554101160#:~:text=%E8%8C%83%E5%BC%8F%EF%BC%88Paradigm%EF%BC%89%E6%98%AF%E7%AC%A6%E5%90%88%E6%9F%90%E4%B8%80%E7%A7%8D%E7%BA%A7%E5%88%AB%E7%9A%84%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F%E7%9A%84%E9%9B%86%E5%90%88%E3%80%82%20%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E5%85%B3%E7%B3%BB%E5%BF%85%E9%A1%BB%E6%BB%A1%E8%B6%B3%E4%B8%80%E5%AE%9A%E7%9A%84%E8%A6%81%E6%B1%82%EF%BC%8C%E6%BB%A1%E8%B6%B3%E4%B8%8D%E5%90%8C%E7%A8%8B%E5%BA%A6%E8%A6%81%E6%B1%82%E7%9A%84%E4%B8%BA%E4%B8%8D%E5%90%8C%E8%8C%83%E5%BC%8F%E3%80%82,%E7%9B%AE%E5%89%8D%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%89%E5%85%AD%E7%A7%8D%E8%8C%83%E5%BC%8F%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%B8%89%E8%8C%83%E5%BC%8F%EF%BC%883NF%EF%BC%89%E3%80%81Boyce-Codd%E8%8C%83%E5%BC%8F%EF%BC%88BCNF%EF%BC%89%E3%80%81%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F%EF%BC%884NF%EF%BC%89%E5%92%8C%E7%AC%AC%E4%BA%94%E8%8C%83%E5%BC%8F%EF%BC%885NF%EF%BC%89%E3%80%82%20%E6%BB%A1%E8%B6%B3%E6%9C%80%E4%BD%8E%E8%A6%81%E6%B1%82%E7%9A%84%E8%8C%83%E5%BC%8F%E6%98%AF%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%82%20%E5%9C%A8%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%BB%A1%E8%B6%B3%E6%9B%B4%E5%A4%9A%E8%A6%81%E6%B1%82%E7%9A%84%E7%A7%B0%E4%B8%BA%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%EF%BC%8C%E5%85%B6%E4%BD%99%E8%8C%83%E5%BC%8F%E4%BB%A5%E6%AC%A1%E7%B1%BB%E6%8E%A8%E3%80%82)\n\n关系模式规范化处理的**基本要求**为: \n- 静态关系具有第一范式形式 \n- 动态关系最好具有3NF或BCNF形式 \n\n3种数据依赖间的关系 ：\n- 函数依赖是特殊的多值依赖 \n- 多值依赖又是连接依赖的特例 \n范式1NF、2NF、3NF、BCNF可以看作由符合范式要求的各种关系模式组成的关系模式的集合 \ne.g. 1NF = { R | R 满足第一范式的定义} \n范式间的关系：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120558.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120646.png)\n\n给定关系模式，可以采用规范化算法将其转换为1NF、2NF、3NF、BCNF\n\n对连接依赖和第五范式，无相应的模式规范化算法\n\n#### Normalization Theory归一化理论\n确定特定关系 R 是否处于良好形式（范式） \n关系 R 的形式不好，分解为关系 {R1 R2 ...Rn} 使得 \n- 每个关系都处于良好状态 \n- 分解是无损连接分解\n\n#### Functional Dependencies功能依赖关系\n数据通常存在（完整性）约束（规则）\n例如，预期成立的约束： \n学生和教师通过其 ID 进行唯一标识。 \n每个学生和教师只有一个名字。 \n每个教师和学生只与一个部门相关联。 \n每个部门只有一个预算值，并且只有一个关联的建筑物\n\nDef：满足所有此类现实世界约束的关系实例称为**关系的合法实例** \nDef：**功能依赖**:\n对法律关系的约束询问 \n一组特定属性的值唯一地决定了另一组属性的值 \n函数依赖关系是键概念的泛化\n函数： f： X→Y， x∈X， y∈Y， y = f（x） \n对于 x1，x2∈X，如果 x1=x2，则 f（x1）= f（x2)\n\n#### Relation Instance Satisfy Functional Dependency\n对于关系模式R，$\\alpha$⊆ R，$\\beta$⊆ R,满足函数依赖关系$\\alpha$→$\\beta$\n对于元组 ti 和 tj ∈r（R） 对，使得 $t_i[\\alpha]= t_j [\\alpha ]$，也是 $t_i[\\beta]= t_j [\\beta]$ 的情况\n\n#### Functional Dependency Holds on Schema r(R)功能依赖关系保留在架构 r（R） 上\nDef：让 R 成为关系架构 $\\alpha \\subseteq R$ and $\\beta \\subseteq R$，如果每个实例 r（R） 都满足$\\alpha$→$\\beta$，函数依赖FD在关系模式R上成立/保持$\\alpha$→$\\beta$，每当两个元组 t1 和 t2 在属性$\\alpha$上达成一致时，也就在$\\beta$属性达成一致\n\n#### FD holds on R  vs  FD is satisfied by r(R)FD 保持 R 与 FD 满足 r（R）\n在 R 上定义可能有多个关系实例 r（R），即$r_1（R） ， r_2（R） ， r_3（R） ,..., r_m（R）$\n定义：关系 r（R） 满足$\\alpha$→$\\beta$与$\\alpha$→$\\beta$保留架构 R \n如果$\\alpha$→$\\beta$在 R 上成立，则每个合法 r（R） 都满足此 R \n但是对于模式 R，如果只有一些 ri（R） 满足 R，则$\\alpha$→$\\beta$可能不会对 R 成立。\n\nFD holds on R:\n定义在R的属性间的语义约束，或R的属性间体现出的语义约束\n从设计角度，R应满足的约束\n\nFD is satisfied by r(R):\n根据 R构造的实际数据 r(R) 是否满足语义约束FD\n\n\n#### Keys and Functional Dependencies\nDef：K 是关系架构 R 的超键，当且仅当 K → R \n定义：K 是 R 的候选键，当且仅当 K → R 且 没有 $\\alpha \\subset$K、$\\alpha$→R \nDF 允许我们表达无法用超级键表达的约束。\n\n#### Use of Functional Dependencies\n我们使用 FD 来 测试关系，看看它们是否合法。 如果关系 r 在 FD 集合 F 下是合法的，我们说 r 满足 F \n指定对法律关系集的约束 如果 R 上的所有法律关系都满足 FD 集 F，则 F 对 R 成立。\n注意：关系模式的特定实例可能满足 F 中的 FD，即使 FD 不持有所有法律实例\n\n#### Trivial (平凡) Functional Dependencies\n如果一个关系的所有实例都满足函数依赖关系，那么它就是平凡的\n\n#### Transitive (传递) dependency\n$\\alpha$→$\\beta$但不满足$\\beta$→$\\alpha$，满足$\\beta$→$\\gamma$, 但$\\gamma$ 不在$\\alpha$内，则称$\\alpha$→$\\gamma$满足传递依赖关系\n\n#### Partial (部分) Dependency\ny是a的子集，y→b，a→b为部份依赖\n\n#### Closure of functional dependency set $F^+$\n给定 FD 集 F 可以推断的所有 FD 的集合 \n$F^+$包含 F 中的所有功能依赖项\n\n#### Lossless Decomposition无损分解\n如果至少有如下一个依赖项位于 F+ 中，则将 R 分解为 R1 和 R2 是无损的： \nR1 $\\cap$R2 →R1\nR1$\\cap$ R2→ R2\n\n### 7.3 Normal Forms\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232132.png)\n1NF：第一范式 \n2NF：第二范式 \nBCNF：Boyce-Codd 范式 \n3NF：第三范式\n\n#### Atomic Domains and First Normal Form原子域和第一范式\n如果域的元素是不可分割的单元，则域是原子的\n如果 R 的所有属性的域都是原子的，则关系架构 R 采用第一范式\n原子性实际上是如何使用域元素的一个属性\n\n#### Second Normal Form\n关系模式 R 相对于 DF 集 F 在 2NF 中，如果 \nR 在 1NF 中，并且 \n每个属性 A 都满足其中一个条件：\n- 它出现在候选键中，即它是一个素数属性 // A 是主属性 \n- 它（不是部分）依赖于候选密钥 A是非主属性,完全依赖于候选键\n\n#### Boyce-Codd Normal Form, BCNF\nDef：关系架构 R 在 BCNF 中相对于 FD 集 F \n如果对于 F+ 形式的所有功能依赖关系a→b，a、b属于R，且a→b是传递关系或a是超键\n\n#### Decomposing a Schema into BCNF将架构分解为 BCNF\n设 R 为不在 BCNF 中的架构 R。 让$\\alpha$→$\\beta$违反 BCNF\n我们将 R 分解为\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232656.png)\n\n#### Dependency Preservation依赖关系保留\n对于架构 R，F 是在 R 上的功能依赖，并分解 R 的 {R1， R2,.., Rn}，F 对 Ri 的限制，表示为 Fi，定义为![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100613.png)\n如果满足![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100633.png)\n则分解是依赖保留关系\n\n由于限制中的所有 FD 都只涉及一个关系模式的属性，因此我们通过仅检查一个关系来测试这种依赖关系。 \n注意：限制的定义使用 F+ 中的所有依赖项，而不仅仅是 F 中的依赖项。 \n注意：限制集 F1、F2 、..，Fn 是可以有效检查的 FD 集合。\n\n### 7.5 Algorithms for Decomposition Using FD\n1. Testing for BCNF\n2. Testing Decomposition for BCNF\n3. BCNF decomposition algorithm\n4. Testing for 3NF\n5. 3NF decomposition algorithm\n6. Comparison of BCNF and 3NF\n\n简化测试：要检查关系架构 R 是否在 BCNF 中，只需检查 F 中的 FD 是否违反 BCNF 就足够了，而不是检查 F+ 中的所有 FD。 如果 F 中没有任何依赖项导致违反 BCNF，则 F+ 中的任何依赖项都不会导致违反 BCNF\n\n#### Testing Decomposition for BCNF\n","source":"_posts/Notes/课程/大三（上）/数据库/关系型数据库设计：模式规范化.md","raw":"---\ntitle: 关系型数据库设计：模式规范化\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 数据库\ntags:\n  - 数据库\ndate:\n---\n### 7.1 Good Relational DB Design\n逻辑数据库设计包括:\n- 初始关系架构生成\n- 关系架构规范化\n\n#### Lossless Decomposition无损分解\nDef：设 R 为关系模式，R1 和 R2 构成 R 的分解。 即 R = R1 U R2 \nDef：分解是**无损分解**，如果将模式 R 替换为两个关系模式 R1 U R2 没有丢失信息\n\n#### Normalization Principles归一化原则\n在DBS逻辑设计过程中，将E-R图转换，得到面向应用领域的初始关系模式集\n**初始关系模式集**存在关系模式属性间的**数据依赖 (Data Dependence) 关系**\n- **函数依赖** (functional  dependencies,  FD)\n- **多值依赖** (Mutivalued Dependencies,  MVD)\n- **连接依赖** (Join Dependencies, JD)\n\n直接根据初始关系模式构造DBS，由于初始关系模式中数据依赖关系的存在，  可能会违反DB的完整性约束，导致DBS使用的正确性、性能、效率受到影响\n- 数据冗余问题 pitfalls\n- 插入问题 pitfalls\n- 更新问题 pitfalls\n- 删除问题 pitfalls\n\n等价变换/模式分解: 对初始关系模式集，保证关系模式的：\n- 函数无损连接性（lossless join)，\n- 函数依赖保持性  (dependency preservation)\n\n关系模式集需要规范化处理——等价变换/模式分解\n\n关系模式规范化主要步骤为：\n- 根据函数依赖的Armstrong’s 公理系统和多值依赖的公理系统，从初始关系模式集中已知的函数依赖和多值依赖出发，推导出初始关系模式集中所有的函数依赖和多值依赖\n- 对具有函数依赖和多值依赖的初始关系模式集，采用**模式分解算法**，对其进行（等价）分解和变换，将其转换为各种范式形式，包括：1NF、 2NF、 BCNF、 3NF、 4NF、5NF，以消除函数依赖和多值依赖的负面影响, 保证数据库完整性\n[第一范式、第二范式、第三范式 - 知乎](https://zhuanlan.zhihu.com/p/554101160#:~:text=%E8%8C%83%E5%BC%8F%EF%BC%88Paradigm%EF%BC%89%E6%98%AF%E7%AC%A6%E5%90%88%E6%9F%90%E4%B8%80%E7%A7%8D%E7%BA%A7%E5%88%AB%E7%9A%84%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F%E7%9A%84%E9%9B%86%E5%90%88%E3%80%82%20%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E5%85%B3%E7%B3%BB%E5%BF%85%E9%A1%BB%E6%BB%A1%E8%B6%B3%E4%B8%80%E5%AE%9A%E7%9A%84%E8%A6%81%E6%B1%82%EF%BC%8C%E6%BB%A1%E8%B6%B3%E4%B8%8D%E5%90%8C%E7%A8%8B%E5%BA%A6%E8%A6%81%E6%B1%82%E7%9A%84%E4%B8%BA%E4%B8%8D%E5%90%8C%E8%8C%83%E5%BC%8F%E3%80%82,%E7%9B%AE%E5%89%8D%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%89%E5%85%AD%E7%A7%8D%E8%8C%83%E5%BC%8F%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%B8%89%E8%8C%83%E5%BC%8F%EF%BC%883NF%EF%BC%89%E3%80%81Boyce-Codd%E8%8C%83%E5%BC%8F%EF%BC%88BCNF%EF%BC%89%E3%80%81%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F%EF%BC%884NF%EF%BC%89%E5%92%8C%E7%AC%AC%E4%BA%94%E8%8C%83%E5%BC%8F%EF%BC%885NF%EF%BC%89%E3%80%82%20%E6%BB%A1%E8%B6%B3%E6%9C%80%E4%BD%8E%E8%A6%81%E6%B1%82%E7%9A%84%E8%8C%83%E5%BC%8F%E6%98%AF%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%82%20%E5%9C%A8%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%BB%A1%E8%B6%B3%E6%9B%B4%E5%A4%9A%E8%A6%81%E6%B1%82%E7%9A%84%E7%A7%B0%E4%B8%BA%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%EF%BC%8C%E5%85%B6%E4%BD%99%E8%8C%83%E5%BC%8F%E4%BB%A5%E6%AC%A1%E7%B1%BB%E6%8E%A8%E3%80%82)\n\n关系模式规范化处理的**基本要求**为: \n- 静态关系具有第一范式形式 \n- 动态关系最好具有3NF或BCNF形式 \n\n3种数据依赖间的关系 ：\n- 函数依赖是特殊的多值依赖 \n- 多值依赖又是连接依赖的特例 \n范式1NF、2NF、3NF、BCNF可以看作由符合范式要求的各种关系模式组成的关系模式的集合 \ne.g. 1NF = { R | R 满足第一范式的定义} \n范式间的关系：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120558.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120646.png)\n\n给定关系模式，可以采用规范化算法将其转换为1NF、2NF、3NF、BCNF\n\n对连接依赖和第五范式，无相应的模式规范化算法\n\n#### Normalization Theory归一化理论\n确定特定关系 R 是否处于良好形式（范式） \n关系 R 的形式不好，分解为关系 {R1 R2 ...Rn} 使得 \n- 每个关系都处于良好状态 \n- 分解是无损连接分解\n\n#### Functional Dependencies功能依赖关系\n数据通常存在（完整性）约束（规则）\n例如，预期成立的约束： \n学生和教师通过其 ID 进行唯一标识。 \n每个学生和教师只有一个名字。 \n每个教师和学生只与一个部门相关联。 \n每个部门只有一个预算值，并且只有一个关联的建筑物\n\nDef：满足所有此类现实世界约束的关系实例称为**关系的合法实例** \nDef：**功能依赖**:\n对法律关系的约束询问 \n一组特定属性的值唯一地决定了另一组属性的值 \n函数依赖关系是键概念的泛化\n函数： f： X→Y， x∈X， y∈Y， y = f（x） \n对于 x1，x2∈X，如果 x1=x2，则 f（x1）= f（x2)\n\n#### Relation Instance Satisfy Functional Dependency\n对于关系模式R，$\\alpha$⊆ R，$\\beta$⊆ R,满足函数依赖关系$\\alpha$→$\\beta$\n对于元组 ti 和 tj ∈r（R） 对，使得 $t_i[\\alpha]= t_j [\\alpha ]$，也是 $t_i[\\beta]= t_j [\\beta]$ 的情况\n\n#### Functional Dependency Holds on Schema r(R)功能依赖关系保留在架构 r（R） 上\nDef：让 R 成为关系架构 $\\alpha \\subseteq R$ and $\\beta \\subseteq R$，如果每个实例 r（R） 都满足$\\alpha$→$\\beta$，函数依赖FD在关系模式R上成立/保持$\\alpha$→$\\beta$，每当两个元组 t1 和 t2 在属性$\\alpha$上达成一致时，也就在$\\beta$属性达成一致\n\n#### FD holds on R  vs  FD is satisfied by r(R)FD 保持 R 与 FD 满足 r（R）\n在 R 上定义可能有多个关系实例 r（R），即$r_1（R） ， r_2（R） ， r_3（R） ,..., r_m（R）$\n定义：关系 r（R） 满足$\\alpha$→$\\beta$与$\\alpha$→$\\beta$保留架构 R \n如果$\\alpha$→$\\beta$在 R 上成立，则每个合法 r（R） 都满足此 R \n但是对于模式 R，如果只有一些 ri（R） 满足 R，则$\\alpha$→$\\beta$可能不会对 R 成立。\n\nFD holds on R:\n定义在R的属性间的语义约束，或R的属性间体现出的语义约束\n从设计角度，R应满足的约束\n\nFD is satisfied by r(R):\n根据 R构造的实际数据 r(R) 是否满足语义约束FD\n\n\n#### Keys and Functional Dependencies\nDef：K 是关系架构 R 的超键，当且仅当 K → R \n定义：K 是 R 的候选键，当且仅当 K → R 且 没有 $\\alpha \\subset$K、$\\alpha$→R \nDF 允许我们表达无法用超级键表达的约束。\n\n#### Use of Functional Dependencies\n我们使用 FD 来 测试关系，看看它们是否合法。 如果关系 r 在 FD 集合 F 下是合法的，我们说 r 满足 F \n指定对法律关系集的约束 如果 R 上的所有法律关系都满足 FD 集 F，则 F 对 R 成立。\n注意：关系模式的特定实例可能满足 F 中的 FD，即使 FD 不持有所有法律实例\n\n#### Trivial (平凡) Functional Dependencies\n如果一个关系的所有实例都满足函数依赖关系，那么它就是平凡的\n\n#### Transitive (传递) dependency\n$\\alpha$→$\\beta$但不满足$\\beta$→$\\alpha$，满足$\\beta$→$\\gamma$, 但$\\gamma$ 不在$\\alpha$内，则称$\\alpha$→$\\gamma$满足传递依赖关系\n\n#### Partial (部分) Dependency\ny是a的子集，y→b，a→b为部份依赖\n\n#### Closure of functional dependency set $F^+$\n给定 FD 集 F 可以推断的所有 FD 的集合 \n$F^+$包含 F 中的所有功能依赖项\n\n#### Lossless Decomposition无损分解\n如果至少有如下一个依赖项位于 F+ 中，则将 R 分解为 R1 和 R2 是无损的： \nR1 $\\cap$R2 →R1\nR1$\\cap$ R2→ R2\n\n### 7.3 Normal Forms\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232132.png)\n1NF：第一范式 \n2NF：第二范式 \nBCNF：Boyce-Codd 范式 \n3NF：第三范式\n\n#### Atomic Domains and First Normal Form原子域和第一范式\n如果域的元素是不可分割的单元，则域是原子的\n如果 R 的所有属性的域都是原子的，则关系架构 R 采用第一范式\n原子性实际上是如何使用域元素的一个属性\n\n#### Second Normal Form\n关系模式 R 相对于 DF 集 F 在 2NF 中，如果 \nR 在 1NF 中，并且 \n每个属性 A 都满足其中一个条件：\n- 它出现在候选键中，即它是一个素数属性 // A 是主属性 \n- 它（不是部分）依赖于候选密钥 A是非主属性,完全依赖于候选键\n\n#### Boyce-Codd Normal Form, BCNF\nDef：关系架构 R 在 BCNF 中相对于 FD 集 F \n如果对于 F+ 形式的所有功能依赖关系a→b，a、b属于R，且a→b是传递关系或a是超键\n\n#### Decomposing a Schema into BCNF将架构分解为 BCNF\n设 R 为不在 BCNF 中的架构 R。 让$\\alpha$→$\\beta$违反 BCNF\n我们将 R 分解为\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232656.png)\n\n#### Dependency Preservation依赖关系保留\n对于架构 R，F 是在 R 上的功能依赖，并分解 R 的 {R1， R2,.., Rn}，F 对 Ri 的限制，表示为 Fi，定义为![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100613.png)\n如果满足![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100633.png)\n则分解是依赖保留关系\n\n由于限制中的所有 FD 都只涉及一个关系模式的属性，因此我们通过仅检查一个关系来测试这种依赖关系。 \n注意：限制的定义使用 F+ 中的所有依赖项，而不仅仅是 F 中的依赖项。 \n注意：限制集 F1、F2 、..，Fn 是可以有效检查的 FD 集合。\n\n### 7.5 Algorithms for Decomposition Using FD\n1. Testing for BCNF\n2. Testing Decomposition for BCNF\n3. BCNF decomposition algorithm\n4. Testing for 3NF\n5. 3NF decomposition algorithm\n6. Comparison of BCNF and 3NF\n\n简化测试：要检查关系架构 R 是否在 BCNF 中，只需检查 F 中的 FD 是否违反 BCNF 就足够了，而不是检查 F+ 中的所有 FD。 如果 F 中没有任何依赖项导致违反 BCNF，则 F+ 中的任何依赖项都不会导致违反 BCNF\n\n#### Testing Decomposition for BCNF\n","slug":"Notes/课程/大三（上）/数据库/关系型数据库设计：模式规范化","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr129001gvw8cb0l3hj4h","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"7-1-Good-Relational-DB-Design\"><a href=\"#7-1-Good-Relational-DB-Design\" class=\"headerlink\" title=\"7.1 Good Relational DB Design\"></a>7.1 Good Relational DB Design</h3><p>逻辑数据库设计包括:</p>\n<ul>\n<li>初始关系架构生成</li>\n<li>关系架构规范化</li>\n</ul>\n<h4 id=\"Lossless-Decomposition无损分解\"><a href=\"#Lossless-Decomposition无损分解\" class=\"headerlink\" title=\"Lossless Decomposition无损分解\"></a>Lossless Decomposition无损分解</h4><p>Def：设 R 为关系模式，R1 和 R2 构成 R 的分解。 即 R &#x3D; R1 U R2<br>Def：分解是<strong>无损分解</strong>，如果将模式 R 替换为两个关系模式 R1 U R2 没有丢失信息</p>\n<h4 id=\"Normalization-Principles归一化原则\"><a href=\"#Normalization-Principles归一化原则\" class=\"headerlink\" title=\"Normalization Principles归一化原则\"></a>Normalization Principles归一化原则</h4><p>在DBS逻辑设计过程中，将E-R图转换，得到面向应用领域的初始关系模式集<br><strong>初始关系模式集</strong>存在关系模式属性间的<strong>数据依赖 (Data Dependence) 关系</strong></p>\n<ul>\n<li><strong>函数依赖</strong> (functional  dependencies,  FD)</li>\n<li><strong>多值依赖</strong> (Mutivalued Dependencies,  MVD)</li>\n<li><strong>连接依赖</strong> (Join Dependencies, JD)</li>\n</ul>\n<p>直接根据初始关系模式构造DBS，由于初始关系模式中数据依赖关系的存在，  可能会违反DB的完整性约束，导致DBS使用的正确性、性能、效率受到影响</p>\n<ul>\n<li>数据冗余问题 pitfalls</li>\n<li>插入问题 pitfalls</li>\n<li>更新问题 pitfalls</li>\n<li>删除问题 pitfalls</li>\n</ul>\n<p>等价变换&#x2F;模式分解: 对初始关系模式集，保证关系模式的：</p>\n<ul>\n<li>函数无损连接性（lossless join)，</li>\n<li>函数依赖保持性  (dependency preservation)</li>\n</ul>\n<p>关系模式集需要规范化处理——等价变换&#x2F;模式分解</p>\n<p>关系模式规范化主要步骤为：</p>\n<ul>\n<li>根据函数依赖的Armstrong’s 公理系统和多值依赖的公理系统，从初始关系模式集中已知的函数依赖和多值依赖出发，推导出初始关系模式集中所有的函数依赖和多值依赖</li>\n<li>对具有函数依赖和多值依赖的初始关系模式集，采用<strong>模式分解算法</strong>，对其进行（等价）分解和变换，将其转换为各种范式形式，包括：1NF、 2NF、 BCNF、 3NF、 4NF、5NF，以消除函数依赖和多值依赖的负面影响, 保证数据库完整性<br><a href=\"https://zhuanlan.zhihu.com/p/554101160#:~:text=%E8%8C%83%E5%BC%8F%EF%BC%88Paradigm%EF%BC%89%E6%98%AF%E7%AC%A6%E5%90%88%E6%9F%90%E4%B8%80%E7%A7%8D%E7%BA%A7%E5%88%AB%E7%9A%84%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F%E7%9A%84%E9%9B%86%E5%90%88%E3%80%82%20%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E5%85%B3%E7%B3%BB%E5%BF%85%E9%A1%BB%E6%BB%A1%E8%B6%B3%E4%B8%80%E5%AE%9A%E7%9A%84%E8%A6%81%E6%B1%82%EF%BC%8C%E6%BB%A1%E8%B6%B3%E4%B8%8D%E5%90%8C%E7%A8%8B%E5%BA%A6%E8%A6%81%E6%B1%82%E7%9A%84%E4%B8%BA%E4%B8%8D%E5%90%8C%E8%8C%83%E5%BC%8F%E3%80%82,%E7%9B%AE%E5%89%8D%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%89%E5%85%AD%E7%A7%8D%E8%8C%83%E5%BC%8F%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%B8%89%E8%8C%83%E5%BC%8F%EF%BC%883NF%EF%BC%89%E3%80%81Boyce-Codd%E8%8C%83%E5%BC%8F%EF%BC%88BCNF%EF%BC%89%E3%80%81%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F%EF%BC%884NF%EF%BC%89%E5%92%8C%E7%AC%AC%E4%BA%94%E8%8C%83%E5%BC%8F%EF%BC%885NF%EF%BC%89%E3%80%82%20%E6%BB%A1%E8%B6%B3%E6%9C%80%E4%BD%8E%E8%A6%81%E6%B1%82%E7%9A%84%E8%8C%83%E5%BC%8F%E6%98%AF%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%82%20%E5%9C%A8%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%BB%A1%E8%B6%B3%E6%9B%B4%E5%A4%9A%E8%A6%81%E6%B1%82%E7%9A%84%E7%A7%B0%E4%B8%BA%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%EF%BC%8C%E5%85%B6%E4%BD%99%E8%8C%83%E5%BC%8F%E4%BB%A5%E6%AC%A1%E7%B1%BB%E6%8E%A8%E3%80%82\">第一范式、第二范式、第三范式 - 知乎</a></li>\n</ul>\n<p>关系模式规范化处理的<strong>基本要求</strong>为: </p>\n<ul>\n<li>静态关系具有第一范式形式 </li>\n<li>动态关系最好具有3NF或BCNF形式</li>\n</ul>\n<p>3种数据依赖间的关系 ：</p>\n<ul>\n<li>函数依赖是特殊的多值依赖 </li>\n<li>多值依赖又是连接依赖的特例<br>范式1NF、2NF、3NF、BCNF可以看作由符合范式要求的各种关系模式组成的关系模式的集合<br>e.g. 1NF &#x3D; { R | R 满足第一范式的定义}<br>范式间的关系：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120558.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120646.png\" alt=\"image.png\"></li>\n</ul>\n<p>给定关系模式，可以采用规范化算法将其转换为1NF、2NF、3NF、BCNF</p>\n<p>对连接依赖和第五范式，无相应的模式规范化算法</p>\n<h4 id=\"Normalization-Theory归一化理论\"><a href=\"#Normalization-Theory归一化理论\" class=\"headerlink\" title=\"Normalization Theory归一化理论\"></a>Normalization Theory归一化理论</h4><p>确定特定关系 R 是否处于良好形式（范式）<br>关系 R 的形式不好，分解为关系 {R1 R2 …Rn} 使得 </p>\n<ul>\n<li>每个关系都处于良好状态 </li>\n<li>分解是无损连接分解</li>\n</ul>\n<h4 id=\"Functional-Dependencies功能依赖关系\"><a href=\"#Functional-Dependencies功能依赖关系\" class=\"headerlink\" title=\"Functional Dependencies功能依赖关系\"></a>Functional Dependencies功能依赖关系</h4><p>数据通常存在（完整性）约束（规则）<br>例如，预期成立的约束：<br>学生和教师通过其 ID 进行唯一标识。<br>每个学生和教师只有一个名字。<br>每个教师和学生只与一个部门相关联。<br>每个部门只有一个预算值，并且只有一个关联的建筑物</p>\n<p>Def：满足所有此类现实世界约束的关系实例称为<strong>关系的合法实例</strong><br>Def：<strong>功能依赖</strong>:<br>对法律关系的约束询问<br>一组特定属性的值唯一地决定了另一组属性的值<br>函数依赖关系是键概念的泛化<br>函数： f： X→Y， x∈X， y∈Y， y &#x3D; f（x）<br>对于 x1，x2∈X，如果 x1&#x3D;x2，则 f（x1）&#x3D; f（x2)</p>\n<h4 id=\"Relation-Instance-Satisfy-Functional-Dependency\"><a href=\"#Relation-Instance-Satisfy-Functional-Dependency\" class=\"headerlink\" title=\"Relation Instance Satisfy Functional Dependency\"></a>Relation Instance Satisfy Functional Dependency</h4><p>对于关系模式R，$\\alpha$⊆ R，$\\beta$⊆ R,满足函数依赖关系$\\alpha$→$\\beta$<br>对于元组 ti 和 tj ∈r（R） 对，使得 $t_i[\\alpha]&#x3D; t_j [\\alpha ]$，也是 $t_i[\\beta]&#x3D; t_j [\\beta]$ 的情况</p>\n<h4 id=\"Functional-Dependency-Holds-on-Schema-r-R-功能依赖关系保留在架构-r（R）-上\"><a href=\"#Functional-Dependency-Holds-on-Schema-r-R-功能依赖关系保留在架构-r（R）-上\" class=\"headerlink\" title=\"Functional Dependency Holds on Schema r(R)功能依赖关系保留在架构 r（R） 上\"></a>Functional Dependency Holds on Schema r(R)功能依赖关系保留在架构 r（R） 上</h4><p>Def：让 R 成为关系架构 $\\alpha \\subseteq R$ and $\\beta \\subseteq R$，如果每个实例 r（R） 都满足$\\alpha$→$\\beta$，函数依赖FD在关系模式R上成立&#x2F;保持$\\alpha$→$\\beta$，每当两个元组 t1 和 t2 在属性$\\alpha$上达成一致时，也就在$\\beta$属性达成一致</p>\n<h4 id=\"FD-holds-on-R-vs-FD-is-satisfied-by-r-R-FD-保持-R-与-FD-满足-r（R）\"><a href=\"#FD-holds-on-R-vs-FD-is-satisfied-by-r-R-FD-保持-R-与-FD-满足-r（R）\" class=\"headerlink\" title=\"FD holds on R  vs  FD is satisfied by r(R)FD 保持 R 与 FD 满足 r（R）\"></a>FD holds on R  vs  FD is satisfied by r(R)FD 保持 R 与 FD 满足 r（R）</h4><p>在 R 上定义可能有多个关系实例 r（R），即$r_1（R） ， r_2（R） ， r_3（R） ,…, r_m（R）$<br>定义：关系 r（R） 满足$\\alpha$→$\\beta$与$\\alpha$→$\\beta$保留架构 R<br>如果$\\alpha$→$\\beta$在 R 上成立，则每个合法 r（R） 都满足此 R<br>但是对于模式 R，如果只有一些 ri（R） 满足 R，则$\\alpha$→$\\beta$可能不会对 R 成立。</p>\n<p>FD holds on R:<br>定义在R的属性间的语义约束，或R的属性间体现出的语义约束<br>从设计角度，R应满足的约束</p>\n<p>FD is satisfied by r(R):<br>根据 R构造的实际数据 r(R) 是否满足语义约束FD</p>\n<h4 id=\"Keys-and-Functional-Dependencies\"><a href=\"#Keys-and-Functional-Dependencies\" class=\"headerlink\" title=\"Keys and Functional Dependencies\"></a>Keys and Functional Dependencies</h4><p>Def：K 是关系架构 R 的超键，当且仅当 K → R<br>定义：K 是 R 的候选键，当且仅当 K → R 且 没有 $\\alpha \\subset$K、$\\alpha$→R<br>DF 允许我们表达无法用超级键表达的约束。</p>\n<h4 id=\"Use-of-Functional-Dependencies\"><a href=\"#Use-of-Functional-Dependencies\" class=\"headerlink\" title=\"Use of Functional Dependencies\"></a>Use of Functional Dependencies</h4><p>我们使用 FD 来 测试关系，看看它们是否合法。 如果关系 r 在 FD 集合 F 下是合法的，我们说 r 满足 F<br>指定对法律关系集的约束 如果 R 上的所有法律关系都满足 FD 集 F，则 F 对 R 成立。<br>注意：关系模式的特定实例可能满足 F 中的 FD，即使 FD 不持有所有法律实例</p>\n<h4 id=\"Trivial-平凡-Functional-Dependencies\"><a href=\"#Trivial-平凡-Functional-Dependencies\" class=\"headerlink\" title=\"Trivial (平凡) Functional Dependencies\"></a>Trivial (平凡) Functional Dependencies</h4><p>如果一个关系的所有实例都满足函数依赖关系，那么它就是平凡的</p>\n<h4 id=\"Transitive-传递-dependency\"><a href=\"#Transitive-传递-dependency\" class=\"headerlink\" title=\"Transitive (传递) dependency\"></a>Transitive (传递) dependency</h4><p>$\\alpha$→$\\beta$但不满足$\\beta$→$\\alpha$，满足$\\beta$→$\\gamma$, 但$\\gamma$ 不在$\\alpha$内，则称$\\alpha$→$\\gamma$满足传递依赖关系</p>\n<h4 id=\"Partial-部分-Dependency\"><a href=\"#Partial-部分-Dependency\" class=\"headerlink\" title=\"Partial (部分) Dependency\"></a>Partial (部分) Dependency</h4><p>y是a的子集，y→b，a→b为部份依赖</p>\n<h4 id=\"Closure-of-functional-dependency-set-F\"><a href=\"#Closure-of-functional-dependency-set-F\" class=\"headerlink\" title=\"Closure of functional dependency set $F^+$\"></a>Closure of functional dependency set $F^+$</h4><p>给定 FD 集 F 可以推断的所有 FD 的集合<br>$F^+$包含 F 中的所有功能依赖项</p>\n<h4 id=\"Lossless-Decomposition无损分解-1\"><a href=\"#Lossless-Decomposition无损分解-1\" class=\"headerlink\" title=\"Lossless Decomposition无损分解\"></a>Lossless Decomposition无损分解</h4><p>如果至少有如下一个依赖项位于 F+ 中，则将 R 分解为 R1 和 R2 是无损的：<br>R1 $\\cap$R2 →R1<br>R1$\\cap$ R2→ R2</p>\n<h3 id=\"7-3-Normal-Forms\"><a href=\"#7-3-Normal-Forms\" class=\"headerlink\" title=\"7.3 Normal Forms\"></a>7.3 Normal Forms</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232132.png\" alt=\"image.png\"><br>1NF：第一范式<br>2NF：第二范式<br>BCNF：Boyce-Codd 范式<br>3NF：第三范式</p>\n<h4 id=\"Atomic-Domains-and-First-Normal-Form原子域和第一范式\"><a href=\"#Atomic-Domains-and-First-Normal-Form原子域和第一范式\" class=\"headerlink\" title=\"Atomic Domains and First Normal Form原子域和第一范式\"></a>Atomic Domains and First Normal Form原子域和第一范式</h4><p>如果域的元素是不可分割的单元，则域是原子的<br>如果 R 的所有属性的域都是原子的，则关系架构 R 采用第一范式<br>原子性实际上是如何使用域元素的一个属性</p>\n<h4 id=\"Second-Normal-Form\"><a href=\"#Second-Normal-Form\" class=\"headerlink\" title=\"Second Normal Form\"></a>Second Normal Form</h4><p>关系模式 R 相对于 DF 集 F 在 2NF 中，如果<br>R 在 1NF 中，并且<br>每个属性 A 都满足其中一个条件：</p>\n<ul>\n<li>它出现在候选键中，即它是一个素数属性 &#x2F;&#x2F; A 是主属性 </li>\n<li>它（不是部分）依赖于候选密钥 A是非主属性,完全依赖于候选键</li>\n</ul>\n<h4 id=\"Boyce-Codd-Normal-Form-BCNF\"><a href=\"#Boyce-Codd-Normal-Form-BCNF\" class=\"headerlink\" title=\"Boyce-Codd Normal Form, BCNF\"></a>Boyce-Codd Normal Form, BCNF</h4><p>Def：关系架构 R 在 BCNF 中相对于 FD 集 F<br>如果对于 F+ 形式的所有功能依赖关系a→b，a、b属于R，且a→b是传递关系或a是超键</p>\n<h4 id=\"Decomposing-a-Schema-into-BCNF将架构分解为-BCNF\"><a href=\"#Decomposing-a-Schema-into-BCNF将架构分解为-BCNF\" class=\"headerlink\" title=\"Decomposing a Schema into BCNF将架构分解为 BCNF\"></a>Decomposing a Schema into BCNF将架构分解为 BCNF</h4><p>设 R 为不在 BCNF 中的架构 R。 让$\\alpha$→$\\beta$违反 BCNF<br>我们将 R 分解为<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232656.png\" alt=\"image.png\"></p>\n<h4 id=\"Dependency-Preservation依赖关系保留\"><a href=\"#Dependency-Preservation依赖关系保留\" class=\"headerlink\" title=\"Dependency Preservation依赖关系保留\"></a>Dependency Preservation依赖关系保留</h4><p>对于架构 R，F 是在 R 上的功能依赖，并分解 R 的 {R1， R2,.., Rn}，F 对 Ri 的限制，表示为 Fi，定义为<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100613.png\" alt=\"image.png\"><br>如果满足<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100633.png\" alt=\"image.png\"><br>则分解是依赖保留关系</p>\n<p>由于限制中的所有 FD 都只涉及一个关系模式的属性，因此我们通过仅检查一个关系来测试这种依赖关系。<br>注意：限制的定义使用 F+ 中的所有依赖项，而不仅仅是 F 中的依赖项。<br>注意：限制集 F1、F2 、..，Fn 是可以有效检查的 FD 集合。</p>\n<h3 id=\"7-5-Algorithms-for-Decomposition-Using-FD\"><a href=\"#7-5-Algorithms-for-Decomposition-Using-FD\" class=\"headerlink\" title=\"7.5 Algorithms for Decomposition Using FD\"></a>7.5 Algorithms for Decomposition Using FD</h3><ol>\n<li>Testing for BCNF</li>\n<li>Testing Decomposition for BCNF</li>\n<li>BCNF decomposition algorithm</li>\n<li>Testing for 3NF</li>\n<li>3NF decomposition algorithm</li>\n<li>Comparison of BCNF and 3NF</li>\n</ol>\n<p>简化测试：要检查关系架构 R 是否在 BCNF 中，只需检查 F 中的 FD 是否违反 BCNF 就足够了，而不是检查 F+ 中的所有 FD。 如果 F 中没有任何依赖项导致违反 BCNF，则 F+ 中的任何依赖项都不会导致违反 BCNF</p>\n<h4 id=\"Testing-Decomposition-for-BCNF\"><a href=\"#Testing-Decomposition-for-BCNF\" class=\"headerlink\" title=\"Testing Decomposition for BCNF\"></a>Testing Decomposition for BCNF</h4>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"7-1-Good-Relational-DB-Design\"><a href=\"#7-1-Good-Relational-DB-Design\" class=\"headerlink\" title=\"7.1 Good Relational DB Design\"></a>7.1 Good Relational DB Design</h3><p>逻辑数据库设计包括:</p>\n<ul>\n<li>初始关系架构生成</li>\n<li>关系架构规范化</li>\n</ul>\n<h4 id=\"Lossless-Decomposition无损分解\"><a href=\"#Lossless-Decomposition无损分解\" class=\"headerlink\" title=\"Lossless Decomposition无损分解\"></a>Lossless Decomposition无损分解</h4><p>Def：设 R 为关系模式，R1 和 R2 构成 R 的分解。 即 R &#x3D; R1 U R2<br>Def：分解是<strong>无损分解</strong>，如果将模式 R 替换为两个关系模式 R1 U R2 没有丢失信息</p>\n<h4 id=\"Normalization-Principles归一化原则\"><a href=\"#Normalization-Principles归一化原则\" class=\"headerlink\" title=\"Normalization Principles归一化原则\"></a>Normalization Principles归一化原则</h4><p>在DBS逻辑设计过程中，将E-R图转换，得到面向应用领域的初始关系模式集<br><strong>初始关系模式集</strong>存在关系模式属性间的<strong>数据依赖 (Data Dependence) 关系</strong></p>\n<ul>\n<li><strong>函数依赖</strong> (functional  dependencies,  FD)</li>\n<li><strong>多值依赖</strong> (Mutivalued Dependencies,  MVD)</li>\n<li><strong>连接依赖</strong> (Join Dependencies, JD)</li>\n</ul>\n<p>直接根据初始关系模式构造DBS，由于初始关系模式中数据依赖关系的存在，  可能会违反DB的完整性约束，导致DBS使用的正确性、性能、效率受到影响</p>\n<ul>\n<li>数据冗余问题 pitfalls</li>\n<li>插入问题 pitfalls</li>\n<li>更新问题 pitfalls</li>\n<li>删除问题 pitfalls</li>\n</ul>\n<p>等价变换&#x2F;模式分解: 对初始关系模式集，保证关系模式的：</p>\n<ul>\n<li>函数无损连接性（lossless join)，</li>\n<li>函数依赖保持性  (dependency preservation)</li>\n</ul>\n<p>关系模式集需要规范化处理——等价变换&#x2F;模式分解</p>\n<p>关系模式规范化主要步骤为：</p>\n<ul>\n<li>根据函数依赖的Armstrong’s 公理系统和多值依赖的公理系统，从初始关系模式集中已知的函数依赖和多值依赖出发，推导出初始关系模式集中所有的函数依赖和多值依赖</li>\n<li>对具有函数依赖和多值依赖的初始关系模式集，采用<strong>模式分解算法</strong>，对其进行（等价）分解和变换，将其转换为各种范式形式，包括：1NF、 2NF、 BCNF、 3NF、 4NF、5NF，以消除函数依赖和多值依赖的负面影响, 保证数据库完整性<br><a href=\"https://zhuanlan.zhihu.com/p/554101160#:~:text=%E8%8C%83%E5%BC%8F%EF%BC%88Paradigm%EF%BC%89%E6%98%AF%E7%AC%A6%E5%90%88%E6%9F%90%E4%B8%80%E7%A7%8D%E7%BA%A7%E5%88%AB%E7%9A%84%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F%E7%9A%84%E9%9B%86%E5%90%88%E3%80%82%20%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E5%85%B3%E7%B3%BB%E5%BF%85%E9%A1%BB%E6%BB%A1%E8%B6%B3%E4%B8%80%E5%AE%9A%E7%9A%84%E8%A6%81%E6%B1%82%EF%BC%8C%E6%BB%A1%E8%B6%B3%E4%B8%8D%E5%90%8C%E7%A8%8B%E5%BA%A6%E8%A6%81%E6%B1%82%E7%9A%84%E4%B8%BA%E4%B8%8D%E5%90%8C%E8%8C%83%E5%BC%8F%E3%80%82,%E7%9B%AE%E5%89%8D%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%89%E5%85%AD%E7%A7%8D%E8%8C%83%E5%BC%8F%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%E3%80%81%E7%AC%AC%E4%B8%89%E8%8C%83%E5%BC%8F%EF%BC%883NF%EF%BC%89%E3%80%81Boyce-Codd%E8%8C%83%E5%BC%8F%EF%BC%88BCNF%EF%BC%89%E3%80%81%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F%EF%BC%884NF%EF%BC%89%E5%92%8C%E7%AC%AC%E4%BA%94%E8%8C%83%E5%BC%8F%EF%BC%885NF%EF%BC%89%E3%80%82%20%E6%BB%A1%E8%B6%B3%E6%9C%80%E4%BD%8E%E8%A6%81%E6%B1%82%E7%9A%84%E8%8C%83%E5%BC%8F%E6%98%AF%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%EF%BC%881NF%EF%BC%89%E3%80%82%20%E5%9C%A8%E7%AC%AC%E4%B8%80%E8%8C%83%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%BB%A1%E8%B6%B3%E6%9B%B4%E5%A4%9A%E8%A6%81%E6%B1%82%E7%9A%84%E7%A7%B0%E4%B8%BA%E7%AC%AC%E4%BA%8C%E8%8C%83%E5%BC%8F%EF%BC%882NF%EF%BC%89%EF%BC%8C%E5%85%B6%E4%BD%99%E8%8C%83%E5%BC%8F%E4%BB%A5%E6%AC%A1%E7%B1%BB%E6%8E%A8%E3%80%82\">第一范式、第二范式、第三范式 - 知乎</a></li>\n</ul>\n<p>关系模式规范化处理的<strong>基本要求</strong>为: </p>\n<ul>\n<li>静态关系具有第一范式形式 </li>\n<li>动态关系最好具有3NF或BCNF形式</li>\n</ul>\n<p>3种数据依赖间的关系 ：</p>\n<ul>\n<li>函数依赖是特殊的多值依赖 </li>\n<li>多值依赖又是连接依赖的特例<br>范式1NF、2NF、3NF、BCNF可以看作由符合范式要求的各种关系模式组成的关系模式的集合<br>e.g. 1NF &#x3D; { R | R 满足第一范式的定义}<br>范式间的关系：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120558.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115120646.png\" alt=\"image.png\"></li>\n</ul>\n<p>给定关系模式，可以采用规范化算法将其转换为1NF、2NF、3NF、BCNF</p>\n<p>对连接依赖和第五范式，无相应的模式规范化算法</p>\n<h4 id=\"Normalization-Theory归一化理论\"><a href=\"#Normalization-Theory归一化理论\" class=\"headerlink\" title=\"Normalization Theory归一化理论\"></a>Normalization Theory归一化理论</h4><p>确定特定关系 R 是否处于良好形式（范式）<br>关系 R 的形式不好，分解为关系 {R1 R2 …Rn} 使得 </p>\n<ul>\n<li>每个关系都处于良好状态 </li>\n<li>分解是无损连接分解</li>\n</ul>\n<h4 id=\"Functional-Dependencies功能依赖关系\"><a href=\"#Functional-Dependencies功能依赖关系\" class=\"headerlink\" title=\"Functional Dependencies功能依赖关系\"></a>Functional Dependencies功能依赖关系</h4><p>数据通常存在（完整性）约束（规则）<br>例如，预期成立的约束：<br>学生和教师通过其 ID 进行唯一标识。<br>每个学生和教师只有一个名字。<br>每个教师和学生只与一个部门相关联。<br>每个部门只有一个预算值，并且只有一个关联的建筑物</p>\n<p>Def：满足所有此类现实世界约束的关系实例称为<strong>关系的合法实例</strong><br>Def：<strong>功能依赖</strong>:<br>对法律关系的约束询问<br>一组特定属性的值唯一地决定了另一组属性的值<br>函数依赖关系是键概念的泛化<br>函数： f： X→Y， x∈X， y∈Y， y &#x3D; f（x）<br>对于 x1，x2∈X，如果 x1&#x3D;x2，则 f（x1）&#x3D; f（x2)</p>\n<h4 id=\"Relation-Instance-Satisfy-Functional-Dependency\"><a href=\"#Relation-Instance-Satisfy-Functional-Dependency\" class=\"headerlink\" title=\"Relation Instance Satisfy Functional Dependency\"></a>Relation Instance Satisfy Functional Dependency</h4><p>对于关系模式R，$\\alpha$⊆ R，$\\beta$⊆ R,满足函数依赖关系$\\alpha$→$\\beta$<br>对于元组 ti 和 tj ∈r（R） 对，使得 $t_i[\\alpha]&#x3D; t_j [\\alpha ]$，也是 $t_i[\\beta]&#x3D; t_j [\\beta]$ 的情况</p>\n<h4 id=\"Functional-Dependency-Holds-on-Schema-r-R-功能依赖关系保留在架构-r（R）-上\"><a href=\"#Functional-Dependency-Holds-on-Schema-r-R-功能依赖关系保留在架构-r（R）-上\" class=\"headerlink\" title=\"Functional Dependency Holds on Schema r(R)功能依赖关系保留在架构 r（R） 上\"></a>Functional Dependency Holds on Schema r(R)功能依赖关系保留在架构 r（R） 上</h4><p>Def：让 R 成为关系架构 $\\alpha \\subseteq R$ and $\\beta \\subseteq R$，如果每个实例 r（R） 都满足$\\alpha$→$\\beta$，函数依赖FD在关系模式R上成立&#x2F;保持$\\alpha$→$\\beta$，每当两个元组 t1 和 t2 在属性$\\alpha$上达成一致时，也就在$\\beta$属性达成一致</p>\n<h4 id=\"FD-holds-on-R-vs-FD-is-satisfied-by-r-R-FD-保持-R-与-FD-满足-r（R）\"><a href=\"#FD-holds-on-R-vs-FD-is-satisfied-by-r-R-FD-保持-R-与-FD-满足-r（R）\" class=\"headerlink\" title=\"FD holds on R  vs  FD is satisfied by r(R)FD 保持 R 与 FD 满足 r（R）\"></a>FD holds on R  vs  FD is satisfied by r(R)FD 保持 R 与 FD 满足 r（R）</h4><p>在 R 上定义可能有多个关系实例 r（R），即$r_1（R） ， r_2（R） ， r_3（R） ,…, r_m（R）$<br>定义：关系 r（R） 满足$\\alpha$→$\\beta$与$\\alpha$→$\\beta$保留架构 R<br>如果$\\alpha$→$\\beta$在 R 上成立，则每个合法 r（R） 都满足此 R<br>但是对于模式 R，如果只有一些 ri（R） 满足 R，则$\\alpha$→$\\beta$可能不会对 R 成立。</p>\n<p>FD holds on R:<br>定义在R的属性间的语义约束，或R的属性间体现出的语义约束<br>从设计角度，R应满足的约束</p>\n<p>FD is satisfied by r(R):<br>根据 R构造的实际数据 r(R) 是否满足语义约束FD</p>\n<h4 id=\"Keys-and-Functional-Dependencies\"><a href=\"#Keys-and-Functional-Dependencies\" class=\"headerlink\" title=\"Keys and Functional Dependencies\"></a>Keys and Functional Dependencies</h4><p>Def：K 是关系架构 R 的超键，当且仅当 K → R<br>定义：K 是 R 的候选键，当且仅当 K → R 且 没有 $\\alpha \\subset$K、$\\alpha$→R<br>DF 允许我们表达无法用超级键表达的约束。</p>\n<h4 id=\"Use-of-Functional-Dependencies\"><a href=\"#Use-of-Functional-Dependencies\" class=\"headerlink\" title=\"Use of Functional Dependencies\"></a>Use of Functional Dependencies</h4><p>我们使用 FD 来 测试关系，看看它们是否合法。 如果关系 r 在 FD 集合 F 下是合法的，我们说 r 满足 F<br>指定对法律关系集的约束 如果 R 上的所有法律关系都满足 FD 集 F，则 F 对 R 成立。<br>注意：关系模式的特定实例可能满足 F 中的 FD，即使 FD 不持有所有法律实例</p>\n<h4 id=\"Trivial-平凡-Functional-Dependencies\"><a href=\"#Trivial-平凡-Functional-Dependencies\" class=\"headerlink\" title=\"Trivial (平凡) Functional Dependencies\"></a>Trivial (平凡) Functional Dependencies</h4><p>如果一个关系的所有实例都满足函数依赖关系，那么它就是平凡的</p>\n<h4 id=\"Transitive-传递-dependency\"><a href=\"#Transitive-传递-dependency\" class=\"headerlink\" title=\"Transitive (传递) dependency\"></a>Transitive (传递) dependency</h4><p>$\\alpha$→$\\beta$但不满足$\\beta$→$\\alpha$，满足$\\beta$→$\\gamma$, 但$\\gamma$ 不在$\\alpha$内，则称$\\alpha$→$\\gamma$满足传递依赖关系</p>\n<h4 id=\"Partial-部分-Dependency\"><a href=\"#Partial-部分-Dependency\" class=\"headerlink\" title=\"Partial (部分) Dependency\"></a>Partial (部分) Dependency</h4><p>y是a的子集，y→b，a→b为部份依赖</p>\n<h4 id=\"Closure-of-functional-dependency-set-F\"><a href=\"#Closure-of-functional-dependency-set-F\" class=\"headerlink\" title=\"Closure of functional dependency set $F^+$\"></a>Closure of functional dependency set $F^+$</h4><p>给定 FD 集 F 可以推断的所有 FD 的集合<br>$F^+$包含 F 中的所有功能依赖项</p>\n<h4 id=\"Lossless-Decomposition无损分解-1\"><a href=\"#Lossless-Decomposition无损分解-1\" class=\"headerlink\" title=\"Lossless Decomposition无损分解\"></a>Lossless Decomposition无损分解</h4><p>如果至少有如下一个依赖项位于 F+ 中，则将 R 分解为 R1 和 R2 是无损的：<br>R1 $\\cap$R2 →R1<br>R1$\\cap$ R2→ R2</p>\n<h3 id=\"7-3-Normal-Forms\"><a href=\"#7-3-Normal-Forms\" class=\"headerlink\" title=\"7.3 Normal Forms\"></a>7.3 Normal Forms</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232132.png\" alt=\"image.png\"><br>1NF：第一范式<br>2NF：第二范式<br>BCNF：Boyce-Codd 范式<br>3NF：第三范式</p>\n<h4 id=\"Atomic-Domains-and-First-Normal-Form原子域和第一范式\"><a href=\"#Atomic-Domains-and-First-Normal-Form原子域和第一范式\" class=\"headerlink\" title=\"Atomic Domains and First Normal Form原子域和第一范式\"></a>Atomic Domains and First Normal Form原子域和第一范式</h4><p>如果域的元素是不可分割的单元，则域是原子的<br>如果 R 的所有属性的域都是原子的，则关系架构 R 采用第一范式<br>原子性实际上是如何使用域元素的一个属性</p>\n<h4 id=\"Second-Normal-Form\"><a href=\"#Second-Normal-Form\" class=\"headerlink\" title=\"Second Normal Form\"></a>Second Normal Form</h4><p>关系模式 R 相对于 DF 集 F 在 2NF 中，如果<br>R 在 1NF 中，并且<br>每个属性 A 都满足其中一个条件：</p>\n<ul>\n<li>它出现在候选键中，即它是一个素数属性 &#x2F;&#x2F; A 是主属性 </li>\n<li>它（不是部分）依赖于候选密钥 A是非主属性,完全依赖于候选键</li>\n</ul>\n<h4 id=\"Boyce-Codd-Normal-Form-BCNF\"><a href=\"#Boyce-Codd-Normal-Form-BCNF\" class=\"headerlink\" title=\"Boyce-Codd Normal Form, BCNF\"></a>Boyce-Codd Normal Form, BCNF</h4><p>Def：关系架构 R 在 BCNF 中相对于 FD 集 F<br>如果对于 F+ 形式的所有功能依赖关系a→b，a、b属于R，且a→b是传递关系或a是超键</p>\n<h4 id=\"Decomposing-a-Schema-into-BCNF将架构分解为-BCNF\"><a href=\"#Decomposing-a-Schema-into-BCNF将架构分解为-BCNF\" class=\"headerlink\" title=\"Decomposing a Schema into BCNF将架构分解为 BCNF\"></a>Decomposing a Schema into BCNF将架构分解为 BCNF</h4><p>设 R 为不在 BCNF 中的架构 R。 让$\\alpha$→$\\beta$违反 BCNF<br>我们将 R 分解为<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231119232656.png\" alt=\"image.png\"></p>\n<h4 id=\"Dependency-Preservation依赖关系保留\"><a href=\"#Dependency-Preservation依赖关系保留\" class=\"headerlink\" title=\"Dependency Preservation依赖关系保留\"></a>Dependency Preservation依赖关系保留</h4><p>对于架构 R，F 是在 R 上的功能依赖，并分解 R 的 {R1， R2,.., Rn}，F 对 Ri 的限制，表示为 Fi，定义为<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100613.png\" alt=\"image.png\"><br>如果满足<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231127100633.png\" alt=\"image.png\"><br>则分解是依赖保留关系</p>\n<p>由于限制中的所有 FD 都只涉及一个关系模式的属性，因此我们通过仅检查一个关系来测试这种依赖关系。<br>注意：限制的定义使用 F+ 中的所有依赖项，而不仅仅是 F 中的依赖项。<br>注意：限制集 F1、F2 、..，Fn 是可以有效检查的 FD 集合。</p>\n<h3 id=\"7-5-Algorithms-for-Decomposition-Using-FD\"><a href=\"#7-5-Algorithms-for-Decomposition-Using-FD\" class=\"headerlink\" title=\"7.5 Algorithms for Decomposition Using FD\"></a>7.5 Algorithms for Decomposition Using FD</h3><ol>\n<li>Testing for BCNF</li>\n<li>Testing Decomposition for BCNF</li>\n<li>BCNF decomposition algorithm</li>\n<li>Testing for 3NF</li>\n<li>3NF decomposition algorithm</li>\n<li>Comparison of BCNF and 3NF</li>\n</ol>\n<p>简化测试：要检查关系架构 R 是否在 BCNF 中，只需检查 F 中的 FD 是否违反 BCNF 就足够了，而不是检查 F+ 中的所有 FD。 如果 F 中没有任何依赖项导致违反 BCNF，则 F+ 中的任何依赖项都不会导致违反 BCNF</p>\n<h4 id=\"Testing-Decomposition-for-BCNF\"><a href=\"#Testing-Decomposition-for-BCNF\" class=\"headerlink\" title=\"Testing Decomposition for BCNF\"></a>Testing Decomposition for BCNF</h4>"},{"title":"第三章作业","date":"2023-11-20T07:22:53.443Z","_content":"### 计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\n\n#### AlexNet\n[proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231120165524.png)\n\n根据论文中给出的网络结构\nAlexNet共有五层卷积层和三层全连接层\n其中第一层卷积层有96个卷积核，3通道，大小为11×11，步长为4\n第二层卷积层有256个卷积核，48通道，大小为5×5\n第三次卷积层有384个卷积核，256通道，大小为3×3\n第四层卷积层有384个卷积核，192通道，大小为3×3\n第五层卷积层有256个卷积核，192通道，大小为3×3\n三层全连接层的神经元数量分别为4096、4096、1000\n神经元数目：\n每个卷积层的神经元数目为feature map大小\n55* 55* 96+ 27* 27* 256+ 13* 13* 384+13* 13* 384+13* 13* 256+4096+4096+1000 = 659272\n可训练参数数目：34848+307200+884736+663552+442368+ 6* 6* 256* 4096+4096* 4096+4096* 1000 = 60954656\n\n\n#### VGG19\n卷积层1-2：64个3x3的卷积核\n卷积层3-4：128个3x3的卷积核\n卷积层5-8：256个3x3的卷积核\n卷积层9-12：512个3x3的卷积核\n卷积层13-16：512个3x3的卷积核\n全连接层1：输入神经元数量为 7x7x512，输出神经元数量为 4096\n全连接层2：输入神经元数量为 4096，输出神经元数量为 4096\n全连接层3（输出层）：输入神经元数量为 4096，输出神经元数量为类别数（通常为1000）\n参数数量: 3584 + 147712 + 590336 + 2360320 + 4719616 + 102764544 + 16781312 + 4097000 = 138,357,104\n\n总神经元数量 = (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 = 64,614,528\n\n#### ResNet152\n由于网络的参数数量过于庞大，引入torchinfo库协助计算\n```\nfrom torchinfo import summary  \nfrom torchvision import models  \n  \nnet = models.resnet152()  \nsummary(net, input_size=(1, 3, 224, 224))\n```\n得到的结果如下：\n```\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet                                   [1, 1000]                 --\n├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n├─ReLU: 1-3                              [1, 64, 112, 112]         --\n├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n├─Sequential: 1-5                        [1, 256, 56, 56]          --\n│    └─Bottleneck: 2-1                   [1, 256, 56, 56]          --\n│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           4,096\n│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n│    │    └─Conv2d: 3-7                  [1, 256, 56, 56]          16,384\n│    │    └─BatchNorm2d: 3-8             [1, 256, 56, 56]          512\n│    │    └─Sequential: 3-9              [1, 256, 56, 56]          16,896\n│    │    └─ReLU: 3-10                   [1, 256, 56, 56]          --\n│    └─Bottleneck: 2-2                   [1, 256, 56, 56]          --\n\n# 由于层数较多，不方便全部展示，因此在此删去了一部分\n\n│    │    └─Conv2d: 3-443                [1, 2048, 7, 7]           1,048,576\n│    │    └─BatchNorm2d: 3-444           [1, 2048, 7, 7]           4,096\n│    │    └─ReLU: 3-445                  [1, 2048, 7, 7]           --\n│    └─Bottleneck: 2-50                  [1, 2048, 7, 7]           --\n│    │    └─Conv2d: 3-446                [1, 512, 7, 7]            1,048,576\n│    │    └─BatchNorm2d: 3-447           [1, 512, 7, 7]            1,024\n│    │    └─ReLU: 3-448                  [1, 512, 7, 7]            --\n│    │    └─Conv2d: 3-449                [1, 512, 7, 7]            2,359,296\n│    │    └─BatchNorm2d: 3-450           [1, 512, 7, 7]            1,024\n│    │    └─ReLU: 3-451                  [1, 512, 7, 7]            --\n│    │    └─Conv2d: 3-452                [1, 2048, 7, 7]           1,048,576\n│    │    └─BatchNorm2d: 3-453           [1, 2048, 7, 7]           4,096\n│    │    └─ReLU: 3-454                  [1, 2048, 7, 7]           --\n├─AdaptiveAvgPool2d: 1-9                 [1, 2048, 1, 1]           --\n├─Linear: 1-10                           [1, 1000]                 2,049,000\n==========================================================================================\nTotal params: 60,192,808\nTrainable params: 60,192,808\nNon-trainable params: 0\nTotal mult-adds (G): 11.51\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 360.87\nParams size (MB): 240.77\nEstimated Total Size (MB): 602.25\n==========================================================================================\n\n```\n\n### 全连接层神经元数量计算过程：\n\n1. 全连接层1：4096个神经元\n2. 全连接层2：4096个神经元\n3. 全连接层3（输出层）：1000个神经元（如果用于ImageNet分类任务）\n\n现在，考虑到一些层的输出特征图大小相同，我们将每组卷积层的神经元数量乘以相应的层数，然后将所有层的神经元数量相加。\n\n总神经元数量 = (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 = 64,614,528 个神经元\n\n### 简述错误率与IoU、mAP的关系\n错误率是指分类或检测模型在执行特定任务时所犯的错误百分比\nIoU即交并比，是指检测框与真实框的交集面积与并集面积之比\nmAP中文翻译过来叫做平均精度均值，其中AP为平均精度（Average Precision），mAP是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合评价\n错误率是用于衡量模型性能的一种最简单的指标，反应的是一个大致的情况。IoU和mAP则是度量模型性能更详细的指标，其中mAP使用了IoU的概念，提供了一个比IoU更全面的度量方式\n\n### 简述R-CNN、Fast R-CNN和 Faster R-CNN 的区别\n\nR-CNN 是一种基于区域建议的目标检测方法，对每个候选区域独立地进行卷积特征提取，速度比较慢。\nFast R-CNN 通过在整个图像上运行卷积网络，然后在提取的特征图上选择感兴趣的区域（RoI，Region of Interest）来改进 R-CNN。不同于 R-CNN 中独立处理每个候选区域，Fast R-CNN 在整个图像上运行卷积网络一次，然后通过RoI池化层将每个区域映射到卷积特征图上。特征提取是在整个图像上进行的，而不是独立处理每个区域，这样做可以加快速度。\nFaster R-CNN 是进一步改进的方法，引入了RPN（Region Proposal Network）来生成候选区域。RPN是一个用于在图像中生成区域建议的神经网络，与整个检测网络（如Fast R-CNN）共享卷积层。这样就可以利用RPN直接从特征图中生成候选框，不再需要独立的区域提议步骤。\n\n\n### 简述LSTM标准模型中三个门的主要作用，并给出计算公式\n\n#### 遗忘门\n主要作用是决定哪些信息应该被忘记或丢弃。它通过一个sigmoid激活函数来输出一个0到1之间的值，表示每个记忆单元中信息保留的程度。当遗忘门输出接近1时，表示保留所有信息；当接近0时，表示尽可能忘记先前的信息。\n$$\\boldsymbol{f}^{(t)}=\\sigma\\left(U^{f} \\boldsymbol{x}^{(t)}+W^{f} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{f}\\right)=\\sigma\\left(\\mathcal{W}_{f}\\left(\\begin{array}{l}\n\\boldsymbol{h}^{(t-1)} \\\\\n\\boldsymbol{x}^{(t)}\n\\end{array}\\right)\\right)$$\n#### 输入门\n控制新信息的输入程度。通过sigmoid激活函数和tanh激活函数来筛选和创建新的候选值，然后将其结合起来更新单元的状态。\n$$\\boldsymbol{i}^{(t)}=\\sigma\\left(U^{i} \\boldsymbol{x}^{(t)}+W^{i} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{i}\\right)=\\sigma\\left(\\mathcal{W}_{i}\\left(\\begin{array}{l}\n\\boldsymbol{h}^{(t-1)} \\\\\n\\boldsymbol{x}^{(t)}\n\\end{array}\\right)\\right)$$\n#### 输出门\n决定当前时刻的隐藏状态应该输出多少信息。它根据当前的输入和前一时刻的隐藏状态，结合记忆单元的状态，输出一个0到1之间的值。\n$$\\boldsymbol{g}^{(t)}=\\sigma\\left(U^{o} \\boldsymbol{x}^{(t)}+W^{o} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{o}\\right)=\\sigma\\left(\\mathcal{W}_{o}\\left(\\begin{array}{l}\n\\boldsymbol{h}^{(t)} \\\\\n\\boldsymbol{x}^{(t)}\n\\end{array}\\right)\\right)$$\n\n\n### 简述GAN的训练过程\n模型由两部分组成：\n- 生成器：生成器的任务是生成与真实数据相似的虚假数据，目标是让生成器尽可能地欺骗判别器，使其生成的数据在视觉上或在数据分布上与真实数据难以区分。\n- 判别器：判别器是一个二分类器，它的任务是将真实数据和由生成器产生的假数据区分开来。判别器的目标是尽可能地准确地区分真实数据和虚假数据。\n生成器生成一批假样本，然后将这些假样本输入给判别器，判别器评估并给出对这些样本的判断，判别器接收一批真实样本和由生成器生成的假样本，然后分别对它们进行分类。\n优化判别器：$$J^{(D)}=-\\mathrm{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log (D(\\boldsymbol{x}))]-\\mathrm{E}_{z \\sim p_{\\boldsymbol{z}}(z)}[\\log (1-D(G(\\boldsymbol{z})))]$$\n优化生成器：$$J^{(G)}=\\mathrm{E}_{\\boldsymbol{z} \\sim p_{z}(z)}[\\log (1-D(G(z)))]$$\n","source":"_posts/Notes/课程/大三（上）/智能计算系统/第三章作业.md","raw":"---\ntitle: 第三章作业\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 智能计算系统\ndate:\ntags:\n---\n### 计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\n\n#### AlexNet\n[proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231120165524.png)\n\n根据论文中给出的网络结构\nAlexNet共有五层卷积层和三层全连接层\n其中第一层卷积层有96个卷积核，3通道，大小为11×11，步长为4\n第二层卷积层有256个卷积核，48通道，大小为5×5\n第三次卷积层有384个卷积核，256通道，大小为3×3\n第四层卷积层有384个卷积核，192通道，大小为3×3\n第五层卷积层有256个卷积核，192通道，大小为3×3\n三层全连接层的神经元数量分别为4096、4096、1000\n神经元数目：\n每个卷积层的神经元数目为feature map大小\n55* 55* 96+ 27* 27* 256+ 13* 13* 384+13* 13* 384+13* 13* 256+4096+4096+1000 = 659272\n可训练参数数目：34848+307200+884736+663552+442368+ 6* 6* 256* 4096+4096* 4096+4096* 1000 = 60954656\n\n\n#### VGG19\n卷积层1-2：64个3x3的卷积核\n卷积层3-4：128个3x3的卷积核\n卷积层5-8：256个3x3的卷积核\n卷积层9-12：512个3x3的卷积核\n卷积层13-16：512个3x3的卷积核\n全连接层1：输入神经元数量为 7x7x512，输出神经元数量为 4096\n全连接层2：输入神经元数量为 4096，输出神经元数量为 4096\n全连接层3（输出层）：输入神经元数量为 4096，输出神经元数量为类别数（通常为1000）\n参数数量: 3584 + 147712 + 590336 + 2360320 + 4719616 + 102764544 + 16781312 + 4097000 = 138,357,104\n\n总神经元数量 = (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 = 64,614,528\n\n#### ResNet152\n由于网络的参数数量过于庞大，引入torchinfo库协助计算\n```\nfrom torchinfo import summary  \nfrom torchvision import models  \n  \nnet = models.resnet152()  \nsummary(net, input_size=(1, 3, 224, 224))\n```\n得到的结果如下：\n```\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet                                   [1, 1000]                 --\n├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n├─ReLU: 1-3                              [1, 64, 112, 112]         --\n├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n├─Sequential: 1-5                        [1, 256, 56, 56]          --\n│    └─Bottleneck: 2-1                   [1, 256, 56, 56]          --\n│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           4,096\n│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n│    │    └─Conv2d: 3-7                  [1, 256, 56, 56]          16,384\n│    │    └─BatchNorm2d: 3-8             [1, 256, 56, 56]          512\n│    │    └─Sequential: 3-9              [1, 256, 56, 56]          16,896\n│    │    └─ReLU: 3-10                   [1, 256, 56, 56]          --\n│    └─Bottleneck: 2-2                   [1, 256, 56, 56]          --\n\n# 由于层数较多，不方便全部展示，因此在此删去了一部分\n\n│    │    └─Conv2d: 3-443                [1, 2048, 7, 7]           1,048,576\n│    │    └─BatchNorm2d: 3-444           [1, 2048, 7, 7]           4,096\n│    │    └─ReLU: 3-445                  [1, 2048, 7, 7]           --\n│    └─Bottleneck: 2-50                  [1, 2048, 7, 7]           --\n│    │    └─Conv2d: 3-446                [1, 512, 7, 7]            1,048,576\n│    │    └─BatchNorm2d: 3-447           [1, 512, 7, 7]            1,024\n│    │    └─ReLU: 3-448                  [1, 512, 7, 7]            --\n│    │    └─Conv2d: 3-449                [1, 512, 7, 7]            2,359,296\n│    │    └─BatchNorm2d: 3-450           [1, 512, 7, 7]            1,024\n│    │    └─ReLU: 3-451                  [1, 512, 7, 7]            --\n│    │    └─Conv2d: 3-452                [1, 2048, 7, 7]           1,048,576\n│    │    └─BatchNorm2d: 3-453           [1, 2048, 7, 7]           4,096\n│    │    └─ReLU: 3-454                  [1, 2048, 7, 7]           --\n├─AdaptiveAvgPool2d: 1-9                 [1, 2048, 1, 1]           --\n├─Linear: 1-10                           [1, 1000]                 2,049,000\n==========================================================================================\nTotal params: 60,192,808\nTrainable params: 60,192,808\nNon-trainable params: 0\nTotal mult-adds (G): 11.51\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 360.87\nParams size (MB): 240.77\nEstimated Total Size (MB): 602.25\n==========================================================================================\n\n```\n\n### 全连接层神经元数量计算过程：\n\n1. 全连接层1：4096个神经元\n2. 全连接层2：4096个神经元\n3. 全连接层3（输出层）：1000个神经元（如果用于ImageNet分类任务）\n\n现在，考虑到一些层的输出特征图大小相同，我们将每组卷积层的神经元数量乘以相应的层数，然后将所有层的神经元数量相加。\n\n总神经元数量 = (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 = 64,614,528 个神经元\n\n### 简述错误率与IoU、mAP的关系\n错误率是指分类或检测模型在执行特定任务时所犯的错误百分比\nIoU即交并比，是指检测框与真实框的交集面积与并集面积之比\nmAP中文翻译过来叫做平均精度均值，其中AP为平均精度（Average Precision），mAP是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合评价\n错误率是用于衡量模型性能的一种最简单的指标，反应的是一个大致的情况。IoU和mAP则是度量模型性能更详细的指标，其中mAP使用了IoU的概念，提供了一个比IoU更全面的度量方式\n\n### 简述R-CNN、Fast R-CNN和 Faster R-CNN 的区别\n\nR-CNN 是一种基于区域建议的目标检测方法，对每个候选区域独立地进行卷积特征提取，速度比较慢。\nFast R-CNN 通过在整个图像上运行卷积网络，然后在提取的特征图上选择感兴趣的区域（RoI，Region of Interest）来改进 R-CNN。不同于 R-CNN 中独立处理每个候选区域，Fast R-CNN 在整个图像上运行卷积网络一次，然后通过RoI池化层将每个区域映射到卷积特征图上。特征提取是在整个图像上进行的，而不是独立处理每个区域，这样做可以加快速度。\nFaster R-CNN 是进一步改进的方法，引入了RPN（Region Proposal Network）来生成候选区域。RPN是一个用于在图像中生成区域建议的神经网络，与整个检测网络（如Fast R-CNN）共享卷积层。这样就可以利用RPN直接从特征图中生成候选框，不再需要独立的区域提议步骤。\n\n\n### 简述LSTM标准模型中三个门的主要作用，并给出计算公式\n\n#### 遗忘门\n主要作用是决定哪些信息应该被忘记或丢弃。它通过一个sigmoid激活函数来输出一个0到1之间的值，表示每个记忆单元中信息保留的程度。当遗忘门输出接近1时，表示保留所有信息；当接近0时，表示尽可能忘记先前的信息。\n$$\\boldsymbol{f}^{(t)}=\\sigma\\left(U^{f} \\boldsymbol{x}^{(t)}+W^{f} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{f}\\right)=\\sigma\\left(\\mathcal{W}_{f}\\left(\\begin{array}{l}\n\\boldsymbol{h}^{(t-1)} \\\\\n\\boldsymbol{x}^{(t)}\n\\end{array}\\right)\\right)$$\n#### 输入门\n控制新信息的输入程度。通过sigmoid激活函数和tanh激活函数来筛选和创建新的候选值，然后将其结合起来更新单元的状态。\n$$\\boldsymbol{i}^{(t)}=\\sigma\\left(U^{i} \\boldsymbol{x}^{(t)}+W^{i} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{i}\\right)=\\sigma\\left(\\mathcal{W}_{i}\\left(\\begin{array}{l}\n\\boldsymbol{h}^{(t-1)} \\\\\n\\boldsymbol{x}^{(t)}\n\\end{array}\\right)\\right)$$\n#### 输出门\n决定当前时刻的隐藏状态应该输出多少信息。它根据当前的输入和前一时刻的隐藏状态，结合记忆单元的状态，输出一个0到1之间的值。\n$$\\boldsymbol{g}^{(t)}=\\sigma\\left(U^{o} \\boldsymbol{x}^{(t)}+W^{o} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{o}\\right)=\\sigma\\left(\\mathcal{W}_{o}\\left(\\begin{array}{l}\n\\boldsymbol{h}^{(t)} \\\\\n\\boldsymbol{x}^{(t)}\n\\end{array}\\right)\\right)$$\n\n\n### 简述GAN的训练过程\n模型由两部分组成：\n- 生成器：生成器的任务是生成与真实数据相似的虚假数据，目标是让生成器尽可能地欺骗判别器，使其生成的数据在视觉上或在数据分布上与真实数据难以区分。\n- 判别器：判别器是一个二分类器，它的任务是将真实数据和由生成器产生的假数据区分开来。判别器的目标是尽可能地准确地区分真实数据和虚假数据。\n生成器生成一批假样本，然后将这些假样本输入给判别器，判别器评估并给出对这些样本的判断，判别器接收一批真实样本和由生成器生成的假样本，然后分别对它们进行分类。\n优化判别器：$$J^{(D)}=-\\mathrm{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log (D(\\boldsymbol{x}))]-\\mathrm{E}_{z \\sim p_{\\boldsymbol{z}}(z)}[\\log (1-D(G(\\boldsymbol{z})))]$$\n优化生成器：$$J^{(G)}=\\mathrm{E}_{\\boldsymbol{z} \\sim p_{z}(z)}[\\log (1-D(G(z)))]$$\n","slug":"Notes/课程/大三（上）/智能计算系统/第三章作业","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12a001jvw8c07an5vfz","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\"><a href=\"#计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\" class=\"headerlink\" title=\"计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\"></a>计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目</h3><h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><p><a href=\"https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\">proceedings.neurips.cc&#x2F;paper&#x2F;2012&#x2F;file&#x2F;c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231120165524.png\" alt=\"image.png\"></p>\n<p>根据论文中给出的网络结构<br>AlexNet共有五层卷积层和三层全连接层<br>其中第一层卷积层有96个卷积核，3通道，大小为11×11，步长为4<br>第二层卷积层有256个卷积核，48通道，大小为5×5<br>第三次卷积层有384个卷积核，256通道，大小为3×3<br>第四层卷积层有384个卷积核，192通道，大小为3×3<br>第五层卷积层有256个卷积核，192通道，大小为3×3<br>三层全连接层的神经元数量分别为4096、4096、1000<br>神经元数目：<br>每个卷积层的神经元数目为feature map大小<br>55* 55* 96+ 27* 27* 256+ 13* 13* 384+13* 13* 384+13* 13* 256+4096+4096+1000 &#x3D; 659272<br>可训练参数数目：34848+307200+884736+663552+442368+ 6* 6* 256* 4096+4096* 4096+4096* 1000 &#x3D; 60954656</p>\n<h4 id=\"VGG19\"><a href=\"#VGG19\" class=\"headerlink\" title=\"VGG19\"></a>VGG19</h4><p>卷积层1-2：64个3x3的卷积核<br>卷积层3-4：128个3x3的卷积核<br>卷积层5-8：256个3x3的卷积核<br>卷积层9-12：512个3x3的卷积核<br>卷积层13-16：512个3x3的卷积核<br>全连接层1：输入神经元数量为 7x7x512，输出神经元数量为 4096<br>全连接层2：输入神经元数量为 4096，输出神经元数量为 4096<br>全连接层3（输出层）：输入神经元数量为 4096，输出神经元数量为类别数（通常为1000）<br>参数数量: 3584 + 147712 + 590336 + 2360320 + 4719616 + 102764544 + 16781312 + 4097000 &#x3D; 138,357,104</p>\n<p>总神经元数量 &#x3D; (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 &#x3D; 64,614,528</p>\n<h4 id=\"ResNet152\"><a href=\"#ResNet152\" class=\"headerlink\" title=\"ResNet152\"></a>ResNet152</h4><p>由于网络的参数数量过于庞大，引入torchinfo库协助计算</p>\n<figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-keyword\">from</span> torchinfo <span class=\"hljs-keyword\">import</span> <span class=\"hljs-keyword\">summary</span>  <br><span class=\"hljs-keyword\">from</span> torchvision <span class=\"hljs-keyword\">import</span> models  <br>  <br>net = models.resnet152()  <br><span class=\"hljs-keyword\">summary</span>(net, input_size=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">224</span>, <span class=\"hljs-number\">224</span>))<br></code></pre></td></tr></table></figure>\n<p>得到的结果如下：</p>\n<figure class=\"highlight nestedtext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nestedtext\">==========================================================================================<br>Layer (type:depth-idx)                   Output Shape              Param #<br><span class=\"hljs-attribute\">==========================================================================================</span><br><span class=\"hljs-attribute\">ResNet                                   [1, 1000]                 --</span><br><span class=\"hljs-attribute\">├─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-1                            [1, 64, 112, 112]         9,408</span><br><span class=\"hljs-attribute\">├─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-2                       [1, 64, 112, 112]         128</span><br><span class=\"hljs-attribute\">├─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-3                              [1, 64, 112, 112]         --</span><br><span class=\"hljs-attribute\">├─MaxPool2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-4                         [1, 64, 56, 56]           --</span><br><span class=\"hljs-attribute\">├─Sequential</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-5                        [1, 256, 56, 56]          --</span><br><span class=\"hljs-attribute\">│    └─Bottleneck</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">2-1                   [1, 256, 56, 56]          --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-1                  [1, 64, 56, 56]           4,096</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-2             [1, 64, 56, 56]           128</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-3                    [1, 64, 56, 56]           --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-4                  [1, 64, 56, 56]           36,864</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-5             [1, 64, 56, 56]           128</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-6                    [1, 64, 56, 56]           --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-7                  [1, 256, 56, 56]          16,384</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-8             [1, 256, 56, 56]          512</span><br><span class=\"hljs-attribute\">│    │    └─Sequential</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-9              [1, 256, 56, 56]          16,896</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-10                   [1, 256, 56, 56]          --</span><br><span class=\"hljs-attribute\">│    └─Bottleneck</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">2-2                   [1, 256, 56, 56]          --</span><br><br><span class=\"hljs-comment\"># 由于层数较多，不方便全部展示，因此在此删去了一部分</span><br><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-443                [1, 2048, 7, 7]           1,048,576</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-444           [1, 2048, 7, 7]           4,096</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-445                  [1, 2048, 7, 7]           --</span><br><span class=\"hljs-attribute\">│    └─Bottleneck</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">2-50                  [1, 2048, 7, 7]           --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-446                [1, 512, 7, 7]            1,048,576</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-447           [1, 512, 7, 7]            1,024</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-448                  [1, 512, 7, 7]            --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-449                [1, 512, 7, 7]            2,359,296</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-450           [1, 512, 7, 7]            1,024</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-451                  [1, 512, 7, 7]            --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-452                [1, 2048, 7, 7]           1,048,576</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-453           [1, 2048, 7, 7]           4,096</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-454                  [1, 2048, 7, 7]           --</span><br><span class=\"hljs-attribute\">├─AdaptiveAvgPool2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-9                 [1, 2048, 1, 1]           --</span><br><span class=\"hljs-attribute\">├─Linear</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-10                           [1, 1000]                 2,049,000</span><br><span class=\"hljs-attribute\">==========================================================================================</span><br><span class=\"hljs-attribute\">Total params</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">60,192,808</span><br><span class=\"hljs-attribute\">Trainable params</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">60,192,808</span><br><span class=\"hljs-attribute\">Non-trainable params</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">0</span><br><span class=\"hljs-attribute\">Total mult-adds (G)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">11.51</span><br><span class=\"hljs-attribute\">==========================================================================================</span><br><span class=\"hljs-attribute\">Input size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">0.60</span><br><span class=\"hljs-attribute\">Forward/backward pass size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">360.87</span><br><span class=\"hljs-attribute\">Params size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">240.77</span><br><span class=\"hljs-attribute\">Estimated Total Size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">602.25</span><br>==========================================================================================<br><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"全连接层神经元数量计算过程：\"><a href=\"#全连接层神经元数量计算过程：\" class=\"headerlink\" title=\"全连接层神经元数量计算过程：\"></a>全连接层神经元数量计算过程：</h3><ol>\n<li>全连接层1：4096个神经元</li>\n<li>全连接层2：4096个神经元</li>\n<li>全连接层3（输出层）：1000个神经元（如果用于ImageNet分类任务）</li>\n</ol>\n<p>现在，考虑到一些层的输出特征图大小相同，我们将每组卷积层的神经元数量乘以相应的层数，然后将所有层的神经元数量相加。</p>\n<p>总神经元数量 &#x3D; (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 &#x3D; 64,614,528 个神经元</p>\n<h3 id=\"简述错误率与IoU、mAP的关系\"><a href=\"#简述错误率与IoU、mAP的关系\" class=\"headerlink\" title=\"简述错误率与IoU、mAP的关系\"></a>简述错误率与IoU、mAP的关系</h3><p>错误率是指分类或检测模型在执行特定任务时所犯的错误百分比<br>IoU即交并比，是指检测框与真实框的交集面积与并集面积之比<br>mAP中文翻译过来叫做平均精度均值，其中AP为平均精度（Average Precision），mAP是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合评价<br>错误率是用于衡量模型性能的一种最简单的指标，反应的是一个大致的情况。IoU和mAP则是度量模型性能更详细的指标，其中mAP使用了IoU的概念，提供了一个比IoU更全面的度量方式</p>\n<h3 id=\"简述R-CNN、Fast-R-CNN和-Faster-R-CNN-的区别\"><a href=\"#简述R-CNN、Fast-R-CNN和-Faster-R-CNN-的区别\" class=\"headerlink\" title=\"简述R-CNN、Fast R-CNN和 Faster R-CNN 的区别\"></a>简述R-CNN、Fast R-CNN和 Faster R-CNN 的区别</h3><p>R-CNN 是一种基于区域建议的目标检测方法，对每个候选区域独立地进行卷积特征提取，速度比较慢。<br>Fast R-CNN 通过在整个图像上运行卷积网络，然后在提取的特征图上选择感兴趣的区域（RoI，Region of Interest）来改进 R-CNN。不同于 R-CNN 中独立处理每个候选区域，Fast R-CNN 在整个图像上运行卷积网络一次，然后通过RoI池化层将每个区域映射到卷积特征图上。特征提取是在整个图像上进行的，而不是独立处理每个区域，这样做可以加快速度。<br>Faster R-CNN 是进一步改进的方法，引入了RPN（Region Proposal Network）来生成候选区域。RPN是一个用于在图像中生成区域建议的神经网络，与整个检测网络（如Fast R-CNN）共享卷积层。这样就可以利用RPN直接从特征图中生成候选框，不再需要独立的区域提议步骤。</p>\n<h3 id=\"简述LSTM标准模型中三个门的主要作用，并给出计算公式\"><a href=\"#简述LSTM标准模型中三个门的主要作用，并给出计算公式\" class=\"headerlink\" title=\"简述LSTM标准模型中三个门的主要作用，并给出计算公式\"></a>简述LSTM标准模型中三个门的主要作用，并给出计算公式</h3><h4 id=\"遗忘门\"><a href=\"#遗忘门\" class=\"headerlink\" title=\"遗忘门\"></a>遗忘门</h4><p>主要作用是决定哪些信息应该被忘记或丢弃。它通过一个sigmoid激活函数来输出一个0到1之间的值，表示每个记忆单元中信息保留的程度。当遗忘门输出接近1时，表示保留所有信息；当接近0时，表示尽可能忘记先前的信息。<br>$$\\boldsymbol{f}^{(t)}&#x3D;\\sigma\\left(U^{f} \\boldsymbol{x}^{(t)}+W^{f} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{f}\\right)&#x3D;\\sigma\\left(\\mathcal{W}_{f}\\left(\\begin{array}{l}<br>\\boldsymbol{h}^{(t-1)} \\<br>\\boldsymbol{x}^{(t)}<br>\\end{array}\\right)\\right)$$</p>\n<h4 id=\"输入门\"><a href=\"#输入门\" class=\"headerlink\" title=\"输入门\"></a>输入门</h4><p>控制新信息的输入程度。通过sigmoid激活函数和tanh激活函数来筛选和创建新的候选值，然后将其结合起来更新单元的状态。<br>$$\\boldsymbol{i}^{(t)}&#x3D;\\sigma\\left(U^{i} \\boldsymbol{x}^{(t)}+W^{i} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{i}\\right)&#x3D;\\sigma\\left(\\mathcal{W}_{i}\\left(\\begin{array}{l}<br>\\boldsymbol{h}^{(t-1)} \\<br>\\boldsymbol{x}^{(t)}<br>\\end{array}\\right)\\right)$$</p>\n<h4 id=\"输出门\"><a href=\"#输出门\" class=\"headerlink\" title=\"输出门\"></a>输出门</h4><p>决定当前时刻的隐藏状态应该输出多少信息。它根据当前的输入和前一时刻的隐藏状态，结合记忆单元的状态，输出一个0到1之间的值。<br>$$\\boldsymbol{g}^{(t)}&#x3D;\\sigma\\left(U^{o} \\boldsymbol{x}^{(t)}+W^{o} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{o}\\right)&#x3D;\\sigma\\left(\\mathcal{W}_{o}\\left(\\begin{array}{l}<br>\\boldsymbol{h}^{(t)} \\<br>\\boldsymbol{x}^{(t)}<br>\\end{array}\\right)\\right)$$</p>\n<h3 id=\"简述GAN的训练过程\"><a href=\"#简述GAN的训练过程\" class=\"headerlink\" title=\"简述GAN的训练过程\"></a>简述GAN的训练过程</h3><p>模型由两部分组成：</p>\n<ul>\n<li>生成器：生成器的任务是生成与真实数据相似的虚假数据，目标是让生成器尽可能地欺骗判别器，使其生成的数据在视觉上或在数据分布上与真实数据难以区分。</li>\n<li>判别器：判别器是一个二分类器，它的任务是将真实数据和由生成器产生的假数据区分开来。判别器的目标是尽可能地准确地区分真实数据和虚假数据。<br>生成器生成一批假样本，然后将这些假样本输入给判别器，判别器评估并给出对这些样本的判断，判别器接收一批真实样本和由生成器生成的假样本，然后分别对它们进行分类。<br>优化判别器：$$J^{(D)}&#x3D;-\\mathrm{E}<em>{\\boldsymbol{x} \\sim p</em>{\\text {data }}(\\boldsymbol{x})}[\\log (D(\\boldsymbol{x}))]-\\mathrm{E}<em>{z \\sim p</em>{\\boldsymbol{z}}(z)}[\\log (1-D(G(\\boldsymbol{z})))]$$<br>优化生成器：$$J^{(G)}&#x3D;\\mathrm{E}<em>{\\boldsymbol{z} \\sim p</em>{z}(z)}[\\log (1-D(G(z)))]$$</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\"><a href=\"#计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\" class=\"headerlink\" title=\"计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目\"></a>计算AlexNet、VGG19、ResNet152三个网络中的神经元数目及可训练的参数数目</h3><h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><p><a href=\"https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\">proceedings.neurips.cc&#x2F;paper&#x2F;2012&#x2F;file&#x2F;c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231120165524.png\" alt=\"image.png\"></p>\n<p>根据论文中给出的网络结构<br>AlexNet共有五层卷积层和三层全连接层<br>其中第一层卷积层有96个卷积核，3通道，大小为11×11，步长为4<br>第二层卷积层有256个卷积核，48通道，大小为5×5<br>第三次卷积层有384个卷积核，256通道，大小为3×3<br>第四层卷积层有384个卷积核，192通道，大小为3×3<br>第五层卷积层有256个卷积核，192通道，大小为3×3<br>三层全连接层的神经元数量分别为4096、4096、1000<br>神经元数目：<br>每个卷积层的神经元数目为feature map大小<br>55* 55* 96+ 27* 27* 256+ 13* 13* 384+13* 13* 384+13* 13* 256+4096+4096+1000 &#x3D; 659272<br>可训练参数数目：34848+307200+884736+663552+442368+ 6* 6* 256* 4096+4096* 4096+4096* 1000 &#x3D; 60954656</p>\n<h4 id=\"VGG19\"><a href=\"#VGG19\" class=\"headerlink\" title=\"VGG19\"></a>VGG19</h4><p>卷积层1-2：64个3x3的卷积核<br>卷积层3-4：128个3x3的卷积核<br>卷积层5-8：256个3x3的卷积核<br>卷积层9-12：512个3x3的卷积核<br>卷积层13-16：512个3x3的卷积核<br>全连接层1：输入神经元数量为 7x7x512，输出神经元数量为 4096<br>全连接层2：输入神经元数量为 4096，输出神经元数量为 4096<br>全连接层3（输出层）：输入神经元数量为 4096，输出神经元数量为类别数（通常为1000）<br>参数数量: 3584 + 147712 + 590336 + 2360320 + 4719616 + 102764544 + 16781312 + 4097000 &#x3D; 138,357,104</p>\n<p>总神经元数量 &#x3D; (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 &#x3D; 64,614,528</p>\n<h4 id=\"ResNet152\"><a href=\"#ResNet152\" class=\"headerlink\" title=\"ResNet152\"></a>ResNet152</h4><p>由于网络的参数数量过于庞大，引入torchinfo库协助计算</p>\n<figure class=\"highlight pgsql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs pgsql\"><span class=\"hljs-keyword\">from</span> torchinfo <span class=\"hljs-keyword\">import</span> <span class=\"hljs-keyword\">summary</span>  <br><span class=\"hljs-keyword\">from</span> torchvision <span class=\"hljs-keyword\">import</span> models  <br>  <br>net = models.resnet152()  <br><span class=\"hljs-keyword\">summary</span>(net, input_size=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">224</span>, <span class=\"hljs-number\">224</span>))<br></code></pre></td></tr></table></figure>\n<p>得到的结果如下：</p>\n<figure class=\"highlight nestedtext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs nestedtext\">==========================================================================================<br>Layer (type:depth-idx)                   Output Shape              Param #<br><span class=\"hljs-attribute\">==========================================================================================</span><br><span class=\"hljs-attribute\">ResNet                                   [1, 1000]                 --</span><br><span class=\"hljs-attribute\">├─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-1                            [1, 64, 112, 112]         9,408</span><br><span class=\"hljs-attribute\">├─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-2                       [1, 64, 112, 112]         128</span><br><span class=\"hljs-attribute\">├─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-3                              [1, 64, 112, 112]         --</span><br><span class=\"hljs-attribute\">├─MaxPool2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-4                         [1, 64, 56, 56]           --</span><br><span class=\"hljs-attribute\">├─Sequential</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-5                        [1, 256, 56, 56]          --</span><br><span class=\"hljs-attribute\">│    └─Bottleneck</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">2-1                   [1, 256, 56, 56]          --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-1                  [1, 64, 56, 56]           4,096</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-2             [1, 64, 56, 56]           128</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-3                    [1, 64, 56, 56]           --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-4                  [1, 64, 56, 56]           36,864</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-5             [1, 64, 56, 56]           128</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-6                    [1, 64, 56, 56]           --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-7                  [1, 256, 56, 56]          16,384</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-8             [1, 256, 56, 56]          512</span><br><span class=\"hljs-attribute\">│    │    └─Sequential</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-9              [1, 256, 56, 56]          16,896</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-10                   [1, 256, 56, 56]          --</span><br><span class=\"hljs-attribute\">│    └─Bottleneck</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">2-2                   [1, 256, 56, 56]          --</span><br><br><span class=\"hljs-comment\"># 由于层数较多，不方便全部展示，因此在此删去了一部分</span><br><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-443                [1, 2048, 7, 7]           1,048,576</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-444           [1, 2048, 7, 7]           4,096</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-445                  [1, 2048, 7, 7]           --</span><br><span class=\"hljs-attribute\">│    └─Bottleneck</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">2-50                  [1, 2048, 7, 7]           --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-446                [1, 512, 7, 7]            1,048,576</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-447           [1, 512, 7, 7]            1,024</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-448                  [1, 512, 7, 7]            --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-449                [1, 512, 7, 7]            2,359,296</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-450           [1, 512, 7, 7]            1,024</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-451                  [1, 512, 7, 7]            --</span><br><span class=\"hljs-attribute\">│    │    └─Conv2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-452                [1, 2048, 7, 7]           1,048,576</span><br><span class=\"hljs-attribute\">│    │    └─BatchNorm2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-453           [1, 2048, 7, 7]           4,096</span><br><span class=\"hljs-attribute\">│    │    └─ReLU</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">3-454                  [1, 2048, 7, 7]           --</span><br><span class=\"hljs-attribute\">├─AdaptiveAvgPool2d</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-9                 [1, 2048, 1, 1]           --</span><br><span class=\"hljs-attribute\">├─Linear</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">1-10                           [1, 1000]                 2,049,000</span><br><span class=\"hljs-attribute\">==========================================================================================</span><br><span class=\"hljs-attribute\">Total params</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">60,192,808</span><br><span class=\"hljs-attribute\">Trainable params</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">60,192,808</span><br><span class=\"hljs-attribute\">Non-trainable params</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">0</span><br><span class=\"hljs-attribute\">Total mult-adds (G)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">11.51</span><br><span class=\"hljs-attribute\">==========================================================================================</span><br><span class=\"hljs-attribute\">Input size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">0.60</span><br><span class=\"hljs-attribute\">Forward/backward pass size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">360.87</span><br><span class=\"hljs-attribute\">Params size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">240.77</span><br><span class=\"hljs-attribute\">Estimated Total Size (MB)</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">602.25</span><br>==========================================================================================<br><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"全连接层神经元数量计算过程：\"><a href=\"#全连接层神经元数量计算过程：\" class=\"headerlink\" title=\"全连接层神经元数量计算过程：\"></a>全连接层神经元数量计算过程：</h3><ol>\n<li>全连接层1：4096个神经元</li>\n<li>全连接层2：4096个神经元</li>\n<li>全连接层3（输出层）：1000个神经元（如果用于ImageNet分类任务）</li>\n</ol>\n<p>现在，考虑到一些层的输出特征图大小相同，我们将每组卷积层的神经元数量乘以相应的层数，然后将所有层的神经元数量相加。</p>\n<p>总神经元数量 &#x3D; (3,161,600 * 2) + (1,605,632 * 2) + (802,816 * 4) + (401,408 * 4) + (100,352 * 4) + 4096 + 4096 + 1000 &#x3D; 64,614,528 个神经元</p>\n<h3 id=\"简述错误率与IoU、mAP的关系\"><a href=\"#简述错误率与IoU、mAP的关系\" class=\"headerlink\" title=\"简述错误率与IoU、mAP的关系\"></a>简述错误率与IoU、mAP的关系</h3><p>错误率是指分类或检测模型在执行特定任务时所犯的错误百分比<br>IoU即交并比，是指检测框与真实框的交集面积与并集面积之比<br>mAP中文翻译过来叫做平均精度均值，其中AP为平均精度（Average Precision），mAP是把每个类别的AP都单独拿出来，然后计算所有类别AP的平均值，代表着对检测到的目标平均精度的一个综合评价<br>错误率是用于衡量模型性能的一种最简单的指标，反应的是一个大致的情况。IoU和mAP则是度量模型性能更详细的指标，其中mAP使用了IoU的概念，提供了一个比IoU更全面的度量方式</p>\n<h3 id=\"简述R-CNN、Fast-R-CNN和-Faster-R-CNN-的区别\"><a href=\"#简述R-CNN、Fast-R-CNN和-Faster-R-CNN-的区别\" class=\"headerlink\" title=\"简述R-CNN、Fast R-CNN和 Faster R-CNN 的区别\"></a>简述R-CNN、Fast R-CNN和 Faster R-CNN 的区别</h3><p>R-CNN 是一种基于区域建议的目标检测方法，对每个候选区域独立地进行卷积特征提取，速度比较慢。<br>Fast R-CNN 通过在整个图像上运行卷积网络，然后在提取的特征图上选择感兴趣的区域（RoI，Region of Interest）来改进 R-CNN。不同于 R-CNN 中独立处理每个候选区域，Fast R-CNN 在整个图像上运行卷积网络一次，然后通过RoI池化层将每个区域映射到卷积特征图上。特征提取是在整个图像上进行的，而不是独立处理每个区域，这样做可以加快速度。<br>Faster R-CNN 是进一步改进的方法，引入了RPN（Region Proposal Network）来生成候选区域。RPN是一个用于在图像中生成区域建议的神经网络，与整个检测网络（如Fast R-CNN）共享卷积层。这样就可以利用RPN直接从特征图中生成候选框，不再需要独立的区域提议步骤。</p>\n<h3 id=\"简述LSTM标准模型中三个门的主要作用，并给出计算公式\"><a href=\"#简述LSTM标准模型中三个门的主要作用，并给出计算公式\" class=\"headerlink\" title=\"简述LSTM标准模型中三个门的主要作用，并给出计算公式\"></a>简述LSTM标准模型中三个门的主要作用，并给出计算公式</h3><h4 id=\"遗忘门\"><a href=\"#遗忘门\" class=\"headerlink\" title=\"遗忘门\"></a>遗忘门</h4><p>主要作用是决定哪些信息应该被忘记或丢弃。它通过一个sigmoid激活函数来输出一个0到1之间的值，表示每个记忆单元中信息保留的程度。当遗忘门输出接近1时，表示保留所有信息；当接近0时，表示尽可能忘记先前的信息。<br>$$\\boldsymbol{f}^{(t)}&#x3D;\\sigma\\left(U^{f} \\boldsymbol{x}^{(t)}+W^{f} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{f}\\right)&#x3D;\\sigma\\left(\\mathcal{W}_{f}\\left(\\begin{array}{l}<br>\\boldsymbol{h}^{(t-1)} \\<br>\\boldsymbol{x}^{(t)}<br>\\end{array}\\right)\\right)$$</p>\n<h4 id=\"输入门\"><a href=\"#输入门\" class=\"headerlink\" title=\"输入门\"></a>输入门</h4><p>控制新信息的输入程度。通过sigmoid激活函数和tanh激活函数来筛选和创建新的候选值，然后将其结合起来更新单元的状态。<br>$$\\boldsymbol{i}^{(t)}&#x3D;\\sigma\\left(U^{i} \\boldsymbol{x}^{(t)}+W^{i} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{i}\\right)&#x3D;\\sigma\\left(\\mathcal{W}_{i}\\left(\\begin{array}{l}<br>\\boldsymbol{h}^{(t-1)} \\<br>\\boldsymbol{x}^{(t)}<br>\\end{array}\\right)\\right)$$</p>\n<h4 id=\"输出门\"><a href=\"#输出门\" class=\"headerlink\" title=\"输出门\"></a>输出门</h4><p>决定当前时刻的隐藏状态应该输出多少信息。它根据当前的输入和前一时刻的隐藏状态，结合记忆单元的状态，输出一个0到1之间的值。<br>$$\\boldsymbol{g}^{(t)}&#x3D;\\sigma\\left(U^{o} \\boldsymbol{x}^{(t)}+W^{o} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}^{o}\\right)&#x3D;\\sigma\\left(\\mathcal{W}_{o}\\left(\\begin{array}{l}<br>\\boldsymbol{h}^{(t)} \\<br>\\boldsymbol{x}^{(t)}<br>\\end{array}\\right)\\right)$$</p>\n<h3 id=\"简述GAN的训练过程\"><a href=\"#简述GAN的训练过程\" class=\"headerlink\" title=\"简述GAN的训练过程\"></a>简述GAN的训练过程</h3><p>模型由两部分组成：</p>\n<ul>\n<li>生成器：生成器的任务是生成与真实数据相似的虚假数据，目标是让生成器尽可能地欺骗判别器，使其生成的数据在视觉上或在数据分布上与真实数据难以区分。</li>\n<li>判别器：判别器是一个二分类器，它的任务是将真实数据和由生成器产生的假数据区分开来。判别器的目标是尽可能地准确地区分真实数据和虚假数据。<br>生成器生成一批假样本，然后将这些假样本输入给判别器，判别器评估并给出对这些样本的判断，判别器接收一批真实样本和由生成器生成的假样本，然后分别对它们进行分类。<br>优化判别器：$$J^{(D)}&#x3D;-\\mathrm{E}<em>{\\boldsymbol{x} \\sim p</em>{\\text {data }}(\\boldsymbol{x})}[\\log (D(\\boldsymbol{x}))]-\\mathrm{E}<em>{z \\sim p</em>{\\boldsymbol{z}}(z)}[\\log (1-D(G(\\boldsymbol{z})))]$$<br>优化生成器：$$J^{(G)}&#x3D;\\mathrm{E}<em>{\\boldsymbol{z} \\sim p</em>{z}(z)}[\\log (1-D(G(z)))]$$</li>\n</ul>\n"},{"title":"编程框架使用","date":"2023-11-09T00:25:42.360Z","_content":"深度学习编程框架：将深度学习算法中的基本操作封装成一系列组件，这一系列深度学习组件，即构成一套深度学习框架\n\n### pytorch概述\n##### 主要优点\n简洁易懂：API设计简洁一致，基本上是tensor、autograd、nn三层封装\n便于调试：采用动态图，可以进行调试\n强大高效：提供了非常丰富的模型组件\n\n#### pytorch和tensorflow\nTensorflow在工业界拥有完备的解决方案和用户基础\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234517.png)\n\n### pytorch编程模型及基本用法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234940.png)\n\n#### Numpy基础\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080600.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080608.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080629.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080859.png)\n\n#### ndarray的属性\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081100.png)\n\n#### 形状操作\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081251.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081536.png)\n\n#### 索引\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081931.png)\n\n\n### Pytorch基础\n张量是现代深度学习框架的核心数据结构之一，包括PyTorch和TensorFlow等。它类似于 NumPy中的多维数组（ndarray），在PyTorch中，几乎所有的数据都表示为张量。这包括输 入数据、模型参数、梯度等。\n支持GPU加速：PyTorch能够利用GPU加速计算，而张量是在GPU上进行计算的主要数据类 型。\n张量是计算图上的数据载体，用张量统一表示所有的数据，张量在计算图的节点之间传递。\n张量可以看做是n维的数组，数组的维数即为张量的阶数\n\n#### tensor创建\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082757.png)\n\n#### tensor与array的转换\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082837.png)\n\n#### 张量的数据类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083145.png)\n\n不同数据分布对位宽的需求是不同的\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083621.png)\n不同层的数据对于位宽的需求是不一样的，每层数据都有其保持网络收敛的最低位宽要求，每层数据的位宽需求与数据分布之间存在关系\n\n#### 张量属性的转换\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116084631.png)\n\n#### 张量的数据格式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116085711.png)\n\n#### 张量的切片\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101105106.png)\n\n#### 张量维度的压缩、扩展\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116090605.png)\n\n#### 原地操作\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091113.png)\n原地操作能够节省内存占用，在进行深度学习算法推理时，使用原地操作能够有效减少模型占用的内存\n原地操作会覆盖原张量，如果在模型训练时使用原地操 作来更新张量梯度，则每次迭代计算所的梯度值将被覆 盖，从而破坏模型的训练过程\n对于多个张量同时引用一个张量的情况，对该张量进行 原地操作会影响其他张量的操作\n\n#### 广播机制\n对于参与计算操作的多个张量，如果张量维度不匹配，可以 使用PyTorch的广播机制对不匹配的张量维度进行扩展，最终 将这些张量均扩展为维度相同\n广播条件：\n- 每个张量都有至少1个维度\n- 从**张量末尾的维度开始对齐**扩展，在对齐后的同一维度中，仅下列情况下才允许进行广播操作：1）维度尺寸相同；2）维度尺寸不同 但其中一个维度尺寸为1；3）其中一个张量没有该维度\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091810.png)\n\n#### 计算图\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110110.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110417.png)\n\n### 基于Pytorch的模型推理实现\n#### 读取输入图像\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093426.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093434.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110938.png)\n\n#### 构建神经网络\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101150707.png)\n","source":"_posts/Notes/课程/大三（上）/智能计算系统/编程框架使用.md","raw":"---\ntitle: 编程框架使用\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 智能计算系统\ntags:\n  - 智能计算系统\n  - Pytorch\ndate:\n---\n深度学习编程框架：将深度学习算法中的基本操作封装成一系列组件，这一系列深度学习组件，即构成一套深度学习框架\n\n### pytorch概述\n##### 主要优点\n简洁易懂：API设计简洁一致，基本上是tensor、autograd、nn三层封装\n便于调试：采用动态图，可以进行调试\n强大高效：提供了非常丰富的模型组件\n\n#### pytorch和tensorflow\nTensorflow在工业界拥有完备的解决方案和用户基础\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234517.png)\n\n### pytorch编程模型及基本用法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234940.png)\n\n#### Numpy基础\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080600.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080608.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080629.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080859.png)\n\n#### ndarray的属性\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081100.png)\n\n#### 形状操作\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081251.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081536.png)\n\n#### 索引\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081931.png)\n\n\n### Pytorch基础\n张量是现代深度学习框架的核心数据结构之一，包括PyTorch和TensorFlow等。它类似于 NumPy中的多维数组（ndarray），在PyTorch中，几乎所有的数据都表示为张量。这包括输 入数据、模型参数、梯度等。\n支持GPU加速：PyTorch能够利用GPU加速计算，而张量是在GPU上进行计算的主要数据类 型。\n张量是计算图上的数据载体，用张量统一表示所有的数据，张量在计算图的节点之间传递。\n张量可以看做是n维的数组，数组的维数即为张量的阶数\n\n#### tensor创建\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082757.png)\n\n#### tensor与array的转换\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082837.png)\n\n#### 张量的数据类型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083145.png)\n\n不同数据分布对位宽的需求是不同的\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083621.png)\n不同层的数据对于位宽的需求是不一样的，每层数据都有其保持网络收敛的最低位宽要求，每层数据的位宽需求与数据分布之间存在关系\n\n#### 张量属性的转换\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116084631.png)\n\n#### 张量的数据格式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116085711.png)\n\n#### 张量的切片\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101105106.png)\n\n#### 张量维度的压缩、扩展\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116090605.png)\n\n#### 原地操作\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091113.png)\n原地操作能够节省内存占用，在进行深度学习算法推理时，使用原地操作能够有效减少模型占用的内存\n原地操作会覆盖原张量，如果在模型训练时使用原地操 作来更新张量梯度，则每次迭代计算所的梯度值将被覆 盖，从而破坏模型的训练过程\n对于多个张量同时引用一个张量的情况，对该张量进行 原地操作会影响其他张量的操作\n\n#### 广播机制\n对于参与计算操作的多个张量，如果张量维度不匹配，可以 使用PyTorch的广播机制对不匹配的张量维度进行扩展，最终 将这些张量均扩展为维度相同\n广播条件：\n- 每个张量都有至少1个维度\n- 从**张量末尾的维度开始对齐**扩展，在对齐后的同一维度中，仅下列情况下才允许进行广播操作：1）维度尺寸相同；2）维度尺寸不同 但其中一个维度尺寸为1；3）其中一个张量没有该维度\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091810.png)\n\n#### 计算图\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110110.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110417.png)\n\n### 基于Pytorch的模型推理实现\n#### 读取输入图像\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093426.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093434.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110938.png)\n\n#### 构建神经网络\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101150707.png)\n","slug":"Notes/课程/大三（上）/智能计算系统/编程框架使用","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12a001mvw8c7fa5d1n5","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>深度学习编程框架：将深度学习算法中的基本操作封装成一系列组件，这一系列深度学习组件，即构成一套深度学习框架</p>\n<h3 id=\"pytorch概述\"><a href=\"#pytorch概述\" class=\"headerlink\" title=\"pytorch概述\"></a>pytorch概述</h3><h5 id=\"主要优点\"><a href=\"#主要优点\" class=\"headerlink\" title=\"主要优点\"></a>主要优点</h5><p>简洁易懂：API设计简洁一致，基本上是tensor、autograd、nn三层封装<br>便于调试：采用动态图，可以进行调试<br>强大高效：提供了非常丰富的模型组件</p>\n<h4 id=\"pytorch和tensorflow\"><a href=\"#pytorch和tensorflow\" class=\"headerlink\" title=\"pytorch和tensorflow\"></a>pytorch和tensorflow</h4><p>Tensorflow在工业界拥有完备的解决方案和用户基础<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234517.png\" alt=\"image.png\"></p>\n<h3 id=\"pytorch编程模型及基本用法\"><a href=\"#pytorch编程模型及基本用法\" class=\"headerlink\" title=\"pytorch编程模型及基本用法\"></a>pytorch编程模型及基本用法</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234940.png\" alt=\"image.png\"></p>\n<h4 id=\"Numpy基础\"><a href=\"#Numpy基础\" class=\"headerlink\" title=\"Numpy基础\"></a>Numpy基础</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080600.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080608.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080629.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080859.png\" alt=\"image.png\"></p>\n<h4 id=\"ndarray的属性\"><a href=\"#ndarray的属性\" class=\"headerlink\" title=\"ndarray的属性\"></a>ndarray的属性</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081100.png\" alt=\"image.png\"></p>\n<h4 id=\"形状操作\"><a href=\"#形状操作\" class=\"headerlink\" title=\"形状操作\"></a>形状操作</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081251.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081536.png\" alt=\"image.png\"></p>\n<h4 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081931.png\" alt=\"image.png\"></p>\n<h3 id=\"Pytorch基础\"><a href=\"#Pytorch基础\" class=\"headerlink\" title=\"Pytorch基础\"></a>Pytorch基础</h3><p>张量是现代深度学习框架的核心数据结构之一，包括PyTorch和TensorFlow等。它类似于 NumPy中的多维数组（ndarray），在PyTorch中，几乎所有的数据都表示为张量。这包括输 入数据、模型参数、梯度等。<br>支持GPU加速：PyTorch能够利用GPU加速计算，而张量是在GPU上进行计算的主要数据类 型。<br>张量是计算图上的数据载体，用张量统一表示所有的数据，张量在计算图的节点之间传递。<br>张量可以看做是n维的数组，数组的维数即为张量的阶数</p>\n<h4 id=\"tensor创建\"><a href=\"#tensor创建\" class=\"headerlink\" title=\"tensor创建\"></a>tensor创建</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082757.png\" alt=\"image.png\"></p>\n<h4 id=\"tensor与array的转换\"><a href=\"#tensor与array的转换\" class=\"headerlink\" title=\"tensor与array的转换\"></a>tensor与array的转换</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082837.png\" alt=\"image.png\"></p>\n<h4 id=\"张量的数据类型\"><a href=\"#张量的数据类型\" class=\"headerlink\" title=\"张量的数据类型\"></a>张量的数据类型</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083145.png\" alt=\"image.png\"></p>\n<p>不同数据分布对位宽的需求是不同的<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083621.png\" alt=\"image.png\"><br>不同层的数据对于位宽的需求是不一样的，每层数据都有其保持网络收敛的最低位宽要求，每层数据的位宽需求与数据分布之间存在关系</p>\n<h4 id=\"张量属性的转换\"><a href=\"#张量属性的转换\" class=\"headerlink\" title=\"张量属性的转换\"></a>张量属性的转换</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116084631.png\" alt=\"image.png\"></p>\n<h4 id=\"张量的数据格式\"><a href=\"#张量的数据格式\" class=\"headerlink\" title=\"张量的数据格式\"></a>张量的数据格式</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116085711.png\" alt=\"image.png\"></p>\n<h4 id=\"张量的切片\"><a href=\"#张量的切片\" class=\"headerlink\" title=\"张量的切片\"></a>张量的切片</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101105106.png\" alt=\"image.png\"></p>\n<h4 id=\"张量维度的压缩、扩展\"><a href=\"#张量维度的压缩、扩展\" class=\"headerlink\" title=\"张量维度的压缩、扩展\"></a>张量维度的压缩、扩展</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116090605.png\" alt=\"image.png\"></p>\n<h4 id=\"原地操作\"><a href=\"#原地操作\" class=\"headerlink\" title=\"原地操作\"></a>原地操作</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091113.png\" alt=\"image.png\"><br>原地操作能够节省内存占用，在进行深度学习算法推理时，使用原地操作能够有效减少模型占用的内存<br>原地操作会覆盖原张量，如果在模型训练时使用原地操 作来更新张量梯度，则每次迭代计算所的梯度值将被覆 盖，从而破坏模型的训练过程<br>对于多个张量同时引用一个张量的情况，对该张量进行 原地操作会影响其他张量的操作</p>\n<h4 id=\"广播机制\"><a href=\"#广播机制\" class=\"headerlink\" title=\"广播机制\"></a>广播机制</h4><p>对于参与计算操作的多个张量，如果张量维度不匹配，可以 使用PyTorch的广播机制对不匹配的张量维度进行扩展，最终 将这些张量均扩展为维度相同<br>广播条件：</p>\n<ul>\n<li>每个张量都有至少1个维度</li>\n<li>从<strong>张量末尾的维度开始对齐</strong>扩展，在对齐后的同一维度中，仅下列情况下才允许进行广播操作：1）维度尺寸相同；2）维度尺寸不同 但其中一个维度尺寸为1；3）其中一个张量没有该维度<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091810.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"计算图\"><a href=\"#计算图\" class=\"headerlink\" title=\"计算图\"></a>计算图</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110110.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110417.png\" alt=\"image.png\"></p>\n<h3 id=\"基于Pytorch的模型推理实现\"><a href=\"#基于Pytorch的模型推理实现\" class=\"headerlink\" title=\"基于Pytorch的模型推理实现\"></a>基于Pytorch的模型推理实现</h3><h4 id=\"读取输入图像\"><a href=\"#读取输入图像\" class=\"headerlink\" title=\"读取输入图像\"></a>读取输入图像</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093426.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093434.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110938.png\" alt=\"image.png\"></p>\n<h4 id=\"构建神经网络\"><a href=\"#构建神经网络\" class=\"headerlink\" title=\"构建神经网络\"></a>构建神经网络</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101150707.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>深度学习编程框架：将深度学习算法中的基本操作封装成一系列组件，这一系列深度学习组件，即构成一套深度学习框架</p>\n<h3 id=\"pytorch概述\"><a href=\"#pytorch概述\" class=\"headerlink\" title=\"pytorch概述\"></a>pytorch概述</h3><h5 id=\"主要优点\"><a href=\"#主要优点\" class=\"headerlink\" title=\"主要优点\"></a>主要优点</h5><p>简洁易懂：API设计简洁一致，基本上是tensor、autograd、nn三层封装<br>便于调试：采用动态图，可以进行调试<br>强大高效：提供了非常丰富的模型组件</p>\n<h4 id=\"pytorch和tensorflow\"><a href=\"#pytorch和tensorflow\" class=\"headerlink\" title=\"pytorch和tensorflow\"></a>pytorch和tensorflow</h4><p>Tensorflow在工业界拥有完备的解决方案和用户基础<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234517.png\" alt=\"image.png\"></p>\n<h3 id=\"pytorch编程模型及基本用法\"><a href=\"#pytorch编程模型及基本用法\" class=\"headerlink\" title=\"pytorch编程模型及基本用法\"></a>pytorch编程模型及基本用法</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231231234940.png\" alt=\"image.png\"></p>\n<h4 id=\"Numpy基础\"><a href=\"#Numpy基础\" class=\"headerlink\" title=\"Numpy基础\"></a>Numpy基础</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080600.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080608.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080629.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116080859.png\" alt=\"image.png\"></p>\n<h4 id=\"ndarray的属性\"><a href=\"#ndarray的属性\" class=\"headerlink\" title=\"ndarray的属性\"></a>ndarray的属性</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081100.png\" alt=\"image.png\"></p>\n<h4 id=\"形状操作\"><a href=\"#形状操作\" class=\"headerlink\" title=\"形状操作\"></a>形状操作</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081251.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081536.png\" alt=\"image.png\"></p>\n<h4 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116081931.png\" alt=\"image.png\"></p>\n<h3 id=\"Pytorch基础\"><a href=\"#Pytorch基础\" class=\"headerlink\" title=\"Pytorch基础\"></a>Pytorch基础</h3><p>张量是现代深度学习框架的核心数据结构之一，包括PyTorch和TensorFlow等。它类似于 NumPy中的多维数组（ndarray），在PyTorch中，几乎所有的数据都表示为张量。这包括输 入数据、模型参数、梯度等。<br>支持GPU加速：PyTorch能够利用GPU加速计算，而张量是在GPU上进行计算的主要数据类 型。<br>张量是计算图上的数据载体，用张量统一表示所有的数据，张量在计算图的节点之间传递。<br>张量可以看做是n维的数组，数组的维数即为张量的阶数</p>\n<h4 id=\"tensor创建\"><a href=\"#tensor创建\" class=\"headerlink\" title=\"tensor创建\"></a>tensor创建</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082757.png\" alt=\"image.png\"></p>\n<h4 id=\"tensor与array的转换\"><a href=\"#tensor与array的转换\" class=\"headerlink\" title=\"tensor与array的转换\"></a>tensor与array的转换</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116082837.png\" alt=\"image.png\"></p>\n<h4 id=\"张量的数据类型\"><a href=\"#张量的数据类型\" class=\"headerlink\" title=\"张量的数据类型\"></a>张量的数据类型</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083145.png\" alt=\"image.png\"></p>\n<p>不同数据分布对位宽的需求是不同的<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116083621.png\" alt=\"image.png\"><br>不同层的数据对于位宽的需求是不一样的，每层数据都有其保持网络收敛的最低位宽要求，每层数据的位宽需求与数据分布之间存在关系</p>\n<h4 id=\"张量属性的转换\"><a href=\"#张量属性的转换\" class=\"headerlink\" title=\"张量属性的转换\"></a>张量属性的转换</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116084631.png\" alt=\"image.png\"></p>\n<h4 id=\"张量的数据格式\"><a href=\"#张量的数据格式\" class=\"headerlink\" title=\"张量的数据格式\"></a>张量的数据格式</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116085711.png\" alt=\"image.png\"></p>\n<h4 id=\"张量的切片\"><a href=\"#张量的切片\" class=\"headerlink\" title=\"张量的切片\"></a>张量的切片</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101105106.png\" alt=\"image.png\"></p>\n<h4 id=\"张量维度的压缩、扩展\"><a href=\"#张量维度的压缩、扩展\" class=\"headerlink\" title=\"张量维度的压缩、扩展\"></a>张量维度的压缩、扩展</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116090605.png\" alt=\"image.png\"></p>\n<h4 id=\"原地操作\"><a href=\"#原地操作\" class=\"headerlink\" title=\"原地操作\"></a>原地操作</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091113.png\" alt=\"image.png\"><br>原地操作能够节省内存占用，在进行深度学习算法推理时，使用原地操作能够有效减少模型占用的内存<br>原地操作会覆盖原张量，如果在模型训练时使用原地操 作来更新张量梯度，则每次迭代计算所的梯度值将被覆 盖，从而破坏模型的训练过程<br>对于多个张量同时引用一个张量的情况，对该张量进行 原地操作会影响其他张量的操作</p>\n<h4 id=\"广播机制\"><a href=\"#广播机制\" class=\"headerlink\" title=\"广播机制\"></a>广播机制</h4><p>对于参与计算操作的多个张量，如果张量维度不匹配，可以 使用PyTorch的广播机制对不匹配的张量维度进行扩展，最终 将这些张量均扩展为维度相同<br>广播条件：</p>\n<ul>\n<li>每个张量都有至少1个维度</li>\n<li>从<strong>张量末尾的维度开始对齐</strong>扩展，在对齐后的同一维度中，仅下列情况下才允许进行广播操作：1）维度尺寸相同；2）维度尺寸不同 但其中一个维度尺寸为1；3）其中一个张量没有该维度<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116091810.png\" alt=\"image.png\"></li>\n</ul>\n<h4 id=\"计算图\"><a href=\"#计算图\" class=\"headerlink\" title=\"计算图\"></a>计算图</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110110.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110417.png\" alt=\"image.png\"></p>\n<h3 id=\"基于Pytorch的模型推理实现\"><a href=\"#基于Pytorch的模型推理实现\" class=\"headerlink\" title=\"基于Pytorch的模型推理实现\"></a>基于Pytorch的模型推理实现</h3><h4 id=\"读取输入图像\"><a href=\"#读取输入图像\" class=\"headerlink\" title=\"读取输入图像\"></a>读取输入图像</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093426.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116093434.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101110938.png\" alt=\"image.png\"></p>\n<h4 id=\"构建神经网络\"><a href=\"#构建神经网络\" class=\"headerlink\" title=\"构建神经网络\"></a>构建神经网络</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101150707.png\" alt=\"image.png\"></p>\n"},{"title":"A2分析与简答","date":"2023-10-30T11:44:59.000Z","_content":"#### • A2.1 (5 points) 尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch_size的选择依据和影响。\n\nEpoch代表全部的训练数据在模型中训练的次数\nIteration表示在一个Epoch中参数更新的次数\nBatch表示一次正向和反向传播中的一组数据样本\nbatch_size表示Batch中的数据量。小的batch_size可能会导致训练过程不稳定，但梯度更新更快，更容易得到更好的模型效果。大的batch_size可以提高内存利用率，训练效果比较稳定，但模型收敛速度较慢，过大的batch_size得到的模型效果普遍较差。\n\n\n#### • A2.2 (5 points) 以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。 \n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100205.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100233.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031101845.png)\n\n该网络表达式为：$$a_{1} = w_{1}\\cdot x + b_{1}$$$$a_{2} = w_{1}\\cdot a_{1} + b_{2}$$\n均方误差损失函数表达式为：$$L\\left(y, a_{2}\\right)=\\left(y-a_{2}\\right)^{2}$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203612.png)\n\n\n交叉熵损失函数表达式为：$$L\\left(y, a_{2}\\right)=-\\left[y \\log \\left(a_{2}\\right)+(1-y) \\log \\left(1-a_{2}\\right)\\right]$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203715.png)\n由图像可以显然看出其非凸性\n\n\n\n#### • A2.3 (5 points) 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\n回归模型中输出加入高斯噪音，用函数表示如下：$$y=f(x, \\theta)+\\epsilon$$其中 $x$ 为输入数据，$y$ 为输出数据，$\\epsilon$ 为高斯噪音，在此假设其概率密度函数为 $\\mathcal{N}\\left(0, \\sigma^{2}\\right)$.\n对于一个期望为 $\\mu$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)$$\n样本 $y_i$ 服从期望为 $f(x_i, \\theta)$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$p\\left(y_{i} \\mid x_{i}, \\theta\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}{2 \\sigma^{2}}}$$\n所有样本的联合概率密度函数为：$$p(\\mathbf{y} \\mid \\mathbf{x}, \\theta)=\\prod_{i=1}^{N} p\\left(y_{i} \\mid x_{i}, \\theta\\right)=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{N / 2}} e^{-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}$$\n极大似然估计的目的是找到使得观测数据出现的概率最大的参数值 $\\theta$ ，即调整 $\\theta$ 的值使得上述联合概率密度函数的值最大，即使得 $\\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小。\n\n样本的均方误差可以表示为：$$MSELoss=\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$$最小化均方误差即使得 $\\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小，这与极大似然估计的目的等价。\n\n\n#### • A2.4 (5 points) 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\n[极大似然估计与最小化交叉熵损失或者KL散度为什么等价？ - 知乎](https://zhuanlan.zhihu.com/p/84764177)\n分类问题的联合概率分布为：$$p(X \\mid \\theta) = \\prod_{i=1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)$$\n对参数 $\\theta$ 的极大似然估计为：$$\\begin{array}{l}\n\\theta=\\underset{\\theta}{\\operatorname{argmax}} p(X \\mid \\theta) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\prod_{i=1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\sum_{i=1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)\n\\end{array}$$抽样服从真实样本分布，可得：$$\\begin{array}{l}\n=\\underset{\\theta}{\\operatorname{argmax}} E_{x \\sim p_{\\text {data }}(x)} \\log p(x \\mid \\theta) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\int_{x} p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x \\\\\n=\\underset{\\theta}{\\operatorname{argmin}} \\int_{x}-p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x\n\\end{array}$$即为交叉熵损失。\n\n\n\n#### • A2.5 (5 points) 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\n[l1 相比于 l2 为什么容易获得稀疏解？ - 知乎](https://www.zhihu.com/question/37096933/answer/70426653)\nL1正则化项是参数的绝对值之和，它在零附近不可导，这使得在优化过程中，某些参数可能会被推到零，从而使得模型变得稀疏；\nL2正则化项是参数的平方和，它是光滑可导的，在优化过程中会使得所有参数都变得较小，但通常不会等于零，从而得到一个相对平滑的解。\n\n#### • A2.6 (5 points) 分析Batch normalization对参数优化起到什么作用、如何起到这种作用。\n允许使用更大的学习率，加快模型的学习速度。减轻了对参数初始化的依赖，一定程度上增加了泛化能力。\nBatch normalization对每个batch中的数据进行归一化，使得其均值为0，方差为1，然后进行缩放和偏移操作（scale and shift），维持数据的原始分布，其均值和方差分别为可学习的参数β和γ，通过反向传播更新。","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/A2分析与简答.md","raw":"---\ntitle: A2分析与简答\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 深度学习\ndate:\n---\n#### • A2.1 (5 points) 尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch_size的选择依据和影响。\n\nEpoch代表全部的训练数据在模型中训练的次数\nIteration表示在一个Epoch中参数更新的次数\nBatch表示一次正向和反向传播中的一组数据样本\nbatch_size表示Batch中的数据量。小的batch_size可能会导致训练过程不稳定，但梯度更新更快，更容易得到更好的模型效果。大的batch_size可以提高内存利用率，训练效果比较稳定，但模型收敛速度较慢，过大的batch_size得到的模型效果普遍较差。\n\n\n#### • A2.2 (5 points) 以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。 \n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100205.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100233.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031101845.png)\n\n该网络表达式为：$$a_{1} = w_{1}\\cdot x + b_{1}$$$$a_{2} = w_{1}\\cdot a_{1} + b_{2}$$\n均方误差损失函数表达式为：$$L\\left(y, a_{2}\\right)=\\left(y-a_{2}\\right)^{2}$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203612.png)\n\n\n交叉熵损失函数表达式为：$$L\\left(y, a_{2}\\right)=-\\left[y \\log \\left(a_{2}\\right)+(1-y) \\log \\left(1-a_{2}\\right)\\right]$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203715.png)\n由图像可以显然看出其非凸性\n\n\n\n#### • A2.3 (5 points) 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\n回归模型中输出加入高斯噪音，用函数表示如下：$$y=f(x, \\theta)+\\epsilon$$其中 $x$ 为输入数据，$y$ 为输出数据，$\\epsilon$ 为高斯噪音，在此假设其概率密度函数为 $\\mathcal{N}\\left(0, \\sigma^{2}\\right)$.\n对于一个期望为 $\\mu$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)$$\n样本 $y_i$ 服从期望为 $f(x_i, \\theta)$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$p\\left(y_{i} \\mid x_{i}, \\theta\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}{2 \\sigma^{2}}}$$\n所有样本的联合概率密度函数为：$$p(\\mathbf{y} \\mid \\mathbf{x}, \\theta)=\\prod_{i=1}^{N} p\\left(y_{i} \\mid x_{i}, \\theta\\right)=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{N / 2}} e^{-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}$$\n极大似然估计的目的是找到使得观测数据出现的概率最大的参数值 $\\theta$ ，即调整 $\\theta$ 的值使得上述联合概率密度函数的值最大，即使得 $\\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小。\n\n样本的均方误差可以表示为：$$MSELoss=\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$$最小化均方误差即使得 $\\sum_{i=1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小，这与极大似然估计的目的等价。\n\n\n#### • A2.4 (5 points) 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\n[极大似然估计与最小化交叉熵损失或者KL散度为什么等价？ - 知乎](https://zhuanlan.zhihu.com/p/84764177)\n分类问题的联合概率分布为：$$p(X \\mid \\theta) = \\prod_{i=1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)$$\n对参数 $\\theta$ 的极大似然估计为：$$\\begin{array}{l}\n\\theta=\\underset{\\theta}{\\operatorname{argmax}} p(X \\mid \\theta) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\prod_{i=1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\sum_{i=1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)\n\\end{array}$$抽样服从真实样本分布，可得：$$\\begin{array}{l}\n=\\underset{\\theta}{\\operatorname{argmax}} E_{x \\sim p_{\\text {data }}(x)} \\log p(x \\mid \\theta) \\\\\n=\\underset{\\theta}{\\operatorname{argmax}} \\int_{x} p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x \\\\\n=\\underset{\\theta}{\\operatorname{argmin}} \\int_{x}-p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x\n\\end{array}$$即为交叉熵损失。\n\n\n\n#### • A2.5 (5 points) 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\n[l1 相比于 l2 为什么容易获得稀疏解？ - 知乎](https://www.zhihu.com/question/37096933/answer/70426653)\nL1正则化项是参数的绝对值之和，它在零附近不可导，这使得在优化过程中，某些参数可能会被推到零，从而使得模型变得稀疏；\nL2正则化项是参数的平方和，它是光滑可导的，在优化过程中会使得所有参数都变得较小，但通常不会等于零，从而得到一个相对平滑的解。\n\n#### • A2.6 (5 points) 分析Batch normalization对参数优化起到什么作用、如何起到这种作用。\n允许使用更大的学习率，加快模型的学习速度。减轻了对参数初始化的依赖，一定程度上增加了泛化能力。\nBatch normalization对每个batch中的数据进行归一化，使得其均值为0，方差为1，然后进行缩放和偏移操作（scale and shift），维持数据的原始分布，其均值和方差分别为可学习的参数β和γ，通过反向传播更新。","slug":"Notes/课程/大三（上）/神经网络与深度学习/A2分析与简答","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12b001pvw8cel9u1pa7","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h4 id=\"•-A2-1-5-points-尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch-size的选择依据和影响。\"><a href=\"#•-A2-1-5-points-尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch-size的选择依据和影响。\" class=\"headerlink\" title=\"• A2.1 (5 points) 尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch_size的选择依据和影响。\"></a>• A2.1 (5 points) 尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch_size的选择依据和影响。</h4><p>Epoch代表全部的训练数据在模型中训练的次数<br>Iteration表示在一个Epoch中参数更新的次数<br>Batch表示一次正向和反向传播中的一组数据样本<br>batch_size表示Batch中的数据量。小的batch_size可能会导致训练过程不稳定，但梯度更新更快，更容易得到更好的模型效果。大的batch_size可以提高内存利用率，训练效果比较稳定，但模型收敛速度较慢，过大的batch_size得到的模型效果普遍较差。</p>\n<h4 id=\"•-A2-2-5-points-以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。\"><a href=\"#•-A2-2-5-points-以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。\" class=\"headerlink\" title=\"• A2.2 (5 points) 以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。\"></a>• A2.2 (5 points) 以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100205.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100233.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031101845.png\" alt=\"image.png\"></p>\n<p>该网络表达式为：$$a_{1} &#x3D; w_{1}\\cdot x + b_{1}$$$$a_{2} &#x3D; w_{1}\\cdot a_{1} + b_{2}$$<br>均方误差损失函数表达式为：$$L\\left(y, a_{2}\\right)&#x3D;\\left(y-a_{2}\\right)^{2}$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203612.png\" alt=\"image.png\"></p>\n<p>交叉熵损失函数表达式为：$$L\\left(y, a_{2}\\right)&#x3D;-\\left[y \\log \\left(a_{2}\\right)+(1-y) \\log \\left(1-a_{2}\\right)\\right]$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203715.png\" alt=\"image.png\"><br>由图像可以显然看出其非凸性</p>\n<h4 id=\"•-A2-3-5-points-尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\"><a href=\"#•-A2-3-5-points-尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\" class=\"headerlink\" title=\"• A2.3 (5 points) 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\"></a>• A2.3 (5 points) 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。</h4><p>回归模型中输出加入高斯噪音，用函数表示如下：$$y&#x3D;f(x, \\theta)+\\epsilon$$其中 $x$ 为输入数据，$y$ 为输出数据，$\\epsilon$ 为高斯噪音，在此假设其概率密度函数为 $\\mathcal{N}\\left(0, \\sigma^{2}\\right)$.<br>对于一个期望为 $\\mu$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$f(x)&#x3D;\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)$$<br>样本 $y_i$ 服从期望为 $f(x_i, \\theta)$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$p\\left(y_{i} \\mid x_{i}, \\theta\\right)&#x3D;\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}{2 \\sigma^{2}}}$$<br>所有样本的联合概率密度函数为：$$p(\\mathbf{y} \\mid \\mathbf{x}, \\theta)&#x3D;\\prod_{i&#x3D;1}^{N} p\\left(y_{i} \\mid x_{i}, \\theta\\right)&#x3D;\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{N &#x2F; 2}} e^{-\\frac{1}{2 \\sigma^{2}} \\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}$$<br>极大似然估计的目的是找到使得观测数据出现的概率最大的参数值 $\\theta$ ，即调整 $\\theta$ 的值使得上述联合概率密度函数的值最大，即使得 $\\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小。</p>\n<p>样本的均方误差可以表示为：$$MSELoss&#x3D;\\frac{1}{N} \\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$$最小化均方误差即使得 $\\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小，这与极大似然估计的目的等价。</p>\n<h4 id=\"•-A2-4-5-points-尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\"><a href=\"#•-A2-4-5-points-尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\" class=\"headerlink\" title=\"• A2.4 (5 points) 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\"></a>• A2.4 (5 points) 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然</h4><p><a href=\"https://zhuanlan.zhihu.com/p/84764177\">极大似然估计与最小化交叉熵损失或者KL散度为什么等价？ - 知乎</a><br>分类问题的联合概率分布为：$$p(X \\mid \\theta) &#x3D; \\prod_{i&#x3D;1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) &#x3D; \\frac{1}{N} \\sum_{i&#x3D;1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)$$<br>对参数 $\\theta$ 的极大似然估计为：$$\\begin{array}{l}<br>\\theta&#x3D;\\underset{\\theta}{\\operatorname{argmax}} p(X \\mid \\theta) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\prod_{i&#x3D;1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\prod_{i&#x3D;1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\sum_{i&#x3D;1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)<br>\\end{array}$$抽样服从真实样本分布，可得：$$\\begin{array}{l}<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} E_{x \\sim p_{\\text {data }}(x)} \\log p(x \\mid \\theta) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\int_{x} p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmin}} \\int_{x}-p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x<br>\\end{array}$$即为交叉熵损失。</p>\n<h4 id=\"•-A2-5-5-points-分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\"><a href=\"#•-A2-5-5-points-分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\" class=\"headerlink\" title=\"• A2.5 (5 points) 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\"></a>• A2.5 (5 points) 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。</h4><p><a href=\"https://www.zhihu.com/question/37096933/answer/70426653\">l1 相比于 l2 为什么容易获得稀疏解？ - 知乎</a><br>L1正则化项是参数的绝对值之和，它在零附近不可导，这使得在优化过程中，某些参数可能会被推到零，从而使得模型变得稀疏；<br>L2正则化项是参数的平方和，它是光滑可导的，在优化过程中会使得所有参数都变得较小，但通常不会等于零，从而得到一个相对平滑的解。</p>\n<h4 id=\"•-A2-6-5-points-分析Batch-normalization对参数优化起到什么作用、如何起到这种作用。\"><a href=\"#•-A2-6-5-points-分析Batch-normalization对参数优化起到什么作用、如何起到这种作用。\" class=\"headerlink\" title=\"• A2.6 (5 points) 分析Batch normalization对参数优化起到什么作用、如何起到这种作用。\"></a>• A2.6 (5 points) 分析Batch normalization对参数优化起到什么作用、如何起到这种作用。</h4><p>允许使用更大的学习率，加快模型的学习速度。减轻了对参数初始化的依赖，一定程度上增加了泛化能力。<br>Batch normalization对每个batch中的数据进行归一化，使得其均值为0，方差为1，然后进行缩放和偏移操作（scale and shift），维持数据的原始分布，其均值和方差分别为可学习的参数β和γ，通过反向传播更新。</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"•-A2-1-5-points-尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch-size的选择依据和影响。\"><a href=\"#•-A2-1-5-points-尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch-size的选择依据和影响。\" class=\"headerlink\" title=\"• A2.1 (5 points) 尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch_size的选择依据和影响。\"></a>• A2.1 (5 points) 尝试解释Epoch、Iteration、Batch几个概念及其不同，尝试说明batch_size的选择依据和影响。</h4><p>Epoch代表全部的训练数据在模型中训练的次数<br>Iteration表示在一个Epoch中参数更新的次数<br>Batch表示一次正向和反向传播中的一组数据样本<br>batch_size表示Batch中的数据量。小的batch_size可能会导致训练过程不稳定，但梯度更新更快，更容易得到更好的模型效果。大的batch_size可以提高内存利用率，训练效果比较稳定，但模型收敛速度较慢，过大的batch_size得到的模型效果普遍较差。</p>\n<h4 id=\"•-A2-2-5-points-以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。\"><a href=\"#•-A2-2-5-points-以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。\" class=\"headerlink\" title=\"• A2.2 (5 points) 以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。\"></a>• A2.2 (5 points) 以一个简单的1-1-1结构的两层神经网络为例，分别采用均方误差损失函数和交叉熵损失函数，说明这两种函数关于参数的非凸性（可作图示意和说明）。</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100205.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031100233.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231031101845.png\" alt=\"image.png\"></p>\n<p>该网络表达式为：$$a_{1} &#x3D; w_{1}\\cdot x + b_{1}$$$$a_{2} &#x3D; w_{1}\\cdot a_{1} + b_{2}$$<br>均方误差损失函数表达式为：$$L\\left(y, a_{2}\\right)&#x3D;\\left(y-a_{2}\\right)^{2}$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203612.png\" alt=\"image.png\"></p>\n<p>交叉熵损失函数表达式为：$$L\\left(y, a_{2}\\right)&#x3D;-\\left[y \\log \\left(a_{2}\\right)+(1-y) \\log \\left(1-a_{2}\\right)\\right]$$画出函数关于 $w_{1}$ 和 $b_{1}$ 的曲面图如下：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231101203715.png\" alt=\"image.png\"><br>由图像可以显然看出其非凸性</p>\n<h4 id=\"•-A2-3-5-points-尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\"><a href=\"#•-A2-3-5-points-尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\" class=\"headerlink\" title=\"• A2.3 (5 points) 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。\"></a>• A2.3 (5 points) 尝试推导：在回归问题中，假设输出中包含高斯噪音，则最小化均方误差等价于极大似然。</h4><p>回归模型中输出加入高斯噪音，用函数表示如下：$$y&#x3D;f(x, \\theta)+\\epsilon$$其中 $x$ 为输入数据，$y$ 为输出数据，$\\epsilon$ 为高斯噪音，在此假设其概率密度函数为 $\\mathcal{N}\\left(0, \\sigma^{2}\\right)$.<br>对于一个期望为 $\\mu$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$f(x)&#x3D;\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)$$<br>样本 $y_i$ 服从期望为 $f(x_i, \\theta)$ ，方差为 $\\sigma^{2}$ 的正态分布，其概率密度函数为：$$p\\left(y_{i} \\mid x_{i}, \\theta\\right)&#x3D;\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}{2 \\sigma^{2}}}$$<br>所有样本的联合概率密度函数为：$$p(\\mathbf{y} \\mid \\mathbf{x}, \\theta)&#x3D;\\prod_{i&#x3D;1}^{N} p\\left(y_{i} \\mid x_{i}, \\theta\\right)&#x3D;\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{N &#x2F; 2}} e^{-\\frac{1}{2 \\sigma^{2}} \\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}}$$<br>极大似然估计的目的是找到使得观测数据出现的概率最大的参数值 $\\theta$ ，即调整 $\\theta$ 的值使得上述联合概率密度函数的值最大，即使得 $\\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小。</p>\n<p>样本的均方误差可以表示为：$$MSELoss&#x3D;\\frac{1}{N} \\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$$最小化均方误差即使得 $\\sum_{i&#x3D;1}^{N}\\left(y_{i}-f\\left(x_{i}, \\theta\\right)\\right)^{2}$ 的值最小，这与极大似然估计的目的等价。</p>\n<h4 id=\"•-A2-4-5-points-尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\"><a href=\"#•-A2-4-5-points-尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\" class=\"headerlink\" title=\"• A2.4 (5 points) 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然\"></a>• A2.4 (5 points) 尝试推导：在分类问题中，最小化交叉熵损失等价于极大化似然</h4><p><a href=\"https://zhuanlan.zhihu.com/p/84764177\">极大似然估计与最小化交叉熵损失或者KL散度为什么等价？ - 知乎</a><br>分类问题的联合概率分布为：$$p(X \\mid \\theta) &#x3D; \\prod_{i&#x3D;1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) &#x3D; \\frac{1}{N} \\sum_{i&#x3D;1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)$$<br>对参数 $\\theta$ 的极大似然估计为：$$\\begin{array}{l}<br>\\theta&#x3D;\\underset{\\theta}{\\operatorname{argmax}} p(X \\mid \\theta) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\prod_{i&#x3D;1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\prod_{i&#x3D;1}^{N} p\\left(x^{(i)} \\mid \\theta\\right) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\frac{1}{N} \\sum_{i&#x3D;1}^{N} \\log p\\left(x^{(i)} \\mid \\theta\\right)<br>\\end{array}$$抽样服从真实样本分布，可得：$$\\begin{array}{l}<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} E_{x \\sim p_{\\text {data }}(x)} \\log p(x \\mid \\theta) \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmax}} \\int_{x} p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x \\<br>&#x3D;\\underset{\\theta}{\\operatorname{argmin}} \\int_{x}-p_{\\text {data }}(x) \\log p(x \\mid \\theta) d x<br>\\end{array}$$即为交叉熵损失。</p>\n<h4 id=\"•-A2-5-5-points-分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\"><a href=\"#•-A2-5-5-points-分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\" class=\"headerlink\" title=\"• A2.5 (5 points) 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。\"></a>• A2.5 (5 points) 分析为什么L1正则化倾向于得到稀疏解、为什么L2正则化倾向于得到平滑的解。</h4><p><a href=\"https://www.zhihu.com/question/37096933/answer/70426653\">l1 相比于 l2 为什么容易获得稀疏解？ - 知乎</a><br>L1正则化项是参数的绝对值之和，它在零附近不可导，这使得在优化过程中，某些参数可能会被推到零，从而使得模型变得稀疏；<br>L2正则化项是参数的平方和，它是光滑可导的，在优化过程中会使得所有参数都变得较小，但通常不会等于零，从而得到一个相对平滑的解。</p>\n<h4 id=\"•-A2-6-5-points-分析Batch-normalization对参数优化起到什么作用、如何起到这种作用。\"><a href=\"#•-A2-6-5-points-分析Batch-normalization对参数优化起到什么作用、如何起到这种作用。\" class=\"headerlink\" title=\"• A2.6 (5 points) 分析Batch normalization对参数优化起到什么作用、如何起到这种作用。\"></a>• A2.6 (5 points) 分析Batch normalization对参数优化起到什么作用、如何起到这种作用。</h4><p>允许使用更大的学习率，加快模型的学习速度。减轻了对参数初始化的依赖，一定程度上增加了泛化能力。<br>Batch normalization对每个batch中的数据进行归一化，使得其均值为0，方差为1，然后进行缩放和偏移操作（scale and shift），维持数据的原始分布，其均值和方差分别为可学习的参数β和γ，通过反向传播更新。</p>\n"},{"title":"Memory内存","date":"2023-11-01T01:52:38.626Z","_content":"\n\n\n程序需要从硬盘读取到内存才能执行\nCPU可以直接访问的只有内存和寄存器\n内存单元只看到地址流和读写请求\n内存访问需要很多周期，导致停顿（stall）\nCache（高速缓存）设置在内存和CPU寄存器之间\n\n指令和数据到内存地址的地址绑定可以发生在三个不同的阶段\n编译时：如果内存位置先验已知，则可以生成绝对代码；如果起始位置发生变化，则必须重新编译代码 \n加载时间：如果编译时内存位置未知，则必须生成可重定位代码 \n执行时间：如果进程可以在执行期间从一个内存段移动到另一个内存段，则绑定会延迟到运行时 需要硬件支持地址映射（例如基址和限制寄存器）\n\n调用函数会分配一个调用框架来存储所有局部变量以及返回给被调用者所需的上下文","source":"_posts/Notes/课程/大三（上）/操作系统/Memory内存.md","raw":"---\ntitle: Memory内存\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 操作系统\ntags:\n  - 操作系统\ndate:\n---\n\n\n\n程序需要从硬盘读取到内存才能执行\nCPU可以直接访问的只有内存和寄存器\n内存单元只看到地址流和读写请求\n内存访问需要很多周期，导致停顿（stall）\nCache（高速缓存）设置在内存和CPU寄存器之间\n\n指令和数据到内存地址的地址绑定可以发生在三个不同的阶段\n编译时：如果内存位置先验已知，则可以生成绝对代码；如果起始位置发生变化，则必须重新编译代码 \n加载时间：如果编译时内存位置未知，则必须生成可重定位代码 \n执行时间：如果进程可以在执行期间从一个内存段移动到另一个内存段，则绑定会延迟到运行时 需要硬件支持地址映射（例如基址和限制寄存器）\n\n调用函数会分配一个调用框架来存储所有局部变量以及返回给被调用者所需的上下文","slug":"Notes/课程/大三（上）/操作系统/Memory内存","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12b001svw8c8l96hhyh","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>程序需要从硬盘读取到内存才能执行<br>CPU可以直接访问的只有内存和寄存器<br>内存单元只看到地址流和读写请求<br>内存访问需要很多周期，导致停顿（stall）<br>Cache（高速缓存）设置在内存和CPU寄存器之间</p>\n<p>指令和数据到内存地址的地址绑定可以发生在三个不同的阶段<br>编译时：如果内存位置先验已知，则可以生成绝对代码；如果起始位置发生变化，则必须重新编译代码<br>加载时间：如果编译时内存位置未知，则必须生成可重定位代码<br>执行时间：如果进程可以在执行期间从一个内存段移动到另一个内存段，则绑定会延迟到运行时 需要硬件支持地址映射（例如基址和限制寄存器）</p>\n<p>调用函数会分配一个调用框架来存储所有局部变量以及返回给被调用者所需的上下文</p>\n","site":{"data":{}},"excerpt":"","more":"<p>程序需要从硬盘读取到内存才能执行<br>CPU可以直接访问的只有内存和寄存器<br>内存单元只看到地址流和读写请求<br>内存访问需要很多周期，导致停顿（stall）<br>Cache（高速缓存）设置在内存和CPU寄存器之间</p>\n<p>指令和数据到内存地址的地址绑定可以发生在三个不同的阶段<br>编译时：如果内存位置先验已知，则可以生成绝对代码；如果起始位置发生变化，则必须重新编译代码<br>加载时间：如果编译时内存位置未知，则必须生成可重定位代码<br>执行时间：如果进程可以在执行期间从一个内存段移动到另一个内存段，则绑定会延迟到运行时 需要硬件支持地址映射（例如基址和限制寄存器）</p>\n<p>调用函数会分配一个调用框架来存储所有局部变量以及返回给被调用者所需的上下文</p>\n"},{"title":"A6分析与简答","date":"2023-12-11T04:50:42.123Z","_content":"### A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用。\n[一文读懂卷积神经网络中的1x1卷积核 - 知乎](https://zhuanlan.zhihu.com/p/40050371)\n\n1. 改变通道数\n\t当卷积核的个数大于或小于输入通道数时，可以改变输出的通道数\n2. 增加非线性 \n\t1x1卷积核可以在不改变特征维度的情况下添加非线性激活\n### A6.2 (5分) 计算函数$𝑦 = max(𝑥_1,⋯,𝑥_𝐷)$和函数$𝑦 = argmax(𝑥_1,⋯,𝑥_𝐷)$的梯度。\n$𝑦 = max(𝑥_1,⋯,𝑥_𝐷)$：如果对于输入向量中的某个元素$𝑥_𝑖$，它是最大值，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为\n\n$$\n\\frac{\\partial y}{\\partial x_{i}}=\\left\\{\\begin{array}{ll}\n1, & \\text { if } x_{i}=max(𝑥_1,⋯,𝑥_𝐷) \\\\\n0, & \\text { otherwise }\n\\end{array}\\right.\n$$\n\nargmax(𝑥_1,⋯,𝑥_𝐷)输出的是使得函数取得最大值的元素的索引位置.如果对于输入向量中的某个元素𝑥𝑖，它的索引位置与最大值所在的位置相同，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为\n\n$$\n\\frac{\\partial y}{\\partial x_{i}}=\\left\\{\\begin{array}{ll}\n1, & \\text { if } i=argmax(𝑥_1,⋯,𝑥_𝐷) \\\\\n0, & \\text { otherwise }\n\\end{array}\\right.\n$$\n\n### A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\n[[循环神经网络#长短时记忆网络（LSTM）]]\n[LSTM - 长短期记忆递归神经网络 - 知乎](https://zhuanlan.zhihu.com/p/123857569)\n[Fetching Title#6dum](https://zhuanlan.zhihu.com/p/136223550)\n\n遗忘门\n\n$$f_{t}=\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)$$\n\n记忆门\n\n$$\\begin{array}{l}\ni_{t}=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\\n\\widetilde{C}_{t}=\\tanh \\left(W_{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right) \\\\\n\n\\end{array}$$\n\n输出门\n\n$$\\begin{array}{l}\no_{t}=\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right.) \\\\\nh_{t}=o_{t} * \\tanh \\left(C_{t}\\right)\n\\end{array}$$\n\n单元状态更新\n\n$$C_{t}=f_{t} * C_{t-1}+i_{t} * \\widetilde{C_{t}}$$\n\n对于参数$W_i$, $W_f$,$W_o$, 统一用$W$表示：\n\n$$\\frac{\\partial L_{t}}{\\partial W}=\\frac{\\partial L_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial C_{t}} * \\ldots *\\left(\\prod_{k=2}^{t} \\frac{\\partial C_{k}}{\\partial C_{k-1}}\\right) * \\frac{\\partial C_{1}}{\\partial W}$$\n\n\n\nLSTM 通过记忆单元 C 来缓解梯度消失问题。针对 $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$ 求得，\n\n$$\n\\begin{aligned}\n\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}= & \\frac{\\partial C^{(k)}}{\\partial f^{(k)}} \\frac{\\partial f^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}+\\frac{\\partial C^{(k)}}{\\partial i^{(k)}} \\frac{\\partial i^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}} \\\\\n& +\\frac{\\partial C^{(k)}}{\\partial a^{(k)}} \\frac{\\partial a^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}\n\\end{aligned}\n$$\n\n\n具体计算后得到，\n\n$$\n\\begin{array}{c}\n\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}=C^{(k-1)} \\sigma^{\\prime}(\\cdot) W_{f} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\\\\n+a^{(k)} \\sigma^{\\prime}(\\cdot) W_{i} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\\\\n+i^{(k)} \\tanh ^{\\prime}(\\cdot) W_{c} * o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\\\\n+f^{(t)} \\\\\n\\prod_{k=t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}=\\left(f^{(k)} f^{(k+1)} \\ldots f^{(T)}\\right)+\\text { other }\n\\end{array}\n$$\n\n\n在LSTM迭代过程中，针对  $\\prod_{k=t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  而言，每一步  $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  可以自主的选择在  $[0,1]$  之间，或者大于1，因为  $f^{(k)}$  是可训练学习的。那么整体  $\\prod_{k=t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  也就不会一直减小，远距离梯度不至于完全消失，也就能够解决RNN中存在的梯度消失问题。\n### A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\n[LSTM如何解决RNN带来的梯度消失问题 - 知乎](https://zhuanlan.zhihu.com/p/136223550)\n[Transformer/CNN/RNN的对比（时间复杂度，序列操作数，最大路径长度） - 知乎](https://zhuanlan.zhihu.com/p/264749298)\n基于卷积或循环网络的序列编码都是一种局部的编码方式，只建模了输入信息的局部依赖关系．虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。\nRNN梯度消失的原因是，随着梯度的传导，梯度被近距离梯度主导，模型难以学习到远距离的信息。具体原因也就是  $\\prod_{k=t+1}^{T} \\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  部分，在迭代过程中，每一步  $\\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  始终在  $[0,1]$  之间或者始终大于1。\n因此，卷积层和循环层不适合用于建模长距离依赖关系。\n如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：\n1. 增加网络的层数，通过一个深层网络来获取远距离的信息交互\n2. 使用全连接网络。\n而全连接网络无法处理变长序列，自注意力模型可以解决这个问题，因为其连接权重是动态学习的。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231211190703.png)\n\n#### 复杂度：\n循环层：\n$h_{t}=f\\left(U x_{t}+W h_{t-1}\\right)$\n\n-  $U x_{t}: d \\times m  与  m \\times 1$  运算，复杂度为  $\\mathcal{O}(m d)$ ， m为input size\n-  $W h_{t-1}: d \\times d  与  d \\times 1$  运算，复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$\n\n故一次操作的时间复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$ ， n次序列操作后的总时间复杂度为  $\\mathcal{O}\\left(n d^{2}\\right)$ \n\n卷积层：\n注: 这里保证输入输出都是一样的，即均是  $n \\times d$ \n- 为了保证输入和输出在第一个维度都相同，故需要对输入进行padding操作，因为这里kernel size为k， (实际kernel的形状为  $k \\times d$  ) 如果不 padding的话，那么输出的第一个维度为 $n-k+1$ ，因为这里stride是为1的。为了保证输入输出相同，则需要对序列的前后分别padding长度为 $(k-1) / 2$  。\n- 大小为 $k \\times d$ 的卷积核一次运算的复杂度为: $\\mathcal{O}(k d)$  ，一共做了 n 次，故复杂度为  $\\mathcal{O}(n k d)$ \n- 为了保证第二个维度在第二个维度都相同，故需要 d 个卷积核，所以卷积操作总的时间复杂度为  $\\mathcal{O}\\left(n k d^{2}\\right)$ \n\n自注意力层：\n$A(Q, K, V)={Softmax}\\left(Q K^{T}\\right)$\n-  $Q, K, V: n \\times d$ \n- 相似度计算  $Q K^{T}: n \\times d  与  d \\times n$ 运算，得到 $n \\times n$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ \n- softmax计算: 对每行做softmax，复杂度为 $\\mathcal{O}(n)$ ,则 $\\mathrm{n}$ 行的复杂度为  $\\mathcal{O}\\left(n^{2}\\right)$ \n- 加权和: $n \\times n$ 与 $n \\times d$ 运算，得到 $n \\times d$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ .故最后self-attention的时间复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ \n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/A6分析与简答.md","raw":"---\ntitle: A6分析与简答\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 深度学习\n  - 神经网络\ndate:\n---\n### A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用。\n[一文读懂卷积神经网络中的1x1卷积核 - 知乎](https://zhuanlan.zhihu.com/p/40050371)\n\n1. 改变通道数\n\t当卷积核的个数大于或小于输入通道数时，可以改变输出的通道数\n2. 增加非线性 \n\t1x1卷积核可以在不改变特征维度的情况下添加非线性激活\n### A6.2 (5分) 计算函数$𝑦 = max(𝑥_1,⋯,𝑥_𝐷)$和函数$𝑦 = argmax(𝑥_1,⋯,𝑥_𝐷)$的梯度。\n$𝑦 = max(𝑥_1,⋯,𝑥_𝐷)$：如果对于输入向量中的某个元素$𝑥_𝑖$，它是最大值，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为\n\n$$\n\\frac{\\partial y}{\\partial x_{i}}=\\left\\{\\begin{array}{ll}\n1, & \\text { if } x_{i}=max(𝑥_1,⋯,𝑥_𝐷) \\\\\n0, & \\text { otherwise }\n\\end{array}\\right.\n$$\n\nargmax(𝑥_1,⋯,𝑥_𝐷)输出的是使得函数取得最大值的元素的索引位置.如果对于输入向量中的某个元素𝑥𝑖，它的索引位置与最大值所在的位置相同，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为\n\n$$\n\\frac{\\partial y}{\\partial x_{i}}=\\left\\{\\begin{array}{ll}\n1, & \\text { if } i=argmax(𝑥_1,⋯,𝑥_𝐷) \\\\\n0, & \\text { otherwise }\n\\end{array}\\right.\n$$\n\n### A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\n[[循环神经网络#长短时记忆网络（LSTM）]]\n[LSTM - 长短期记忆递归神经网络 - 知乎](https://zhuanlan.zhihu.com/p/123857569)\n[Fetching Title#6dum](https://zhuanlan.zhihu.com/p/136223550)\n\n遗忘门\n\n$$f_{t}=\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)$$\n\n记忆门\n\n$$\\begin{array}{l}\ni_{t}=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\\n\\widetilde{C}_{t}=\\tanh \\left(W_{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right) \\\\\n\n\\end{array}$$\n\n输出门\n\n$$\\begin{array}{l}\no_{t}=\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right.) \\\\\nh_{t}=o_{t} * \\tanh \\left(C_{t}\\right)\n\\end{array}$$\n\n单元状态更新\n\n$$C_{t}=f_{t} * C_{t-1}+i_{t} * \\widetilde{C_{t}}$$\n\n对于参数$W_i$, $W_f$,$W_o$, 统一用$W$表示：\n\n$$\\frac{\\partial L_{t}}{\\partial W}=\\frac{\\partial L_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial C_{t}} * \\ldots *\\left(\\prod_{k=2}^{t} \\frac{\\partial C_{k}}{\\partial C_{k-1}}\\right) * \\frac{\\partial C_{1}}{\\partial W}$$\n\n\n\nLSTM 通过记忆单元 C 来缓解梯度消失问题。针对 $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$ 求得，\n\n$$\n\\begin{aligned}\n\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}= & \\frac{\\partial C^{(k)}}{\\partial f^{(k)}} \\frac{\\partial f^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}+\\frac{\\partial C^{(k)}}{\\partial i^{(k)}} \\frac{\\partial i^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}} \\\\\n& +\\frac{\\partial C^{(k)}}{\\partial a^{(k)}} \\frac{\\partial a^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}\n\\end{aligned}\n$$\n\n\n具体计算后得到，\n\n$$\n\\begin{array}{c}\n\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}=C^{(k-1)} \\sigma^{\\prime}(\\cdot) W_{f} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\\\\n+a^{(k)} \\sigma^{\\prime}(\\cdot) W_{i} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\\\\n+i^{(k)} \\tanh ^{\\prime}(\\cdot) W_{c} * o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\\\\n+f^{(t)} \\\\\n\\prod_{k=t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}=\\left(f^{(k)} f^{(k+1)} \\ldots f^{(T)}\\right)+\\text { other }\n\\end{array}\n$$\n\n\n在LSTM迭代过程中，针对  $\\prod_{k=t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  而言，每一步  $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  可以自主的选择在  $[0,1]$  之间，或者大于1，因为  $f^{(k)}$  是可训练学习的。那么整体  $\\prod_{k=t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  也就不会一直减小，远距离梯度不至于完全消失，也就能够解决RNN中存在的梯度消失问题。\n### A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\n[LSTM如何解决RNN带来的梯度消失问题 - 知乎](https://zhuanlan.zhihu.com/p/136223550)\n[Transformer/CNN/RNN的对比（时间复杂度，序列操作数，最大路径长度） - 知乎](https://zhuanlan.zhihu.com/p/264749298)\n基于卷积或循环网络的序列编码都是一种局部的编码方式，只建模了输入信息的局部依赖关系．虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。\nRNN梯度消失的原因是，随着梯度的传导，梯度被近距离梯度主导，模型难以学习到远距离的信息。具体原因也就是  $\\prod_{k=t+1}^{T} \\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  部分，在迭代过程中，每一步  $\\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  始终在  $[0,1]$  之间或者始终大于1。\n因此，卷积层和循环层不适合用于建模长距离依赖关系。\n如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：\n1. 增加网络的层数，通过一个深层网络来获取远距离的信息交互\n2. 使用全连接网络。\n而全连接网络无法处理变长序列，自注意力模型可以解决这个问题，因为其连接权重是动态学习的。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231211190703.png)\n\n#### 复杂度：\n循环层：\n$h_{t}=f\\left(U x_{t}+W h_{t-1}\\right)$\n\n-  $U x_{t}: d \\times m  与  m \\times 1$  运算，复杂度为  $\\mathcal{O}(m d)$ ， m为input size\n-  $W h_{t-1}: d \\times d  与  d \\times 1$  运算，复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$\n\n故一次操作的时间复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$ ， n次序列操作后的总时间复杂度为  $\\mathcal{O}\\left(n d^{2}\\right)$ \n\n卷积层：\n注: 这里保证输入输出都是一样的，即均是  $n \\times d$ \n- 为了保证输入和输出在第一个维度都相同，故需要对输入进行padding操作，因为这里kernel size为k， (实际kernel的形状为  $k \\times d$  ) 如果不 padding的话，那么输出的第一个维度为 $n-k+1$ ，因为这里stride是为1的。为了保证输入输出相同，则需要对序列的前后分别padding长度为 $(k-1) / 2$  。\n- 大小为 $k \\times d$ 的卷积核一次运算的复杂度为: $\\mathcal{O}(k d)$  ，一共做了 n 次，故复杂度为  $\\mathcal{O}(n k d)$ \n- 为了保证第二个维度在第二个维度都相同，故需要 d 个卷积核，所以卷积操作总的时间复杂度为  $\\mathcal{O}\\left(n k d^{2}\\right)$ \n\n自注意力层：\n$A(Q, K, V)={Softmax}\\left(Q K^{T}\\right)$\n-  $Q, K, V: n \\times d$ \n- 相似度计算  $Q K^{T}: n \\times d  与  d \\times n$ 运算，得到 $n \\times n$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ \n- softmax计算: 对每行做softmax，复杂度为 $\\mathcal{O}(n)$ ,则 $\\mathrm{n}$ 行的复杂度为  $\\mathcal{O}\\left(n^{2}\\right)$ \n- 加权和: $n \\times n$ 与 $n \\times d$ 运算，得到 $n \\times d$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ .故最后self-attention的时间复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ \n","slug":"Notes/课程/大三（上）/神经网络与深度学习/A6分析与简答","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12c001vvw8c68z32q8p","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"A6-1-5分-分析卷积神经网络中用1×1的卷积核的作用。\"><a href=\"#A6-1-5分-分析卷积神经网络中用1×1的卷积核的作用。\" class=\"headerlink\" title=\"A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用。\"></a>A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用。</h3><p><a href=\"https://zhuanlan.zhihu.com/p/40050371\">一文读懂卷积神经网络中的1x1卷积核 - 知乎</a></p>\n<ol>\n<li>改变通道数<br> 当卷积核的个数大于或小于输入通道数时，可以改变输出的通道数</li>\n<li>增加非线性<br> 1x1卷积核可以在不改变特征维度的情况下添加非线性激活</li>\n</ol>\n<h3 id=\"A6-2-5分-计算函数-𝑦-max-𝑥-1-⋯-𝑥-𝐷-和函数-𝑦-argmax-𝑥-1-⋯-𝑥-𝐷-的梯度。\"><a href=\"#A6-2-5分-计算函数-𝑦-max-𝑥-1-⋯-𝑥-𝐷-和函数-𝑦-argmax-𝑥-1-⋯-𝑥-𝐷-的梯度。\" class=\"headerlink\" title=\"A6.2 (5分) 计算函数$𝑦 &#x3D; max(𝑥_1,⋯,𝑥_𝐷)$和函数$𝑦 &#x3D; argmax(𝑥_1,⋯,𝑥_𝐷)$的梯度。\"></a>A6.2 (5分) 计算函数$𝑦 &#x3D; max(𝑥_1,⋯,𝑥_𝐷)$和函数$𝑦 &#x3D; argmax(𝑥_1,⋯,𝑥_𝐷)$的梯度。</h3><p>$𝑦 &#x3D; max(𝑥_1,⋯,𝑥_𝐷)$：如果对于输入向量中的某个元素$𝑥_𝑖$，它是最大值，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为</p>\n<p>$$<br>\\frac{\\partial y}{\\partial x_{i}}&#x3D;\\left{\\begin{array}{ll}<br>1, &amp; \\text { if } x_{i}&#x3D;max(𝑥_1,⋯,𝑥_𝐷) \\<br>0, &amp; \\text { otherwise }<br>\\end{array}\\right.<br>$$</p>\n<p>argmax(𝑥_1,⋯,𝑥_𝐷)输出的是使得函数取得最大值的元素的索引位置.如果对于输入向量中的某个元素𝑥𝑖，它的索引位置与最大值所在的位置相同，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为</p>\n<p>$$<br>\\frac{\\partial y}{\\partial x_{i}}&#x3D;\\left{\\begin{array}{ll}<br>1, &amp; \\text { if } i&#x3D;argmax(𝑥_1,⋯,𝑥_𝐷) \\<br>0, &amp; \\text { otherwise }<br>\\end{array}\\right.<br>$$</p>\n<h3 id=\"A6-3-5分-推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\"><a href=\"#A6-3-5分-推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\" class=\"headerlink\" title=\"A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\"></a>A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。</h3><p>[[循环神经网络#长短时记忆网络（LSTM）]]<br><a href=\"https://zhuanlan.zhihu.com/p/123857569\">LSTM - 长短期记忆递归神经网络 - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/136223550\">Fetching Title#6dum</a></p>\n<p>遗忘门</p>\n<p>$$f_{t}&#x3D;\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)$$</p>\n<p>记忆门</p>\n<p>$$\\begin{array}{l}<br>i_{t}&#x3D;\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\<br>\\widetilde{C}<em>{t}&#x3D;\\tanh \\left(W</em>{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right) \\</p>\n<p>\\end{array}$$</p>\n<p>输出门</p>\n<p>$$\\begin{array}{l}<br>o_{t}&#x3D;\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right.) \\<br>h_{t}&#x3D;o_{t} * \\tanh \\left(C_{t}\\right)<br>\\end{array}$$</p>\n<p>单元状态更新</p>\n<p>$$C_{t}&#x3D;f_{t} * C_{t-1}+i_{t} * \\widetilde{C_{t}}$$</p>\n<p>对于参数$W_i$, $W_f$,$W_o$, 统一用$W$表示：</p>\n<p>$$\\frac{\\partial L_{t}}{\\partial W}&#x3D;\\frac{\\partial L_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial C_{t}} * \\ldots *\\left(\\prod_{k&#x3D;2}^{t} \\frac{\\partial C_{k}}{\\partial C_{k-1}}\\right) * \\frac{\\partial C_{1}}{\\partial W}$$</p>\n<p>LSTM 通过记忆单元 C 来缓解梯度消失问题。针对 $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$ 求得，</p>\n<p>$$<br>\\begin{aligned}<br>\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}&#x3D; &amp; \\frac{\\partial C^{(k)}}{\\partial f^{(k)}} \\frac{\\partial f^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}+\\frac{\\partial C^{(k)}}{\\partial i^{(k)}} \\frac{\\partial i^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}} \\<br>&amp; +\\frac{\\partial C^{(k)}}{\\partial a^{(k)}} \\frac{\\partial a^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}<br>\\end{aligned}<br>$$</p>\n<p>具体计算后得到，</p>\n<p>$$<br>\\begin{array}{c}<br>\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}&#x3D;C^{(k-1)} \\sigma^{\\prime}(\\cdot) W_{f} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\<br>+a^{(k)} \\sigma^{\\prime}(\\cdot) W_{i} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\<br>+i^{(k)} \\tanh ^{\\prime}(\\cdot) W_{c} * o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\<br>+f^{(t)} \\<br>\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}&#x3D;\\left(f^{(k)} f^{(k+1)} \\ldots f^{(T)}\\right)+\\text { other }<br>\\end{array}<br>$$</p>\n<p>在LSTM迭代过程中，针对  $\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  而言，每一步  $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  可以自主的选择在  $[0,1]$  之间，或者大于1，因为  $f^{(k)}$  是可训练学习的。那么整体  $\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  也就不会一直减小，远距离梯度不至于完全消失，也就能够解决RNN中存在的梯度消失问题。</p>\n<h3 id=\"A6-4-5分-当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\"><a href=\"#A6-4-5分-当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\" class=\"headerlink\" title=\"A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\"></a>A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。</h3><p><a href=\"https://zhuanlan.zhihu.com/p/136223550\">LSTM如何解决RNN带来的梯度消失问题 - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/264749298\">Transformer&#x2F;CNN&#x2F;RNN的对比（时间复杂度，序列操作数，最大路径长度） - 知乎</a><br>基于卷积或循环网络的序列编码都是一种局部的编码方式，只建模了输入信息的局部依赖关系．虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。<br>RNN梯度消失的原因是，随着梯度的传导，梯度被近距离梯度主导，模型难以学习到远距离的信息。具体原因也就是  $\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  部分，在迭代过程中，每一步  $\\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  始终在  $[0,1]$  之间或者始终大于1。<br>因此，卷积层和循环层不适合用于建模长距离依赖关系。<br>如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：</p>\n<ol>\n<li>增加网络的层数，通过一个深层网络来获取远距离的信息交互</li>\n<li>使用全连接网络。<br>而全连接网络无法处理变长序列，自注意力模型可以解决这个问题，因为其连接权重是动态学习的。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231211190703.png\" alt=\"image.png\"></li>\n</ol>\n<h4 id=\"复杂度：\"><a href=\"#复杂度：\" class=\"headerlink\" title=\"复杂度：\"></a>复杂度：</h4><p>循环层：<br>$h_{t}&#x3D;f\\left(U x_{t}+W h_{t-1}\\right)$</p>\n<ul>\n<li>$U x_{t}: d \\times m  与  m \\times 1$  运算，复杂度为  $\\mathcal{O}(m d)$ ， m为input size</li>\n<li>$W h_{t-1}: d \\times d  与  d \\times 1$  运算，复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$</li>\n</ul>\n<p>故一次操作的时间复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$ ， n次序列操作后的总时间复杂度为  $\\mathcal{O}\\left(n d^{2}\\right)$ </p>\n<p>卷积层：<br>注: 这里保证输入输出都是一样的，即均是  $n \\times d$ </p>\n<ul>\n<li>为了保证输入和输出在第一个维度都相同，故需要对输入进行padding操作，因为这里kernel size为k， (实际kernel的形状为  $k \\times d$  ) 如果不 padding的话，那么输出的第一个维度为 $n-k+1$ ，因为这里stride是为1的。为了保证输入输出相同，则需要对序列的前后分别padding长度为 $(k-1) &#x2F; 2$  。</li>\n<li>大小为 $k \\times d$ 的卷积核一次运算的复杂度为: $\\mathcal{O}(k d)$  ，一共做了 n 次，故复杂度为  $\\mathcal{O}(n k d)$ </li>\n<li>为了保证第二个维度在第二个维度都相同，故需要 d 个卷积核，所以卷积操作总的时间复杂度为  $\\mathcal{O}\\left(n k d^{2}\\right)$</li>\n</ul>\n<p>自注意力层：<br>$A(Q, K, V)&#x3D;{Softmax}\\left(Q K^{T}\\right)$</p>\n<ul>\n<li>$Q, K, V: n \\times d$ </li>\n<li>相似度计算  $Q K^{T}: n \\times d  与  d \\times n$ 运算，得到 $n \\times n$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ </li>\n<li>softmax计算: 对每行做softmax，复杂度为 $\\mathcal{O}(n)$ ,则 $\\mathrm{n}$ 行的复杂度为  $\\mathcal{O}\\left(n^{2}\\right)$ </li>\n<li>加权和: $n \\times n$ 与 $n \\times d$ 运算，得到 $n \\times d$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ .故最后self-attention的时间复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"A6-1-5分-分析卷积神经网络中用1×1的卷积核的作用。\"><a href=\"#A6-1-5分-分析卷积神经网络中用1×1的卷积核的作用。\" class=\"headerlink\" title=\"A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用。\"></a>A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用。</h3><p><a href=\"https://zhuanlan.zhihu.com/p/40050371\">一文读懂卷积神经网络中的1x1卷积核 - 知乎</a></p>\n<ol>\n<li>改变通道数<br> 当卷积核的个数大于或小于输入通道数时，可以改变输出的通道数</li>\n<li>增加非线性<br> 1x1卷积核可以在不改变特征维度的情况下添加非线性激活</li>\n</ol>\n<h3 id=\"A6-2-5分-计算函数-𝑦-max-𝑥-1-⋯-𝑥-𝐷-和函数-𝑦-argmax-𝑥-1-⋯-𝑥-𝐷-的梯度。\"><a href=\"#A6-2-5分-计算函数-𝑦-max-𝑥-1-⋯-𝑥-𝐷-和函数-𝑦-argmax-𝑥-1-⋯-𝑥-𝐷-的梯度。\" class=\"headerlink\" title=\"A6.2 (5分) 计算函数$𝑦 &#x3D; max(𝑥_1,⋯,𝑥_𝐷)$和函数$𝑦 &#x3D; argmax(𝑥_1,⋯,𝑥_𝐷)$的梯度。\"></a>A6.2 (5分) 计算函数$𝑦 &#x3D; max(𝑥_1,⋯,𝑥_𝐷)$和函数$𝑦 &#x3D; argmax(𝑥_1,⋯,𝑥_𝐷)$的梯度。</h3><p>$𝑦 &#x3D; max(𝑥_1,⋯,𝑥_𝐷)$：如果对于输入向量中的某个元素$𝑥_𝑖$，它是最大值，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为</p>\n<p>$$<br>\\frac{\\partial y}{\\partial x_{i}}&#x3D;\\left{\\begin{array}{ll}<br>1, &amp; \\text { if } x_{i}&#x3D;max(𝑥_1,⋯,𝑥_𝐷) \\<br>0, &amp; \\text { otherwise }<br>\\end{array}\\right.<br>$$</p>\n<p>argmax(𝑥_1,⋯,𝑥_𝐷)输出的是使得函数取得最大值的元素的索引位置.如果对于输入向量中的某个元素𝑥𝑖，它的索引位置与最大值所在的位置相同，则其梯度为1，其他所有元素的梯度都为0。用数学符号表示为</p>\n<p>$$<br>\\frac{\\partial y}{\\partial x_{i}}&#x3D;\\left{\\begin{array}{ll}<br>1, &amp; \\text { if } i&#x3D;argmax(𝑥_1,⋯,𝑥_𝐷) \\<br>0, &amp; \\text { otherwise }<br>\\end{array}\\right.<br>$$</p>\n<h3 id=\"A6-3-5分-推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\"><a href=\"#A6-3-5分-推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\" class=\"headerlink\" title=\"A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。\"></a>A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。</h3><p>[[循环神经网络#长短时记忆网络（LSTM）]]<br><a href=\"https://zhuanlan.zhihu.com/p/123857569\">LSTM - 长短期记忆递归神经网络 - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/136223550\">Fetching Title#6dum</a></p>\n<p>遗忘门</p>\n<p>$$f_{t}&#x3D;\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)$$</p>\n<p>记忆门</p>\n<p>$$\\begin{array}{l}<br>i_{t}&#x3D;\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\<br>\\widetilde{C}<em>{t}&#x3D;\\tanh \\left(W</em>{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right) \\</p>\n<p>\\end{array}$$</p>\n<p>输出门</p>\n<p>$$\\begin{array}{l}<br>o_{t}&#x3D;\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right.) \\<br>h_{t}&#x3D;o_{t} * \\tanh \\left(C_{t}\\right)<br>\\end{array}$$</p>\n<p>单元状态更新</p>\n<p>$$C_{t}&#x3D;f_{t} * C_{t-1}+i_{t} * \\widetilde{C_{t}}$$</p>\n<p>对于参数$W_i$, $W_f$,$W_o$, 统一用$W$表示：</p>\n<p>$$\\frac{\\partial L_{t}}{\\partial W}&#x3D;\\frac{\\partial L_{t}}{\\partial h_{t}} * \\frac{\\partial h_{t}}{\\partial C_{t}} * \\ldots *\\left(\\prod_{k&#x3D;2}^{t} \\frac{\\partial C_{k}}{\\partial C_{k-1}}\\right) * \\frac{\\partial C_{1}}{\\partial W}$$</p>\n<p>LSTM 通过记忆单元 C 来缓解梯度消失问题。针对 $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$ 求得，</p>\n<p>$$<br>\\begin{aligned}<br>\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}&#x3D; &amp; \\frac{\\partial C^{(k)}}{\\partial f^{(k)}} \\frac{\\partial f^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}+\\frac{\\partial C^{(k)}}{\\partial i^{(k)}} \\frac{\\partial i^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}} \\<br>&amp; +\\frac{\\partial C^{(k)}}{\\partial a^{(k)}} \\frac{\\partial a^{(k)}}{\\partial h^{(k-1)}} \\frac{\\partial h^{(k-1)}}{\\partial C^{(k-1)}}<br>\\end{aligned}<br>$$</p>\n<p>具体计算后得到，</p>\n<p>$$<br>\\begin{array}{c}<br>\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}&#x3D;C^{(k-1)} \\sigma^{\\prime}(\\cdot) W_{f} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\<br>+a^{(k)} \\sigma^{\\prime}(\\cdot) W_{i} o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\<br>+i^{(k)} \\tanh ^{\\prime}(\\cdot) W_{c} * o^{(k-1)} \\tanh ^{\\prime}\\left(C^{(k-1)}\\right) \\<br>+f^{(t)} \\<br>\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}&#x3D;\\left(f^{(k)} f^{(k+1)} \\ldots f^{(T)}\\right)+\\text { other }<br>\\end{array}<br>$$</p>\n<p>在LSTM迭代过程中，针对  $\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  而言，每一步  $\\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  可以自主的选择在  $[0,1]$  之间，或者大于1，因为  $f^{(k)}$  是可训练学习的。那么整体  $\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial C^{(k)}}{\\partial C^{(k-1)}}$  也就不会一直减小，远距离梯度不至于完全消失，也就能够解决RNN中存在的梯度消失问题。</p>\n<h3 id=\"A6-4-5分-当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\"><a href=\"#A6-4-5分-当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\" class=\"headerlink\" title=\"A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。\"></a>A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。</h3><p><a href=\"https://zhuanlan.zhihu.com/p/136223550\">LSTM如何解决RNN带来的梯度消失问题 - 知乎</a><br><a href=\"https://zhuanlan.zhihu.com/p/264749298\">Transformer&#x2F;CNN&#x2F;RNN的对比（时间复杂度，序列操作数，最大路径长度） - 知乎</a><br>基于卷积或循环网络的序列编码都是一种局部的编码方式，只建模了输入信息的局部依赖关系．虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。<br>RNN梯度消失的原因是，随着梯度的传导，梯度被近距离梯度主导，模型难以学习到远距离的信息。具体原因也就是  $\\prod_{k&#x3D;t+1}^{T} \\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  部分，在迭代过程中，每一步  $\\frac{\\partial h^{(k)}}{\\partial h^{(k-1)}}$  始终在  $[0,1]$  之间或者始终大于1。<br>因此，卷积层和循环层不适合用于建模长距离依赖关系。<br>如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：</p>\n<ol>\n<li>增加网络的层数，通过一个深层网络来获取远距离的信息交互</li>\n<li>使用全连接网络。<br>而全连接网络无法处理变长序列，自注意力模型可以解决这个问题，因为其连接权重是动态学习的。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231211190703.png\" alt=\"image.png\"></li>\n</ol>\n<h4 id=\"复杂度：\"><a href=\"#复杂度：\" class=\"headerlink\" title=\"复杂度：\"></a>复杂度：</h4><p>循环层：<br>$h_{t}&#x3D;f\\left(U x_{t}+W h_{t-1}\\right)$</p>\n<ul>\n<li>$U x_{t}: d \\times m  与  m \\times 1$  运算，复杂度为  $\\mathcal{O}(m d)$ ， m为input size</li>\n<li>$W h_{t-1}: d \\times d  与  d \\times 1$  运算，复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$</li>\n</ul>\n<p>故一次操作的时间复杂度为  $\\mathcal{O}\\left(d^{2}\\right)$ ， n次序列操作后的总时间复杂度为  $\\mathcal{O}\\left(n d^{2}\\right)$ </p>\n<p>卷积层：<br>注: 这里保证输入输出都是一样的，即均是  $n \\times d$ </p>\n<ul>\n<li>为了保证输入和输出在第一个维度都相同，故需要对输入进行padding操作，因为这里kernel size为k， (实际kernel的形状为  $k \\times d$  ) 如果不 padding的话，那么输出的第一个维度为 $n-k+1$ ，因为这里stride是为1的。为了保证输入输出相同，则需要对序列的前后分别padding长度为 $(k-1) &#x2F; 2$  。</li>\n<li>大小为 $k \\times d$ 的卷积核一次运算的复杂度为: $\\mathcal{O}(k d)$  ，一共做了 n 次，故复杂度为  $\\mathcal{O}(n k d)$ </li>\n<li>为了保证第二个维度在第二个维度都相同，故需要 d 个卷积核，所以卷积操作总的时间复杂度为  $\\mathcal{O}\\left(n k d^{2}\\right)$</li>\n</ul>\n<p>自注意力层：<br>$A(Q, K, V)&#x3D;{Softmax}\\left(Q K^{T}\\right)$</p>\n<ul>\n<li>$Q, K, V: n \\times d$ </li>\n<li>相似度计算  $Q K^{T}: n \\times d  与  d \\times n$ 运算，得到 $n \\times n$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ </li>\n<li>softmax计算: 对每行做softmax，复杂度为 $\\mathcal{O}(n)$ ,则 $\\mathrm{n}$ 行的复杂度为  $\\mathcal{O}\\left(n^{2}\\right)$ </li>\n<li>加权和: $n \\times n$ 与 $n \\times d$ 运算，得到 $n \\times d$ 矩阵，复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$ .故最后self-attention的时间复杂度为 $\\mathcal{O}\\left(n^{2} d\\right)$</li>\n</ul>\n"},{"title":"编程框架机理","date":"2023-11-28T02:44:05.364Z","_content":"### Pytorch的设计原则\n1. 要性能更要易用\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153815.png)\n\n2. 简介抽象而非隐藏细节\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153840.png)\n\n3. 始于python，忠于python\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153906.png)\n\n\n### Pytorch的计算图机制\n常见求导方法：\n- 手动求解法\n- 数值求导法\n- 符号求导法\n- 自动求导法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154036.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154049.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154058.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154115.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154144.png)\n\n\n### 分布式计算机制\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155704.png)\n\n不同的分布式计算方法：\n数据并行：对数据进行分区\n模型并行：对程序进行分区\n混合并行：同时对数据和程序进行分区\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155828.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155942.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160028.png)\n\n### 编译机制\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160441.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160452.png)\n","source":"_posts/Notes/课程/大三（上）/智能计算系统/编程框架机理.md","raw":"---\ntitle: 编程框架机理\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 智能计算系统\ntags:\n  - 智能计算系统\ndate:\n---\n### Pytorch的设计原则\n1. 要性能更要易用\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153815.png)\n\n2. 简介抽象而非隐藏细节\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153840.png)\n\n3. 始于python，忠于python\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153906.png)\n\n\n### Pytorch的计算图机制\n常见求导方法：\n- 手动求解法\n- 数值求导法\n- 符号求导法\n- 自动求导法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154036.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154049.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154058.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154115.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154144.png)\n\n\n### 分布式计算机制\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155704.png)\n\n不同的分布式计算方法：\n数据并行：对数据进行分区\n模型并行：对程序进行分区\n混合并行：同时对数据和程序进行分区\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155828.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155942.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160028.png)\n\n### 编译机制\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160441.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160452.png)\n","slug":"Notes/课程/大三（上）/智能计算系统/编程框架机理","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12c001yvw8camv3dces","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"Pytorch的设计原则\"><a href=\"#Pytorch的设计原则\" class=\"headerlink\" title=\"Pytorch的设计原则\"></a>Pytorch的设计原则</h3><ol>\n<li><p>要性能更要易用<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153815.png\" alt=\"image.png\"></p>\n</li>\n<li><p>简介抽象而非隐藏细节<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153840.png\" alt=\"image.png\"></p>\n</li>\n<li><p>始于python，忠于python<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153906.png\" alt=\"image.png\"></p>\n</li>\n</ol>\n<h3 id=\"Pytorch的计算图机制\"><a href=\"#Pytorch的计算图机制\" class=\"headerlink\" title=\"Pytorch的计算图机制\"></a>Pytorch的计算图机制</h3><p>常见求导方法：</p>\n<ul>\n<li>手动求解法</li>\n<li>数值求导法</li>\n<li>符号求导法</li>\n<li>自动求导法<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154036.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154049.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154058.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154115.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154144.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"分布式计算机制\"><a href=\"#分布式计算机制\" class=\"headerlink\" title=\"分布式计算机制\"></a>分布式计算机制</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155704.png\" alt=\"image.png\"></p>\n<p>不同的分布式计算方法：<br>数据并行：对数据进行分区<br>模型并行：对程序进行分区<br>混合并行：同时对数据和程序进行分区</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155828.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155942.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160028.png\" alt=\"image.png\"></p>\n<h3 id=\"编译机制\"><a href=\"#编译机制\" class=\"headerlink\" title=\"编译机制\"></a>编译机制</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160441.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160452.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Pytorch的设计原则\"><a href=\"#Pytorch的设计原则\" class=\"headerlink\" title=\"Pytorch的设计原则\"></a>Pytorch的设计原则</h3><ol>\n<li><p>要性能更要易用<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153815.png\" alt=\"image.png\"></p>\n</li>\n<li><p>简介抽象而非隐藏细节<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153840.png\" alt=\"image.png\"></p>\n</li>\n<li><p>始于python，忠于python<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101153906.png\" alt=\"image.png\"></p>\n</li>\n</ol>\n<h3 id=\"Pytorch的计算图机制\"><a href=\"#Pytorch的计算图机制\" class=\"headerlink\" title=\"Pytorch的计算图机制\"></a>Pytorch的计算图机制</h3><p>常见求导方法：</p>\n<ul>\n<li>手动求解法</li>\n<li>数值求导法</li>\n<li>符号求导法</li>\n<li>自动求导法<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154036.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154049.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154058.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154115.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101154144.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"分布式计算机制\"><a href=\"#分布式计算机制\" class=\"headerlink\" title=\"分布式计算机制\"></a>分布式计算机制</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155704.png\" alt=\"image.png\"></p>\n<p>不同的分布式计算方法：<br>数据并行：对数据进行分区<br>模型并行：对程序进行分区<br>混合并行：同时对数据和程序进行分区</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155828.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101155942.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160028.png\" alt=\"image.png\"></p>\n<h3 id=\"编译机制\"><a href=\"#编译机制\" class=\"headerlink\" title=\"编译机制\"></a>编译机制</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160441.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240101160452.png\" alt=\"image.png\"></p>\n"},{"title":"卷积神经网络","date":"2023-10-12T01:52:46.091Z","_content":"全连接神经网络：\n- 参数太多\n- 训练困难\n- 网络结构复杂\n- 没有考虑到图像的空间结构信息\n\n### 动机\n比起浅层神经网络，深度神经网络会更难训练，然而，如果训练好一个深度网络，它会比浅层网络强大的多\n因此，有必要开发一种能够训练的深度网络结构\n卷积神经网络可以简化网络结构，同时利用空间结构信息\n### 卷积神经网络\n#### 卷积运算\n卷积经常用在信号处理中，用于计算信号的**延迟累积**\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101111.png)\n\n#### 局部感受野\n单个视觉细胞仅对**部分区域**的**特定模式**反应\n局部感受：对外部世界由局部到全局的感知\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101800.png)\n\n一个小区域的输入像素连接到隐藏层的一个神经元\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102036.png)\n输入图像中的此类区域称为隐藏神经元的局部感受野\n然后在整张图片上滑动这个局部感受野，每个不同的局部感受野对应于隐藏层的一个不同的神经元\n\n#### 权值共享\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102551.png)\n每个隐藏层神经元使用相同权值与偏差\n\n步长（Stride）：每次局部感受野移动的幅度。相邻的子区域很相似，没有必要检测所有的子区域\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012103227.png)\n\n第一个隐藏层中所有神经元都检测到了相同特征，只是在不同的输入图像的位置\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012104314.png)\n\n因此，从输入层到隐藏层的映射通常称之为特征映射（feature map）\n共享的权值和偏置被称为卷积核（kernel）或者滤波器（filter）\n\n滤波器越多，特征映射的深度越大，得到的关于输入的信息就越多\n共享权值极大降低了CNN的参数规模\n\n#### 池化\n池化层通常用在卷积层之后，以简化卷积层输出的信息\n- Max-pooling：输出池化矩阵中最大激活值 ![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111003.png)\n- Mean-pooling：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111104.png)\n\n池化的主要目的：\n- 减少参数，降低计算量，控制过拟合\n- 使得特征具有局部的转移和扭曲不变性\n\n### 卷积神经网络中的反向传播\n\n\n\n### 典型的卷积网络\n- ImageNet\n- LeNet\n- AlexNet\n- VGGNet\n- GoogLeNet\n- ResNet\n\n### 应用\nCNNs迁移学习框架\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012114823.png)\n\n图像描述生成\n\n图像表征提取","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/卷积神经网络.md","raw":"---\ntitle: 卷积神经网络\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 神经网络\ndate:\n---\n全连接神经网络：\n- 参数太多\n- 训练困难\n- 网络结构复杂\n- 没有考虑到图像的空间结构信息\n\n### 动机\n比起浅层神经网络，深度神经网络会更难训练，然而，如果训练好一个深度网络，它会比浅层网络强大的多\n因此，有必要开发一种能够训练的深度网络结构\n卷积神经网络可以简化网络结构，同时利用空间结构信息\n### 卷积神经网络\n#### 卷积运算\n卷积经常用在信号处理中，用于计算信号的**延迟累积**\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101111.png)\n\n#### 局部感受野\n单个视觉细胞仅对**部分区域**的**特定模式**反应\n局部感受：对外部世界由局部到全局的感知\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101800.png)\n\n一个小区域的输入像素连接到隐藏层的一个神经元\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102036.png)\n输入图像中的此类区域称为隐藏神经元的局部感受野\n然后在整张图片上滑动这个局部感受野，每个不同的局部感受野对应于隐藏层的一个不同的神经元\n\n#### 权值共享\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102551.png)\n每个隐藏层神经元使用相同权值与偏差\n\n步长（Stride）：每次局部感受野移动的幅度。相邻的子区域很相似，没有必要检测所有的子区域\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012103227.png)\n\n第一个隐藏层中所有神经元都检测到了相同特征，只是在不同的输入图像的位置\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012104314.png)\n\n因此，从输入层到隐藏层的映射通常称之为特征映射（feature map）\n共享的权值和偏置被称为卷积核（kernel）或者滤波器（filter）\n\n滤波器越多，特征映射的深度越大，得到的关于输入的信息就越多\n共享权值极大降低了CNN的参数规模\n\n#### 池化\n池化层通常用在卷积层之后，以简化卷积层输出的信息\n- Max-pooling：输出池化矩阵中最大激活值 ![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111003.png)\n- Mean-pooling：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111104.png)\n\n池化的主要目的：\n- 减少参数，降低计算量，控制过拟合\n- 使得特征具有局部的转移和扭曲不变性\n\n### 卷积神经网络中的反向传播\n\n\n\n### 典型的卷积网络\n- ImageNet\n- LeNet\n- AlexNet\n- VGGNet\n- GoogLeNet\n- ResNet\n\n### 应用\nCNNs迁移学习框架\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012114823.png)\n\n图像描述生成\n\n图像表征提取","slug":"Notes/课程/大三（上）/神经网络与深度学习/卷积神经网络","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12d0021vw8cee0dfhuv","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>全连接神经网络：</p>\n<ul>\n<li>参数太多</li>\n<li>训练困难</li>\n<li>网络结构复杂</li>\n<li>没有考虑到图像的空间结构信息</li>\n</ul>\n<h3 id=\"动机\"><a href=\"#动机\" class=\"headerlink\" title=\"动机\"></a>动机</h3><p>比起浅层神经网络，深度神经网络会更难训练，然而，如果训练好一个深度网络，它会比浅层网络强大的多<br>因此，有必要开发一种能够训练的深度网络结构<br>卷积神经网络可以简化网络结构，同时利用空间结构信息</p>\n<h3 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><h4 id=\"卷积运算\"><a href=\"#卷积运算\" class=\"headerlink\" title=\"卷积运算\"></a>卷积运算</h4><p>卷积经常用在信号处理中，用于计算信号的<strong>延迟累积</strong><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101111.png\" alt=\"image.png\"></p>\n<h4 id=\"局部感受野\"><a href=\"#局部感受野\" class=\"headerlink\" title=\"局部感受野\"></a>局部感受野</h4><p>单个视觉细胞仅对<strong>部分区域</strong>的<strong>特定模式</strong>反应<br>局部感受：对外部世界由局部到全局的感知<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101800.png\" alt=\"image.png\"></p>\n<p>一个小区域的输入像素连接到隐藏层的一个神经元<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102036.png\" alt=\"image.png\"><br>输入图像中的此类区域称为隐藏神经元的局部感受野<br>然后在整张图片上滑动这个局部感受野，每个不同的局部感受野对应于隐藏层的一个不同的神经元</p>\n<h4 id=\"权值共享\"><a href=\"#权值共享\" class=\"headerlink\" title=\"权值共享\"></a>权值共享</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102551.png\" alt=\"image.png\"><br>每个隐藏层神经元使用相同权值与偏差</p>\n<p>步长（Stride）：每次局部感受野移动的幅度。相邻的子区域很相似，没有必要检测所有的子区域<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012103227.png\" alt=\"image.png\"></p>\n<p>第一个隐藏层中所有神经元都检测到了相同特征，只是在不同的输入图像的位置<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012104314.png\" alt=\"image.png\"></p>\n<p>因此，从输入层到隐藏层的映射通常称之为特征映射（feature map）<br>共享的权值和偏置被称为卷积核（kernel）或者滤波器（filter）</p>\n<p>滤波器越多，特征映射的深度越大，得到的关于输入的信息就越多<br>共享权值极大降低了CNN的参数规模</p>\n<h4 id=\"池化\"><a href=\"#池化\" class=\"headerlink\" title=\"池化\"></a>池化</h4><p>池化层通常用在卷积层之后，以简化卷积层输出的信息</p>\n<ul>\n<li>Max-pooling：输出池化矩阵中最大激活值 <img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111003.png\" alt=\"image.png\"></li>\n<li>Mean-pooling：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111104.png\" alt=\"image.png\"></li>\n</ul>\n<p>池化的主要目的：</p>\n<ul>\n<li>减少参数，降低计算量，控制过拟合</li>\n<li>使得特征具有局部的转移和扭曲不变性</li>\n</ul>\n<h3 id=\"卷积神经网络中的反向传播\"><a href=\"#卷积神经网络中的反向传播\" class=\"headerlink\" title=\"卷积神经网络中的反向传播\"></a>卷积神经网络中的反向传播</h3><h3 id=\"典型的卷积网络\"><a href=\"#典型的卷积网络\" class=\"headerlink\" title=\"典型的卷积网络\"></a>典型的卷积网络</h3><ul>\n<li>ImageNet</li>\n<li>LeNet</li>\n<li>AlexNet</li>\n<li>VGGNet</li>\n<li>GoogLeNet</li>\n<li>ResNet</li>\n</ul>\n<h3 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h3><p>CNNs迁移学习框架<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012114823.png\" alt=\"image.png\"></p>\n<p>图像描述生成</p>\n<p>图像表征提取</p>\n","site":{"data":{}},"excerpt":"","more":"<p>全连接神经网络：</p>\n<ul>\n<li>参数太多</li>\n<li>训练困难</li>\n<li>网络结构复杂</li>\n<li>没有考虑到图像的空间结构信息</li>\n</ul>\n<h3 id=\"动机\"><a href=\"#动机\" class=\"headerlink\" title=\"动机\"></a>动机</h3><p>比起浅层神经网络，深度神经网络会更难训练，然而，如果训练好一个深度网络，它会比浅层网络强大的多<br>因此，有必要开发一种能够训练的深度网络结构<br>卷积神经网络可以简化网络结构，同时利用空间结构信息</p>\n<h3 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><h4 id=\"卷积运算\"><a href=\"#卷积运算\" class=\"headerlink\" title=\"卷积运算\"></a>卷积运算</h4><p>卷积经常用在信号处理中，用于计算信号的<strong>延迟累积</strong><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101111.png\" alt=\"image.png\"></p>\n<h4 id=\"局部感受野\"><a href=\"#局部感受野\" class=\"headerlink\" title=\"局部感受野\"></a>局部感受野</h4><p>单个视觉细胞仅对<strong>部分区域</strong>的<strong>特定模式</strong>反应<br>局部感受：对外部世界由局部到全局的感知<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012101800.png\" alt=\"image.png\"></p>\n<p>一个小区域的输入像素连接到隐藏层的一个神经元<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102036.png\" alt=\"image.png\"><br>输入图像中的此类区域称为隐藏神经元的局部感受野<br>然后在整张图片上滑动这个局部感受野，每个不同的局部感受野对应于隐藏层的一个不同的神经元</p>\n<h4 id=\"权值共享\"><a href=\"#权值共享\" class=\"headerlink\" title=\"权值共享\"></a>权值共享</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012102551.png\" alt=\"image.png\"><br>每个隐藏层神经元使用相同权值与偏差</p>\n<p>步长（Stride）：每次局部感受野移动的幅度。相邻的子区域很相似，没有必要检测所有的子区域<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012103227.png\" alt=\"image.png\"></p>\n<p>第一个隐藏层中所有神经元都检测到了相同特征，只是在不同的输入图像的位置<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012104314.png\" alt=\"image.png\"></p>\n<p>因此，从输入层到隐藏层的映射通常称之为特征映射（feature map）<br>共享的权值和偏置被称为卷积核（kernel）或者滤波器（filter）</p>\n<p>滤波器越多，特征映射的深度越大，得到的关于输入的信息就越多<br>共享权值极大降低了CNN的参数规模</p>\n<h4 id=\"池化\"><a href=\"#池化\" class=\"headerlink\" title=\"池化\"></a>池化</h4><p>池化层通常用在卷积层之后，以简化卷积层输出的信息</p>\n<ul>\n<li>Max-pooling：输出池化矩阵中最大激活值 <img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111003.png\" alt=\"image.png\"></li>\n<li>Mean-pooling：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012111104.png\" alt=\"image.png\"></li>\n</ul>\n<p>池化的主要目的：</p>\n<ul>\n<li>减少参数，降低计算量，控制过拟合</li>\n<li>使得特征具有局部的转移和扭曲不变性</li>\n</ul>\n<h3 id=\"卷积神经网络中的反向传播\"><a href=\"#卷积神经网络中的反向传播\" class=\"headerlink\" title=\"卷积神经网络中的反向传播\"></a>卷积神经网络中的反向传播</h3><h3 id=\"典型的卷积网络\"><a href=\"#典型的卷积网络\" class=\"headerlink\" title=\"典型的卷积网络\"></a>典型的卷积网络</h3><ul>\n<li>ImageNet</li>\n<li>LeNet</li>\n<li>AlexNet</li>\n<li>VGGNet</li>\n<li>GoogLeNet</li>\n<li>ResNet</li>\n</ul>\n<h3 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h3><p>CNNs迁移学习框架<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231012114823.png\" alt=\"image.png\"></p>\n<p>图像描述生成</p>\n<p>图像表征提取</p>\n"},{"title":"前馈神经网络","date":"2023-09-24T02:21:57.962Z","_content":"\n### 激活函数\n**激活函数：最关键部分**\n- 激活函数：连续并可导的非线性函数\n- 激活函数及其导函数要尽可能简单\n- 激活函数的导函数要在一个合适的区间内\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921102752.png)\n\n#### Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\n>![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105310.png)\n\n- Logistic函数：Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到 (0, 1)\n- Tanh函数：Tanh函数可以看作放大并平移的Logistic函数，其值域是(−1, 1)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105110.png)\nTanh 函数的输出是零中心化的（Zero-Centered），而 Logistic 函数的输出恒大于 0．非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下 降的收敛速度变慢．\n- Hard-Logistic函数和Hard-Tanh函数：分段函数来近似Logistic和Tanh\n\n#### ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）\nReLU为左饱和函数，在 𝑥 > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。\n>死亡 ReLU 问题：ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当更新后，第一个隐藏层中的某个 ReLU 神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活。\n\n为避免ReLU的问题，有几种ReLU的变种\n- 带泄露的ReLU（Leaky ReLU）：在输入 𝑥 < 0时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活\n- 带参数的ReLU（Parametric ReLU，PReLU）：引入一个可学习的参数\n- ELU（Exponential Linear Unit，指数线性单元）\n- Softplus 函数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924110331.png)\n\n### 网络结构\n\n- **前馈网络**：整个网络中的信息是朝一个方向传播，没有反向的信息传播。包括全连接前馈网络、卷积神经网络等。\n- **反馈网络（记忆网络）**：和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递。\n- **图网络**：图网络是定义在图结构数据上的神经网络，图中每个节点都由 一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个 节点可以收到来自相邻节点或自身的信息\n\n\n### 前馈神经网络\n每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层．第0层称为输入层，最后一层称0为输出层，其他中间层称为隐藏层．整个网络中无反馈，信号从输入层向输出层单向传播。\n传播公式：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112107.png)\n\n通用近似定理：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112220.png)\n根据通用近似定理，前馈神经网络可以以任意精度拟合任意定义在实数空间的有界闭集函数\n\n**输出层**： 根据任务确定输出层的激活函数\n- 回归任务：根据输出的值域选择激活函数\n- 分类任务：softmax函数\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110000.png)\n\n### 反向传播算法\n\n假设采用随机梯度下降进行神经网络参数学习，给定一个样本 (𝒙, 𝒚)，将其 输入到神经网络模型中，得到网络输出为 𝒚̂．假设损失函数为 ℒ(𝒚, 𝒚)̂，要进行参数学习就需要计算损失函数关于每个参数的导数。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130625.png)\n\n根据损失函数计算参数的偏导数：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130632.png)\n\n\n偏导数![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130544.png)\n表示第𝑙层神经元对最终损失的影响，也反映了最终损失对第𝑙 层神经元的敏感程度，因此一般称为第𝑙 层神经元的误差项，用𝛿 (𝑙) 来表示．![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130525.png)误差项𝛿 (𝑙) 也间接反映了不同神经元对网络能力的贡献程度\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131107.png)\n第𝑙 层的误差项可以通过第𝑙 + 1层的误差项计算得到，这就是误差的反向传播（BackPropagation，BP）。反向传播算法的含义是： 第 𝑙 层的一个神经元的误差项（或敏感性）是所有与该神经元相连的第 𝑙 + 1 层 的神经元的误差项的权重和．然后，再乘上该神经元激活函数的梯度。\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921111754.png)\n需要会推导\n\n### 优化问题\n神经网络的参数学习比线性模型要更加困难，主要原因有两点：1）非凸优化问题和2）梯度消失问题．\n#### 非凸优化问题\n>**凸优化问题**:\n对于目标函数，我们限定是凸函数；对于优化变量的可行域（注意，还要包括目标函数定义域的约束），我们限定它是凸集。同时满足这两个限制条件的最优化问题称为凸优化问题，这类问题有一个非常好性质，那就是局部最优解一定是全局最优解。\n\n神经网络的优化问题是一个非凸优化问题![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131753.png)\n#### 梯度消失问题\n误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数．当我们使 用Sigmoid型函数：Logistic函数𝜎(𝑥)或Tanh函数时，其导数为![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132011.png)\n其导数的值域都小于等于1\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132016.png)\n由于 Sigmoid 型函数的饱和性，饱和区的导数更是接近于 0．这样，误差经过每一层传递都会不断衰减．当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练．这就是所谓的梯度消失问题（Vanishing Gradient Problem）.\n在深度神经网络中，减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数，比如ReLU等。","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/前馈神经网络.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntitle: 前馈神经网络\ntags:\n  - 神经网络\ndate:\n---\n\n### 激活函数\n**激活函数：最关键部分**\n- 激活函数：连续并可导的非线性函数\n- 激活函数及其导函数要尽可能简单\n- 激活函数的导函数要在一个合适的区间内\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921102752.png)\n\n#### Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\n>![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105310.png)\n\n- Logistic函数：Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到 (0, 1)\n- Tanh函数：Tanh函数可以看作放大并平移的Logistic函数，其值域是(−1, 1)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105110.png)\nTanh 函数的输出是零中心化的（Zero-Centered），而 Logistic 函数的输出恒大于 0．非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下 降的收敛速度变慢．\n- Hard-Logistic函数和Hard-Tanh函数：分段函数来近似Logistic和Tanh\n\n#### ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）\nReLU为左饱和函数，在 𝑥 > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。\n>死亡 ReLU 问题：ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当更新后，第一个隐藏层中的某个 ReLU 神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活。\n\n为避免ReLU的问题，有几种ReLU的变种\n- 带泄露的ReLU（Leaky ReLU）：在输入 𝑥 < 0时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活\n- 带参数的ReLU（Parametric ReLU，PReLU）：引入一个可学习的参数\n- ELU（Exponential Linear Unit，指数线性单元）\n- Softplus 函数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924110331.png)\n\n### 网络结构\n\n- **前馈网络**：整个网络中的信息是朝一个方向传播，没有反向的信息传播。包括全连接前馈网络、卷积神经网络等。\n- **反馈网络（记忆网络）**：和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递。\n- **图网络**：图网络是定义在图结构数据上的神经网络，图中每个节点都由 一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个 节点可以收到来自相邻节点或自身的信息\n\n\n### 前馈神经网络\n每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层．第0层称为输入层，最后一层称0为输出层，其他中间层称为隐藏层．整个网络中无反馈，信号从输入层向输出层单向传播。\n传播公式：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112107.png)\n\n通用近似定理：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112220.png)\n根据通用近似定理，前馈神经网络可以以任意精度拟合任意定义在实数空间的有界闭集函数\n\n**输出层**： 根据任务确定输出层的激活函数\n- 回归任务：根据输出的值域选择激活函数\n- 分类任务：softmax函数\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110000.png)\n\n### 反向传播算法\n\n假设采用随机梯度下降进行神经网络参数学习，给定一个样本 (𝒙, 𝒚)，将其 输入到神经网络模型中，得到网络输出为 𝒚̂．假设损失函数为 ℒ(𝒚, 𝒚)̂，要进行参数学习就需要计算损失函数关于每个参数的导数。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130625.png)\n\n根据损失函数计算参数的偏导数：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130632.png)\n\n\n偏导数![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130544.png)\n表示第𝑙层神经元对最终损失的影响，也反映了最终损失对第𝑙 层神经元的敏感程度，因此一般称为第𝑙 层神经元的误差项，用𝛿 (𝑙) 来表示．![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130525.png)误差项𝛿 (𝑙) 也间接反映了不同神经元对网络能力的贡献程度\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131107.png)\n第𝑙 层的误差项可以通过第𝑙 + 1层的误差项计算得到，这就是误差的反向传播（BackPropagation，BP）。反向传播算法的含义是： 第 𝑙 层的一个神经元的误差项（或敏感性）是所有与该神经元相连的第 𝑙 + 1 层 的神经元的误差项的权重和．然后，再乘上该神经元激活函数的梯度。\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921111754.png)\n需要会推导\n\n### 优化问题\n神经网络的参数学习比线性模型要更加困难，主要原因有两点：1）非凸优化问题和2）梯度消失问题．\n#### 非凸优化问题\n>**凸优化问题**:\n对于目标函数，我们限定是凸函数；对于优化变量的可行域（注意，还要包括目标函数定义域的约束），我们限定它是凸集。同时满足这两个限制条件的最优化问题称为凸优化问题，这类问题有一个非常好性质，那就是局部最优解一定是全局最优解。\n\n神经网络的优化问题是一个非凸优化问题![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131753.png)\n#### 梯度消失问题\n误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数．当我们使 用Sigmoid型函数：Logistic函数𝜎(𝑥)或Tanh函数时，其导数为![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132011.png)\n其导数的值域都小于等于1\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132016.png)\n由于 Sigmoid 型函数的饱和性，饱和区的导数更是接近于 0．这样，误差经过每一层传递都会不断衰减．当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练．这就是所谓的梯度消失问题（Vanishing Gradient Problem）.\n在深度神经网络中，减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数，比如ReLU等。","slug":"Notes/课程/大三（上）/神经网络与深度学习/前馈神经网络","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12d0024vw8cgpk19t43","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><p><strong>激活函数：最关键部分</strong></p>\n<ul>\n<li>激活函数：连续并可导的非线性函数</li>\n<li>激活函数及其导函数要尽可能简单</li>\n<li>激活函数的导函数要在一个合适的区间内<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921102752.png\"></li>\n</ul>\n<h4 id=\"Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\"><a href=\"#Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\" class=\"headerlink\" title=\"Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\"></a>Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数</h4><blockquote>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105310.png\" alt=\"image.png\"></p>\n</blockquote>\n<ul>\n<li>Logistic函数：Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到 (0, 1)</li>\n<li>Tanh函数：Tanh函数可以看作放大并平移的Logistic函数，其值域是(−1, 1)<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105110.png\" alt=\"image.png\"><br>Tanh 函数的输出是零中心化的（Zero-Centered），而 Logistic 函数的输出恒大于 0．非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下 降的收敛速度变慢．</li>\n<li>Hard-Logistic函数和Hard-Tanh函数：分段函数来近似Logistic和Tanh</li>\n</ul>\n<h4 id=\"ReLU函数：ReLU（Rectified-Linear-Unit，修正线性单元）\"><a href=\"#ReLU函数：ReLU（Rectified-Linear-Unit，修正线性单元）\" class=\"headerlink\" title=\"ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）\"></a>ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）</h4><p>ReLU为左饱和函数，在 𝑥 &gt; 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。</p>\n<blockquote>\n<p>死亡 ReLU 问题：ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当更新后，第一个隐藏层中的某个 ReLU 神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活。</p>\n</blockquote>\n<p>为避免ReLU的问题，有几种ReLU的变种</p>\n<ul>\n<li>带泄露的ReLU（Leaky ReLU）：在输入 𝑥 &lt; 0时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活</li>\n<li>带参数的ReLU（Parametric ReLU，PReLU）：引入一个可学习的参数</li>\n<li>ELU（Exponential Linear Unit，指数线性单元）</li>\n<li>Softplus 函数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924110331.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h3><ul>\n<li><strong>前馈网络</strong>：整个网络中的信息是朝一个方向传播，没有反向的信息传播。包括全连接前馈网络、卷积神经网络等。</li>\n<li><strong>反馈网络（记忆网络）</strong>：和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递。</li>\n<li><strong>图网络</strong>：图网络是定义在图结构数据上的神经网络，图中每个节点都由 一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个 节点可以收到来自相邻节点或自身的信息</li>\n</ul>\n<h3 id=\"前馈神经网络\"><a href=\"#前馈神经网络\" class=\"headerlink\" title=\"前馈神经网络\"></a>前馈神经网络</h3><p>每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层．第0层称为输入层，最后一层称0为输出层，其他中间层称为隐藏层．整个网络中无反馈，信号从输入层向输出层单向传播。<br>传播公式：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112107.png\" alt=\"image.png\"></p>\n<p>通用近似定理：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112220.png\" alt=\"image.png\"><br>根据通用近似定理，前馈神经网络可以以任意精度拟合任意定义在实数空间的有界闭集函数</p>\n<p><strong>输出层</strong>： 根据任务确定输出层的激活函数</p>\n<ul>\n<li>回归任务：根据输出的值域选择激活函数</li>\n<li>分类任务：softmax函数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110000.png\"></li>\n</ul>\n<h3 id=\"反向传播算法\"><a href=\"#反向传播算法\" class=\"headerlink\" title=\"反向传播算法\"></a>反向传播算法</h3><p>假设采用随机梯度下降进行神经网络参数学习，给定一个样本 (𝒙, 𝒚)，将其 输入到神经网络模型中，得到网络输出为 𝒚̂．假设损失函数为 ℒ(𝒚, 𝒚)̂，要进行参数学习就需要计算损失函数关于每个参数的导数。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130625.png\" alt=\"image.png\"></p>\n<p>根据损失函数计算参数的偏导数：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130632.png\" alt=\"image.png\"></p>\n<p>偏导数<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130544.png\" alt=\"image.png\"><br>表示第𝑙层神经元对最终损失的影响，也反映了最终损失对第𝑙 层神经元的敏感程度，因此一般称为第𝑙 层神经元的误差项，用𝛿 (𝑙) 来表示．<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130525.png\" alt=\"image.png\">误差项𝛿 (𝑙) 也间接反映了不同神经元对网络能力的贡献程度<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131107.png\" alt=\"image.png\"><br>第𝑙 层的误差项可以通过第𝑙 + 1层的误差项计算得到，这就是误差的反向传播（BackPropagation，BP）。反向传播算法的含义是： 第 𝑙 层的一个神经元的误差项（或敏感性）是所有与该神经元相连的第 𝑙 + 1 层 的神经元的误差项的权重和．然后，再乘上该神经元激活函数的梯度。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921111754.png\"><br>需要会推导</p>\n<h3 id=\"优化问题\"><a href=\"#优化问题\" class=\"headerlink\" title=\"优化问题\"></a>优化问题</h3><p>神经网络的参数学习比线性模型要更加困难，主要原因有两点：1）非凸优化问题和2）梯度消失问题．</p>\n<h4 id=\"非凸优化问题\"><a href=\"#非凸优化问题\" class=\"headerlink\" title=\"非凸优化问题\"></a>非凸优化问题</h4><blockquote>\n<p><strong>凸优化问题</strong>:<br>对于目标函数，我们限定是凸函数；对于优化变量的可行域（注意，还要包括目标函数定义域的约束），我们限定它是凸集。同时满足这两个限制条件的最优化问题称为凸优化问题，这类问题有一个非常好性质，那就是局部最优解一定是全局最优解。</p>\n</blockquote>\n<p>神经网络的优化问题是一个非凸优化问题<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131753.png\" alt=\"image.png\"></p>\n<h4 id=\"梯度消失问题\"><a href=\"#梯度消失问题\" class=\"headerlink\" title=\"梯度消失问题\"></a>梯度消失问题</h4><p>误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数．当我们使 用Sigmoid型函数：Logistic函数𝜎(𝑥)或Tanh函数时，其导数为<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132011.png\" alt=\"image.png\"><br>其导数的值域都小于等于1<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132016.png\" alt=\"image.png\"><br>由于 Sigmoid 型函数的饱和性，饱和区的导数更是接近于 0．这样，误差经过每一层传递都会不断衰减．当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练．这就是所谓的梯度消失问题（Vanishing Gradient Problem）.<br>在深度神经网络中，减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数，比如ReLU等。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><p><strong>激活函数：最关键部分</strong></p>\n<ul>\n<li>激活函数：连续并可导的非线性函数</li>\n<li>激活函数及其导函数要尽可能简单</li>\n<li>激活函数的导函数要在一个合适的区间内<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921102752.png\"></li>\n</ul>\n<h4 id=\"Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\"><a href=\"#Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\" class=\"headerlink\" title=\"Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数\"></a>Sigmoid型函数：指一类S型曲线函数，为两端饱和函数，包括Logistic函数和Tanh函数</h4><blockquote>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105310.png\" alt=\"image.png\"></p>\n</blockquote>\n<ul>\n<li>Logistic函数：Logistic 函数可以看成是一个“挤压”函数，把一个实数域的输入“挤压”到 (0, 1)</li>\n<li>Tanh函数：Tanh函数可以看作放大并平移的Logistic函数，其值域是(−1, 1)<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924105110.png\" alt=\"image.png\"><br>Tanh 函数的输出是零中心化的（Zero-Centered），而 Logistic 函数的输出恒大于 0．非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下 降的收敛速度变慢．</li>\n<li>Hard-Logistic函数和Hard-Tanh函数：分段函数来近似Logistic和Tanh</li>\n</ul>\n<h4 id=\"ReLU函数：ReLU（Rectified-Linear-Unit，修正线性单元）\"><a href=\"#ReLU函数：ReLU（Rectified-Linear-Unit，修正线性单元）\" class=\"headerlink\" title=\"ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）\"></a>ReLU函数：ReLU（Rectified Linear Unit，修正线性单元）</h4><p>ReLU为左饱和函数，在 𝑥 &gt; 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。</p>\n<blockquote>\n<p>死亡 ReLU 问题：ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次不恰当更新后，第一个隐藏层中的某个 ReLU 神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活。</p>\n</blockquote>\n<p>为避免ReLU的问题，有几种ReLU的变种</p>\n<ul>\n<li>带泄露的ReLU（Leaky ReLU）：在输入 𝑥 &lt; 0时，保持一个很小的梯度𝛾．这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活</li>\n<li>带参数的ReLU（Parametric ReLU，PReLU）：引入一个可学习的参数</li>\n<li>ELU（Exponential Linear Unit，指数线性单元）</li>\n<li>Softplus 函数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924110331.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"网络结构\"><a href=\"#网络结构\" class=\"headerlink\" title=\"网络结构\"></a>网络结构</h3><ul>\n<li><strong>前馈网络</strong>：整个网络中的信息是朝一个方向传播，没有反向的信息传播。包括全连接前馈网络、卷积神经网络等。</li>\n<li><strong>反馈网络（记忆网络）</strong>：和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递。</li>\n<li><strong>图网络</strong>：图网络是定义在图结构数据上的神经网络，图中每个节点都由 一个或一组神经元构成．节点之间的连接可以是有向的，也可以是无向的．每个 节点可以收到来自相邻节点或自身的信息</li>\n</ul>\n<h3 id=\"前馈神经网络\"><a href=\"#前馈神经网络\" class=\"headerlink\" title=\"前馈神经网络\"></a>前馈神经网络</h3><p>每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层．第0层称为输入层，最后一层称0为输出层，其他中间层称为隐藏层．整个网络中无反馈，信号从输入层向输出层单向传播。<br>传播公式：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112107.png\" alt=\"image.png\"></p>\n<p>通用近似定理：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924112220.png\" alt=\"image.png\"><br>根据通用近似定理，前馈神经网络可以以任意精度拟合任意定义在实数空间的有界闭集函数</p>\n<p><strong>输出层</strong>： 根据任务确定输出层的激活函数</p>\n<ul>\n<li>回归任务：根据输出的值域选择激活函数</li>\n<li>分类任务：softmax函数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921110000.png\"></li>\n</ul>\n<h3 id=\"反向传播算法\"><a href=\"#反向传播算法\" class=\"headerlink\" title=\"反向传播算法\"></a>反向传播算法</h3><p>假设采用随机梯度下降进行神经网络参数学习，给定一个样本 (𝒙, 𝒚)，将其 输入到神经网络模型中，得到网络输出为 𝒚̂．假设损失函数为 ℒ(𝒚, 𝒚)̂，要进行参数学习就需要计算损失函数关于每个参数的导数。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130625.png\" alt=\"image.png\"></p>\n<p>根据损失函数计算参数的偏导数：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130632.png\" alt=\"image.png\"></p>\n<p>偏导数<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130544.png\" alt=\"image.png\"><br>表示第𝑙层神经元对最终损失的影响，也反映了最终损失对第𝑙 层神经元的敏感程度，因此一般称为第𝑙 层神经元的误差项，用𝛿 (𝑙) 来表示．<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924130525.png\" alt=\"image.png\">误差项𝛿 (𝑙) 也间接反映了不同神经元对网络能力的贡献程度<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131107.png\" alt=\"image.png\"><br>第𝑙 层的误差项可以通过第𝑙 + 1层的误差项计算得到，这就是误差的反向传播（BackPropagation，BP）。反向传播算法的含义是： 第 𝑙 层的一个神经元的误差项（或敏感性）是所有与该神经元相连的第 𝑙 + 1 层 的神经元的误差项的权重和．然后，再乘上该神经元激活函数的梯度。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921111754.png\"><br>需要会推导</p>\n<h3 id=\"优化问题\"><a href=\"#优化问题\" class=\"headerlink\" title=\"优化问题\"></a>优化问题</h3><p>神经网络的参数学习比线性模型要更加困难，主要原因有两点：1）非凸优化问题和2）梯度消失问题．</p>\n<h4 id=\"非凸优化问题\"><a href=\"#非凸优化问题\" class=\"headerlink\" title=\"非凸优化问题\"></a>非凸优化问题</h4><blockquote>\n<p><strong>凸优化问题</strong>:<br>对于目标函数，我们限定是凸函数；对于优化变量的可行域（注意，还要包括目标函数定义域的约束），我们限定它是凸集。同时满足这两个限制条件的最优化问题称为凸优化问题，这类问题有一个非常好性质，那就是局部最优解一定是全局最优解。</p>\n</blockquote>\n<p>神经网络的优化问题是一个非凸优化问题<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924131753.png\" alt=\"image.png\"></p>\n<h4 id=\"梯度消失问题\"><a href=\"#梯度消失问题\" class=\"headerlink\" title=\"梯度消失问题\"></a>梯度消失问题</h4><p>误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数．当我们使 用Sigmoid型函数：Logistic函数𝜎(𝑥)或Tanh函数时，其导数为<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132011.png\" alt=\"image.png\"><br>其导数的值域都小于等于1<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230924132016.png\" alt=\"image.png\"><br>由于 Sigmoid 型函数的饱和性，饱和区的导数更是接近于 0．这样，误差经过每一层传递都会不断衰减．当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练．这就是所谓的梯度消失问题（Vanishing Gradient Problem）.<br>在深度神经网络中，减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数，比如ReLU等。</p>\n"},{"title":"图神经网络","date":"2023-10-26T01:53:12.478Z","_content":"图是一种描述样本间关系的通用语言\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026095632.png)\n\n## 图神经网络\n图神经网络是一种基于图结构数据的深度学习方法，学习图结构数据中的节点特征、边特征、图级表示\n- 图是图神经网络研究的基本对象;𝐺=(𝑉,𝐸)\n- 是描述复杂事务的数据表示形式，由节点和边组成；\n- 可描述不规则数据（非欧式数据），充分利用数据间关系信息\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100249.png)\n\n使用神经网络变换、聚合来自目标节点其邻居的信息，迭代生成节点嵌入表示\n1. 对于单个节点，求取邻居节点的表示的均值获得邻居表示\n2. 通过一个全连接层对邻居表示和该节点的自身表示进行线性加权\n3. 通过非线性激活函数得到该节点的聚合表示\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100526.png)\n\n- 节点在每一层都有一个表示\n- 模型可以设计任意多层\n- 输入特征可以视为第0层的节点表示\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026101101.png)\n\n### 图卷积神经网络\n#### 基于谱域的方法\n𝑮=𝑽,𝑬,𝑨,𝑿, 𝑽为节点集合, 𝑬为边集合, 𝐀为邻接矩阵, 𝑿是节点特征矩阵\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102249.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102316.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102345.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102358.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102419.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102735.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102806.png)\n\n#### 基于空间结构的方法\n- 对于每个节点，选择固定数量的节点作为其邻近节点\n- 按照一定的邻近度量根据邻近度进行排序\n- 参数共享\nGraphSAGE\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103613.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103647.png)\n\n### 图注意力神经网络\n人脑通过注意力来解决信息超载问题\n#### 注意力机制\nAttention 机制选择一些关键的信息输入进行处理，来提高神经网络的效率。具体通过在模型训练和推断过程中动态调整模型对数据/特征不同部分的权重提升模型的学习效果\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104614.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104623.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104643.png)\n\n#### 自注意力机制\n当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，但是只建模了输入信息的局部依赖关系\n自注意力模型：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105602.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105742.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105815.png)\n\n#### 多头自注意力模型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105855.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105910.png)\n\n#### 图注意力网络\nGAT 在图神经网络中引入了Attention，关注邻居节点集合中比较重要的部分。GAT 采用masked attention的方式——仅将注意力分配到节点i的邻居节点集𝑁i上，即j ∈𝑁i\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026110938.png)\n对于单个节点，将其表示作为查询𝑞，邻居节点的表示作为键和值<𝑘,𝑣>，执行注意力操作的结果即为该节点的聚合表示：\n1. 获得第𝑖个节点的查询，获得和其相邻的所有节点的键和值\n2. 计算其和所有邻居节点的相关性\n- 将查询和键拼接起来，然后通过激活函数为LeakyReLU的全连接神经网络进行变换，最终得到一个代表相关分数的实数\n3. 计算归一化注意力得分\n4. 以注意力得分为权重，对值进行加权求和，计算节点的聚合表示图注意力网络\n5. 最终的聚合结果可以由多个注意力头的结果拼接得到\n\n## 图汇聚\n图汇聚（Graph Pooling）：获取一个完整图级别的表示\n\n平面图汇聚：\n- 直接从节点表示生成图级别表示\n- 最大汇聚、平均汇聚、求和汇聚、注意力汇聚\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111724.png)\n\n\n层次图汇聚：逐步粗化图得到图级别表示\n- 基于降采样的层次图汇聚\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111802.png)\n\n- 基于超节点的层次图汇聚\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111833.png)\n\n## 异构图神经网络\n异构信息网络的核心：\n1. 元路径（Meta-path）：具有语义含义的路径\n2. 语义空间（Semantic Space）\n\n同构图：节点类型和边的类型只有一种的图\n\n异构图：相比于以往的同构图𝐺= (𝑉, 𝐸) ，异构图多了两个属性R、T ，其中R表示边的类型，T表示节点的类型，可以表示为：𝐺= (𝑉, 𝐸, 𝑅, 𝑇)。异构图神经网络的关键在于如何设计合适的聚合函数以捕获邻域所包含的语义。\n聚合公式：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112122.png)\n异构图神经网络的特点：\n1. 不同meta-path 对应不同语义空间\n2. 每个语义空间内节点的邻居和信息均不同\n3. 节点的最终表征与每个语义空间均有关\n异构图注意力网络：\n1. 使用meta-path 异构信息网络投影到多个同构图\n2. 在每个同构图内使用注意力网络整合邻居信息\n3. 对不同meta-path 对应的多个同构图使用全局注意力机制\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112241.png)\n\n## 图神经网络的应用\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112424.png)\n\n### 文本分类\n文本分类是自然语言处理（NLP）中的一项基础任务，它的目标是将输入的文本分配给一个或多个类别。常见的文本分类应用包括情感分析、主题分类、垃圾邮件检测等。\n面向文本数据的图构建：文本数据（词、文档）不直接包含关系信息，但可以基于统计进行关系构建\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113021.png)\n\n### 知识图谱嵌入\n定义：知识图谱嵌入(Knowledge Graph Embeddings, KGEs)，也称为知识图谱表示学习(Knowledge Graph Representing Learning)，是将包含实体和关系的知识图谱的组成部分嵌入到低维度、连续的向量空间中，从而**在保持知识图谱固有结构的同时简化操作**。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113151.png)\n\n模型概述：\n1. 多模态实体嵌入模块：编码器通过多层感知器网络学习不同模态（图片、文本）数据的实体嵌入。\n2. 局部对比多关系特征聚合模块：将实体的局部邻域的关系类型区分为多关系子图，通过注意力机制进行关系内、关系间特征聚合。\n3. 高阶对比编码模块：显式对比来自实体及其高阶邻居的编码来丰富实体特征。\n\n## 总结\n图是一个非常强大的工具，几乎所有的数据都能够表示成图\n- 计算机视觉、生物化学、推荐系统、交通、程序验证、程序推理......\n然而：\n- 在图上进行优化非常困难，因为图是稀疏架构，每个节点的边数高度可变，空间效率低\n- 图神经网络对超参数非常敏感","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/图神经网络.md","raw":"---\ntitle: 图神经网络\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 神经网络\ndate:\n---\n图是一种描述样本间关系的通用语言\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026095632.png)\n\n## 图神经网络\n图神经网络是一种基于图结构数据的深度学习方法，学习图结构数据中的节点特征、边特征、图级表示\n- 图是图神经网络研究的基本对象;𝐺=(𝑉,𝐸)\n- 是描述复杂事务的数据表示形式，由节点和边组成；\n- 可描述不规则数据（非欧式数据），充分利用数据间关系信息\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100249.png)\n\n使用神经网络变换、聚合来自目标节点其邻居的信息，迭代生成节点嵌入表示\n1. 对于单个节点，求取邻居节点的表示的均值获得邻居表示\n2. 通过一个全连接层对邻居表示和该节点的自身表示进行线性加权\n3. 通过非线性激活函数得到该节点的聚合表示\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100526.png)\n\n- 节点在每一层都有一个表示\n- 模型可以设计任意多层\n- 输入特征可以视为第0层的节点表示\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026101101.png)\n\n### 图卷积神经网络\n#### 基于谱域的方法\n𝑮=𝑽,𝑬,𝑨,𝑿, 𝑽为节点集合, 𝑬为边集合, 𝐀为邻接矩阵, 𝑿是节点特征矩阵\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102249.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102316.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102345.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102358.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102419.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102735.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102806.png)\n\n#### 基于空间结构的方法\n- 对于每个节点，选择固定数量的节点作为其邻近节点\n- 按照一定的邻近度量根据邻近度进行排序\n- 参数共享\nGraphSAGE\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103613.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103647.png)\n\n### 图注意力神经网络\n人脑通过注意力来解决信息超载问题\n#### 注意力机制\nAttention 机制选择一些关键的信息输入进行处理，来提高神经网络的效率。具体通过在模型训练和推断过程中动态调整模型对数据/特征不同部分的权重提升模型的学习效果\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104614.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104623.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104643.png)\n\n#### 自注意力机制\n当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，但是只建模了输入信息的局部依赖关系\n自注意力模型：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105602.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105742.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105815.png)\n\n#### 多头自注意力模型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105855.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105910.png)\n\n#### 图注意力网络\nGAT 在图神经网络中引入了Attention，关注邻居节点集合中比较重要的部分。GAT 采用masked attention的方式——仅将注意力分配到节点i的邻居节点集𝑁i上，即j ∈𝑁i\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026110938.png)\n对于单个节点，将其表示作为查询𝑞，邻居节点的表示作为键和值<𝑘,𝑣>，执行注意力操作的结果即为该节点的聚合表示：\n1. 获得第𝑖个节点的查询，获得和其相邻的所有节点的键和值\n2. 计算其和所有邻居节点的相关性\n- 将查询和键拼接起来，然后通过激活函数为LeakyReLU的全连接神经网络进行变换，最终得到一个代表相关分数的实数\n3. 计算归一化注意力得分\n4. 以注意力得分为权重，对值进行加权求和，计算节点的聚合表示图注意力网络\n5. 最终的聚合结果可以由多个注意力头的结果拼接得到\n\n## 图汇聚\n图汇聚（Graph Pooling）：获取一个完整图级别的表示\n\n平面图汇聚：\n- 直接从节点表示生成图级别表示\n- 最大汇聚、平均汇聚、求和汇聚、注意力汇聚\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111724.png)\n\n\n层次图汇聚：逐步粗化图得到图级别表示\n- 基于降采样的层次图汇聚\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111802.png)\n\n- 基于超节点的层次图汇聚\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111833.png)\n\n## 异构图神经网络\n异构信息网络的核心：\n1. 元路径（Meta-path）：具有语义含义的路径\n2. 语义空间（Semantic Space）\n\n同构图：节点类型和边的类型只有一种的图\n\n异构图：相比于以往的同构图𝐺= (𝑉, 𝐸) ，异构图多了两个属性R、T ，其中R表示边的类型，T表示节点的类型，可以表示为：𝐺= (𝑉, 𝐸, 𝑅, 𝑇)。异构图神经网络的关键在于如何设计合适的聚合函数以捕获邻域所包含的语义。\n聚合公式：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112122.png)\n异构图神经网络的特点：\n1. 不同meta-path 对应不同语义空间\n2. 每个语义空间内节点的邻居和信息均不同\n3. 节点的最终表征与每个语义空间均有关\n异构图注意力网络：\n1. 使用meta-path 异构信息网络投影到多个同构图\n2. 在每个同构图内使用注意力网络整合邻居信息\n3. 对不同meta-path 对应的多个同构图使用全局注意力机制\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112241.png)\n\n## 图神经网络的应用\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112424.png)\n\n### 文本分类\n文本分类是自然语言处理（NLP）中的一项基础任务，它的目标是将输入的文本分配给一个或多个类别。常见的文本分类应用包括情感分析、主题分类、垃圾邮件检测等。\n面向文本数据的图构建：文本数据（词、文档）不直接包含关系信息，但可以基于统计进行关系构建\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113021.png)\n\n### 知识图谱嵌入\n定义：知识图谱嵌入(Knowledge Graph Embeddings, KGEs)，也称为知识图谱表示学习(Knowledge Graph Representing Learning)，是将包含实体和关系的知识图谱的组成部分嵌入到低维度、连续的向量空间中，从而**在保持知识图谱固有结构的同时简化操作**。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113151.png)\n\n模型概述：\n1. 多模态实体嵌入模块：编码器通过多层感知器网络学习不同模态（图片、文本）数据的实体嵌入。\n2. 局部对比多关系特征聚合模块：将实体的局部邻域的关系类型区分为多关系子图，通过注意力机制进行关系内、关系间特征聚合。\n3. 高阶对比编码模块：显式对比来自实体及其高阶邻居的编码来丰富实体特征。\n\n## 总结\n图是一个非常强大的工具，几乎所有的数据都能够表示成图\n- 计算机视觉、生物化学、推荐系统、交通、程序验证、程序推理......\n然而：\n- 在图上进行优化非常困难，因为图是稀疏架构，每个节点的边数高度可变，空间效率低\n- 图神经网络对超参数非常敏感","slug":"Notes/课程/大三（上）/神经网络与深度学习/图神经网络","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12g003wvw8cfmtuftxy","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>图是一种描述样本间关系的通用语言<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026095632.png\" alt=\"image.png\"></p>\n<h2 id=\"图神经网络\"><a href=\"#图神经网络\" class=\"headerlink\" title=\"图神经网络\"></a>图神经网络</h2><p>图神经网络是一种基于图结构数据的深度学习方法，学习图结构数据中的节点特征、边特征、图级表示</p>\n<ul>\n<li>图是图神经网络研究的基本对象;𝐺&#x3D;(𝑉,𝐸)</li>\n<li>是描述复杂事务的数据表示形式，由节点和边组成；</li>\n<li>可描述不规则数据（非欧式数据），充分利用数据间关系信息<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100249.png\" alt=\"image.png\"></li>\n</ul>\n<p>使用神经网络变换、聚合来自目标节点其邻居的信息，迭代生成节点嵌入表示</p>\n<ol>\n<li>对于单个节点，求取邻居节点的表示的均值获得邻居表示</li>\n<li>通过一个全连接层对邻居表示和该节点的自身表示进行线性加权</li>\n<li>通过非线性激活函数得到该节点的聚合表示<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100526.png\" alt=\"image.png\"></li>\n</ol>\n<ul>\n<li>节点在每一层都有一个表示</li>\n<li>模型可以设计任意多层</li>\n<li>输入特征可以视为第0层的节点表示<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026101101.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"图卷积神经网络\"><a href=\"#图卷积神经网络\" class=\"headerlink\" title=\"图卷积神经网络\"></a>图卷积神经网络</h3><h4 id=\"基于谱域的方法\"><a href=\"#基于谱域的方法\" class=\"headerlink\" title=\"基于谱域的方法\"></a>基于谱域的方法</h4><p>𝑮&#x3D;𝑽,𝑬,𝑨,𝑿, 𝑽为节点集合, 𝑬为边集合, 𝐀为邻接矩阵, 𝑿是节点特征矩阵<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102249.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102316.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102345.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102358.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102419.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102735.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102806.png\" alt=\"image.png\"></p>\n<h4 id=\"基于空间结构的方法\"><a href=\"#基于空间结构的方法\" class=\"headerlink\" title=\"基于空间结构的方法\"></a>基于空间结构的方法</h4><ul>\n<li>对于每个节点，选择固定数量的节点作为其邻近节点</li>\n<li>按照一定的邻近度量根据邻近度进行排序</li>\n<li>参数共享<br>GraphSAGE<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103613.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103647.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"图注意力神经网络\"><a href=\"#图注意力神经网络\" class=\"headerlink\" title=\"图注意力神经网络\"></a>图注意力神经网络</h3><p>人脑通过注意力来解决信息超载问题</p>\n<h4 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h4><p>Attention 机制选择一些关键的信息输入进行处理，来提高神经网络的效率。具体通过在模型训练和推断过程中动态调整模型对数据&#x2F;特征不同部分的权重提升模型的学习效果<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104614.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104623.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104643.png\" alt=\"image.png\"></p>\n<h4 id=\"自注意力机制\"><a href=\"#自注意力机制\" class=\"headerlink\" title=\"自注意力机制\"></a>自注意力机制</h4><p>当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，但是只建模了输入信息的局部依赖关系<br>自注意力模型：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105602.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105742.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105815.png\" alt=\"image.png\"></p>\n<h4 id=\"多头自注意力模型\"><a href=\"#多头自注意力模型\" class=\"headerlink\" title=\"多头自注意力模型\"></a>多头自注意力模型</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105855.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105910.png\" alt=\"image.png\"></p>\n<h4 id=\"图注意力网络\"><a href=\"#图注意力网络\" class=\"headerlink\" title=\"图注意力网络\"></a>图注意力网络</h4><p>GAT 在图神经网络中引入了Attention，关注邻居节点集合中比较重要的部分。GAT 采用masked attention的方式——仅将注意力分配到节点i的邻居节点集𝑁i上，即j ∈𝑁i<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026110938.png\" alt=\"image.png\"><br>对于单个节点，将其表示作为查询𝑞，邻居节点的表示作为键和值&lt;𝑘,𝑣&gt;，执行注意力操作的结果即为该节点的聚合表示：</p>\n<ol>\n<li>获得第𝑖个节点的查询，获得和其相邻的所有节点的键和值</li>\n<li>计算其和所有邻居节点的相关性</li>\n</ol>\n<ul>\n<li>将查询和键拼接起来，然后通过激活函数为LeakyReLU的全连接神经网络进行变换，最终得到一个代表相关分数的实数</li>\n</ul>\n<ol start=\"3\">\n<li>计算归一化注意力得分</li>\n<li>以注意力得分为权重，对值进行加权求和，计算节点的聚合表示图注意力网络</li>\n<li>最终的聚合结果可以由多个注意力头的结果拼接得到</li>\n</ol>\n<h2 id=\"图汇聚\"><a href=\"#图汇聚\" class=\"headerlink\" title=\"图汇聚\"></a>图汇聚</h2><p>图汇聚（Graph Pooling）：获取一个完整图级别的表示</p>\n<p>平面图汇聚：</p>\n<ul>\n<li>直接从节点表示生成图级别表示</li>\n<li>最大汇聚、平均汇聚、求和汇聚、注意力汇聚<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111724.png\" alt=\"image.png\"></li>\n</ul>\n<p>层次图汇聚：逐步粗化图得到图级别表示</p>\n<ul>\n<li><p>基于降采样的层次图汇聚<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111802.png\" alt=\"image.png\"></p>\n</li>\n<li><p>基于超节点的层次图汇聚<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111833.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<h2 id=\"异构图神经网络\"><a href=\"#异构图神经网络\" class=\"headerlink\" title=\"异构图神经网络\"></a>异构图神经网络</h2><p>异构信息网络的核心：</p>\n<ol>\n<li>元路径（Meta-path）：具有语义含义的路径</li>\n<li>语义空间（Semantic Space）</li>\n</ol>\n<p>同构图：节点类型和边的类型只有一种的图</p>\n<p>异构图：相比于以往的同构图𝐺&#x3D; (𝑉, 𝐸) ，异构图多了两个属性R、T ，其中R表示边的类型，T表示节点的类型，可以表示为：𝐺&#x3D; (𝑉, 𝐸, 𝑅, 𝑇)。异构图神经网络的关键在于如何设计合适的聚合函数以捕获邻域所包含的语义。<br>聚合公式：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112122.png\" alt=\"image.png\"><br>异构图神经网络的特点：</p>\n<ol>\n<li>不同meta-path 对应不同语义空间</li>\n<li>每个语义空间内节点的邻居和信息均不同</li>\n<li>节点的最终表征与每个语义空间均有关<br>异构图注意力网络：</li>\n<li>使用meta-path 异构信息网络投影到多个同构图</li>\n<li>在每个同构图内使用注意力网络整合邻居信息</li>\n<li>对不同meta-path 对应的多个同构图使用全局注意力机制<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112241.png\" alt=\"image.png\"></li>\n</ol>\n<h2 id=\"图神经网络的应用\"><a href=\"#图神经网络的应用\" class=\"headerlink\" title=\"图神经网络的应用\"></a>图神经网络的应用</h2><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112424.png\" alt=\"image.png\"></p>\n<h3 id=\"文本分类\"><a href=\"#文本分类\" class=\"headerlink\" title=\"文本分类\"></a>文本分类</h3><p>文本分类是自然语言处理（NLP）中的一项基础任务，它的目标是将输入的文本分配给一个或多个类别。常见的文本分类应用包括情感分析、主题分类、垃圾邮件检测等。<br>面向文本数据的图构建：文本数据（词、文档）不直接包含关系信息，但可以基于统计进行关系构建<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113021.png\" alt=\"image.png\"></p>\n<h3 id=\"知识图谱嵌入\"><a href=\"#知识图谱嵌入\" class=\"headerlink\" title=\"知识图谱嵌入\"></a>知识图谱嵌入</h3><p>定义：知识图谱嵌入(Knowledge Graph Embeddings, KGEs)，也称为知识图谱表示学习(Knowledge Graph Representing Learning)，是将包含实体和关系的知识图谱的组成部分嵌入到低维度、连续的向量空间中，从而<strong>在保持知识图谱固有结构的同时简化操作</strong>。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113151.png\" alt=\"image.png\"></p>\n<p>模型概述：</p>\n<ol>\n<li>多模态实体嵌入模块：编码器通过多层感知器网络学习不同模态（图片、文本）数据的实体嵌入。</li>\n<li>局部对比多关系特征聚合模块：将实体的局部邻域的关系类型区分为多关系子图，通过注意力机制进行关系内、关系间特征聚合。</li>\n<li>高阶对比编码模块：显式对比来自实体及其高阶邻居的编码来丰富实体特征。</li>\n</ol>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>图是一个非常强大的工具，几乎所有的数据都能够表示成图</p>\n<ul>\n<li>计算机视觉、生物化学、推荐系统、交通、程序验证、程序推理……<br>然而：</li>\n<li>在图上进行优化非常困难，因为图是稀疏架构，每个节点的边数高度可变，空间效率低</li>\n<li>图神经网络对超参数非常敏感</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>图是一种描述样本间关系的通用语言<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026095632.png\" alt=\"image.png\"></p>\n<h2 id=\"图神经网络\"><a href=\"#图神经网络\" class=\"headerlink\" title=\"图神经网络\"></a>图神经网络</h2><p>图神经网络是一种基于图结构数据的深度学习方法，学习图结构数据中的节点特征、边特征、图级表示</p>\n<ul>\n<li>图是图神经网络研究的基本对象;𝐺&#x3D;(𝑉,𝐸)</li>\n<li>是描述复杂事务的数据表示形式，由节点和边组成；</li>\n<li>可描述不规则数据（非欧式数据），充分利用数据间关系信息<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100249.png\" alt=\"image.png\"></li>\n</ul>\n<p>使用神经网络变换、聚合来自目标节点其邻居的信息，迭代生成节点嵌入表示</p>\n<ol>\n<li>对于单个节点，求取邻居节点的表示的均值获得邻居表示</li>\n<li>通过一个全连接层对邻居表示和该节点的自身表示进行线性加权</li>\n<li>通过非线性激活函数得到该节点的聚合表示<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026100526.png\" alt=\"image.png\"></li>\n</ol>\n<ul>\n<li>节点在每一层都有一个表示</li>\n<li>模型可以设计任意多层</li>\n<li>输入特征可以视为第0层的节点表示<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026101101.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"图卷积神经网络\"><a href=\"#图卷积神经网络\" class=\"headerlink\" title=\"图卷积神经网络\"></a>图卷积神经网络</h3><h4 id=\"基于谱域的方法\"><a href=\"#基于谱域的方法\" class=\"headerlink\" title=\"基于谱域的方法\"></a>基于谱域的方法</h4><p>𝑮&#x3D;𝑽,𝑬,𝑨,𝑿, 𝑽为节点集合, 𝑬为边集合, 𝐀为邻接矩阵, 𝑿是节点特征矩阵<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102249.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102316.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102345.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102358.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102419.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102735.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026102806.png\" alt=\"image.png\"></p>\n<h4 id=\"基于空间结构的方法\"><a href=\"#基于空间结构的方法\" class=\"headerlink\" title=\"基于空间结构的方法\"></a>基于空间结构的方法</h4><ul>\n<li>对于每个节点，选择固定数量的节点作为其邻近节点</li>\n<li>按照一定的邻近度量根据邻近度进行排序</li>\n<li>参数共享<br>GraphSAGE<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103613.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026103647.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"图注意力神经网络\"><a href=\"#图注意力神经网络\" class=\"headerlink\" title=\"图注意力神经网络\"></a>图注意力神经网络</h3><p>人脑通过注意力来解决信息超载问题</p>\n<h4 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h4><p>Attention 机制选择一些关键的信息输入进行处理，来提高神经网络的效率。具体通过在模型训练和推断过程中动态调整模型对数据&#x2F;特征不同部分的权重提升模型的学习效果<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104614.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104623.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026104643.png\" alt=\"image.png\"></p>\n<h4 id=\"自注意力机制\"><a href=\"#自注意力机制\" class=\"headerlink\" title=\"自注意力机制\"></a>自注意力机制</h4><p>当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，但是只建模了输入信息的局部依赖关系<br>自注意力模型：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105602.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105742.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105815.png\" alt=\"image.png\"></p>\n<h4 id=\"多头自注意力模型\"><a href=\"#多头自注意力模型\" class=\"headerlink\" title=\"多头自注意力模型\"></a>多头自注意力模型</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105855.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026105910.png\" alt=\"image.png\"></p>\n<h4 id=\"图注意力网络\"><a href=\"#图注意力网络\" class=\"headerlink\" title=\"图注意力网络\"></a>图注意力网络</h4><p>GAT 在图神经网络中引入了Attention，关注邻居节点集合中比较重要的部分。GAT 采用masked attention的方式——仅将注意力分配到节点i的邻居节点集𝑁i上，即j ∈𝑁i<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026110938.png\" alt=\"image.png\"><br>对于单个节点，将其表示作为查询𝑞，邻居节点的表示作为键和值&lt;𝑘,𝑣&gt;，执行注意力操作的结果即为该节点的聚合表示：</p>\n<ol>\n<li>获得第𝑖个节点的查询，获得和其相邻的所有节点的键和值</li>\n<li>计算其和所有邻居节点的相关性</li>\n</ol>\n<ul>\n<li>将查询和键拼接起来，然后通过激活函数为LeakyReLU的全连接神经网络进行变换，最终得到一个代表相关分数的实数</li>\n</ul>\n<ol start=\"3\">\n<li>计算归一化注意力得分</li>\n<li>以注意力得分为权重，对值进行加权求和，计算节点的聚合表示图注意力网络</li>\n<li>最终的聚合结果可以由多个注意力头的结果拼接得到</li>\n</ol>\n<h2 id=\"图汇聚\"><a href=\"#图汇聚\" class=\"headerlink\" title=\"图汇聚\"></a>图汇聚</h2><p>图汇聚（Graph Pooling）：获取一个完整图级别的表示</p>\n<p>平面图汇聚：</p>\n<ul>\n<li>直接从节点表示生成图级别表示</li>\n<li>最大汇聚、平均汇聚、求和汇聚、注意力汇聚<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111724.png\" alt=\"image.png\"></li>\n</ul>\n<p>层次图汇聚：逐步粗化图得到图级别表示</p>\n<ul>\n<li><p>基于降采样的层次图汇聚<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111802.png\" alt=\"image.png\"></p>\n</li>\n<li><p>基于超节点的层次图汇聚<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026111833.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<h2 id=\"异构图神经网络\"><a href=\"#异构图神经网络\" class=\"headerlink\" title=\"异构图神经网络\"></a>异构图神经网络</h2><p>异构信息网络的核心：</p>\n<ol>\n<li>元路径（Meta-path）：具有语义含义的路径</li>\n<li>语义空间（Semantic Space）</li>\n</ol>\n<p>同构图：节点类型和边的类型只有一种的图</p>\n<p>异构图：相比于以往的同构图𝐺&#x3D; (𝑉, 𝐸) ，异构图多了两个属性R、T ，其中R表示边的类型，T表示节点的类型，可以表示为：𝐺&#x3D; (𝑉, 𝐸, 𝑅, 𝑇)。异构图神经网络的关键在于如何设计合适的聚合函数以捕获邻域所包含的语义。<br>聚合公式：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112122.png\" alt=\"image.png\"><br>异构图神经网络的特点：</p>\n<ol>\n<li>不同meta-path 对应不同语义空间</li>\n<li>每个语义空间内节点的邻居和信息均不同</li>\n<li>节点的最终表征与每个语义空间均有关<br>异构图注意力网络：</li>\n<li>使用meta-path 异构信息网络投影到多个同构图</li>\n<li>在每个同构图内使用注意力网络整合邻居信息</li>\n<li>对不同meta-path 对应的多个同构图使用全局注意力机制<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112241.png\" alt=\"image.png\"></li>\n</ol>\n<h2 id=\"图神经网络的应用\"><a href=\"#图神经网络的应用\" class=\"headerlink\" title=\"图神经网络的应用\"></a>图神经网络的应用</h2><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026112424.png\" alt=\"image.png\"></p>\n<h3 id=\"文本分类\"><a href=\"#文本分类\" class=\"headerlink\" title=\"文本分类\"></a>文本分类</h3><p>文本分类是自然语言处理（NLP）中的一项基础任务，它的目标是将输入的文本分配给一个或多个类别。常见的文本分类应用包括情感分析、主题分类、垃圾邮件检测等。<br>面向文本数据的图构建：文本数据（词、文档）不直接包含关系信息，但可以基于统计进行关系构建<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113021.png\" alt=\"image.png\"></p>\n<h3 id=\"知识图谱嵌入\"><a href=\"#知识图谱嵌入\" class=\"headerlink\" title=\"知识图谱嵌入\"></a>知识图谱嵌入</h3><p>定义：知识图谱嵌入(Knowledge Graph Embeddings, KGEs)，也称为知识图谱表示学习(Knowledge Graph Representing Learning)，是将包含实体和关系的知识图谱的组成部分嵌入到低维度、连续的向量空间中，从而<strong>在保持知识图谱固有结构的同时简化操作</strong>。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231026113151.png\" alt=\"image.png\"></p>\n<p>模型概述：</p>\n<ol>\n<li>多模态实体嵌入模块：编码器通过多层感知器网络学习不同模态（图片、文本）数据的实体嵌入。</li>\n<li>局部对比多关系特征聚合模块：将实体的局部邻域的关系类型区分为多关系子图，通过注意力机制进行关系内、关系间特征聚合。</li>\n<li>高阶对比编码模块：显式对比来自实体及其高阶邻居的编码来丰富实体特征。</li>\n</ol>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>图是一个非常强大的工具，几乎所有的数据都能够表示成图</p>\n<ul>\n<li>计算机视觉、生物化学、推荐系统、交通、程序验证、程序推理……<br>然而：</li>\n<li>在图上进行优化非常困难，因为图是稀疏架构，每个节点的边数高度可变，空间效率低</li>\n<li>图神经网络对超参数非常敏感</li>\n</ul>\n"},{"title":"强化学习","date":"2023-11-16T02:17:52.527Z","_content":"### 强化学习\n一种试错型学习范式\n随即环境，智能体的动作引起环境的变化\n评价：包含噪声的延迟奖励\n目标：最大化长期累计回报\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102158.png)\n\n#### 状态\n状态是用于决定下一步发生什么的信息\n形式上，状态是一个关于历史信息的表示\n历史是一个状态、动作和奖励组成的序列\n\n#### 环境状态\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102437.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102701.png)\n\n\n完全可观测性：智能体能够直接观测到环境状态$O_t = s_t$\n部分可观测性：智能体间接观察环境$O_t != s_t$\n\n#### 目标\n智能体的目标：最大化其收到的奖励总和\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103205.png)\n\n\n### 马尔可夫决策过程\n理想情况下，一个状态应该总结过去的“经历”，以便保留所有必要的信息，也就是说，它应该具有马尔可夫性：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103615.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104447.png)\n\n#### 策略\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104526.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105033.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105039.png)\n\n#### 贝尔曼等式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105248.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105313.png)\n\n#### 策略改进\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111336.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111525.png)\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/强化学习.md","raw":"---\ntitle: 强化学习\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 深度学习\ndate:\n---\n### 强化学习\n一种试错型学习范式\n随即环境，智能体的动作引起环境的变化\n评价：包含噪声的延迟奖励\n目标：最大化长期累计回报\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102158.png)\n\n#### 状态\n状态是用于决定下一步发生什么的信息\n形式上，状态是一个关于历史信息的表示\n历史是一个状态、动作和奖励组成的序列\n\n#### 环境状态\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102437.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102701.png)\n\n\n完全可观测性：智能体能够直接观测到环境状态$O_t = s_t$\n部分可观测性：智能体间接观察环境$O_t != s_t$\n\n#### 目标\n智能体的目标：最大化其收到的奖励总和\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103205.png)\n\n\n### 马尔可夫决策过程\n理想情况下，一个状态应该总结过去的“经历”，以便保留所有必要的信息，也就是说，它应该具有马尔可夫性：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103615.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104447.png)\n\n#### 策略\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104526.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105033.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105039.png)\n\n#### 贝尔曼等式\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105248.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105313.png)\n\n#### 策略改进\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111336.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111525.png)\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/强化学习","published":1,"updated":"2024-03-02T04:19:43.791Z","_id":"clt9kr12g003xvw8c4iui360g","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h3><p>一种试错型学习范式<br>随即环境，智能体的动作引起环境的变化<br>评价：包含噪声的延迟奖励<br>目标：最大化长期累计回报<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102158.png\" alt=\"image.png\"></p>\n<h4 id=\"状态\"><a href=\"#状态\" class=\"headerlink\" title=\"状态\"></a>状态</h4><p>状态是用于决定下一步发生什么的信息<br>形式上，状态是一个关于历史信息的表示<br>历史是一个状态、动作和奖励组成的序列</p>\n<h4 id=\"环境状态\"><a href=\"#环境状态\" class=\"headerlink\" title=\"环境状态\"></a>环境状态</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102437.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102701.png\" alt=\"image.png\"></p>\n<p>完全可观测性：智能体能够直接观测到环境状态$O_t &#x3D; s_t$<br>部分可观测性：智能体间接观察环境$O_t !&#x3D; s_t$</p>\n<h4 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h4><p>智能体的目标：最大化其收到的奖励总和<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103205.png\" alt=\"image.png\"></p>\n<h3 id=\"马尔可夫决策过程\"><a href=\"#马尔可夫决策过程\" class=\"headerlink\" title=\"马尔可夫决策过程\"></a>马尔可夫决策过程</h3><p>理想情况下，一个状态应该总结过去的“经历”，以便保留所有必要的信息，也就是说，它应该具有马尔可夫性：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103615.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104447.png\" alt=\"image.png\"></p>\n<h4 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104526.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105033.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105039.png\" alt=\"image.png\"></p>\n<h4 id=\"贝尔曼等式\"><a href=\"#贝尔曼等式\" class=\"headerlink\" title=\"贝尔曼等式\"></a>贝尔曼等式</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105248.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105313.png\" alt=\"image.png\"></p>\n<h4 id=\"策略改进\"><a href=\"#策略改进\" class=\"headerlink\" title=\"策略改进\"></a>策略改进</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111336.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111525.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h3><p>一种试错型学习范式<br>随即环境，智能体的动作引起环境的变化<br>评价：包含噪声的延迟奖励<br>目标：最大化长期累计回报<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102158.png\" alt=\"image.png\"></p>\n<h4 id=\"状态\"><a href=\"#状态\" class=\"headerlink\" title=\"状态\"></a>状态</h4><p>状态是用于决定下一步发生什么的信息<br>形式上，状态是一个关于历史信息的表示<br>历史是一个状态、动作和奖励组成的序列</p>\n<h4 id=\"环境状态\"><a href=\"#环境状态\" class=\"headerlink\" title=\"环境状态\"></a>环境状态</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102437.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116102701.png\" alt=\"image.png\"></p>\n<p>完全可观测性：智能体能够直接观测到环境状态$O_t &#x3D; s_t$<br>部分可观测性：智能体间接观察环境$O_t !&#x3D; s_t$</p>\n<h4 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h4><p>智能体的目标：最大化其收到的奖励总和<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103205.png\" alt=\"image.png\"></p>\n<h3 id=\"马尔可夫决策过程\"><a href=\"#马尔可夫决策过程\" class=\"headerlink\" title=\"马尔可夫决策过程\"></a>马尔可夫决策过程</h3><p>理想情况下，一个状态应该总结过去的“经历”，以便保留所有必要的信息，也就是说，它应该具有马尔可夫性：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116103615.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104447.png\" alt=\"image.png\"></p>\n<h4 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116104526.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105033.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105039.png\" alt=\"image.png\"></p>\n<h4 id=\"贝尔曼等式\"><a href=\"#贝尔曼等式\" class=\"headerlink\" title=\"贝尔曼等式\"></a>贝尔曼等式</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105248.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116105313.png\" alt=\"image.png\"></p>\n<h4 id=\"策略改进\"><a href=\"#策略改进\" class=\"headerlink\" title=\"策略改进\"></a>策略改进</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111336.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231116111525.png\" alt=\"image.png\"></p>\n"},{"title":"实验 用numpy搭建全连接神经网络用于手写数字识别","date":"2023-10-10T12:36:33.957Z","_content":"[【代码+原理讲解】使用Numpy实现一个简单的四层全连接神经网络（手写数字识别，mnist数据集，正确率98.58%） - 知乎](https://zhuanlan.zhihu.com/p/377634925)\n[入门讲解：使用numpy实现简单的神经网络（BP算法）-CSDN博客](https://blog.csdn.net/weixin_44023658/article/details/105694079)\n结合代码和公式对全连接神经网络的实现进行分析\n\n### 数据处理\n```\n# 标准化处理  \nif normalize:  \n    for _ in ('train_img', 'test_img'):  \n        dataset[_] = dataset[_].astype(np.float32) / 255.0  \n# one_hot_label处理  \nif one_hot_label:  \n    for _ in ('train_label', 'test_label'):  \n        t = np.zeros((dataset[_].size, 10))  \n        for idx, row in enumerate(t):  \n            row[dataset[_][idx]] = 1  \n        dataset[_] = t  \n# 展平处理  \nif flatten:  \n    for _ in ('train_img', 'test_img'):  \n        dataset[_] = dataset[_].reshape(-1, 784)  \n# 划分验证集  \nif val_data:  \n    x_val_data, x_test_data = np.split(dataset['test_img'], 2)  \n    y_val_data, y_test_data = np.split(dataset['test_label'], 2)  \n    return dataset['train_img'], dataset['train_label'], x_val_data, y_val_data, x_test_data, y_test_data\n```\n\n-  标准化处理：将数据归一化\n\n- one hot处理：将数据处理成one hot形式，即维度扩充为与数据类别相同，数据为哪个类别，其相应维度上的值为1，否则为0\n\n- 展平处理：将28\\*28的图像转换成一个维度上784的大小\n\n- 划分验证集：原数据集为训练集：测试集60000：10000，把测试集中的5000条作为验证集\n\n### 全连接层\n```\nclass Net(object):  \n    def __init__(self, num_input, num_output):  \n        self.num_input = num_input  \n        self.num_output = num_output  \n        self.w = np.random.normal(loc=0.0, scale=0.01, size=(self.num_input, self.num_output))  # 随机初始化参数 假设为(n, m)  \n        self.bias = np.zeros([1, self.num_output])  # 初始化为0  (1, m)  \n        self.input_data = np.zeros(0)  \n        self.output_data = np.zeros(0)  \n        self.grad_w = np.zeros(0)  \n        self.grad_b = np.zeros(0)  \n  \n    def forward(self, input_data):  \n        self.input_data = input_data  # 假设input_data = (1, n)  \n        self.output_data = np.matmul(self.input_data, self.w) + self.bias  # (1, n) * (n, m) = (1, m) m为下一层的输入维度  \n        return self.output_data  \n  \n    def backward(self, grad):  \n        self.grad_w = np.dot(self.input_data.T, grad)  # (n, 1) * (1, m) = (n, m)  \n        self.grad_b = np.sum(grad, axis=0)  \n        next_grad = np.dot(grad, self.w.T)  # (1, m) * (m, n) = (1, n)  \n        return next_grad  \n  \n    def backward_with_l2(self, grad, lamb, batch_size):  \n        self.grad_w = np.dot(self.input_data.T, grad) + (lamb / batch_size) * self.w  \n        self.grad_b = np.sum(grad, axis=0)  \n        next_grad = np.dot(grad, self.w.T)  \n        return next_grad  \n  \n    def update(self, lr):  \n        self.w = self.w - lr * self.grad_w  \n        self.bias = self.bias - lr * self.grad_b\n```\n\n定义一个类，在其中实现的功能有：\n- 初始化：在创建一层全连接层时，需要初始化w和b，w为（m，n）其中m为输入数据的维度，n为下一层的输入维度，b为（1，n）.公式如下:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231017235953.png)\n\n- 前馈函数：在前馈函数中实现上面的公式\n\n- 反向传播：需要根据损失更新参数w和b的值，因此分别对w和b求偏导\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018001633.png)\n\n前一层的梯度可以根据后一层的梯度得到:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001328.png)\n根据上面这个公式，可以发现，**前一层可以使用后一层的误差项来得到自己的误差项**，而不需要从最后用链式法则进行推导。因此称为反向**传播**。\n推导过程如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001444.png)\n\n\n- 更新参数：在原来的参数上减去梯度方向得到新的参数，实验中往往需要学习率来控制更新的程度\n\n### 激活函数\n```\nclass ReLU(object):  \n  \n    def __init__(self):  \n        self.input_data = np.zeros(0)  \n  \n    def forward(self, input_data):  \n        self.input_data = input_data  \n        output_data = np.maximum(0, input_data)  # (1, n)  \n        return output_data  \n  \n    def backward(self, grad):  \n        next_grad = grad  # (1, n) * (1, n) 逐元素相乘  \n        next_grad[self.input_data < 0] = 0  \n        return next_grad\n```\n激活函数同样需要两个，一个实现前向传播，一个实现反向传播\n\n### Softmax+交叉熵损失\n```\nclass Softmax(object):  \n    def __init__(self):  \n        self.prob = np.zeros(0)  \n        self.batch_size = []  \n        self.label = []  \n  \n    def forward(self, input_data):  \n        input_max = np.max(input_data, axis=1, keepdims=True)  \n        input_exp = np.exp(input_data - input_max)  \n        self.prob = input_exp / np.sum(input_exp, axis=1, keepdims=True)  \n        return self.prob  \n  \n    def get_loss(self, label):  # 计算损失  \n        self.label = label  \n        self.batch_size = self.prob.shape[0]  \n        loss = -np.sum(label * np.log(self.prob + 1e-7)) / self.batch_size  \n        return loss  \n  \n    def backward(self):  \n        grad = (self.prob - self.label) / self.batch_size  \n        return grad\n```\n在Softmax中除了实现前向和后向传播外，添加了用交叉熵计算损失的函数，这是因为在softmax后加交叉熵，反向传播的公式会更简便。\n\n### MSE损失（Mean squared error均方误差）\n```\ndef MSE_loss(self, y_pre, y):  \n    loss = 0.5 * np.sum((y_pre - y) ** 2) / batch_size  \n    grad = (y_pre - y) / batch_size  \n    return loss, grad\n```\n\n### 整体的传播\n```\ndef forward(self, input_data):  # 神经网络的前向传播  \n    h1 = self.fc1.forward(input_data)  \n    h1 = self.relu1.forward(h1)  \n    # h1 = self.sigmoid1.forward(h1)  \n    h2 = self.fc2.forward(h1)  \n    h2 = self.relu2.forward(h2)  \n    # h2 = self.sigmoid2.forward(h2)  \n    h3 = self.fc3.forward(h2)  \n    # prob = self.softmax.forward(h1)  \n    return h3  \n  \ndef backward(self, y_pre, y):  # 神经网络的反向传播  \n    _, grad = self.MSE_loss(y_pre, y)  \n    # grad = self.softmax.backward()  \n    dh3 = self.fc3.backward(grad)  \n    dh2 = self.relu2.backward(dh3)  \n    # dh2 = self.sigmoid2.backward(dh3)  \n    dh2 = self.fc2.backward(dh2)  \n    dh1 = self.relu1.backward(dh2)  \n    # dh1 = self.sigmoid1.backward(dh2)  \n    dh1 = self.fc1.backward(dh1)\n    \ndef update(self, lr):  \n    self.fc1.update(lr)  \n    self.fc2.update(lr)  \n    self.fc3.update(lr)\n```\n### 实验结果\n使用mini-batch GD，使用效果较好的模型参数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002745.png)\n得到实验结果如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002819.png)\n最终测试集准确率稳定在98%以上。\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/实验 用numpy搭建全连接神经网络用于手写数字识别.md","raw":"---\ntitle: 实验 用numpy搭建全连接神经网络用于手写数字识别\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - Python\n  - 神经网络\ndate:\n---\n[【代码+原理讲解】使用Numpy实现一个简单的四层全连接神经网络（手写数字识别，mnist数据集，正确率98.58%） - 知乎](https://zhuanlan.zhihu.com/p/377634925)\n[入门讲解：使用numpy实现简单的神经网络（BP算法）-CSDN博客](https://blog.csdn.net/weixin_44023658/article/details/105694079)\n结合代码和公式对全连接神经网络的实现进行分析\n\n### 数据处理\n```\n# 标准化处理  \nif normalize:  \n    for _ in ('train_img', 'test_img'):  \n        dataset[_] = dataset[_].astype(np.float32) / 255.0  \n# one_hot_label处理  \nif one_hot_label:  \n    for _ in ('train_label', 'test_label'):  \n        t = np.zeros((dataset[_].size, 10))  \n        for idx, row in enumerate(t):  \n            row[dataset[_][idx]] = 1  \n        dataset[_] = t  \n# 展平处理  \nif flatten:  \n    for _ in ('train_img', 'test_img'):  \n        dataset[_] = dataset[_].reshape(-1, 784)  \n# 划分验证集  \nif val_data:  \n    x_val_data, x_test_data = np.split(dataset['test_img'], 2)  \n    y_val_data, y_test_data = np.split(dataset['test_label'], 2)  \n    return dataset['train_img'], dataset['train_label'], x_val_data, y_val_data, x_test_data, y_test_data\n```\n\n-  标准化处理：将数据归一化\n\n- one hot处理：将数据处理成one hot形式，即维度扩充为与数据类别相同，数据为哪个类别，其相应维度上的值为1，否则为0\n\n- 展平处理：将28\\*28的图像转换成一个维度上784的大小\n\n- 划分验证集：原数据集为训练集：测试集60000：10000，把测试集中的5000条作为验证集\n\n### 全连接层\n```\nclass Net(object):  \n    def __init__(self, num_input, num_output):  \n        self.num_input = num_input  \n        self.num_output = num_output  \n        self.w = np.random.normal(loc=0.0, scale=0.01, size=(self.num_input, self.num_output))  # 随机初始化参数 假设为(n, m)  \n        self.bias = np.zeros([1, self.num_output])  # 初始化为0  (1, m)  \n        self.input_data = np.zeros(0)  \n        self.output_data = np.zeros(0)  \n        self.grad_w = np.zeros(0)  \n        self.grad_b = np.zeros(0)  \n  \n    def forward(self, input_data):  \n        self.input_data = input_data  # 假设input_data = (1, n)  \n        self.output_data = np.matmul(self.input_data, self.w) + self.bias  # (1, n) * (n, m) = (1, m) m为下一层的输入维度  \n        return self.output_data  \n  \n    def backward(self, grad):  \n        self.grad_w = np.dot(self.input_data.T, grad)  # (n, 1) * (1, m) = (n, m)  \n        self.grad_b = np.sum(grad, axis=0)  \n        next_grad = np.dot(grad, self.w.T)  # (1, m) * (m, n) = (1, n)  \n        return next_grad  \n  \n    def backward_with_l2(self, grad, lamb, batch_size):  \n        self.grad_w = np.dot(self.input_data.T, grad) + (lamb / batch_size) * self.w  \n        self.grad_b = np.sum(grad, axis=0)  \n        next_grad = np.dot(grad, self.w.T)  \n        return next_grad  \n  \n    def update(self, lr):  \n        self.w = self.w - lr * self.grad_w  \n        self.bias = self.bias - lr * self.grad_b\n```\n\n定义一个类，在其中实现的功能有：\n- 初始化：在创建一层全连接层时，需要初始化w和b，w为（m，n）其中m为输入数据的维度，n为下一层的输入维度，b为（1，n）.公式如下:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231017235953.png)\n\n- 前馈函数：在前馈函数中实现上面的公式\n\n- 反向传播：需要根据损失更新参数w和b的值，因此分别对w和b求偏导\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018001633.png)\n\n前一层的梯度可以根据后一层的梯度得到:\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001328.png)\n根据上面这个公式，可以发现，**前一层可以使用后一层的误差项来得到自己的误差项**，而不需要从最后用链式法则进行推导。因此称为反向**传播**。\n推导过程如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001444.png)\n\n\n- 更新参数：在原来的参数上减去梯度方向得到新的参数，实验中往往需要学习率来控制更新的程度\n\n### 激活函数\n```\nclass ReLU(object):  \n  \n    def __init__(self):  \n        self.input_data = np.zeros(0)  \n  \n    def forward(self, input_data):  \n        self.input_data = input_data  \n        output_data = np.maximum(0, input_data)  # (1, n)  \n        return output_data  \n  \n    def backward(self, grad):  \n        next_grad = grad  # (1, n) * (1, n) 逐元素相乘  \n        next_grad[self.input_data < 0] = 0  \n        return next_grad\n```\n激活函数同样需要两个，一个实现前向传播，一个实现反向传播\n\n### Softmax+交叉熵损失\n```\nclass Softmax(object):  \n    def __init__(self):  \n        self.prob = np.zeros(0)  \n        self.batch_size = []  \n        self.label = []  \n  \n    def forward(self, input_data):  \n        input_max = np.max(input_data, axis=1, keepdims=True)  \n        input_exp = np.exp(input_data - input_max)  \n        self.prob = input_exp / np.sum(input_exp, axis=1, keepdims=True)  \n        return self.prob  \n  \n    def get_loss(self, label):  # 计算损失  \n        self.label = label  \n        self.batch_size = self.prob.shape[0]  \n        loss = -np.sum(label * np.log(self.prob + 1e-7)) / self.batch_size  \n        return loss  \n  \n    def backward(self):  \n        grad = (self.prob - self.label) / self.batch_size  \n        return grad\n```\n在Softmax中除了实现前向和后向传播外，添加了用交叉熵计算损失的函数，这是因为在softmax后加交叉熵，反向传播的公式会更简便。\n\n### MSE损失（Mean squared error均方误差）\n```\ndef MSE_loss(self, y_pre, y):  \n    loss = 0.5 * np.sum((y_pre - y) ** 2) / batch_size  \n    grad = (y_pre - y) / batch_size  \n    return loss, grad\n```\n\n### 整体的传播\n```\ndef forward(self, input_data):  # 神经网络的前向传播  \n    h1 = self.fc1.forward(input_data)  \n    h1 = self.relu1.forward(h1)  \n    # h1 = self.sigmoid1.forward(h1)  \n    h2 = self.fc2.forward(h1)  \n    h2 = self.relu2.forward(h2)  \n    # h2 = self.sigmoid2.forward(h2)  \n    h3 = self.fc3.forward(h2)  \n    # prob = self.softmax.forward(h1)  \n    return h3  \n  \ndef backward(self, y_pre, y):  # 神经网络的反向传播  \n    _, grad = self.MSE_loss(y_pre, y)  \n    # grad = self.softmax.backward()  \n    dh3 = self.fc3.backward(grad)  \n    dh2 = self.relu2.backward(dh3)  \n    # dh2 = self.sigmoid2.backward(dh3)  \n    dh2 = self.fc2.backward(dh2)  \n    dh1 = self.relu1.backward(dh2)  \n    # dh1 = self.sigmoid1.backward(dh2)  \n    dh1 = self.fc1.backward(dh1)\n    \ndef update(self, lr):  \n    self.fc1.update(lr)  \n    self.fc2.update(lr)  \n    self.fc3.update(lr)\n```\n### 实验结果\n使用mini-batch GD，使用效果较好的模型参数\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002745.png)\n得到实验结果如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002819.png)\n最终测试集准确率稳定在98%以上。\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/实验 用numpy搭建全连接神经网络用于手写数字识别","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12h003zvw8c8gkadmuy","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><a href=\"https://zhuanlan.zhihu.com/p/377634925\">【代码+原理讲解】使用Numpy实现一个简单的四层全连接神经网络（手写数字识别，mnist数据集，正确率98.58%） - 知乎</a><br><a href=\"https://blog.csdn.net/weixin_44023658/article/details/105694079\">入门讲解：使用numpy实现简单的神经网络（BP算法）-CSDN博客</a><br>结合代码和公式对全连接神经网络的实现进行分析</p>\n<h3 id=\"数据处理\"><a href=\"#数据处理\" class=\"headerlink\" title=\"数据处理\"></a>数据处理</h3><figure class=\"highlight prolog\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs prolog\"># 标准化处理  <br>if normalize:  <br>    for <span class=\"hljs-symbol\">_</span> in (<span class=\"hljs-string\">&#x27;train_img&#x27;</span>, <span class=\"hljs-string\">&#x27;test_img&#x27;</span>):  <br>        dataset[<span class=\"hljs-symbol\">_</span>] = dataset[<span class=\"hljs-symbol\">_</span>].astype(np.float32) / <span class=\"hljs-number\">255.0</span>  <br># one_hot_label处理  <br>if one_hot_label:  <br>    for <span class=\"hljs-symbol\">_</span> in (<span class=\"hljs-string\">&#x27;train_label&#x27;</span>, <span class=\"hljs-string\">&#x27;test_label&#x27;</span>):  <br>        t = np.zeros((dataset[<span class=\"hljs-symbol\">_</span>].size, <span class=\"hljs-number\">10</span>))  <br>        for idx, row in enumerate(t):  <br>            row[dataset[<span class=\"hljs-symbol\">_</span>][idx]] = <span class=\"hljs-number\">1</span>  <br>        dataset[<span class=\"hljs-symbol\">_</span>] = t  <br># 展平处理  <br>if flatten:  <br>    for <span class=\"hljs-symbol\">_</span> in (<span class=\"hljs-string\">&#x27;train_img&#x27;</span>, <span class=\"hljs-string\">&#x27;test_img&#x27;</span>):  <br>        dataset[<span class=\"hljs-symbol\">_</span>] = dataset[<span class=\"hljs-symbol\">_</span>].reshape(<span class=\"hljs-number\">-1</span>, <span class=\"hljs-number\">784</span>)  <br># 划分验证集  <br>if val_data:  <br>    x_val_data, x_test_data = np.split(dataset[<span class=\"hljs-string\">&#x27;test_img&#x27;</span>], <span class=\"hljs-number\">2</span>)  <br>    y_val_data, y_test_data = np.split(dataset[<span class=\"hljs-string\">&#x27;test_label&#x27;</span>], <span class=\"hljs-number\">2</span>)  <br>    return dataset[<span class=\"hljs-string\">&#x27;train_img&#x27;</span>], dataset[<span class=\"hljs-string\">&#x27;train_label&#x27;</span>], x_val_data, y_val_data, x_test_data, y_test_data<br></code></pre></td></tr></table></figure>\n\n<ul>\n<li><p>标准化处理：将数据归一化</p>\n</li>\n<li><p>one hot处理：将数据处理成one hot形式，即维度扩充为与数据类别相同，数据为哪个类别，其相应维度上的值为1，否则为0</p>\n</li>\n<li><p>展平处理：将28*28的图像转换成一个维度上784的大小</p>\n</li>\n<li><p>划分验证集：原数据集为训练集：测试集60000：10000，把测试集中的5000条作为验证集</p>\n</li>\n</ul>\n<h3 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Net</span>(<span class=\"hljs-title class_ inherited__\">object</span>):  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, num_input, num_output</span>):  <br>        self.num_input = num_input  <br>        self.num_output = num_output  <br>        self.w = np.random.normal(loc=<span class=\"hljs-number\">0.0</span>, scale=<span class=\"hljs-number\">0.01</span>, size=(self.num_input, self.num_output))  <span class=\"hljs-comment\"># 随机初始化参数 假设为(n, m)  </span><br>        self.bias = np.zeros([<span class=\"hljs-number\">1</span>, self.num_output])  <span class=\"hljs-comment\"># 初始化为0  (1, m)  </span><br>        self.input_data = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.output_data = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.grad_w = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.grad_b = np.zeros(<span class=\"hljs-number\">0</span>)  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, input_data</span>):  <br>        self.input_data = input_data  <span class=\"hljs-comment\"># 假设input_data = (1, n)  </span><br>        self.output_data = np.matmul(self.input_data, self.w) + self.bias  <span class=\"hljs-comment\"># (1, n) * (n, m) = (1, m) m为下一层的输入维度  </span><br>        <span class=\"hljs-keyword\">return</span> self.output_data  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self, grad</span>):  <br>        self.grad_w = np.dot(self.input_data.T, grad)  <span class=\"hljs-comment\"># (n, 1) * (1, m) = (n, m)  </span><br>        self.grad_b = np.<span class=\"hljs-built_in\">sum</span>(grad, axis=<span class=\"hljs-number\">0</span>)  <br>        next_grad = np.dot(grad, self.w.T)  <span class=\"hljs-comment\"># (1, m) * (m, n) = (1, n)  </span><br>        <span class=\"hljs-keyword\">return</span> next_grad  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward_with_l2</span>(<span class=\"hljs-params\">self, grad, lamb, batch_size</span>):  <br>        self.grad_w = np.dot(self.input_data.T, grad) + (lamb / batch_size) * self.w  <br>        self.grad_b = np.<span class=\"hljs-built_in\">sum</span>(grad, axis=<span class=\"hljs-number\">0</span>)  <br>        next_grad = np.dot(grad, self.w.T)  <br>        <span class=\"hljs-keyword\">return</span> next_grad  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">update</span>(<span class=\"hljs-params\">self, lr</span>):  <br>        self.w = self.w - lr * self.grad_w  <br>        self.bias = self.bias - lr * self.grad_b<br></code></pre></td></tr></table></figure>\n\n<p>定义一个类，在其中实现的功能有：</p>\n<ul>\n<li><p>初始化：在创建一层全连接层时，需要初始化w和b，w为（m，n）其中m为输入数据的维度，n为下一层的输入维度，b为（1，n）.公式如下:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231017235953.png\" alt=\"image.png\"></p>\n</li>\n<li><p>前馈函数：在前馈函数中实现上面的公式</p>\n</li>\n<li><p>反向传播：需要根据损失更新参数w和b的值，因此分别对w和b求偏导<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018001633.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<p>前一层的梯度可以根据后一层的梯度得到:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001328.png\" alt=\"image.png\"><br>根据上面这个公式，可以发现，<strong>前一层可以使用后一层的误差项来得到自己的误差项</strong>，而不需要从最后用链式法则进行推导。因此称为反向<strong>传播</strong>。<br>推导过程如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001444.png\" alt=\"image.png\"></p>\n<ul>\n<li>更新参数：在原来的参数上减去梯度方向得到新的参数，实验中往往需要学习率来控制更新的程度</li>\n</ul>\n<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><figure class=\"highlight ruby\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ruby\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Re</span>LU(object):  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span></span>):  <br>        <span class=\"hljs-variable language_\">self</span>.input_data = np.zeros(<span class=\"hljs-number\">0</span>)  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span>, input_data</span>):  <br>        <span class=\"hljs-variable language_\">self</span>.input_data = input_data  <br>        output_data = np.maximum(<span class=\"hljs-number\">0</span>, input_data)  <span class=\"hljs-comment\"># (1, n)  </span><br>        <span class=\"hljs-keyword\">return</span> output_data  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span>, grad</span>):  <br>        next_grad = grad  <span class=\"hljs-comment\"># (1, n) * (1, n) 逐元素相乘  </span><br>        next_grad[<span class=\"hljs-variable language_\">self</span>.input_data &lt; <span class=\"hljs-number\">0</span>] = <span class=\"hljs-number\">0</span>  <br>        <span class=\"hljs-keyword\">return</span> next_grad<br></code></pre></td></tr></table></figure>\n<p>激活函数同样需要两个，一个实现前向传播，一个实现反向传播</p>\n<h3 id=\"Softmax-交叉熵损失\"><a href=\"#Softmax-交叉熵损失\" class=\"headerlink\" title=\"Softmax+交叉熵损失\"></a>Softmax+交叉熵损失</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Softmax</span>(<span class=\"hljs-title class_ inherited__\">object</span>):  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self</span>):  <br>        self.prob = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.batch_size = []  <br>        self.label = []  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, input_data</span>):  <br>        input_max = np.<span class=\"hljs-built_in\">max</span>(input_data, axis=<span class=\"hljs-number\">1</span>, keepdims=<span class=\"hljs-literal\">True</span>)  <br>        input_exp = np.exp(input_data - input_max)  <br>        self.prob = input_exp / np.<span class=\"hljs-built_in\">sum</span>(input_exp, axis=<span class=\"hljs-number\">1</span>, keepdims=<span class=\"hljs-literal\">True</span>)  <br>        <span class=\"hljs-keyword\">return</span> self.prob  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_loss</span>(<span class=\"hljs-params\">self, label</span>):  <span class=\"hljs-comment\"># 计算损失  </span><br>        self.label = label  <br>        self.batch_size = self.prob.shape[<span class=\"hljs-number\">0</span>]  <br>        loss = -np.<span class=\"hljs-built_in\">sum</span>(label * np.log(self.prob + <span class=\"hljs-number\">1e-7</span>)) / self.batch_size  <br>        <span class=\"hljs-keyword\">return</span> loss  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self</span>):  <br>        grad = (self.prob - self.label) / self.batch_size  <br>        <span class=\"hljs-keyword\">return</span> grad<br></code></pre></td></tr></table></figure>\n<p>在Softmax中除了实现前向和后向传播外，添加了用交叉熵计算损失的函数，这是因为在softmax后加交叉熵，反向传播的公式会更简便。</p>\n<h3 id=\"MSE损失（Mean-squared-error均方误差）\"><a href=\"#MSE损失（Mean-squared-error均方误差）\" class=\"headerlink\" title=\"MSE损失（Mean squared error均方误差）\"></a>MSE损失（Mean squared error均方误差）</h3><figure class=\"highlight ruby\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ruby\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">MSE_loss</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span>, y_pre, y</span>):  <br>    loss = <span class=\"hljs-number\">0.5</span> * np.sum((y_pre - y) ** <span class=\"hljs-number\">2</span>) / batch_size  <br>    grad = (y_pre - y) / batch_size  <br>    <span class=\"hljs-keyword\">return</span> loss, grad<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"整体的传播\"><a href=\"#整体的传播\" class=\"headerlink\" title=\"整体的传播\"></a>整体的传播</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, input_data</span>):  <span class=\"hljs-comment\"># 神经网络的前向传播  </span><br>    h1 = self.fc1.forward(input_data)  <br>    h1 = self.relu1.forward(h1)  <br>    <span class=\"hljs-comment\"># h1 = self.sigmoid1.forward(h1)  </span><br>    h2 = self.fc2.forward(h1)  <br>    h2 = self.relu2.forward(h2)  <br>    <span class=\"hljs-comment\"># h2 = self.sigmoid2.forward(h2)  </span><br>    h3 = self.fc3.forward(h2)  <br>    <span class=\"hljs-comment\"># prob = self.softmax.forward(h1)  </span><br>    <span class=\"hljs-keyword\">return</span> h3  <br>  <br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self, y_pre, y</span>):  <span class=\"hljs-comment\"># 神经网络的反向传播  </span><br>    _, grad = self.MSE_loss(y_pre, y)  <br>    <span class=\"hljs-comment\"># grad = self.softmax.backward()  </span><br>    dh3 = self.fc3.backward(grad)  <br>    dh2 = self.relu2.backward(dh3)  <br>    <span class=\"hljs-comment\"># dh2 = self.sigmoid2.backward(dh3)  </span><br>    dh2 = self.fc2.backward(dh2)  <br>    dh1 = self.relu1.backward(dh2)  <br>    <span class=\"hljs-comment\"># dh1 = self.sigmoid1.backward(dh2)  </span><br>    dh1 = self.fc1.backward(dh1)<br>    <br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">update</span>(<span class=\"hljs-params\">self, lr</span>):  <br>    self.fc1.update(lr)  <br>    self.fc2.update(lr)  <br>    self.fc3.update(lr)<br></code></pre></td></tr></table></figure>\n<h3 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h3><p>使用mini-batch GD，使用效果较好的模型参数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002745.png\" alt=\"image.png\"><br>得到实验结果如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002819.png\" alt=\"image.png\"><br>最终测试集准确率稳定在98%以上。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://zhuanlan.zhihu.com/p/377634925\">【代码+原理讲解】使用Numpy实现一个简单的四层全连接神经网络（手写数字识别，mnist数据集，正确率98.58%） - 知乎</a><br><a href=\"https://blog.csdn.net/weixin_44023658/article/details/105694079\">入门讲解：使用numpy实现简单的神经网络（BP算法）-CSDN博客</a><br>结合代码和公式对全连接神经网络的实现进行分析</p>\n<h3 id=\"数据处理\"><a href=\"#数据处理\" class=\"headerlink\" title=\"数据处理\"></a>数据处理</h3><figure class=\"highlight prolog\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs prolog\"># 标准化处理  <br>if normalize:  <br>    for <span class=\"hljs-symbol\">_</span> in (<span class=\"hljs-string\">&#x27;train_img&#x27;</span>, <span class=\"hljs-string\">&#x27;test_img&#x27;</span>):  <br>        dataset[<span class=\"hljs-symbol\">_</span>] = dataset[<span class=\"hljs-symbol\">_</span>].astype(np.float32) / <span class=\"hljs-number\">255.0</span>  <br># one_hot_label处理  <br>if one_hot_label:  <br>    for <span class=\"hljs-symbol\">_</span> in (<span class=\"hljs-string\">&#x27;train_label&#x27;</span>, <span class=\"hljs-string\">&#x27;test_label&#x27;</span>):  <br>        t = np.zeros((dataset[<span class=\"hljs-symbol\">_</span>].size, <span class=\"hljs-number\">10</span>))  <br>        for idx, row in enumerate(t):  <br>            row[dataset[<span class=\"hljs-symbol\">_</span>][idx]] = <span class=\"hljs-number\">1</span>  <br>        dataset[<span class=\"hljs-symbol\">_</span>] = t  <br># 展平处理  <br>if flatten:  <br>    for <span class=\"hljs-symbol\">_</span> in (<span class=\"hljs-string\">&#x27;train_img&#x27;</span>, <span class=\"hljs-string\">&#x27;test_img&#x27;</span>):  <br>        dataset[<span class=\"hljs-symbol\">_</span>] = dataset[<span class=\"hljs-symbol\">_</span>].reshape(<span class=\"hljs-number\">-1</span>, <span class=\"hljs-number\">784</span>)  <br># 划分验证集  <br>if val_data:  <br>    x_val_data, x_test_data = np.split(dataset[<span class=\"hljs-string\">&#x27;test_img&#x27;</span>], <span class=\"hljs-number\">2</span>)  <br>    y_val_data, y_test_data = np.split(dataset[<span class=\"hljs-string\">&#x27;test_label&#x27;</span>], <span class=\"hljs-number\">2</span>)  <br>    return dataset[<span class=\"hljs-string\">&#x27;train_img&#x27;</span>], dataset[<span class=\"hljs-string\">&#x27;train_label&#x27;</span>], x_val_data, y_val_data, x_test_data, y_test_data<br></code></pre></td></tr></table></figure>\n\n<ul>\n<li><p>标准化处理：将数据归一化</p>\n</li>\n<li><p>one hot处理：将数据处理成one hot形式，即维度扩充为与数据类别相同，数据为哪个类别，其相应维度上的值为1，否则为0</p>\n</li>\n<li><p>展平处理：将28*28的图像转换成一个维度上784的大小</p>\n</li>\n<li><p>划分验证集：原数据集为训练集：测试集60000：10000，把测试集中的5000条作为验证集</p>\n</li>\n</ul>\n<h3 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Net</span>(<span class=\"hljs-title class_ inherited__\">object</span>):  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, num_input, num_output</span>):  <br>        self.num_input = num_input  <br>        self.num_output = num_output  <br>        self.w = np.random.normal(loc=<span class=\"hljs-number\">0.0</span>, scale=<span class=\"hljs-number\">0.01</span>, size=(self.num_input, self.num_output))  <span class=\"hljs-comment\"># 随机初始化参数 假设为(n, m)  </span><br>        self.bias = np.zeros([<span class=\"hljs-number\">1</span>, self.num_output])  <span class=\"hljs-comment\"># 初始化为0  (1, m)  </span><br>        self.input_data = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.output_data = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.grad_w = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.grad_b = np.zeros(<span class=\"hljs-number\">0</span>)  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, input_data</span>):  <br>        self.input_data = input_data  <span class=\"hljs-comment\"># 假设input_data = (1, n)  </span><br>        self.output_data = np.matmul(self.input_data, self.w) + self.bias  <span class=\"hljs-comment\"># (1, n) * (n, m) = (1, m) m为下一层的输入维度  </span><br>        <span class=\"hljs-keyword\">return</span> self.output_data  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self, grad</span>):  <br>        self.grad_w = np.dot(self.input_data.T, grad)  <span class=\"hljs-comment\"># (n, 1) * (1, m) = (n, m)  </span><br>        self.grad_b = np.<span class=\"hljs-built_in\">sum</span>(grad, axis=<span class=\"hljs-number\">0</span>)  <br>        next_grad = np.dot(grad, self.w.T)  <span class=\"hljs-comment\"># (1, m) * (m, n) = (1, n)  </span><br>        <span class=\"hljs-keyword\">return</span> next_grad  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward_with_l2</span>(<span class=\"hljs-params\">self, grad, lamb, batch_size</span>):  <br>        self.grad_w = np.dot(self.input_data.T, grad) + (lamb / batch_size) * self.w  <br>        self.grad_b = np.<span class=\"hljs-built_in\">sum</span>(grad, axis=<span class=\"hljs-number\">0</span>)  <br>        next_grad = np.dot(grad, self.w.T)  <br>        <span class=\"hljs-keyword\">return</span> next_grad  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">update</span>(<span class=\"hljs-params\">self, lr</span>):  <br>        self.w = self.w - lr * self.grad_w  <br>        self.bias = self.bias - lr * self.grad_b<br></code></pre></td></tr></table></figure>\n\n<p>定义一个类，在其中实现的功能有：</p>\n<ul>\n<li><p>初始化：在创建一层全连接层时，需要初始化w和b，w为（m，n）其中m为输入数据的维度，n为下一层的输入维度，b为（1，n）.公式如下:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231017235953.png\" alt=\"image.png\"></p>\n</li>\n<li><p>前馈函数：在前馈函数中实现上面的公式</p>\n</li>\n<li><p>反向传播：需要根据损失更新参数w和b的值，因此分别对w和b求偏导<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231018001633.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<p>前一层的梯度可以根据后一层的梯度得到:<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001328.png\" alt=\"image.png\"><br>根据上面这个公式，可以发现，<strong>前一层可以使用后一层的误差项来得到自己的误差项</strong>，而不需要从最后用链式法则进行推导。因此称为反向<strong>传播</strong>。<br>推导过程如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019001444.png\" alt=\"image.png\"></p>\n<ul>\n<li>更新参数：在原来的参数上减去梯度方向得到新的参数，实验中往往需要学习率来控制更新的程度</li>\n</ul>\n<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><figure class=\"highlight ruby\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ruby\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Re</span>LU(object):  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span></span>):  <br>        <span class=\"hljs-variable language_\">self</span>.input_data = np.zeros(<span class=\"hljs-number\">0</span>)  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span>, input_data</span>):  <br>        <span class=\"hljs-variable language_\">self</span>.input_data = input_data  <br>        output_data = np.maximum(<span class=\"hljs-number\">0</span>, input_data)  <span class=\"hljs-comment\"># (1, n)  </span><br>        <span class=\"hljs-keyword\">return</span> output_data  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span>, grad</span>):  <br>        next_grad = grad  <span class=\"hljs-comment\"># (1, n) * (1, n) 逐元素相乘  </span><br>        next_grad[<span class=\"hljs-variable language_\">self</span>.input_data &lt; <span class=\"hljs-number\">0</span>] = <span class=\"hljs-number\">0</span>  <br>        <span class=\"hljs-keyword\">return</span> next_grad<br></code></pre></td></tr></table></figure>\n<p>激活函数同样需要两个，一个实现前向传播，一个实现反向传播</p>\n<h3 id=\"Softmax-交叉熵损失\"><a href=\"#Softmax-交叉熵损失\" class=\"headerlink\" title=\"Softmax+交叉熵损失\"></a>Softmax+交叉熵损失</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Softmax</span>(<span class=\"hljs-title class_ inherited__\">object</span>):  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self</span>):  <br>        self.prob = np.zeros(<span class=\"hljs-number\">0</span>)  <br>        self.batch_size = []  <br>        self.label = []  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, input_data</span>):  <br>        input_max = np.<span class=\"hljs-built_in\">max</span>(input_data, axis=<span class=\"hljs-number\">1</span>, keepdims=<span class=\"hljs-literal\">True</span>)  <br>        input_exp = np.exp(input_data - input_max)  <br>        self.prob = input_exp / np.<span class=\"hljs-built_in\">sum</span>(input_exp, axis=<span class=\"hljs-number\">1</span>, keepdims=<span class=\"hljs-literal\">True</span>)  <br>        <span class=\"hljs-keyword\">return</span> self.prob  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_loss</span>(<span class=\"hljs-params\">self, label</span>):  <span class=\"hljs-comment\"># 计算损失  </span><br>        self.label = label  <br>        self.batch_size = self.prob.shape[<span class=\"hljs-number\">0</span>]  <br>        loss = -np.<span class=\"hljs-built_in\">sum</span>(label * np.log(self.prob + <span class=\"hljs-number\">1e-7</span>)) / self.batch_size  <br>        <span class=\"hljs-keyword\">return</span> loss  <br>  <br>    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self</span>):  <br>        grad = (self.prob - self.label) / self.batch_size  <br>        <span class=\"hljs-keyword\">return</span> grad<br></code></pre></td></tr></table></figure>\n<p>在Softmax中除了实现前向和后向传播外，添加了用交叉熵计算损失的函数，这是因为在softmax后加交叉熵，反向传播的公式会更简便。</p>\n<h3 id=\"MSE损失（Mean-squared-error均方误差）\"><a href=\"#MSE损失（Mean-squared-error均方误差）\" class=\"headerlink\" title=\"MSE损失（Mean squared error均方误差）\"></a>MSE损失（Mean squared error均方误差）</h3><figure class=\"highlight ruby\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs ruby\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">MSE_loss</span>(<span class=\"hljs-params\"><span class=\"hljs-variable language_\">self</span>, y_pre, y</span>):  <br>    loss = <span class=\"hljs-number\">0.5</span> * np.sum((y_pre - y) ** <span class=\"hljs-number\">2</span>) / batch_size  <br>    grad = (y_pre - y) / batch_size  <br>    <span class=\"hljs-keyword\">return</span> loss, grad<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"整体的传播\"><a href=\"#整体的传播\" class=\"headerlink\" title=\"整体的传播\"></a>整体的传播</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, input_data</span>):  <span class=\"hljs-comment\"># 神经网络的前向传播  </span><br>    h1 = self.fc1.forward(input_data)  <br>    h1 = self.relu1.forward(h1)  <br>    <span class=\"hljs-comment\"># h1 = self.sigmoid1.forward(h1)  </span><br>    h2 = self.fc2.forward(h1)  <br>    h2 = self.relu2.forward(h2)  <br>    <span class=\"hljs-comment\"># h2 = self.sigmoid2.forward(h2)  </span><br>    h3 = self.fc3.forward(h2)  <br>    <span class=\"hljs-comment\"># prob = self.softmax.forward(h1)  </span><br>    <span class=\"hljs-keyword\">return</span> h3  <br>  <br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">backward</span>(<span class=\"hljs-params\">self, y_pre, y</span>):  <span class=\"hljs-comment\"># 神经网络的反向传播  </span><br>    _, grad = self.MSE_loss(y_pre, y)  <br>    <span class=\"hljs-comment\"># grad = self.softmax.backward()  </span><br>    dh3 = self.fc3.backward(grad)  <br>    dh2 = self.relu2.backward(dh3)  <br>    <span class=\"hljs-comment\"># dh2 = self.sigmoid2.backward(dh3)  </span><br>    dh2 = self.fc2.backward(dh2)  <br>    dh1 = self.relu1.backward(dh2)  <br>    <span class=\"hljs-comment\"># dh1 = self.sigmoid1.backward(dh2)  </span><br>    dh1 = self.fc1.backward(dh1)<br>    <br><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">update</span>(<span class=\"hljs-params\">self, lr</span>):  <br>    self.fc1.update(lr)  <br>    self.fc2.update(lr)  <br>    self.fc3.update(lr)<br></code></pre></td></tr></table></figure>\n<h3 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h3><p>使用mini-batch GD，使用效果较好的模型参数<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002745.png\" alt=\"image.png\"><br>得到实验结果如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019002819.png\" alt=\"image.png\"><br>最终测试集准确率稳定在98%以上。</p>\n"},{"title":"注意力机制","date":"2023-11-02T02:07:44.926Z","_content":"软性注意力机制：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101323.png)\n\n打分函数：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101459.png)\n### 记忆网络\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102112518.png)\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/注意力机制.md","raw":"---\ntitle: 注意力机制\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 深度学习\n  - 神经网络\ndate:\n---\n软性注意力机制：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101323.png)\n\n打分函数：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101459.png)\n### 记忆网络\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102112518.png)\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/注意力机制","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12h0041vw8c96up9h3y","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>软性注意力机制：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101323.png\" alt=\"image.png\"></p>\n<p>打分函数：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101459.png\" alt=\"image.png\"></p>\n<h3 id=\"记忆网络\"><a href=\"#记忆网络\" class=\"headerlink\" title=\"记忆网络\"></a>记忆网络</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102112518.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>软性注意力机制：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101323.png\" alt=\"image.png\"></p>\n<p>打分函数：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102101459.png\" alt=\"image.png\"></p>\n<h3 id=\"记忆网络\"><a href=\"#记忆网络\" class=\"headerlink\" title=\"记忆网络\"></a>记忆网络</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231102112518.png\" alt=\"image.png\"></p>\n"},{"title":"UML","date":"2023-10-20T02:50:57.049Z","_content":"UML：统一建模语言\n\nUML是一种标准的图形化建模语言，它是面向对象分析与设计的一种标准表示\n- 是一种**可视化的建模语言**，不是一种可视化的程序设计语言\n- 是一种**建模语言规格说明**，不是工具或知识库的规格说明\n\n### UML的基本结构\n基本构造块：\n- 事物 Thing\n- 关系 Relationship\n- 图 Diagram\n\n语义规则：\n- name、scope、visibility、integrity、execution\n\n通用机制：\n- specification、adornment、common division、extensibility mechanism\n\n#### 事物\nStructural thing\n- Class, interface, component, node\n\nBehavior thing\n- use case, Interaction, state machine\n\nGroup thing\n- package\n\nAnnotation thing\n- note\n\n#### 关系\n- Dependenc\n- Association\n- Generalization\n- Realization\n\n### UML的4+1视图\nUML 用模型来描述系统的结构（静态特征）以及行为（动态特征）。从不同的视角为系统的架构建模，形成系统的不同视图（view）， 称为4+1视图，从1个需求的角度出发描述与系统设计的4个维度之间的关系。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020110320.png)\n- 用例视图：表示功能需求，强调从用户的角度看到的或需要的系统功能。这种视图也叫做用户模型视图（user model view） 或场景视图（scenario view）；\n\n- 逻辑视图： 展现软件系统设计的静态结构的组成及特征，也称为结构模型视图（structural model view） 或静态视图（static view）；\n\n- 进程视图：描述软件系统设计的并发和同步等特性，关注系统非功能性需求，也称为行为模型视图（behavioral model view）、过程视图（process view）、 协作视图（collaborative view）和动态视图（dynamic view）；\n\n- 构件视图：关注软件代码的静态组织与管理，也称为实现模型视图（implementation model view ）和开发视图（development view）；\n\n- 部署视图：描述硬件的拓扑结构以及软件和硬件的映射问题，关注系统非功能性需求（性能、可靠性等），也称为环境模型视图或物理视图（physical view）\n\n### UML的9个基本图\n- 用例图（Use case diagram）：（从用户的角度）描述系统的功能；\n\n- 类图（Class diagram）：描述系统的静态结构（类及其相互关系）；\n\n- 对象图（Object diagram）： 描述系统在某个时刻的静态结构（对象及其相互关系）；\n\n- 顺序图（Sequence diagram）：按时间顺序描述系统元素间的交互；\n\n- 协作图（Collaboration diagram）：按照时间和空间的顺序描述系统元素间的交互和它们之间的关系；\n\n- 状态图（State diagram）：描述了系统元素（对象）的状态条件和响应；\n\n- 活动图（Activity diagram）：描述了系统元素之间的活动；\n\n- 构件图（Component diagram）：描述了实现系统的元素（类或包）组织；\n\n- 部署图（Deployment diagram）：描述了环境元素的配置并把实现系统的元素映射到配置上。","source":"_posts/Notes/课程/大三（上）/软件工程/UML.md","raw":"---\ntitle: UML\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 软件工程\ntags:\n  - 软件工程\ndate:\n---\nUML：统一建模语言\n\nUML是一种标准的图形化建模语言，它是面向对象分析与设计的一种标准表示\n- 是一种**可视化的建模语言**，不是一种可视化的程序设计语言\n- 是一种**建模语言规格说明**，不是工具或知识库的规格说明\n\n### UML的基本结构\n基本构造块：\n- 事物 Thing\n- 关系 Relationship\n- 图 Diagram\n\n语义规则：\n- name、scope、visibility、integrity、execution\n\n通用机制：\n- specification、adornment、common division、extensibility mechanism\n\n#### 事物\nStructural thing\n- Class, interface, component, node\n\nBehavior thing\n- use case, Interaction, state machine\n\nGroup thing\n- package\n\nAnnotation thing\n- note\n\n#### 关系\n- Dependenc\n- Association\n- Generalization\n- Realization\n\n### UML的4+1视图\nUML 用模型来描述系统的结构（静态特征）以及行为（动态特征）。从不同的视角为系统的架构建模，形成系统的不同视图（view）， 称为4+1视图，从1个需求的角度出发描述与系统设计的4个维度之间的关系。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020110320.png)\n- 用例视图：表示功能需求，强调从用户的角度看到的或需要的系统功能。这种视图也叫做用户模型视图（user model view） 或场景视图（scenario view）；\n\n- 逻辑视图： 展现软件系统设计的静态结构的组成及特征，也称为结构模型视图（structural model view） 或静态视图（static view）；\n\n- 进程视图：描述软件系统设计的并发和同步等特性，关注系统非功能性需求，也称为行为模型视图（behavioral model view）、过程视图（process view）、 协作视图（collaborative view）和动态视图（dynamic view）；\n\n- 构件视图：关注软件代码的静态组织与管理，也称为实现模型视图（implementation model view ）和开发视图（development view）；\n\n- 部署视图：描述硬件的拓扑结构以及软件和硬件的映射问题，关注系统非功能性需求（性能、可靠性等），也称为环境模型视图或物理视图（physical view）\n\n### UML的9个基本图\n- 用例图（Use case diagram）：（从用户的角度）描述系统的功能；\n\n- 类图（Class diagram）：描述系统的静态结构（类及其相互关系）；\n\n- 对象图（Object diagram）： 描述系统在某个时刻的静态结构（对象及其相互关系）；\n\n- 顺序图（Sequence diagram）：按时间顺序描述系统元素间的交互；\n\n- 协作图（Collaboration diagram）：按照时间和空间的顺序描述系统元素间的交互和它们之间的关系；\n\n- 状态图（State diagram）：描述了系统元素（对象）的状态条件和响应；\n\n- 活动图（Activity diagram）：描述了系统元素之间的活动；\n\n- 构件图（Component diagram）：描述了实现系统的元素（类或包）组织；\n\n- 部署图（Deployment diagram）：描述了环境元素的配置并把实现系统的元素映射到配置上。","slug":"Notes/课程/大三（上）/软件工程/UML","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12i0044vw8c54tu48gt","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>UML：统一建模语言</p>\n<p>UML是一种标准的图形化建模语言，它是面向对象分析与设计的一种标准表示</p>\n<ul>\n<li>是一种<strong>可视化的建模语言</strong>，不是一种可视化的程序设计语言</li>\n<li>是一种<strong>建模语言规格说明</strong>，不是工具或知识库的规格说明</li>\n</ul>\n<h3 id=\"UML的基本结构\"><a href=\"#UML的基本结构\" class=\"headerlink\" title=\"UML的基本结构\"></a>UML的基本结构</h3><p>基本构造块：</p>\n<ul>\n<li>事物 Thing</li>\n<li>关系 Relationship</li>\n<li>图 Diagram</li>\n</ul>\n<p>语义规则：</p>\n<ul>\n<li>name、scope、visibility、integrity、execution</li>\n</ul>\n<p>通用机制：</p>\n<ul>\n<li>specification、adornment、common division、extensibility mechanism</li>\n</ul>\n<h4 id=\"事物\"><a href=\"#事物\" class=\"headerlink\" title=\"事物\"></a>事物</h4><p>Structural thing</p>\n<ul>\n<li>Class, interface, component, node</li>\n</ul>\n<p>Behavior thing</p>\n<ul>\n<li>use case, Interaction, state machine</li>\n</ul>\n<p>Group thing</p>\n<ul>\n<li>package</li>\n</ul>\n<p>Annotation thing</p>\n<ul>\n<li>note</li>\n</ul>\n<h4 id=\"关系\"><a href=\"#关系\" class=\"headerlink\" title=\"关系\"></a>关系</h4><ul>\n<li>Dependenc</li>\n<li>Association</li>\n<li>Generalization</li>\n<li>Realization</li>\n</ul>\n<h3 id=\"UML的4-1视图\"><a href=\"#UML的4-1视图\" class=\"headerlink\" title=\"UML的4+1视图\"></a>UML的4+1视图</h3><p>UML 用模型来描述系统的结构（静态特征）以及行为（动态特征）。从不同的视角为系统的架构建模，形成系统的不同视图（view）， 称为4+1视图，从1个需求的角度出发描述与系统设计的4个维度之间的关系。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020110320.png\" alt=\"image.png\"></p>\n<ul>\n<li><p>用例视图：表示功能需求，强调从用户的角度看到的或需要的系统功能。这种视图也叫做用户模型视图（user model view） 或场景视图（scenario view）；</p>\n</li>\n<li><p>逻辑视图： 展现软件系统设计的静态结构的组成及特征，也称为结构模型视图（structural model view） 或静态视图（static view）；</p>\n</li>\n<li><p>进程视图：描述软件系统设计的并发和同步等特性，关注系统非功能性需求，也称为行为模型视图（behavioral model view）、过程视图（process view）、 协作视图（collaborative view）和动态视图（dynamic view）；</p>\n</li>\n<li><p>构件视图：关注软件代码的静态组织与管理，也称为实现模型视图（implementation model view ）和开发视图（development view）；</p>\n</li>\n<li><p>部署视图：描述硬件的拓扑结构以及软件和硬件的映射问题，关注系统非功能性需求（性能、可靠性等），也称为环境模型视图或物理视图（physical view）</p>\n</li>\n</ul>\n<h3 id=\"UML的9个基本图\"><a href=\"#UML的9个基本图\" class=\"headerlink\" title=\"UML的9个基本图\"></a>UML的9个基本图</h3><ul>\n<li><p>用例图（Use case diagram）：（从用户的角度）描述系统的功能；</p>\n</li>\n<li><p>类图（Class diagram）：描述系统的静态结构（类及其相互关系）；</p>\n</li>\n<li><p>对象图（Object diagram）： 描述系统在某个时刻的静态结构（对象及其相互关系）；</p>\n</li>\n<li><p>顺序图（Sequence diagram）：按时间顺序描述系统元素间的交互；</p>\n</li>\n<li><p>协作图（Collaboration diagram）：按照时间和空间的顺序描述系统元素间的交互和它们之间的关系；</p>\n</li>\n<li><p>状态图（State diagram）：描述了系统元素（对象）的状态条件和响应；</p>\n</li>\n<li><p>活动图（Activity diagram）：描述了系统元素之间的活动；</p>\n</li>\n<li><p>构件图（Component diagram）：描述了实现系统的元素（类或包）组织；</p>\n</li>\n<li><p>部署图（Deployment diagram）：描述了环境元素的配置并把实现系统的元素映射到配置上。</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>UML：统一建模语言</p>\n<p>UML是一种标准的图形化建模语言，它是面向对象分析与设计的一种标准表示</p>\n<ul>\n<li>是一种<strong>可视化的建模语言</strong>，不是一种可视化的程序设计语言</li>\n<li>是一种<strong>建模语言规格说明</strong>，不是工具或知识库的规格说明</li>\n</ul>\n<h3 id=\"UML的基本结构\"><a href=\"#UML的基本结构\" class=\"headerlink\" title=\"UML的基本结构\"></a>UML的基本结构</h3><p>基本构造块：</p>\n<ul>\n<li>事物 Thing</li>\n<li>关系 Relationship</li>\n<li>图 Diagram</li>\n</ul>\n<p>语义规则：</p>\n<ul>\n<li>name、scope、visibility、integrity、execution</li>\n</ul>\n<p>通用机制：</p>\n<ul>\n<li>specification、adornment、common division、extensibility mechanism</li>\n</ul>\n<h4 id=\"事物\"><a href=\"#事物\" class=\"headerlink\" title=\"事物\"></a>事物</h4><p>Structural thing</p>\n<ul>\n<li>Class, interface, component, node</li>\n</ul>\n<p>Behavior thing</p>\n<ul>\n<li>use case, Interaction, state machine</li>\n</ul>\n<p>Group thing</p>\n<ul>\n<li>package</li>\n</ul>\n<p>Annotation thing</p>\n<ul>\n<li>note</li>\n</ul>\n<h4 id=\"关系\"><a href=\"#关系\" class=\"headerlink\" title=\"关系\"></a>关系</h4><ul>\n<li>Dependenc</li>\n<li>Association</li>\n<li>Generalization</li>\n<li>Realization</li>\n</ul>\n<h3 id=\"UML的4-1视图\"><a href=\"#UML的4-1视图\" class=\"headerlink\" title=\"UML的4+1视图\"></a>UML的4+1视图</h3><p>UML 用模型来描述系统的结构（静态特征）以及行为（动态特征）。从不同的视角为系统的架构建模，形成系统的不同视图（view）， 称为4+1视图，从1个需求的角度出发描述与系统设计的4个维度之间的关系。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020110320.png\" alt=\"image.png\"></p>\n<ul>\n<li><p>用例视图：表示功能需求，强调从用户的角度看到的或需要的系统功能。这种视图也叫做用户模型视图（user model view） 或场景视图（scenario view）；</p>\n</li>\n<li><p>逻辑视图： 展现软件系统设计的静态结构的组成及特征，也称为结构模型视图（structural model view） 或静态视图（static view）；</p>\n</li>\n<li><p>进程视图：描述软件系统设计的并发和同步等特性，关注系统非功能性需求，也称为行为模型视图（behavioral model view）、过程视图（process view）、 协作视图（collaborative view）和动态视图（dynamic view）；</p>\n</li>\n<li><p>构件视图：关注软件代码的静态组织与管理，也称为实现模型视图（implementation model view ）和开发视图（development view）；</p>\n</li>\n<li><p>部署视图：描述硬件的拓扑结构以及软件和硬件的映射问题，关注系统非功能性需求（性能、可靠性等），也称为环境模型视图或物理视图（physical view）</p>\n</li>\n</ul>\n<h3 id=\"UML的9个基本图\"><a href=\"#UML的9个基本图\" class=\"headerlink\" title=\"UML的9个基本图\"></a>UML的9个基本图</h3><ul>\n<li><p>用例图（Use case diagram）：（从用户的角度）描述系统的功能；</p>\n</li>\n<li><p>类图（Class diagram）：描述系统的静态结构（类及其相互关系）；</p>\n</li>\n<li><p>对象图（Object diagram）： 描述系统在某个时刻的静态结构（对象及其相互关系）；</p>\n</li>\n<li><p>顺序图（Sequence diagram）：按时间顺序描述系统元素间的交互；</p>\n</li>\n<li><p>协作图（Collaboration diagram）：按照时间和空间的顺序描述系统元素间的交互和它们之间的关系；</p>\n</li>\n<li><p>状态图（State diagram）：描述了系统元素（对象）的状态条件和响应；</p>\n</li>\n<li><p>活动图（Activity diagram）：描述了系统元素之间的活动；</p>\n</li>\n<li><p>构件图（Component diagram）：描述了实现系统的元素（类或包）组织；</p>\n</li>\n<li><p>部署图（Deployment diagram）：描述了环境元素的配置并把实现系统的元素映射到配置上。</p>\n</li>\n</ul>\n"},{"title":"循环神经网络","date":"2023-10-19T02:02:51.481Z","_content":"### 概述\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019100538.png)\n\n特点：不同于在样本上做多个独立预测，而是假设样本之间存在关联，进而在样本序列上做预测\n\n采用链式法则表示一个观测序列的联合概率：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101449.png)\n考虑一个观测和所有历史观测之间的依赖关系复杂度随着观测个数指数级增长\n\n马尔可夫模型：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101541.png)\n马尔可夫模型假设当前观察只和较近的观测有关\n\n考虑两个不同的序列（如输入和输出序列），可以使用隐马尔可夫模型：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101946.png)\n\n联合分布为：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102434.png)\n\n最可能的隐状态为：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102453.png)\n\n循环神经网络（Recurrent neural networks，RNN）：\n- HMM是一个生成模型，RNN是一个判别模型\n\n### 循环神经网络（RNN）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103834.png)\nRNN可以被认为是同一网络的多个副本，每个副本都将消息传递给后者\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103930.png)\n可以保留序列前面的特征，但无法建立与序列后面的联系\n\n双向RNN（BRNN）除了前向建模，还添加了一层后向建模\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104056.png)\n\n#### 形式化表示\n在每个时间步应用递归公式：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104638.png)\n\nT个时间步的递归：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104956.png)\n\n### 随时间反向传播（BPTT）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115173012.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115172943.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111347.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111406.png)\n用链式法则：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111434.png)\n一直传播到t=0\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111500.png)\n\n### 梯度弥散和梯度爆炸\n\n\n### 长短时记忆网络（LSTM）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019112925.png)\n\n#### 遗忘门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113336.png)\n\n#### 输入门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113611.png)\n\n#### 单元状态更新\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113642.png)\n\n#### 输出门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113722.png)\n\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/循环神经网络.md","raw":"---\ntitle: 循环神经网络\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 神经网络\ndate:\n---\n### 概述\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019100538.png)\n\n特点：不同于在样本上做多个独立预测，而是假设样本之间存在关联，进而在样本序列上做预测\n\n采用链式法则表示一个观测序列的联合概率：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101449.png)\n考虑一个观测和所有历史观测之间的依赖关系复杂度随着观测个数指数级增长\n\n马尔可夫模型：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101541.png)\n马尔可夫模型假设当前观察只和较近的观测有关\n\n考虑两个不同的序列（如输入和输出序列），可以使用隐马尔可夫模型：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101946.png)\n\n联合分布为：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102434.png)\n\n最可能的隐状态为：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102453.png)\n\n循环神经网络（Recurrent neural networks，RNN）：\n- HMM是一个生成模型，RNN是一个判别模型\n\n### 循环神经网络（RNN）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103834.png)\nRNN可以被认为是同一网络的多个副本，每个副本都将消息传递给后者\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103930.png)\n可以保留序列前面的特征，但无法建立与序列后面的联系\n\n双向RNN（BRNN）除了前向建模，还添加了一层后向建模\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104056.png)\n\n#### 形式化表示\n在每个时间步应用递归公式：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104638.png)\n\nT个时间步的递归：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104956.png)\n\n### 随时间反向传播（BPTT）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115173012.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115172943.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111347.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111406.png)\n用链式法则：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111434.png)\n一直传播到t=0\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111500.png)\n\n### 梯度弥散和梯度爆炸\n\n\n### 长短时记忆网络（LSTM）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019112925.png)\n\n#### 遗忘门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113336.png)\n\n#### 输入门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113611.png)\n\n#### 单元状态更新\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113642.png)\n\n#### 输出门\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113722.png)\n\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/循环神经网络","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12i0047vw8c2ayz424a","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019100538.png\" alt=\"image.png\"></p>\n<p>特点：不同于在样本上做多个独立预测，而是假设样本之间存在关联，进而在样本序列上做预测</p>\n<p>采用链式法则表示一个观测序列的联合概率：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101449.png\" alt=\"image.png\"><br>考虑一个观测和所有历史观测之间的依赖关系复杂度随着观测个数指数级增长</p>\n<p>马尔可夫模型：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101541.png\" alt=\"image.png\"><br>马尔可夫模型假设当前观察只和较近的观测有关</p>\n<p>考虑两个不同的序列（如输入和输出序列），可以使用隐马尔可夫模型：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101946.png\" alt=\"image.png\"></p>\n<p>联合分布为：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102434.png\" alt=\"image.png\"></p>\n<p>最可能的隐状态为：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102453.png\" alt=\"image.png\"></p>\n<p>循环神经网络（Recurrent neural networks，RNN）：</p>\n<ul>\n<li>HMM是一个生成模型，RNN是一个判别模型</li>\n</ul>\n<h3 id=\"循环神经网络（RNN）\"><a href=\"#循环神经网络（RNN）\" class=\"headerlink\" title=\"循环神经网络（RNN）\"></a>循环神经网络（RNN）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103834.png\" alt=\"image.png\"><br>RNN可以被认为是同一网络的多个副本，每个副本都将消息传递给后者<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103930.png\" alt=\"image.png\"><br>可以保留序列前面的特征，但无法建立与序列后面的联系</p>\n<p>双向RNN（BRNN）除了前向建模，还添加了一层后向建模<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104056.png\" alt=\"image.png\"></p>\n<h4 id=\"形式化表示\"><a href=\"#形式化表示\" class=\"headerlink\" title=\"形式化表示\"></a>形式化表示</h4><p>在每个时间步应用递归公式：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104638.png\" alt=\"image.png\"></p>\n<p>T个时间步的递归：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104956.png\" alt=\"image.png\"></p>\n<h3 id=\"随时间反向传播（BPTT）\"><a href=\"#随时间反向传播（BPTT）\" class=\"headerlink\" title=\"随时间反向传播（BPTT）\"></a>随时间反向传播（BPTT）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115173012.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115172943.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111347.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111406.png\" alt=\"image.png\"><br>用链式法则：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111434.png\" alt=\"image.png\"><br>一直传播到t&#x3D;0<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111500.png\" alt=\"image.png\"></p>\n<h3 id=\"梯度弥散和梯度爆炸\"><a href=\"#梯度弥散和梯度爆炸\" class=\"headerlink\" title=\"梯度弥散和梯度爆炸\"></a>梯度弥散和梯度爆炸</h3><h3 id=\"长短时记忆网络（LSTM）\"><a href=\"#长短时记忆网络（LSTM）\" class=\"headerlink\" title=\"长短时记忆网络（LSTM）\"></a>长短时记忆网络（LSTM）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019112925.png\" alt=\"image.png\"></p>\n<h4 id=\"遗忘门\"><a href=\"#遗忘门\" class=\"headerlink\" title=\"遗忘门\"></a>遗忘门</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113336.png\" alt=\"image.png\"></p>\n<h4 id=\"输入门\"><a href=\"#输入门\" class=\"headerlink\" title=\"输入门\"></a>输入门</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113611.png\" alt=\"image.png\"></p>\n<h4 id=\"单元状态更新\"><a href=\"#单元状态更新\" class=\"headerlink\" title=\"单元状态更新\"></a>单元状态更新</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113642.png\" alt=\"image.png\"></p>\n<h4 id=\"输出门\"><a href=\"#输出门\" class=\"headerlink\" title=\"输出门\"></a>输出门</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113722.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019100538.png\" alt=\"image.png\"></p>\n<p>特点：不同于在样本上做多个独立预测，而是假设样本之间存在关联，进而在样本序列上做预测</p>\n<p>采用链式法则表示一个观测序列的联合概率：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101449.png\" alt=\"image.png\"><br>考虑一个观测和所有历史观测之间的依赖关系复杂度随着观测个数指数级增长</p>\n<p>马尔可夫模型：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101541.png\" alt=\"image.png\"><br>马尔可夫模型假设当前观察只和较近的观测有关</p>\n<p>考虑两个不同的序列（如输入和输出序列），可以使用隐马尔可夫模型：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019101946.png\" alt=\"image.png\"></p>\n<p>联合分布为：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102434.png\" alt=\"image.png\"></p>\n<p>最可能的隐状态为：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019102453.png\" alt=\"image.png\"></p>\n<p>循环神经网络（Recurrent neural networks，RNN）：</p>\n<ul>\n<li>HMM是一个生成模型，RNN是一个判别模型</li>\n</ul>\n<h3 id=\"循环神经网络（RNN）\"><a href=\"#循环神经网络（RNN）\" class=\"headerlink\" title=\"循环神经网络（RNN）\"></a>循环神经网络（RNN）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103834.png\" alt=\"image.png\"><br>RNN可以被认为是同一网络的多个副本，每个副本都将消息传递给后者<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019103930.png\" alt=\"image.png\"><br>可以保留序列前面的特征，但无法建立与序列后面的联系</p>\n<p>双向RNN（BRNN）除了前向建模，还添加了一层后向建模<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104056.png\" alt=\"image.png\"></p>\n<h4 id=\"形式化表示\"><a href=\"#形式化表示\" class=\"headerlink\" title=\"形式化表示\"></a>形式化表示</h4><p>在每个时间步应用递归公式：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104638.png\" alt=\"image.png\"></p>\n<p>T个时间步的递归：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019104956.png\" alt=\"image.png\"></p>\n<h3 id=\"随时间反向传播（BPTT）\"><a href=\"#随时间反向传播（BPTT）\" class=\"headerlink\" title=\"随时间反向传播（BPTT）\"></a>随时间反向传播（BPTT）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115173012.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231115172943.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111347.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111406.png\" alt=\"image.png\"><br>用链式法则：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111434.png\" alt=\"image.png\"><br>一直传播到t&#x3D;0<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019111500.png\" alt=\"image.png\"></p>\n<h3 id=\"梯度弥散和梯度爆炸\"><a href=\"#梯度弥散和梯度爆炸\" class=\"headerlink\" title=\"梯度弥散和梯度爆炸\"></a>梯度弥散和梯度爆炸</h3><h3 id=\"长短时记忆网络（LSTM）\"><a href=\"#长短时记忆网络（LSTM）\" class=\"headerlink\" title=\"长短时记忆网络（LSTM）\"></a>长短时记忆网络（LSTM）</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019112925.png\" alt=\"image.png\"></p>\n<h4 id=\"遗忘门\"><a href=\"#遗忘门\" class=\"headerlink\" title=\"遗忘门\"></a>遗忘门</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113336.png\" alt=\"image.png\"></p>\n<h4 id=\"输入门\"><a href=\"#输入门\" class=\"headerlink\" title=\"输入门\"></a>输入门</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113611.png\" alt=\"image.png\"></p>\n<h4 id=\"单元状态更新\"><a href=\"#单元状态更新\" class=\"headerlink\" title=\"单元状态更新\"></a>单元状态更新</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113642.png\" alt=\"image.png\"></p>\n<h4 id=\"输出门\"><a href=\"#输出门\" class=\"headerlink\" title=\"输出门\"></a>输出门</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231019113722.png\" alt=\"image.png\"></p>\n"},{"title":"作业","update":null,"_content":"### 课后阅读1\n问题1：时至今日的软件开发过程中是否还存在软件危机中存在的现象？如果存在，软件工程的方法能否解决？未来的AI能否解决？\n\n存在。软件危机中主要存在以下现象：\n1. 对软件开发成本和进度的估计常常很不准确\n2. 用户对“已完成的”软件系统不满意\n3. 软件常常是不可维护的\n4. 软件通常没有适当的文档资料\n随着人们使用软件工程的方法进行软件开发，这些问题逐渐减少，但不会完全消灭。\n利用软件工程的方法可以有效解决这些问题，软件工程采用工程的概念、原理、技术和方法来开发和维护软件，进行详细需求分析和规划，使用合适的设计模式和架构，使用版本控制系统等特点可以有效解决上述问题。随着AI的发展，未来AI很有可能具备自主设计程序的能力，如果AI能实现正确理解客户需求，合理开发软件功能，自动维护软件运行，则可以开发出优质软件。\n\n\n---\n\n\n问题2：软件工程的知识体系中，你认为哪几个环节与软件质量相关？\n\n需求分析：只有正确理解客户的需求，才能做出好的软件\n软件稳定性和可维护性：当软件在使用中经常出现问题，或需要频繁投入人力进行维护时，意味着软件质量不高。\n软件代码的可读性：高可读性的代码可以使软件在维护和升级时更方便。\n经过测试：软件需要经过大量的测试才能保证质量。\n\n---\n\n### 课后阅读2\n1. 为何瀑布模型的成功率较低？\n\n模型缺乏灵活性。当软件需求不明确或不准确时，可能会在开发过程中产生问题，由于瀑布模型前后阶段的依赖关系，任何变更可能会牵涉多个阶段，使得改动极为困难。因此，软件很可能因为繁琐的工程变动而无法实现其应有的功能。\n\n模型的风险控制能力较弱。软件只有通过测试阶段才能让用户接触最终产品，如果产品与用户需求有偏差，返工修改的代价比较大。软件开发人员只有在后期才能看见开发成果，有可能造成开发成果与预期不匹配。软件体系结构级别风险只有在整体组装测试之后才能发现，前期的错误也只有在固定的测试阶段才能被发现，这种错误修复需要极大的时间代价。上述风险一旦产生，会对软件的开发进度造成很大影响，极有可能导致项目延期等后果。\n\n模型文档控制问题。瀑布模型的软件活动是由文档驱动的，当阶段之间规定过多文档时，会极大增加系统的工作量，占用开发者的工作时间，拖延工程进度。工作中开发人员可能会根据实际情况做出调整，而未及时更新文档，导致文档与实际开发过程之间存在不一致，当管理者通过文档管理项目的开发进度时可能出现问题。\n\n模型工期长，阶段顺序严格。瀑布模型强调在一个阶段完成后才能进入下一个阶段，这会导致开发者的效率较低，造成很长的开发时间，一旦项目变化，会产生大量时间成本。\n\n\n---\n\n\n作业要求：根据老师提供的作业3 的参考答案，完成以下内容\n\n任务1：给出该系统的软件框架结构（如果已经有代码的小组，请结合小组的代码结构进行表述），15%\n\n任务2：给出该系统的界面设计， 20%\n\n任务3：给出顾客使用空调用例以及前台营业员出账单和出详单用例对应SSD中所有消息的动态结构交互图， 50%\n\n消息1：RequestOn 开机请求， 组员完成服务资源无限制的情况；20%\n\n组长完成服务资源数=3且服务请求数>3的具有调度策略的所有情况（优先级策略+时间片轮询策略） 40%\n\n消息2：ChangeTargetTemp 调温 5%\n\n消息3：ChangeFanSpeed 调风 15%\n\n消息4：RequestOff 关机 10%\n\n消息5：CreateInvoice 产生账单 5%\n\n消息6：CreateDR 产生详单 5%\n\n任务4：给出顾客使用空调用例的静态结构类图、给出前台营业员出具账单和出具详单用例的静态结构类图，10%\n\n任务5：填写工作量表， 5%","source":"_posts/Notes/课程/大三（上）/软件工程/作业.md","raw":"---\ntitle: 作业\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 软件工程\ntags:\n  - 软件工程\n  - 作业\nupdate:\n---\n### 课后阅读1\n问题1：时至今日的软件开发过程中是否还存在软件危机中存在的现象？如果存在，软件工程的方法能否解决？未来的AI能否解决？\n\n存在。软件危机中主要存在以下现象：\n1. 对软件开发成本和进度的估计常常很不准确\n2. 用户对“已完成的”软件系统不满意\n3. 软件常常是不可维护的\n4. 软件通常没有适当的文档资料\n随着人们使用软件工程的方法进行软件开发，这些问题逐渐减少，但不会完全消灭。\n利用软件工程的方法可以有效解决这些问题，软件工程采用工程的概念、原理、技术和方法来开发和维护软件，进行详细需求分析和规划，使用合适的设计模式和架构，使用版本控制系统等特点可以有效解决上述问题。随着AI的发展，未来AI很有可能具备自主设计程序的能力，如果AI能实现正确理解客户需求，合理开发软件功能，自动维护软件运行，则可以开发出优质软件。\n\n\n---\n\n\n问题2：软件工程的知识体系中，你认为哪几个环节与软件质量相关？\n\n需求分析：只有正确理解客户的需求，才能做出好的软件\n软件稳定性和可维护性：当软件在使用中经常出现问题，或需要频繁投入人力进行维护时，意味着软件质量不高。\n软件代码的可读性：高可读性的代码可以使软件在维护和升级时更方便。\n经过测试：软件需要经过大量的测试才能保证质量。\n\n---\n\n### 课后阅读2\n1. 为何瀑布模型的成功率较低？\n\n模型缺乏灵活性。当软件需求不明确或不准确时，可能会在开发过程中产生问题，由于瀑布模型前后阶段的依赖关系，任何变更可能会牵涉多个阶段，使得改动极为困难。因此，软件很可能因为繁琐的工程变动而无法实现其应有的功能。\n\n模型的风险控制能力较弱。软件只有通过测试阶段才能让用户接触最终产品，如果产品与用户需求有偏差，返工修改的代价比较大。软件开发人员只有在后期才能看见开发成果，有可能造成开发成果与预期不匹配。软件体系结构级别风险只有在整体组装测试之后才能发现，前期的错误也只有在固定的测试阶段才能被发现，这种错误修复需要极大的时间代价。上述风险一旦产生，会对软件的开发进度造成很大影响，极有可能导致项目延期等后果。\n\n模型文档控制问题。瀑布模型的软件活动是由文档驱动的，当阶段之间规定过多文档时，会极大增加系统的工作量，占用开发者的工作时间，拖延工程进度。工作中开发人员可能会根据实际情况做出调整，而未及时更新文档，导致文档与实际开发过程之间存在不一致，当管理者通过文档管理项目的开发进度时可能出现问题。\n\n模型工期长，阶段顺序严格。瀑布模型强调在一个阶段完成后才能进入下一个阶段，这会导致开发者的效率较低，造成很长的开发时间，一旦项目变化，会产生大量时间成本。\n\n\n---\n\n\n作业要求：根据老师提供的作业3 的参考答案，完成以下内容\n\n任务1：给出该系统的软件框架结构（如果已经有代码的小组，请结合小组的代码结构进行表述），15%\n\n任务2：给出该系统的界面设计， 20%\n\n任务3：给出顾客使用空调用例以及前台营业员出账单和出详单用例对应SSD中所有消息的动态结构交互图， 50%\n\n消息1：RequestOn 开机请求， 组员完成服务资源无限制的情况；20%\n\n组长完成服务资源数=3且服务请求数>3的具有调度策略的所有情况（优先级策略+时间片轮询策略） 40%\n\n消息2：ChangeTargetTemp 调温 5%\n\n消息3：ChangeFanSpeed 调风 15%\n\n消息4：RequestOff 关机 10%\n\n消息5：CreateInvoice 产生账单 5%\n\n消息6：CreateDR 产生详单 5%\n\n任务4：给出顾客使用空调用例的静态结构类图、给出前台营业员出具账单和出具详单用例的静态结构类图，10%\n\n任务5：填写工作量表， 5%","slug":"Notes/课程/大三（上）/软件工程/作业","published":1,"date":"2023-09-21T16:36:34.339Z","updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12j004avw8c9k8vg4hb","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"课后阅读1\"><a href=\"#课后阅读1\" class=\"headerlink\" title=\"课后阅读1\"></a>课后阅读1</h3><p>问题1：时至今日的软件开发过程中是否还存在软件危机中存在的现象？如果存在，软件工程的方法能否解决？未来的AI能否解决？</p>\n<p>存在。软件危机中主要存在以下现象：</p>\n<ol>\n<li>对软件开发成本和进度的估计常常很不准确</li>\n<li>用户对“已完成的”软件系统不满意</li>\n<li>软件常常是不可维护的</li>\n<li>软件通常没有适当的文档资料<br>随着人们使用软件工程的方法进行软件开发，这些问题逐渐减少，但不会完全消灭。<br>利用软件工程的方法可以有效解决这些问题，软件工程采用工程的概念、原理、技术和方法来开发和维护软件，进行详细需求分析和规划，使用合适的设计模式和架构，使用版本控制系统等特点可以有效解决上述问题。随着AI的发展，未来AI很有可能具备自主设计程序的能力，如果AI能实现正确理解客户需求，合理开发软件功能，自动维护软件运行，则可以开发出优质软件。</li>\n</ol>\n<hr>\n<p>问题2：软件工程的知识体系中，你认为哪几个环节与软件质量相关？</p>\n<p>需求分析：只有正确理解客户的需求，才能做出好的软件<br>软件稳定性和可维护性：当软件在使用中经常出现问题，或需要频繁投入人力进行维护时，意味着软件质量不高。<br>软件代码的可读性：高可读性的代码可以使软件在维护和升级时更方便。<br>经过测试：软件需要经过大量的测试才能保证质量。</p>\n<hr>\n<h3 id=\"课后阅读2\"><a href=\"#课后阅读2\" class=\"headerlink\" title=\"课后阅读2\"></a>课后阅读2</h3><ol>\n<li>为何瀑布模型的成功率较低？</li>\n</ol>\n<p>模型缺乏灵活性。当软件需求不明确或不准确时，可能会在开发过程中产生问题，由于瀑布模型前后阶段的依赖关系，任何变更可能会牵涉多个阶段，使得改动极为困难。因此，软件很可能因为繁琐的工程变动而无法实现其应有的功能。</p>\n<p>模型的风险控制能力较弱。软件只有通过测试阶段才能让用户接触最终产品，如果产品与用户需求有偏差，返工修改的代价比较大。软件开发人员只有在后期才能看见开发成果，有可能造成开发成果与预期不匹配。软件体系结构级别风险只有在整体组装测试之后才能发现，前期的错误也只有在固定的测试阶段才能被发现，这种错误修复需要极大的时间代价。上述风险一旦产生，会对软件的开发进度造成很大影响，极有可能导致项目延期等后果。</p>\n<p>模型文档控制问题。瀑布模型的软件活动是由文档驱动的，当阶段之间规定过多文档时，会极大增加系统的工作量，占用开发者的工作时间，拖延工程进度。工作中开发人员可能会根据实际情况做出调整，而未及时更新文档，导致文档与实际开发过程之间存在不一致，当管理者通过文档管理项目的开发进度时可能出现问题。</p>\n<p>模型工期长，阶段顺序严格。瀑布模型强调在一个阶段完成后才能进入下一个阶段，这会导致开发者的效率较低，造成很长的开发时间，一旦项目变化，会产生大量时间成本。</p>\n<hr>\n<p>作业要求：根据老师提供的作业3 的参考答案，完成以下内容</p>\n<p>任务1：给出该系统的软件框架结构（如果已经有代码的小组，请结合小组的代码结构进行表述），15%</p>\n<p>任务2：给出该系统的界面设计， 20%</p>\n<p>任务3：给出顾客使用空调用例以及前台营业员出账单和出详单用例对应SSD中所有消息的动态结构交互图， 50%</p>\n<p>消息1：RequestOn 开机请求， 组员完成服务资源无限制的情况；20%</p>\n<p>组长完成服务资源数&#x3D;3且服务请求数&gt;3的具有调度策略的所有情况（优先级策略+时间片轮询策略） 40%</p>\n<p>消息2：ChangeTargetTemp 调温 5%</p>\n<p>消息3：ChangeFanSpeed 调风 15%</p>\n<p>消息4：RequestOff 关机 10%</p>\n<p>消息5：CreateInvoice 产生账单 5%</p>\n<p>消息6：CreateDR 产生详单 5%</p>\n<p>任务4：给出顾客使用空调用例的静态结构类图、给出前台营业员出具账单和出具详单用例的静态结构类图，10%</p>\n<p>任务5：填写工作量表， 5%</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"课后阅读1\"><a href=\"#课后阅读1\" class=\"headerlink\" title=\"课后阅读1\"></a>课后阅读1</h3><p>问题1：时至今日的软件开发过程中是否还存在软件危机中存在的现象？如果存在，软件工程的方法能否解决？未来的AI能否解决？</p>\n<p>存在。软件危机中主要存在以下现象：</p>\n<ol>\n<li>对软件开发成本和进度的估计常常很不准确</li>\n<li>用户对“已完成的”软件系统不满意</li>\n<li>软件常常是不可维护的</li>\n<li>软件通常没有适当的文档资料<br>随着人们使用软件工程的方法进行软件开发，这些问题逐渐减少，但不会完全消灭。<br>利用软件工程的方法可以有效解决这些问题，软件工程采用工程的概念、原理、技术和方法来开发和维护软件，进行详细需求分析和规划，使用合适的设计模式和架构，使用版本控制系统等特点可以有效解决上述问题。随着AI的发展，未来AI很有可能具备自主设计程序的能力，如果AI能实现正确理解客户需求，合理开发软件功能，自动维护软件运行，则可以开发出优质软件。</li>\n</ol>\n<hr>\n<p>问题2：软件工程的知识体系中，你认为哪几个环节与软件质量相关？</p>\n<p>需求分析：只有正确理解客户的需求，才能做出好的软件<br>软件稳定性和可维护性：当软件在使用中经常出现问题，或需要频繁投入人力进行维护时，意味着软件质量不高。<br>软件代码的可读性：高可读性的代码可以使软件在维护和升级时更方便。<br>经过测试：软件需要经过大量的测试才能保证质量。</p>\n<hr>\n<h3 id=\"课后阅读2\"><a href=\"#课后阅读2\" class=\"headerlink\" title=\"课后阅读2\"></a>课后阅读2</h3><ol>\n<li>为何瀑布模型的成功率较低？</li>\n</ol>\n<p>模型缺乏灵活性。当软件需求不明确或不准确时，可能会在开发过程中产生问题，由于瀑布模型前后阶段的依赖关系，任何变更可能会牵涉多个阶段，使得改动极为困难。因此，软件很可能因为繁琐的工程变动而无法实现其应有的功能。</p>\n<p>模型的风险控制能力较弱。软件只有通过测试阶段才能让用户接触最终产品，如果产品与用户需求有偏差，返工修改的代价比较大。软件开发人员只有在后期才能看见开发成果，有可能造成开发成果与预期不匹配。软件体系结构级别风险只有在整体组装测试之后才能发现，前期的错误也只有在固定的测试阶段才能被发现，这种错误修复需要极大的时间代价。上述风险一旦产生，会对软件的开发进度造成很大影响，极有可能导致项目延期等后果。</p>\n<p>模型文档控制问题。瀑布模型的软件活动是由文档驱动的，当阶段之间规定过多文档时，会极大增加系统的工作量，占用开发者的工作时间，拖延工程进度。工作中开发人员可能会根据实际情况做出调整，而未及时更新文档，导致文档与实际开发过程之间存在不一致，当管理者通过文档管理项目的开发进度时可能出现问题。</p>\n<p>模型工期长，阶段顺序严格。瀑布模型强调在一个阶段完成后才能进入下一个阶段，这会导致开发者的效率较低，造成很长的开发时间，一旦项目变化，会产生大量时间成本。</p>\n<hr>\n<p>作业要求：根据老师提供的作业3 的参考答案，完成以下内容</p>\n<p>任务1：给出该系统的软件框架结构（如果已经有代码的小组，请结合小组的代码结构进行表述），15%</p>\n<p>任务2：给出该系统的界面设计， 20%</p>\n<p>任务3：给出顾客使用空调用例以及前台营业员出账单和出详单用例对应SSD中所有消息的动态结构交互图， 50%</p>\n<p>消息1：RequestOn 开机请求， 组员完成服务资源无限制的情况；20%</p>\n<p>组长完成服务资源数&#x3D;3且服务请求数&gt;3的具有调度策略的所有情况（优先级策略+时间片轮询策略） 40%</p>\n<p>消息2：ChangeTargetTemp 调温 5%</p>\n<p>消息3：ChangeFanSpeed 调风 15%</p>\n<p>消息4：RequestOff 关机 10%</p>\n<p>消息5：CreateInvoice 产生账单 5%</p>\n<p>消息6：CreateDR 产生详单 5%</p>\n<p>任务4：给出顾客使用空调用例的静态结构类图、给出前台营业员出具账单和出具详单用例的静态结构类图，10%</p>\n<p>任务5：填写工作量表， 5%</p>\n"},{"title":"生成对抗网络","date":"2023-11-09T03:19:59.983Z","_content":"VAE通过引入隐变量z定义了一个显式的密度函数\n生成对抗网络（GANs）通过对抗网络生成样本，而无需显式的密度函数\n对抗学习是一种机器学习领域常用的学习策略，通过引入假样本迷惑模型\n\n基本思想：训练两个网络\n- 生成器G：生成虚假样本，试图迷惑判别器\n- 判别器D：试图区分真实样本和虚假样本\n- 对抗学习：二者对抗训练\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114252.png)\n\n零和博弈：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114538.png)\n最终通过充分训练，渴望算法收敛于一个好的关于数据分布的估计$p_g$，由$p_g$生成的样本，被最优的判别器当成真实样本\n\n\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/生成对抗网络.md","raw":"---\ntitle: 生成对抗网络\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 深度学习\ndate:\n---\nVAE通过引入隐变量z定义了一个显式的密度函数\n生成对抗网络（GANs）通过对抗网络生成样本，而无需显式的密度函数\n对抗学习是一种机器学习领域常用的学习策略，通过引入假样本迷惑模型\n\n基本思想：训练两个网络\n- 生成器G：生成虚假样本，试图迷惑判别器\n- 判别器D：试图区分真实样本和虚假样本\n- 对抗学习：二者对抗训练\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114252.png)\n\n零和博弈：![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114538.png)\n最终通过充分训练，渴望算法收敛于一个好的关于数据分布的估计$p_g$，由$p_g$生成的样本，被最优的判别器当成真实样本\n\n\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/生成对抗网络","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12j004dvw8canyq6tw7","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>VAE通过引入隐变量z定义了一个显式的密度函数<br>生成对抗网络（GANs）通过对抗网络生成样本，而无需显式的密度函数<br>对抗学习是一种机器学习领域常用的学习策略，通过引入假样本迷惑模型</p>\n<p>基本思想：训练两个网络</p>\n<ul>\n<li>生成器G：生成虚假样本，试图迷惑判别器</li>\n<li>判别器D：试图区分真实样本和虚假样本</li>\n<li>对抗学习：二者对抗训练</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114252.png\" alt=\"image.png\"></p>\n<p>零和博弈：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114538.png\" alt=\"image.png\"><br>最终通过充分训练，渴望算法收敛于一个好的关于数据分布的估计$p_g$，由$p_g$生成的样本，被最优的判别器当成真实样本</p>\n","site":{"data":{}},"excerpt":"","more":"<p>VAE通过引入隐变量z定义了一个显式的密度函数<br>生成对抗网络（GANs）通过对抗网络生成样本，而无需显式的密度函数<br>对抗学习是一种机器学习领域常用的学习策略，通过引入假样本迷惑模型</p>\n<p>基本思想：训练两个网络</p>\n<ul>\n<li>生成器G：生成虚假样本，试图迷惑判别器</li>\n<li>判别器D：试图区分真实样本和虚假样本</li>\n<li>对抗学习：二者对抗训练</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114252.png\" alt=\"image.png\"></p>\n<p>零和博弈：<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109114538.png\" alt=\"image.png\"><br>最终通过充分训练，渴望算法收敛于一个好的关于数据分布的估计$p_g$，由$p_g$生成的样本，被最优的判别器当成真实样本</p>\n"},{"title":"生成模型","date":"2023-11-09T02:01:30.264Z","_content":"有监督：\n给定数据(x,y)，x为输出，y是对应的标签\n目的：学习一个映射f: x→y\n用于分类、回归、目标检测、语义分割、图像描述等\n\n无监督：\n数据：只有x，没有标签\n目标：学习隐藏的信息（数据背后隐藏的结构、主题、情感等）\n用于聚类、特征降维、特征学习、密度估计等\n\n判别模型：\n同时需要输入X和标签Y，试图通过某个判别函数建模条件分布P(Y|X)\n例如softmax回归，SVM等\n不能建模P(X)，即观测到某个样本的概率\n\n生成模型：\n不需要标签Y，试图建模P(X, Y), P(X|Y), P(X)等\n可以建模P(X)，可以生成新的样本\n\n显式的密度估计：显式定义和求解$p_{model}(x)$，最大似然估计、马尔科夫链\n\n隐式的密度估计：学习一个可以从中抽样出样本的模型$p_{model}(x)$，但并不显式定义它，GAN\n\n### 隐变量模型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109102846.png)\n借助因变量z，$p(x|z;\\theta)$可以用$f(z,\\theta)$逼近，进而把概率密度估计问题转化为函数逼近问题\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103141.png)\n\n#### 蒙特卡洛采样法\n对大多数任务来说，精确估计往往不可行，因而转为近似估计\n蒙特卡洛技术是建立在数值采样基础上的近似推断方法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103422.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103438.png)\n\n穷尽所有的z非常困难\n缩小z的取值范围：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z\n\n### 变分自动编码器\n变分推断是一种通过函数最优化近似估计概率的方法\n基本思想：提出一个分布家族，进一步从中得到一个接近目标分布的分布\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109104935.png)\n编码器：采样神经网络学习z的分布p(z)，例如高斯分布，从p(z)中抽样一个z\n解码器：采用另一个神经网络从p(x|z)抽样得到x\n改良：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105330.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105614.png)\n\n目标函数：学习模型参数$\\theta$，以最大化训练数据的对数似然\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105847.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110011.png)\n\n训练：最大化下界（ELBO）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110043.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110544.png)\n\n#### Conditional VAE\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110715.png)\n\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/生成模型.md","raw":"---\ntitle: 生成模型\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 深度学习\ndate:\n---\n有监督：\n给定数据(x,y)，x为输出，y是对应的标签\n目的：学习一个映射f: x→y\n用于分类、回归、目标检测、语义分割、图像描述等\n\n无监督：\n数据：只有x，没有标签\n目标：学习隐藏的信息（数据背后隐藏的结构、主题、情感等）\n用于聚类、特征降维、特征学习、密度估计等\n\n判别模型：\n同时需要输入X和标签Y，试图通过某个判别函数建模条件分布P(Y|X)\n例如softmax回归，SVM等\n不能建模P(X)，即观测到某个样本的概率\n\n生成模型：\n不需要标签Y，试图建模P(X, Y), P(X|Y), P(X)等\n可以建模P(X)，可以生成新的样本\n\n显式的密度估计：显式定义和求解$p_{model}(x)$，最大似然估计、马尔科夫链\n\n隐式的密度估计：学习一个可以从中抽样出样本的模型$p_{model}(x)$，但并不显式定义它，GAN\n\n### 隐变量模型\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109102846.png)\n借助因变量z，$p(x|z;\\theta)$可以用$f(z,\\theta)$逼近，进而把概率密度估计问题转化为函数逼近问题\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103141.png)\n\n#### 蒙特卡洛采样法\n对大多数任务来说，精确估计往往不可行，因而转为近似估计\n蒙特卡洛技术是建立在数值采样基础上的近似推断方法\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103422.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103438.png)\n\n穷尽所有的z非常困难\n缩小z的取值范围：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z\n\n### 变分自动编码器\n变分推断是一种通过函数最优化近似估计概率的方法\n基本思想：提出一个分布家族，进一步从中得到一个接近目标分布的分布\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109104935.png)\n编码器：采样神经网络学习z的分布p(z)，例如高斯分布，从p(z)中抽样一个z\n解码器：采用另一个神经网络从p(x|z)抽样得到x\n改良：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105330.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105614.png)\n\n目标函数：学习模型参数$\\theta$，以最大化训练数据的对数似然\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105847.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110011.png)\n\n训练：最大化下界（ELBO）\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110043.png)\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110544.png)\n\n#### Conditional VAE\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110715.png)\n\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/生成模型","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12k004gvw8c56wn86f4","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>有监督：<br>给定数据(x,y)，x为输出，y是对应的标签<br>目的：学习一个映射f: x→y<br>用于分类、回归、目标检测、语义分割、图像描述等</p>\n<p>无监督：<br>数据：只有x，没有标签<br>目标：学习隐藏的信息（数据背后隐藏的结构、主题、情感等）<br>用于聚类、特征降维、特征学习、密度估计等</p>\n<p>判别模型：<br>同时需要输入X和标签Y，试图通过某个判别函数建模条件分布P(Y|X)<br>例如softmax回归，SVM等<br>不能建模P(X)，即观测到某个样本的概率</p>\n<p>生成模型：<br>不需要标签Y，试图建模P(X, Y), P(X|Y), P(X)等<br>可以建模P(X)，可以生成新的样本</p>\n<p>显式的密度估计：显式定义和求解$p_{model}(x)$，最大似然估计、马尔科夫链</p>\n<p>隐式的密度估计：学习一个可以从中抽样出样本的模型$p_{model}(x)$，但并不显式定义它，GAN</p>\n<h3 id=\"隐变量模型\"><a href=\"#隐变量模型\" class=\"headerlink\" title=\"隐变量模型\"></a>隐变量模型</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109102846.png\" alt=\"image.png\"><br>借助因变量z，$p(x|z;\\theta)$可以用$f(z,\\theta)$逼近，进而把概率密度估计问题转化为函数逼近问题<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103141.png\" alt=\"image.png\"></p>\n<h4 id=\"蒙特卡洛采样法\"><a href=\"#蒙特卡洛采样法\" class=\"headerlink\" title=\"蒙特卡洛采样法\"></a>蒙特卡洛采样法</h4><p>对大多数任务来说，精确估计往往不可行，因而转为近似估计<br>蒙特卡洛技术是建立在数值采样基础上的近似推断方法<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103422.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103438.png\" alt=\"image.png\"></p>\n<p>穷尽所有的z非常困难<br>缩小z的取值范围：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z</p>\n<h3 id=\"变分自动编码器\"><a href=\"#变分自动编码器\" class=\"headerlink\" title=\"变分自动编码器\"></a>变分自动编码器</h3><p>变分推断是一种通过函数最优化近似估计概率的方法<br>基本思想：提出一个分布家族，进一步从中得到一个接近目标分布的分布<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109104935.png\" alt=\"image.png\"><br>编码器：采样神经网络学习z的分布p(z)，例如高斯分布，从p(z)中抽样一个z<br>解码器：采用另一个神经网络从p(x|z)抽样得到x<br>改良：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105330.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105614.png\" alt=\"image.png\"></p>\n<p>目标函数：学习模型参数$\\theta$，以最大化训练数据的对数似然<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105847.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110011.png\" alt=\"image.png\"></p>\n<p>训练：最大化下界（ELBO）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110043.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110544.png\" alt=\"image.png\"></p>\n<h4 id=\"Conditional-VAE\"><a href=\"#Conditional-VAE\" class=\"headerlink\" title=\"Conditional VAE\"></a>Conditional VAE</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110715.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>有监督：<br>给定数据(x,y)，x为输出，y是对应的标签<br>目的：学习一个映射f: x→y<br>用于分类、回归、目标检测、语义分割、图像描述等</p>\n<p>无监督：<br>数据：只有x，没有标签<br>目标：学习隐藏的信息（数据背后隐藏的结构、主题、情感等）<br>用于聚类、特征降维、特征学习、密度估计等</p>\n<p>判别模型：<br>同时需要输入X和标签Y，试图通过某个判别函数建模条件分布P(Y|X)<br>例如softmax回归，SVM等<br>不能建模P(X)，即观测到某个样本的概率</p>\n<p>生成模型：<br>不需要标签Y，试图建模P(X, Y), P(X|Y), P(X)等<br>可以建模P(X)，可以生成新的样本</p>\n<p>显式的密度估计：显式定义和求解$p_{model}(x)$，最大似然估计、马尔科夫链</p>\n<p>隐式的密度估计：学习一个可以从中抽样出样本的模型$p_{model}(x)$，但并不显式定义它，GAN</p>\n<h3 id=\"隐变量模型\"><a href=\"#隐变量模型\" class=\"headerlink\" title=\"隐变量模型\"></a>隐变量模型</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109102846.png\" alt=\"image.png\"><br>借助因变量z，$p(x|z;\\theta)$可以用$f(z,\\theta)$逼近，进而把概率密度估计问题转化为函数逼近问题<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103141.png\" alt=\"image.png\"></p>\n<h4 id=\"蒙特卡洛采样法\"><a href=\"#蒙特卡洛采样法\" class=\"headerlink\" title=\"蒙特卡洛采样法\"></a>蒙特卡洛采样法</h4><p>对大多数任务来说，精确估计往往不可行，因而转为近似估计<br>蒙特卡洛技术是建立在数值采样基础上的近似推断方法<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103422.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109103438.png\" alt=\"image.png\"></p>\n<p>穷尽所有的z非常困难<br>缩小z的取值范围：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z</p>\n<h3 id=\"变分自动编码器\"><a href=\"#变分自动编码器\" class=\"headerlink\" title=\"变分自动编码器\"></a>变分自动编码器</h3><p>变分推断是一种通过函数最优化近似估计概率的方法<br>基本思想：提出一个分布家族，进一步从中得到一个接近目标分布的分布<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109104935.png\" alt=\"image.png\"><br>编码器：采样神经网络学习z的分布p(z)，例如高斯分布，从p(z)中抽样一个z<br>解码器：采用另一个神经网络从p(x|z)抽样得到x<br>改良：原本从任意分布P(z)中采样得到z→变为从后验分布p(z|X)中采样得到z<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105330.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105614.png\" alt=\"image.png\"></p>\n<p>目标函数：学习模型参数$\\theta$，以最大化训练数据的对数似然<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109105847.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110011.png\" alt=\"image.png\"></p>\n<p>训练：最大化下界（ELBO）<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110043.png\" alt=\"image.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110544.png\" alt=\"image.png\"></p>\n<h4 id=\"Conditional-VAE\"><a href=\"#Conditional-VAE\" class=\"headerlink\" title=\"Conditional VAE\"></a>Conditional VAE</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231109110715.png\" alt=\"image.png\"></p>\n"},{"title":"大作业 酒店控温计费系统 规划","update":null,"_content":"- [x] 需求阶段\n- [x] 设计阶段\n- [x] 开发阶段\n\t第七周：\n\t第八周：前端简单的页面写好，后端功能基本实现\n\t第九周：前后端交互，找验收组开会\n\t第十周：debug，完善功能\n\t\n- [x] 测试阶段：测试阶段\n\n\n做好版本管理，实现\n- [x] 注册\n- [x] 登录\n- [x] 登录：用户登陆成功进入空调管理界面，失败返回该界面重新登陆，在view的login_room中进行实现\n- [x] 管理员界面\n\n用户开启空调功能：\n\ttem_c2中70行点击button后，需要给后端传表单，包含（user_id, room_id）, 传给open_ac函数\n\n\n#### 需求分析：\n**功能**\n客户：设定空调温度和风速\n房间内控制面板：显示使用空调消费的金额\n打印空调使用的详单（什么时间打开，什么时间关闭）\n空调管理员：监控各房间空调的使用状态\n酒店管理员：生成酒店及房间的空调使用统计报表\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234400.png)\n\n\n#### 实现框架：\n\n**Djiago**\n**MVC模型**：\nMVC 模式（Model–view–controller）是软件工程中的一种软件架构模式，把软件系统分为三个基本部分：模型（Model）、视图（View）和控制器（Controller）。\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234417.png)\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234432.png)\n\nDjango 的 MTV 模式本质上和 MVC 是一样的，也是为了各组件间保持松耦合关系，只是定义上有些许不同，Django 的 MTV 分别是指：\n\n- M 表示模型（Model）：编写程序应有的功能，负责业务对象与数据库的映射(ORM)。\n- T 表示模板 (Template)：负责如何把页面(html)展示给用户。\n- V 表示视图（View）：负责业务逻辑，并在适当时候调用 Model和 Template。\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234456.png)\n\nDjango + vue\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921154013.png)\n分工\n1 客户端 @李\n2 服务器 @周 @王\n1 数据库 @高\n\ngit 暂定\n\n- [ ] 学习各自的内容（前端、后端、数据库）\n- [ ] 学习用Git上传代码\n- [x] Github创建一个远程仓库\n\n\n#### 项目功能：\n项目功能图\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231010102445.png)\n\n登陆界面：\n- 用户界面：\n\t可以调节温度风速等，可以查看费用累计\n- 管理界面：\n\t可以查看各房间的空调信息，可以生成指定房间的使用详单\n\n服务器端：\n- 接收用户界面传来的空调数据，①把数据传到管理界面，②把数据传到数据库\n- 根据数据库中的数据计算每个房间的费用，把费用数据传给用户界面（每小时更新一次）\n- 接收管理界面传来的生成详单的请求，根据数据生成详单，传给管理界面\n\n\n##### 功能\n温控类：\n1. 房间号\n2. 空调温度 风速 更改时间点\n3. 用户id\n4. 是否需要维修\n\n\n用户类：\n1. 用户id\n2. 房间号\n3. 入住时间\n4. 退房时间\n\n账单内容：\n1月1号 时间11.00 26度 大风 开\n1月1号 时间12.00 26度 大风 关 1h 10\n\n1月1号 时间11.00 26度 大风 开\n1月1号 时间12.00 26度 大风 关 1h 10\n\n1月1号 时间11.00 26度 大风 开\n1月1号 时间12.00 26度 大风 关 1h 10\n\nsum：100\n\n报表：\n101  空调状态：好/坏  累计费用  用电度数\n102\n103\n\n\n计费系统：\n得到用户使用空调的费用\n\n打印账单：\n后端生成一个表格，传给前端\n\n打印报表：\n后端生成一个表格，传给前端\n\n维修功能：\n用户通知维修，给管理界面\n\n##### 实现\n###### model\n- 正在服务的队列\n- 等待服务的房间对象队列\n- 调度器 调控每个房间的空调状态\n- 服务对象：状态，开始时间，服务时长，房间号，温度，风速，费用\n- 房间对象\n- 生成详单\n\n\n#### 前后端通信\n\n##### 办理入住\n前端传输一个表单，包含三个信息，房间号、用户名、密码![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103141211.png)\n提交后将信息添加到数据库的user表中\n\nTODO：\n- [ ] 当该房间已有用户入住时，办理失败\n- [ ] 用户名是主键，不能和其他用户重复（使用身份证号等）\n\n\n调度队列：\n\n回温系统：\n\n","source":"_posts/Notes/课程/大三（上）/软件工程/大作业 酒店温控计费系统 规划.md","raw":"---\ntitle: 大作业 酒店控温计费系统 规划\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 软件工程\ntags:\n  - 软件工程\n  - 作业\nupdate:\n---\n- [x] 需求阶段\n- [x] 设计阶段\n- [x] 开发阶段\n\t第七周：\n\t第八周：前端简单的页面写好，后端功能基本实现\n\t第九周：前后端交互，找验收组开会\n\t第十周：debug，完善功能\n\t\n- [x] 测试阶段：测试阶段\n\n\n做好版本管理，实现\n- [x] 注册\n- [x] 登录\n- [x] 登录：用户登陆成功进入空调管理界面，失败返回该界面重新登陆，在view的login_room中进行实现\n- [x] 管理员界面\n\n用户开启空调功能：\n\ttem_c2中70行点击button后，需要给后端传表单，包含（user_id, room_id）, 传给open_ac函数\n\n\n#### 需求分析：\n**功能**\n客户：设定空调温度和风速\n房间内控制面板：显示使用空调消费的金额\n打印空调使用的详单（什么时间打开，什么时间关闭）\n空调管理员：监控各房间空调的使用状态\n酒店管理员：生成酒店及房间的空调使用统计报表\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234400.png)\n\n\n#### 实现框架：\n\n**Djiago**\n**MVC模型**：\nMVC 模式（Model–view–controller）是软件工程中的一种软件架构模式，把软件系统分为三个基本部分：模型（Model）、视图（View）和控制器（Controller）。\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234417.png)\n\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234432.png)\n\nDjango 的 MTV 模式本质上和 MVC 是一样的，也是为了各组件间保持松耦合关系，只是定义上有些许不同，Django 的 MTV 分别是指：\n\n- M 表示模型（Model）：编写程序应有的功能，负责业务对象与数据库的映射(ORM)。\n- T 表示模板 (Template)：负责如何把页面(html)展示给用户。\n- V 表示视图（View）：负责业务逻辑，并在适当时候调用 Model和 Template。\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234456.png)\n\nDjango + vue\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921154013.png)\n分工\n1 客户端 @李\n2 服务器 @周 @王\n1 数据库 @高\n\ngit 暂定\n\n- [ ] 学习各自的内容（前端、后端、数据库）\n- [ ] 学习用Git上传代码\n- [x] Github创建一个远程仓库\n\n\n#### 项目功能：\n项目功能图\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231010102445.png)\n\n登陆界面：\n- 用户界面：\n\t可以调节温度风速等，可以查看费用累计\n- 管理界面：\n\t可以查看各房间的空调信息，可以生成指定房间的使用详单\n\n服务器端：\n- 接收用户界面传来的空调数据，①把数据传到管理界面，②把数据传到数据库\n- 根据数据库中的数据计算每个房间的费用，把费用数据传给用户界面（每小时更新一次）\n- 接收管理界面传来的生成详单的请求，根据数据生成详单，传给管理界面\n\n\n##### 功能\n温控类：\n1. 房间号\n2. 空调温度 风速 更改时间点\n3. 用户id\n4. 是否需要维修\n\n\n用户类：\n1. 用户id\n2. 房间号\n3. 入住时间\n4. 退房时间\n\n账单内容：\n1月1号 时间11.00 26度 大风 开\n1月1号 时间12.00 26度 大风 关 1h 10\n\n1月1号 时间11.00 26度 大风 开\n1月1号 时间12.00 26度 大风 关 1h 10\n\n1月1号 时间11.00 26度 大风 开\n1月1号 时间12.00 26度 大风 关 1h 10\n\nsum：100\n\n报表：\n101  空调状态：好/坏  累计费用  用电度数\n102\n103\n\n\n计费系统：\n得到用户使用空调的费用\n\n打印账单：\n后端生成一个表格，传给前端\n\n打印报表：\n后端生成一个表格，传给前端\n\n维修功能：\n用户通知维修，给管理界面\n\n##### 实现\n###### model\n- 正在服务的队列\n- 等待服务的房间对象队列\n- 调度器 调控每个房间的空调状态\n- 服务对象：状态，开始时间，服务时长，房间号，温度，风速，费用\n- 房间对象\n- 生成详单\n\n\n#### 前后端通信\n\n##### 办理入住\n前端传输一个表单，包含三个信息，房间号、用户名、密码![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103141211.png)\n提交后将信息添加到数据库的user表中\n\nTODO：\n- [ ] 当该房间已有用户入住时，办理失败\n- [ ] 用户名是主键，不能和其他用户重复（使用身份证号等）\n\n\n调度队列：\n\n回温系统：\n\n","slug":"Notes/课程/大三（上）/软件工程/大作业 酒店温控计费系统 规划","published":1,"date":"2023-09-21T16:36:34.339Z","updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12k004jvw8cer857c0t","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><ul>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n需求阶段</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n设计阶段</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n开发阶段<br>  第七周：<br>  第八周：前端简单的页面写好，后端功能基本实现<br>  第九周：前后端交互，找验收组开会<br>  第十周：debug，完善功能</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n测试阶段：测试阶段</p>\n</li>\n</ul>\n<p>做好版本管理，实现</p>\n<ul>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 注册</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 登录</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 登录：用户登陆成功进入空调管理界面，失败返回该界面重新登陆，在view的login_room中进行实现</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 管理员界面</li>\n</ul>\n<p>用户开启空调功能：<br>    tem_c2中70行点击button后，需要给后端传表单，包含（user_id, room_id）, 传给open_ac函数</p>\n<h4 id=\"需求分析：\"><a href=\"#需求分析：\" class=\"headerlink\" title=\"需求分析：\"></a>需求分析：</h4><p><strong>功能</strong><br>客户：设定空调温度和风速<br>房间内控制面板：显示使用空调消费的金额<br>打印空调使用的详单（什么时间打开，什么时间关闭）<br>空调管理员：监控各房间空调的使用状态<br>酒店管理员：生成酒店及房间的空调使用统计报表<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234400.png\"></p>\n<h4 id=\"实现框架：\"><a href=\"#实现框架：\" class=\"headerlink\" title=\"实现框架：\"></a>实现框架：</h4><p><strong>Djiago</strong><br><strong>MVC模型</strong>：<br>MVC 模式（Model–view–controller）是软件工程中的一种软件架构模式，把软件系统分为三个基本部分：模型（Model）、视图（View）和控制器（Controller）。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234417.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234432.png\"></p>\n<p>Django 的 MTV 模式本质上和 MVC 是一样的，也是为了各组件间保持松耦合关系，只是定义上有些许不同，Django 的 MTV 分别是指：</p>\n<ul>\n<li>M 表示模型（Model）：编写程序应有的功能，负责业务对象与数据库的映射(ORM)。</li>\n<li>T 表示模板 (Template)：负责如何把页面(html)展示给用户。</li>\n<li>V 表示视图（View）：负责业务逻辑，并在适当时候调用 Model和 Template。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234456.png\"></li>\n</ul>\n<p>Django + vue<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921154013.png\"><br>分工<br>1 客户端 @李<br>2 服务器 @周 @王<br>1 数据库 @高</p>\n<p>git 暂定</p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> 学习各自的内容（前端、后端、数据库）</li>\n<li><input disabled=\"\" type=\"checkbox\"> 学习用Git上传代码</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> Github创建一个远程仓库</li>\n</ul>\n<h4 id=\"项目功能：\"><a href=\"#项目功能：\" class=\"headerlink\" title=\"项目功能：\"></a>项目功能：</h4><p>项目功能图<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231010102445.png\" alt=\"image.png\"></p>\n<p>登陆界面：</p>\n<ul>\n<li>用户界面：<br>  可以调节温度风速等，可以查看费用累计</li>\n<li>管理界面：<br>  可以查看各房间的空调信息，可以生成指定房间的使用详单</li>\n</ul>\n<p>服务器端：</p>\n<ul>\n<li>接收用户界面传来的空调数据，①把数据传到管理界面，②把数据传到数据库</li>\n<li>根据数据库中的数据计算每个房间的费用，把费用数据传给用户界面（每小时更新一次）</li>\n<li>接收管理界面传来的生成详单的请求，根据数据生成详单，传给管理界面</li>\n</ul>\n<h5 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h5><p>温控类：</p>\n<ol>\n<li>房间号</li>\n<li>空调温度 风速 更改时间点</li>\n<li>用户id</li>\n<li>是否需要维修</li>\n</ol>\n<p>用户类：</p>\n<ol>\n<li>用户id</li>\n<li>房间号</li>\n<li>入住时间</li>\n<li>退房时间</li>\n</ol>\n<p>账单内容：<br>1月1号 时间11.00 26度 大风 开<br>1月1号 时间12.00 26度 大风 关 1h 10</p>\n<p>1月1号 时间11.00 26度 大风 开<br>1月1号 时间12.00 26度 大风 关 1h 10</p>\n<p>1月1号 时间11.00 26度 大风 开<br>1月1号 时间12.00 26度 大风 关 1h 10</p>\n<p>sum：100</p>\n<p>报表：<br>101  空调状态：好&#x2F;坏  累计费用  用电度数<br>102<br>103</p>\n<p>计费系统：<br>得到用户使用空调的费用</p>\n<p>打印账单：<br>后端生成一个表格，传给前端</p>\n<p>打印报表：<br>后端生成一个表格，传给前端</p>\n<p>维修功能：<br>用户通知维修，给管理界面</p>\n<h5 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h5><h6 id=\"model\"><a href=\"#model\" class=\"headerlink\" title=\"model\"></a>model</h6><ul>\n<li>正在服务的队列</li>\n<li>等待服务的房间对象队列</li>\n<li>调度器 调控每个房间的空调状态</li>\n<li>服务对象：状态，开始时间，服务时长，房间号，温度，风速，费用</li>\n<li>房间对象</li>\n<li>生成详单</li>\n</ul>\n<h4 id=\"前后端通信\"><a href=\"#前后端通信\" class=\"headerlink\" title=\"前后端通信\"></a>前后端通信</h4><h5 id=\"办理入住\"><a href=\"#办理入住\" class=\"headerlink\" title=\"办理入住\"></a>办理入住</h5><p>前端传输一个表单，包含三个信息，房间号、用户名、密码<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103141211.png\" alt=\"image.png\"><br>提交后将信息添加到数据库的user表中</p>\n<p>TODO：</p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> 当该房间已有用户入住时，办理失败</li>\n<li><input disabled=\"\" type=\"checkbox\"> 用户名是主键，不能和其他用户重复（使用身份证号等）</li>\n</ul>\n<p>调度队列：</p>\n<p>回温系统：</p>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n需求阶段</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n设计阶段</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n开发阶段<br>  第七周：<br>  第八周：前端简单的页面写好，后端功能基本实现<br>  第九周：前后端交互，找验收组开会<br>  第十周：debug，完善功能</p>\n</li>\n<li><p><input checked=\"\" disabled=\"\" type=\"checkbox\"> \n测试阶段：测试阶段</p>\n</li>\n</ul>\n<p>做好版本管理，实现</p>\n<ul>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 注册</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 登录</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 登录：用户登陆成功进入空调管理界面，失败返回该界面重新登陆，在view的login_room中进行实现</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> 管理员界面</li>\n</ul>\n<p>用户开启空调功能：<br>    tem_c2中70行点击button后，需要给后端传表单，包含（user_id, room_id）, 传给open_ac函数</p>\n<h4 id=\"需求分析：\"><a href=\"#需求分析：\" class=\"headerlink\" title=\"需求分析：\"></a>需求分析：</h4><p><strong>功能</strong><br>客户：设定空调温度和风速<br>房间内控制面板：显示使用空调消费的金额<br>打印空调使用的详单（什么时间打开，什么时间关闭）<br>空调管理员：监控各房间空调的使用状态<br>酒店管理员：生成酒店及房间的空调使用统计报表<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234400.png\"></p>\n<h4 id=\"实现框架：\"><a href=\"#实现框架：\" class=\"headerlink\" title=\"实现框架：\"></a>实现框架：</h4><p><strong>Djiago</strong><br><strong>MVC模型</strong>：<br>MVC 模式（Model–view–controller）是软件工程中的一种软件架构模式，把软件系统分为三个基本部分：模型（Model）、视图（View）和控制器（Controller）。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234417.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234432.png\"></p>\n<p>Django 的 MTV 模式本质上和 MVC 是一样的，也是为了各组件间保持松耦合关系，只是定义上有些许不同，Django 的 MTV 分别是指：</p>\n<ul>\n<li>M 表示模型（Model）：编写程序应有的功能，负责业务对象与数据库的映射(ORM)。</li>\n<li>T 表示模板 (Template)：负责如何把页面(html)展示给用户。</li>\n<li>V 表示视图（View）：负责业务逻辑，并在适当时候调用 Model和 Template。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230923234456.png\"></li>\n</ul>\n<p>Django + vue<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230921154013.png\"><br>分工<br>1 客户端 @李<br>2 服务器 @周 @王<br>1 数据库 @高</p>\n<p>git 暂定</p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> 学习各自的内容（前端、后端、数据库）</li>\n<li><input disabled=\"\" type=\"checkbox\"> 学习用Git上传代码</li>\n<li><input checked=\"\" disabled=\"\" type=\"checkbox\"> Github创建一个远程仓库</li>\n</ul>\n<h4 id=\"项目功能：\"><a href=\"#项目功能：\" class=\"headerlink\" title=\"项目功能：\"></a>项目功能：</h4><p>项目功能图<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231010102445.png\" alt=\"image.png\"></p>\n<p>登陆界面：</p>\n<ul>\n<li>用户界面：<br>  可以调节温度风速等，可以查看费用累计</li>\n<li>管理界面：<br>  可以查看各房间的空调信息，可以生成指定房间的使用详单</li>\n</ul>\n<p>服务器端：</p>\n<ul>\n<li>接收用户界面传来的空调数据，①把数据传到管理界面，②把数据传到数据库</li>\n<li>根据数据库中的数据计算每个房间的费用，把费用数据传给用户界面（每小时更新一次）</li>\n<li>接收管理界面传来的生成详单的请求，根据数据生成详单，传给管理界面</li>\n</ul>\n<h5 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h5><p>温控类：</p>\n<ol>\n<li>房间号</li>\n<li>空调温度 风速 更改时间点</li>\n<li>用户id</li>\n<li>是否需要维修</li>\n</ol>\n<p>用户类：</p>\n<ol>\n<li>用户id</li>\n<li>房间号</li>\n<li>入住时间</li>\n<li>退房时间</li>\n</ol>\n<p>账单内容：<br>1月1号 时间11.00 26度 大风 开<br>1月1号 时间12.00 26度 大风 关 1h 10</p>\n<p>1月1号 时间11.00 26度 大风 开<br>1月1号 时间12.00 26度 大风 关 1h 10</p>\n<p>1月1号 时间11.00 26度 大风 开<br>1月1号 时间12.00 26度 大风 关 1h 10</p>\n<p>sum：100</p>\n<p>报表：<br>101  空调状态：好&#x2F;坏  累计费用  用电度数<br>102<br>103</p>\n<p>计费系统：<br>得到用户使用空调的费用</p>\n<p>打印账单：<br>后端生成一个表格，传给前端</p>\n<p>打印报表：<br>后端生成一个表格，传给前端</p>\n<p>维修功能：<br>用户通知维修，给管理界面</p>\n<h5 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h5><h6 id=\"model\"><a href=\"#model\" class=\"headerlink\" title=\"model\"></a>model</h6><ul>\n<li>正在服务的队列</li>\n<li>等待服务的房间对象队列</li>\n<li>调度器 调控每个房间的空调状态</li>\n<li>服务对象：状态，开始时间，服务时长，房间号，温度，风速，费用</li>\n<li>房间对象</li>\n<li>生成详单</li>\n</ul>\n<h4 id=\"前后端通信\"><a href=\"#前后端通信\" class=\"headerlink\" title=\"前后端通信\"></a>前后端通信</h4><h5 id=\"办理入住\"><a href=\"#办理入住\" class=\"headerlink\" title=\"办理入住\"></a>办理入住</h5><p>前端传输一个表单，包含三个信息，房间号、用户名、密码<img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231103141211.png\" alt=\"image.png\"><br>提交后将信息添加到数据库的user表中</p>\n<p>TODO：</p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> 当该房间已有用户入住时，办理失败</li>\n<li><input disabled=\"\" type=\"checkbox\"> 用户名是主键，不能和其他用户重复（使用身份证号等）</li>\n</ul>\n<p>调度队列：</p>\n<p>回温系统：</p>\n"},{"title":"软件生命周期","update":null,"_content":"考核：期中10+平时和作业40+期末50\n\n\n---\n\n软件生命周期指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期\n\n### 软件工程过程\n**软件工程过程**是为了获得**软件产品**，在**软件工具**的支持下由**软件工程师**完成的一系列**软件工程活动**。主要活动有：\n- 编写软件规格说明：规定软件的功能及其使用限制\n- 软件开发：产生满足规格说明的软件\n- 软件确认：通过有效性验证以保证软件能够满足客户的要求\n- 软件演进：为了满足客户的变更要求，软件必须在使用过程中进行不断地改进\n\n工程项目的三个基本目标：\n- 合理的进度\n- 有限的经费\n- 一定的质量\n\n### 软件生命周期\n软件生命周期：指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期。\n\n软件生命周期的六个基本步骤\n- 制定计划  P\n- 需求分析  D\n- 设计  D\n- 程序编码  D\n- 测试  C\n- 运行维护  A\n\n指定出完成开发任务的实施计划\n- 任务列表\n- 每个任务的起止时间\n- 每个任务的责任人\n\n### 软件生命周期模型\n\n#### 传统软件生命周期模型\n- 瀑布模型\n推迟软件实现，强调在软件实现之前必须进行分析和设计工作\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111030.png)\n- 演化模型\n提倡两次开发，第一次是试验开发，探索需求\n- 增量模型\n按优先级逐步将需求进行开发\nA: 0, B: 0, C: 0 --> A: 100, B: 0, C: 0 --> A: 100, B: 100, C: 0 --> A: 100, B: 100, C: 100\n- 喷泉模型\n各个开发阶段没有特定的次序要求，可以并行进行，效率高但难于管理\nA: 0, B: 0, C: 0 --> A: 50, B: 40, C: 20 --> A: 100, B: 70, C: 40 --> A: 100, B: 100, C: 100\n- V模型和W模型\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111109.png)\n- 螺旋模型\n针对大型项目，开发周期长，风险高\n- 构建组装模型\n模块化思想，利用组件库中的软件构件\n- 快速应用开发模型\n增量型软件开发过程模型，强调极短的开发周期，并行开发\n- 原型方法\n根据用户需求快速构建原型，用户根据原型提出修改意见，明确需求。可以作用于上述所有模型\n废弃/追加\n\n#### 新型软件生命周期模型\n- RUP\n四个阶段：初始阶段、细化阶段、构造阶段和交付阶段\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922115056.png)\n- 敏捷及极限编程\n","source":"_posts/Notes/课程/大三（上）/软件工程/软件生命周期.md","raw":"---\ntitle: 软件生命周期\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 软件工程\ntags:\n  - 软件工程\nupdate:\n---\n考核：期中10+平时和作业40+期末50\n\n\n---\n\n软件生命周期指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期\n\n### 软件工程过程\n**软件工程过程**是为了获得**软件产品**，在**软件工具**的支持下由**软件工程师**完成的一系列**软件工程活动**。主要活动有：\n- 编写软件规格说明：规定软件的功能及其使用限制\n- 软件开发：产生满足规格说明的软件\n- 软件确认：通过有效性验证以保证软件能够满足客户的要求\n- 软件演进：为了满足客户的变更要求，软件必须在使用过程中进行不断地改进\n\n工程项目的三个基本目标：\n- 合理的进度\n- 有限的经费\n- 一定的质量\n\n### 软件生命周期\n软件生命周期：指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期。\n\n软件生命周期的六个基本步骤\n- 制定计划  P\n- 需求分析  D\n- 设计  D\n- 程序编码  D\n- 测试  C\n- 运行维护  A\n\n指定出完成开发任务的实施计划\n- 任务列表\n- 每个任务的起止时间\n- 每个任务的责任人\n\n### 软件生命周期模型\n\n#### 传统软件生命周期模型\n- 瀑布模型\n推迟软件实现，强调在软件实现之前必须进行分析和设计工作\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111030.png)\n- 演化模型\n提倡两次开发，第一次是试验开发，探索需求\n- 增量模型\n按优先级逐步将需求进行开发\nA: 0, B: 0, C: 0 --> A: 100, B: 0, C: 0 --> A: 100, B: 100, C: 0 --> A: 100, B: 100, C: 100\n- 喷泉模型\n各个开发阶段没有特定的次序要求，可以并行进行，效率高但难于管理\nA: 0, B: 0, C: 0 --> A: 50, B: 40, C: 20 --> A: 100, B: 70, C: 40 --> A: 100, B: 100, C: 100\n- V模型和W模型\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111109.png)\n- 螺旋模型\n针对大型项目，开发周期长，风险高\n- 构建组装模型\n模块化思想，利用组件库中的软件构件\n- 快速应用开发模型\n增量型软件开发过程模型，强调极短的开发周期，并行开发\n- 原型方法\n根据用户需求快速构建原型，用户根据原型提出修改意见，明确需求。可以作用于上述所有模型\n废弃/追加\n\n#### 新型软件生命周期模型\n- RUP\n四个阶段：初始阶段、细化阶段、构造阶段和交付阶段\n![](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922115056.png)\n- 敏捷及极限编程\n","slug":"Notes/课程/大三（上）/软件工程/软件生命周期","published":1,"date":"2023-09-21T16:36:34.340Z","updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12k004mvw8cbyc02qz0","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>考核：期中10+平时和作业40+期末50</p>\n<hr>\n<p>软件生命周期指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期</p>\n<h3 id=\"软件工程过程\"><a href=\"#软件工程过程\" class=\"headerlink\" title=\"软件工程过程\"></a>软件工程过程</h3><p><strong>软件工程过程</strong>是为了获得<strong>软件产品</strong>，在<strong>软件工具</strong>的支持下由<strong>软件工程师</strong>完成的一系列<strong>软件工程活动</strong>。主要活动有：</p>\n<ul>\n<li>编写软件规格说明：规定软件的功能及其使用限制</li>\n<li>软件开发：产生满足规格说明的软件</li>\n<li>软件确认：通过有效性验证以保证软件能够满足客户的要求</li>\n<li>软件演进：为了满足客户的变更要求，软件必须在使用过程中进行不断地改进</li>\n</ul>\n<p>工程项目的三个基本目标：</p>\n<ul>\n<li>合理的进度</li>\n<li>有限的经费</li>\n<li>一定的质量</li>\n</ul>\n<h3 id=\"软件生命周期\"><a href=\"#软件生命周期\" class=\"headerlink\" title=\"软件生命周期\"></a>软件生命周期</h3><p>软件生命周期：指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期。</p>\n<p>软件生命周期的六个基本步骤</p>\n<ul>\n<li>制定计划  P</li>\n<li>需求分析  D</li>\n<li>设计  D</li>\n<li>程序编码  D</li>\n<li>测试  C</li>\n<li>运行维护  A</li>\n</ul>\n<p>指定出完成开发任务的实施计划</p>\n<ul>\n<li>任务列表</li>\n<li>每个任务的起止时间</li>\n<li>每个任务的责任人</li>\n</ul>\n<h3 id=\"软件生命周期模型\"><a href=\"#软件生命周期模型\" class=\"headerlink\" title=\"软件生命周期模型\"></a>软件生命周期模型</h3><h4 id=\"传统软件生命周期模型\"><a href=\"#传统软件生命周期模型\" class=\"headerlink\" title=\"传统软件生命周期模型\"></a>传统软件生命周期模型</h4><ul>\n<li>瀑布模型<br>推迟软件实现，强调在软件实现之前必须进行分析和设计工作<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111030.png\"></li>\n<li>演化模型<br>提倡两次开发，第一次是试验开发，探索需求</li>\n<li>增量模型<br>按优先级逐步将需求进行开发<br>A: 0, B: 0, C: 0 –&gt; A: 100, B: 0, C: 0 –&gt; A: 100, B: 100, C: 0 –&gt; A: 100, B: 100, C: 100</li>\n<li>喷泉模型<br>各个开发阶段没有特定的次序要求，可以并行进行，效率高但难于管理<br>A: 0, B: 0, C: 0 –&gt; A: 50, B: 40, C: 20 –&gt; A: 100, B: 70, C: 40 –&gt; A: 100, B: 100, C: 100</li>\n<li>V模型和W模型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111109.png\"></li>\n<li>螺旋模型<br>针对大型项目，开发周期长，风险高</li>\n<li>构建组装模型<br>模块化思想，利用组件库中的软件构件</li>\n<li>快速应用开发模型<br>增量型软件开发过程模型，强调极短的开发周期，并行开发</li>\n<li>原型方法<br>根据用户需求快速构建原型，用户根据原型提出修改意见，明确需求。可以作用于上述所有模型<br>废弃&#x2F;追加</li>\n</ul>\n<h4 id=\"新型软件生命周期模型\"><a href=\"#新型软件生命周期模型\" class=\"headerlink\" title=\"新型软件生命周期模型\"></a>新型软件生命周期模型</h4><ul>\n<li>RUP<br>四个阶段：初始阶段、细化阶段、构造阶段和交付阶段<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922115056.png\"></li>\n<li>敏捷及极限编程</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>考核：期中10+平时和作业40+期末50</p>\n<hr>\n<p>软件生命周期指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期</p>\n<h3 id=\"软件工程过程\"><a href=\"#软件工程过程\" class=\"headerlink\" title=\"软件工程过程\"></a>软件工程过程</h3><p><strong>软件工程过程</strong>是为了获得<strong>软件产品</strong>，在<strong>软件工具</strong>的支持下由<strong>软件工程师</strong>完成的一系列<strong>软件工程活动</strong>。主要活动有：</p>\n<ul>\n<li>编写软件规格说明：规定软件的功能及其使用限制</li>\n<li>软件开发：产生满足规格说明的软件</li>\n<li>软件确认：通过有效性验证以保证软件能够满足客户的要求</li>\n<li>软件演进：为了满足客户的变更要求，软件必须在使用过程中进行不断地改进</li>\n</ul>\n<p>工程项目的三个基本目标：</p>\n<ul>\n<li>合理的进度</li>\n<li>有限的经费</li>\n<li>一定的质量</li>\n</ul>\n<h3 id=\"软件生命周期\"><a href=\"#软件生命周期\" class=\"headerlink\" title=\"软件生命周期\"></a>软件生命周期</h3><p>软件生命周期：指软件产品从考虑其概念开始，直至废弃为止的整个时期，包括概念阶段、分析与设计阶段、构造阶段、移交和运行阶段等不同时期。</p>\n<p>软件生命周期的六个基本步骤</p>\n<ul>\n<li>制定计划  P</li>\n<li>需求分析  D</li>\n<li>设计  D</li>\n<li>程序编码  D</li>\n<li>测试  C</li>\n<li>运行维护  A</li>\n</ul>\n<p>指定出完成开发任务的实施计划</p>\n<ul>\n<li>任务列表</li>\n<li>每个任务的起止时间</li>\n<li>每个任务的责任人</li>\n</ul>\n<h3 id=\"软件生命周期模型\"><a href=\"#软件生命周期模型\" class=\"headerlink\" title=\"软件生命周期模型\"></a>软件生命周期模型</h3><h4 id=\"传统软件生命周期模型\"><a href=\"#传统软件生命周期模型\" class=\"headerlink\" title=\"传统软件生命周期模型\"></a>传统软件生命周期模型</h4><ul>\n<li>瀑布模型<br>推迟软件实现，强调在软件实现之前必须进行分析和设计工作<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111030.png\"></li>\n<li>演化模型<br>提倡两次开发，第一次是试验开发，探索需求</li>\n<li>增量模型<br>按优先级逐步将需求进行开发<br>A: 0, B: 0, C: 0 –&gt; A: 100, B: 0, C: 0 –&gt; A: 100, B: 100, C: 0 –&gt; A: 100, B: 100, C: 100</li>\n<li>喷泉模型<br>各个开发阶段没有特定的次序要求，可以并行进行，效率高但难于管理<br>A: 0, B: 0, C: 0 –&gt; A: 50, B: 40, C: 20 –&gt; A: 100, B: 70, C: 40 –&gt; A: 100, B: 100, C: 100</li>\n<li>V模型和W模型<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922111109.png\"></li>\n<li>螺旋模型<br>针对大型项目，开发周期长，风险高</li>\n<li>构建组装模型<br>模块化思想，利用组件库中的软件构件</li>\n<li>快速应用开发模型<br>增量型软件开发过程模型，强调极短的开发周期，并行开发</li>\n<li>原型方法<br>根据用户需求快速构建原型，用户根据原型提出修改意见，明确需求。可以作用于上述所有模型<br>废弃&#x2F;追加</li>\n</ul>\n<h4 id=\"新型软件生命周期模型\"><a href=\"#新型软件生命周期模型\" class=\"headerlink\" title=\"新型软件生命周期模型\"></a>新型软件生命周期模型</h4><ul>\n<li>RUP<br>四个阶段：初始阶段、细化阶段、构造阶段和交付阶段<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20230922115056.png\"></li>\n<li>敏捷及极限编程</li>\n</ul>\n"},{"title":"网络优化与正则化","date":"2023-09-28T01:57:24.012Z","_content":"### 激活函数\n\n\n#### 梯度截断：防止梯度爆炸\n\n\n\n#### 学习率衰减\n- 余弦衰减：不引入任何参数\n- 分段衰减\n- 逆时衰减\n- 指数衰减\n- 自然指数衰减\n\n#### 批量大小\n批量的大小不影响随机梯度的期望，但是会影响随机梯度的方差\n- 批量越大，随机梯度的方差越小，训练稳定，可以设置较大的学习率\n- 批量越小，设置小的学习率\n\n### 优化方法\n#### 动量梯度下降法\n\n#### RMSProp\n\n#### Adam\n\n### 参数初始化\n#### 权重全零初始化\n\n#### 使用较小的随机值初始化权重\n从均值等于0，方差等于0.01的高斯分布中采样\n```\nW = 0.01 * np.random.randn(Din, Dout)\n```\n适合层数较少的神经网络\n\n- Sigmoid或Tanh函数使用Xavier初始化\n- relu使用kaiming初始化\n\n### 损失函数\n#### 交叉熵\n\n### 归一化\n\n归一化（Normalization）方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到[0, 1]或[−1, 1]区间内，或者映射为服从均值为0、方差为1 的标准正态分布\n\n#### 逐层归一化\n##### 批量归一化\n\n### 网络正则化\n\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/网络优化与正则化.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntitle: 网络优化与正则化\ntags:\n  - 神经网络\ndate:\n---\n### 激活函数\n\n\n#### 梯度截断：防止梯度爆炸\n\n\n\n#### 学习率衰减\n- 余弦衰减：不引入任何参数\n- 分段衰减\n- 逆时衰减\n- 指数衰减\n- 自然指数衰减\n\n#### 批量大小\n批量的大小不影响随机梯度的期望，但是会影响随机梯度的方差\n- 批量越大，随机梯度的方差越小，训练稳定，可以设置较大的学习率\n- 批量越小，设置小的学习率\n\n### 优化方法\n#### 动量梯度下降法\n\n#### RMSProp\n\n#### Adam\n\n### 参数初始化\n#### 权重全零初始化\n\n#### 使用较小的随机值初始化权重\n从均值等于0，方差等于0.01的高斯分布中采样\n```\nW = 0.01 * np.random.randn(Din, Dout)\n```\n适合层数较少的神经网络\n\n- Sigmoid或Tanh函数使用Xavier初始化\n- relu使用kaiming初始化\n\n### 损失函数\n#### 交叉熵\n\n### 归一化\n\n归一化（Normalization）方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到[0, 1]或[−1, 1]区间内，或者映射为服从均值为0、方差为1 的标准正态分布\n\n#### 逐层归一化\n##### 批量归一化\n\n### 网络正则化\n\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/网络优化与正则化","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12l004pvw8c16br7ue3","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><h4 id=\"梯度截断：防止梯度爆炸\"><a href=\"#梯度截断：防止梯度爆炸\" class=\"headerlink\" title=\"梯度截断：防止梯度爆炸\"></a>梯度截断：防止梯度爆炸</h4><h4 id=\"学习率衰减\"><a href=\"#学习率衰减\" class=\"headerlink\" title=\"学习率衰减\"></a>学习率衰减</h4><ul>\n<li>余弦衰减：不引入任何参数</li>\n<li>分段衰减</li>\n<li>逆时衰减</li>\n<li>指数衰减</li>\n<li>自然指数衰减</li>\n</ul>\n<h4 id=\"批量大小\"><a href=\"#批量大小\" class=\"headerlink\" title=\"批量大小\"></a>批量大小</h4><p>批量的大小不影响随机梯度的期望，但是会影响随机梯度的方差</p>\n<ul>\n<li>批量越大，随机梯度的方差越小，训练稳定，可以设置较大的学习率</li>\n<li>批量越小，设置小的学习率</li>\n</ul>\n<h3 id=\"优化方法\"><a href=\"#优化方法\" class=\"headerlink\" title=\"优化方法\"></a>优化方法</h3><h4 id=\"动量梯度下降法\"><a href=\"#动量梯度下降法\" class=\"headerlink\" title=\"动量梯度下降法\"></a>动量梯度下降法</h4><h4 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h4><h4 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h4><h3 id=\"参数初始化\"><a href=\"#参数初始化\" class=\"headerlink\" title=\"参数初始化\"></a>参数初始化</h3><h4 id=\"权重全零初始化\"><a href=\"#权重全零初始化\" class=\"headerlink\" title=\"权重全零初始化\"></a>权重全零初始化</h4><h4 id=\"使用较小的随机值初始化权重\"><a href=\"#使用较小的随机值初始化权重\" class=\"headerlink\" title=\"使用较小的随机值初始化权重\"></a>使用较小的随机值初始化权重</h4><p>从均值等于0，方差等于0.01的高斯分布中采样</p>\n<figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs apache\"><span class=\"hljs-attribute\">W</span> = <span class=\"hljs-number\">0</span>.<span class=\"hljs-number\">01</span> * np.random.randn(Din, Dout)<br></code></pre></td></tr></table></figure>\n<p>适合层数较少的神经网络</p>\n<ul>\n<li>Sigmoid或Tanh函数使用Xavier初始化</li>\n<li>relu使用kaiming初始化</li>\n</ul>\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><h4 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a>交叉熵</h4><h3 id=\"归一化\"><a href=\"#归一化\" class=\"headerlink\" title=\"归一化\"></a>归一化</h3><p>归一化（Normalization）方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到[0, 1]或[−1, 1]区间内，或者映射为服从均值为0、方差为1 的标准正态分布</p>\n<h4 id=\"逐层归一化\"><a href=\"#逐层归一化\" class=\"headerlink\" title=\"逐层归一化\"></a>逐层归一化</h4><h5 id=\"批量归一化\"><a href=\"#批量归一化\" class=\"headerlink\" title=\"批量归一化\"></a>批量归一化</h5><h3 id=\"网络正则化\"><a href=\"#网络正则化\" class=\"headerlink\" title=\"网络正则化\"></a>网络正则化</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><h4 id=\"梯度截断：防止梯度爆炸\"><a href=\"#梯度截断：防止梯度爆炸\" class=\"headerlink\" title=\"梯度截断：防止梯度爆炸\"></a>梯度截断：防止梯度爆炸</h4><h4 id=\"学习率衰减\"><a href=\"#学习率衰减\" class=\"headerlink\" title=\"学习率衰减\"></a>学习率衰减</h4><ul>\n<li>余弦衰减：不引入任何参数</li>\n<li>分段衰减</li>\n<li>逆时衰减</li>\n<li>指数衰减</li>\n<li>自然指数衰减</li>\n</ul>\n<h4 id=\"批量大小\"><a href=\"#批量大小\" class=\"headerlink\" title=\"批量大小\"></a>批量大小</h4><p>批量的大小不影响随机梯度的期望，但是会影响随机梯度的方差</p>\n<ul>\n<li>批量越大，随机梯度的方差越小，训练稳定，可以设置较大的学习率</li>\n<li>批量越小，设置小的学习率</li>\n</ul>\n<h3 id=\"优化方法\"><a href=\"#优化方法\" class=\"headerlink\" title=\"优化方法\"></a>优化方法</h3><h4 id=\"动量梯度下降法\"><a href=\"#动量梯度下降法\" class=\"headerlink\" title=\"动量梯度下降法\"></a>动量梯度下降法</h4><h4 id=\"RMSProp\"><a href=\"#RMSProp\" class=\"headerlink\" title=\"RMSProp\"></a>RMSProp</h4><h4 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h4><h3 id=\"参数初始化\"><a href=\"#参数初始化\" class=\"headerlink\" title=\"参数初始化\"></a>参数初始化</h3><h4 id=\"权重全零初始化\"><a href=\"#权重全零初始化\" class=\"headerlink\" title=\"权重全零初始化\"></a>权重全零初始化</h4><h4 id=\"使用较小的随机值初始化权重\"><a href=\"#使用较小的随机值初始化权重\" class=\"headerlink\" title=\"使用较小的随机值初始化权重\"></a>使用较小的随机值初始化权重</h4><p>从均值等于0，方差等于0.01的高斯分布中采样</p>\n<figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs apache\"><span class=\"hljs-attribute\">W</span> = <span class=\"hljs-number\">0</span>.<span class=\"hljs-number\">01</span> * np.random.randn(Din, Dout)<br></code></pre></td></tr></table></figure>\n<p>适合层数较少的神经网络</p>\n<ul>\n<li>Sigmoid或Tanh函数使用Xavier初始化</li>\n<li>relu使用kaiming初始化</li>\n</ul>\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><h4 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a>交叉熵</h4><h3 id=\"归一化\"><a href=\"#归一化\" class=\"headerlink\" title=\"归一化\"></a>归一化</h3><p>归一化（Normalization）方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到[0, 1]或[−1, 1]区间内，或者映射为服从均值为0、方差为1 的标准正态分布</p>\n<h4 id=\"逐层归一化\"><a href=\"#逐层归一化\" class=\"headerlink\" title=\"逐层归一化\"></a>逐层归一化</h4><h5 id=\"批量归一化\"><a href=\"#批量归一化\" class=\"headerlink\" title=\"批量归一化\"></a>批量归一化</h5><h3 id=\"网络正则化\"><a href=\"#网络正则化\" class=\"headerlink\" title=\"网络正则化\"></a>网络正则化</h3>"},{"title":"神经网络与深度学习","update":null,"_content":"<u></u>教材：\n- 神经网络与深度学习，邱锡鹏 [神经网络与深度学习](https://nndl.github.io/)\n- 动手学深度学习 阿斯顿·张、李沐\n- **深度学习，Ian Goddfellow**\n\n公开课：\n- 李宏毅：机器学习\n- CS224n\n- CS231n\n\n考核：平时作业80（选够100分的题量）+其他20（随堂考试、问答）\n\n---\n\n随堂考试：\n- 前馈神经网络的反向传播 [[前馈神经网络#反向传播算法]]\n- 循环神经网络的反向传播 [[循环神经网络#随时间反向传播（BPTT）]]\n- 注意力机制 [[图神经网络#注意力机制]]\n\n11.16左右随堂考试\n\n默写贝尔曼等式\n\n","source":"_posts/Notes/课程/大三（上）/神经网络与深度学习/神经网络与深度学习.md","raw":"---\ntitle: 神经网络与深度学习\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 神经网络与深度学习\ntags:\n  - 神经网络\n  - 深度学习\nupdate:\n---\n<u></u>教材：\n- 神经网络与深度学习，邱锡鹏 [神经网络与深度学习](https://nndl.github.io/)\n- 动手学深度学习 阿斯顿·张、李沐\n- **深度学习，Ian Goddfellow**\n\n公开课：\n- 李宏毅：机器学习\n- CS224n\n- CS231n\n\n考核：平时作业80（选够100分的题量）+其他20（随堂考试、问答）\n\n---\n\n随堂考试：\n- 前馈神经网络的反向传播 [[前馈神经网络#反向传播算法]]\n- 循环神经网络的反向传播 [[循环神经网络#随时间反向传播（BPTT）]]\n- 注意力机制 [[图神经网络#注意力机制]]\n\n11.16左右随堂考试\n\n默写贝尔曼等式\n\n","slug":"Notes/课程/大三（上）/神经网络与深度学习/神经网络与深度学习","published":1,"date":"2023-09-21T16:36:34.338Z","updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12l004svw8c1m6sgu9l","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><u></u>教材：</p>\n<ul>\n<li>神经网络与深度学习，邱锡鹏 <a href=\"https://nndl.github.io/\">神经网络与深度学习</a></li>\n<li>动手学深度学习 阿斯顿·张、李沐</li>\n<li><strong>深度学习，Ian Goddfellow</strong></li>\n</ul>\n<p>公开课：</p>\n<ul>\n<li>李宏毅：机器学习</li>\n<li>CS224n</li>\n<li>CS231n</li>\n</ul>\n<p>考核：平时作业80（选够100分的题量）+其他20（随堂考试、问答）</p>\n<hr>\n<p>随堂考试：</p>\n<ul>\n<li>前馈神经网络的反向传播 [[前馈神经网络#反向传播算法]]</li>\n<li>循环神经网络的反向传播 [[循环神经网络#随时间反向传播（BPTT）]]</li>\n<li>注意力机制 [[图神经网络#注意力机制]]</li>\n</ul>\n<p>11.16左右随堂考试</p>\n<p>默写贝尔曼等式</p>\n","site":{"data":{}},"excerpt":"","more":"<p><u></u>教材：</p>\n<ul>\n<li>神经网络与深度学习，邱锡鹏 <a href=\"https://nndl.github.io/\">神经网络与深度学习</a></li>\n<li>动手学深度学习 阿斯顿·张、李沐</li>\n<li><strong>深度学习，Ian Goddfellow</strong></li>\n</ul>\n<p>公开课：</p>\n<ul>\n<li>李宏毅：机器学习</li>\n<li>CS224n</li>\n<li>CS231n</li>\n</ul>\n<p>考核：平时作业80（选够100分的题量）+其他20（随堂考试、问答）</p>\n<hr>\n<p>随堂考试：</p>\n<ul>\n<li>前馈神经网络的反向传播 [[前馈神经网络#反向传播算法]]</li>\n<li>循环神经网络的反向传播 [[循环神经网络#随时间反向传播（BPTT）]]</li>\n<li>注意力机制 [[图神经网络#注意力机制]]</li>\n</ul>\n<p>11.16左右随堂考试</p>\n<p>默写贝尔曼等式</p>\n"},{"title":"软件需求分析","date":"2023-10-20T02:03:27.778Z","_content":"","source":"_posts/Notes/课程/大三（上）/软件工程/软件需求分析.md","raw":"---\ntitle: 软件需求分析\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 软件工程\ntags:\n  - 软件工程\ndate:\n---\n","slug":"Notes/课程/大三（上）/软件工程/软件需求分析","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12m004vvw8c7ybt27l6","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script>","site":{"data":{}},"excerpt":"","more":""},{"title":"面向对象分析","date":"2023-10-20T03:12:44.344Z","_content":"### 面向对象的需求分析建模\n面向对象分析方法中的需求分析包含两个模型：领域模型和用例模型。\n\n- 领域模型表示了需求分析阶段“当前系统”逻辑模型的静态结构；\n\n- 用例模型是“目标系统”的逻辑模型，定义了“目标系统”做什么的需求。由以下四个部分组成：\n\t- 用例图\n\t- 用例说明\n\t- 系统顺序图（system sequence diagram）\n\t- 操作契约（operation contract）\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020111522.png)\n\n### 领域模型\n领域模型：针对某一特定领域内概念类或者对象的抽象可视化表示。\n主要用于概括地描述业务背景及重要的业务流程，帮助软件开发人员在短时间内尽快了解业务。\n- 业务背景：可由用户需求说明书或者调研报告中具有代表业务概念或者业务对象的词汇获得，这些词汇可统称为“概念类”；并通过能够代表关系的词汇建立概念类之间的关系，表示成能够代表业务知识结构的类图；\n- 业务流程：一般由提交请求的角色及提供服务的对象所执行的活动（活动及任务节点）构成，活动的输出一般有数据对象和传给另一个活动的消息组成，建议使用UML的活动图进行描述。\n\n### 用例模型\n用例模型由以下四个部分组成：\n- 用例图；\n- 用例说明；\n- 系统顺序图（system sequence diagram，option）；\n- 操作契约（operation contract，option）\n\n以用例为核心从使用者的角度描述和解释待构建系统的功能需求\n\n用例模型的基本结构：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027105954.png)\n\n#### 用例图\n用例图由三个基本元素组成：\n- Actor：称为角色或者参与者，表示使用系统的对象，代表角色的不一定是人，也可以是组织、系统或设备；\n- Use_case：称为用例，描述角色如何使用系统功能实现需求目标的一组成功场景和一系列失败场景的集合；\n- Association：表示角色与用例之间的关系，以及用例和子用例之间的关系；\n\n#### 基本用例与子用例\n- 基本用例：与角色直接相关的用例，表示系统的功能需求；\n\n- 子用例：通过场景描述分析归纳出的用例，也表示了系统的功能，是基本用例的一个组成部分：\n\t- 包含子用例：多个基本用例中的某个与角色交互的场景具有相同的操作，且这些场景都是基本用例中必须执行的步骤，可以将其抽取出来作为基本用例的子用例\n\t- 扩展子用例：（多个）基本用例中的某些场景存在相同的条件判断的情况，可以将其抽取出来作为基本用例的子用例；\n\n#### 系统顺序图SSD\n使用UML的sequence diagram描述角色与系统之间的交互场景实例；\n在用例描述的基础上需进一步确定角色与系统之间的交互信息，并以可编程的方式将其命名；\n系统顺序图中“一般”只需要三个UML的符号元素\n- 顺序图中的对象图标：<类名：对象名>\n- 角色，类的特殊标识；\n- 代表软件系统的对象，一般使用system或者系统命名；\n- 角色与system之间的交互信息，简称消息或操作；\n\t- 同步消息：请求必须有应答才能发送后续消息；\n\t- 异步消息：无须等待消息应答就可发送其他消息；\n\t- 创建消息：创建一个对象实例的消息，SSD中不需要；\n\t- 删除消息：删除一个对象实例的消息。\n\n注意：\n1. SSD是用于替代用例说明文本的一种方式；\n2. 图中只有两个对象，表示角色对象与系统对象；\n3. 图中的消息名称及参数要求以可编程的方式命名；\n4. 消息名称和参数可以通过一个列表使用中文说明具体含义；\n5. 用例图中的每个用例都应该对应一张SSD；\n6. 角色发给系统的指令（系统事件）是操作契约关注的元素\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027114827.png)\n\n\n#### 操作契约\n","source":"_posts/Notes/课程/大三（上）/软件工程/面向对象分析.md","raw":"---\ntitle: 面向对象分析\ncategories:\n  - Notes\n  - 课程\n  - 大三（上）\n  - 软件工程\ntags:\n  - 软件工程\ndate:\n---\n### 面向对象的需求分析建模\n面向对象分析方法中的需求分析包含两个模型：领域模型和用例模型。\n\n- 领域模型表示了需求分析阶段“当前系统”逻辑模型的静态结构；\n\n- 用例模型是“目标系统”的逻辑模型，定义了“目标系统”做什么的需求。由以下四个部分组成：\n\t- 用例图\n\t- 用例说明\n\t- 系统顺序图（system sequence diagram）\n\t- 操作契约（operation contract）\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020111522.png)\n\n### 领域模型\n领域模型：针对某一特定领域内概念类或者对象的抽象可视化表示。\n主要用于概括地描述业务背景及重要的业务流程，帮助软件开发人员在短时间内尽快了解业务。\n- 业务背景：可由用户需求说明书或者调研报告中具有代表业务概念或者业务对象的词汇获得，这些词汇可统称为“概念类”；并通过能够代表关系的词汇建立概念类之间的关系，表示成能够代表业务知识结构的类图；\n- 业务流程：一般由提交请求的角色及提供服务的对象所执行的活动（活动及任务节点）构成，活动的输出一般有数据对象和传给另一个活动的消息组成，建议使用UML的活动图进行描述。\n\n### 用例模型\n用例模型由以下四个部分组成：\n- 用例图；\n- 用例说明；\n- 系统顺序图（system sequence diagram，option）；\n- 操作契约（operation contract，option）\n\n以用例为核心从使用者的角度描述和解释待构建系统的功能需求\n\n用例模型的基本结构：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027105954.png)\n\n#### 用例图\n用例图由三个基本元素组成：\n- Actor：称为角色或者参与者，表示使用系统的对象，代表角色的不一定是人，也可以是组织、系统或设备；\n- Use_case：称为用例，描述角色如何使用系统功能实现需求目标的一组成功场景和一系列失败场景的集合；\n- Association：表示角色与用例之间的关系，以及用例和子用例之间的关系；\n\n#### 基本用例与子用例\n- 基本用例：与角色直接相关的用例，表示系统的功能需求；\n\n- 子用例：通过场景描述分析归纳出的用例，也表示了系统的功能，是基本用例的一个组成部分：\n\t- 包含子用例：多个基本用例中的某个与角色交互的场景具有相同的操作，且这些场景都是基本用例中必须执行的步骤，可以将其抽取出来作为基本用例的子用例\n\t- 扩展子用例：（多个）基本用例中的某些场景存在相同的条件判断的情况，可以将其抽取出来作为基本用例的子用例；\n\n#### 系统顺序图SSD\n使用UML的sequence diagram描述角色与系统之间的交互场景实例；\n在用例描述的基础上需进一步确定角色与系统之间的交互信息，并以可编程的方式将其命名；\n系统顺序图中“一般”只需要三个UML的符号元素\n- 顺序图中的对象图标：<类名：对象名>\n- 角色，类的特殊标识；\n- 代表软件系统的对象，一般使用system或者系统命名；\n- 角色与system之间的交互信息，简称消息或操作；\n\t- 同步消息：请求必须有应答才能发送后续消息；\n\t- 异步消息：无须等待消息应答就可发送其他消息；\n\t- 创建消息：创建一个对象实例的消息，SSD中不需要；\n\t- 删除消息：删除一个对象实例的消息。\n\n注意：\n1. SSD是用于替代用例说明文本的一种方式；\n2. 图中只有两个对象，表示角色对象与系统对象；\n3. 图中的消息名称及参数要求以可编程的方式命名；\n4. 消息名称和参数可以通过一个列表使用中文说明具体含义；\n5. 用例图中的每个用例都应该对应一张SSD；\n6. 角色发给系统的指令（系统事件）是操作契约关注的元素\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027114827.png)\n\n\n#### 操作契约\n","slug":"Notes/课程/大三（上）/软件工程/面向对象分析","published":1,"updated":"2024-03-02T04:19:43.792Z","_id":"clt9kr12m004yvw8c0j5r44bt","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"面向对象的需求分析建模\"><a href=\"#面向对象的需求分析建模\" class=\"headerlink\" title=\"面向对象的需求分析建模\"></a>面向对象的需求分析建模</h3><p>面向对象分析方法中的需求分析包含两个模型：领域模型和用例模型。</p>\n<ul>\n<li><p>领域模型表示了需求分析阶段“当前系统”逻辑模型的静态结构；</p>\n</li>\n<li><p>用例模型是“目标系统”的逻辑模型，定义了“目标系统”做什么的需求。由以下四个部分组成：</p>\n<ul>\n<li>用例图</li>\n<li>用例说明</li>\n<li>系统顺序图（system sequence diagram）</li>\n<li>操作契约（operation contract）</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020111522.png\" alt=\"image.png\"></p>\n<h3 id=\"领域模型\"><a href=\"#领域模型\" class=\"headerlink\" title=\"领域模型\"></a>领域模型</h3><p>领域模型：针对某一特定领域内概念类或者对象的抽象可视化表示。<br>主要用于概括地描述业务背景及重要的业务流程，帮助软件开发人员在短时间内尽快了解业务。</p>\n<ul>\n<li>业务背景：可由用户需求说明书或者调研报告中具有代表业务概念或者业务对象的词汇获得，这些词汇可统称为“概念类”；并通过能够代表关系的词汇建立概念类之间的关系，表示成能够代表业务知识结构的类图；</li>\n<li>业务流程：一般由提交请求的角色及提供服务的对象所执行的活动（活动及任务节点）构成，活动的输出一般有数据对象和传给另一个活动的消息组成，建议使用UML的活动图进行描述。</li>\n</ul>\n<h3 id=\"用例模型\"><a href=\"#用例模型\" class=\"headerlink\" title=\"用例模型\"></a>用例模型</h3><p>用例模型由以下四个部分组成：</p>\n<ul>\n<li>用例图；</li>\n<li>用例说明；</li>\n<li>系统顺序图（system sequence diagram，option）；</li>\n<li>操作契约（operation contract，option）</li>\n</ul>\n<p>以用例为核心从使用者的角度描述和解释待构建系统的功能需求</p>\n<p>用例模型的基本结构：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027105954.png\" alt=\"image.png\"></p>\n<h4 id=\"用例图\"><a href=\"#用例图\" class=\"headerlink\" title=\"用例图\"></a>用例图</h4><p>用例图由三个基本元素组成：</p>\n<ul>\n<li>Actor：称为角色或者参与者，表示使用系统的对象，代表角色的不一定是人，也可以是组织、系统或设备；</li>\n<li>Use_case：称为用例，描述角色如何使用系统功能实现需求目标的一组成功场景和一系列失败场景的集合；</li>\n<li>Association：表示角色与用例之间的关系，以及用例和子用例之间的关系；</li>\n</ul>\n<h4 id=\"基本用例与子用例\"><a href=\"#基本用例与子用例\" class=\"headerlink\" title=\"基本用例与子用例\"></a>基本用例与子用例</h4><ul>\n<li><p>基本用例：与角色直接相关的用例，表示系统的功能需求；</p>\n</li>\n<li><p>子用例：通过场景描述分析归纳出的用例，也表示了系统的功能，是基本用例的一个组成部分：</p>\n<ul>\n<li>包含子用例：多个基本用例中的某个与角色交互的场景具有相同的操作，且这些场景都是基本用例中必须执行的步骤，可以将其抽取出来作为基本用例的子用例</li>\n<li>扩展子用例：（多个）基本用例中的某些场景存在相同的条件判断的情况，可以将其抽取出来作为基本用例的子用例；</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"系统顺序图SSD\"><a href=\"#系统顺序图SSD\" class=\"headerlink\" title=\"系统顺序图SSD\"></a>系统顺序图SSD</h4><p>使用UML的sequence diagram描述角色与系统之间的交互场景实例；<br>在用例描述的基础上需进一步确定角色与系统之间的交互信息，并以可编程的方式将其命名；<br>系统顺序图中“一般”只需要三个UML的符号元素</p>\n<ul>\n<li>顺序图中的对象图标：&lt;类名：对象名&gt;</li>\n<li>角色，类的特殊标识；</li>\n<li>代表软件系统的对象，一般使用system或者系统命名；</li>\n<li>角色与system之间的交互信息，简称消息或操作；<ul>\n<li>同步消息：请求必须有应答才能发送后续消息；</li>\n<li>异步消息：无须等待消息应答就可发送其他消息；</li>\n<li>创建消息：创建一个对象实例的消息，SSD中不需要；</li>\n<li>删除消息：删除一个对象实例的消息。</li>\n</ul>\n</li>\n</ul>\n<p>注意：</p>\n<ol>\n<li>SSD是用于替代用例说明文本的一种方式；</li>\n<li>图中只有两个对象，表示角色对象与系统对象；</li>\n<li>图中的消息名称及参数要求以可编程的方式命名；</li>\n<li>消息名称和参数可以通过一个列表使用中文说明具体含义；</li>\n<li>用例图中的每个用例都应该对应一张SSD；</li>\n<li>角色发给系统的指令（系统事件）是操作契约关注的元素<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027114827.png\" alt=\"image.png\"></li>\n</ol>\n<h4 id=\"操作契约\"><a href=\"#操作契约\" class=\"headerlink\" title=\"操作契约\"></a>操作契约</h4>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"面向对象的需求分析建模\"><a href=\"#面向对象的需求分析建模\" class=\"headerlink\" title=\"面向对象的需求分析建模\"></a>面向对象的需求分析建模</h3><p>面向对象分析方法中的需求分析包含两个模型：领域模型和用例模型。</p>\n<ul>\n<li><p>领域模型表示了需求分析阶段“当前系统”逻辑模型的静态结构；</p>\n</li>\n<li><p>用例模型是“目标系统”的逻辑模型，定义了“目标系统”做什么的需求。由以下四个部分组成：</p>\n<ul>\n<li>用例图</li>\n<li>用例说明</li>\n<li>系统顺序图（system sequence diagram）</li>\n<li>操作契约（operation contract）</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231020111522.png\" alt=\"image.png\"></p>\n<h3 id=\"领域模型\"><a href=\"#领域模型\" class=\"headerlink\" title=\"领域模型\"></a>领域模型</h3><p>领域模型：针对某一特定领域内概念类或者对象的抽象可视化表示。<br>主要用于概括地描述业务背景及重要的业务流程，帮助软件开发人员在短时间内尽快了解业务。</p>\n<ul>\n<li>业务背景：可由用户需求说明书或者调研报告中具有代表业务概念或者业务对象的词汇获得，这些词汇可统称为“概念类”；并通过能够代表关系的词汇建立概念类之间的关系，表示成能够代表业务知识结构的类图；</li>\n<li>业务流程：一般由提交请求的角色及提供服务的对象所执行的活动（活动及任务节点）构成，活动的输出一般有数据对象和传给另一个活动的消息组成，建议使用UML的活动图进行描述。</li>\n</ul>\n<h3 id=\"用例模型\"><a href=\"#用例模型\" class=\"headerlink\" title=\"用例模型\"></a>用例模型</h3><p>用例模型由以下四个部分组成：</p>\n<ul>\n<li>用例图；</li>\n<li>用例说明；</li>\n<li>系统顺序图（system sequence diagram，option）；</li>\n<li>操作契约（operation contract，option）</li>\n</ul>\n<p>以用例为核心从使用者的角度描述和解释待构建系统的功能需求</p>\n<p>用例模型的基本结构：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027105954.png\" alt=\"image.png\"></p>\n<h4 id=\"用例图\"><a href=\"#用例图\" class=\"headerlink\" title=\"用例图\"></a>用例图</h4><p>用例图由三个基本元素组成：</p>\n<ul>\n<li>Actor：称为角色或者参与者，表示使用系统的对象，代表角色的不一定是人，也可以是组织、系统或设备；</li>\n<li>Use_case：称为用例，描述角色如何使用系统功能实现需求目标的一组成功场景和一系列失败场景的集合；</li>\n<li>Association：表示角色与用例之间的关系，以及用例和子用例之间的关系；</li>\n</ul>\n<h4 id=\"基本用例与子用例\"><a href=\"#基本用例与子用例\" class=\"headerlink\" title=\"基本用例与子用例\"></a>基本用例与子用例</h4><ul>\n<li><p>基本用例：与角色直接相关的用例，表示系统的功能需求；</p>\n</li>\n<li><p>子用例：通过场景描述分析归纳出的用例，也表示了系统的功能，是基本用例的一个组成部分：</p>\n<ul>\n<li>包含子用例：多个基本用例中的某个与角色交互的场景具有相同的操作，且这些场景都是基本用例中必须执行的步骤，可以将其抽取出来作为基本用例的子用例</li>\n<li>扩展子用例：（多个）基本用例中的某些场景存在相同的条件判断的情况，可以将其抽取出来作为基本用例的子用例；</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"系统顺序图SSD\"><a href=\"#系统顺序图SSD\" class=\"headerlink\" title=\"系统顺序图SSD\"></a>系统顺序图SSD</h4><p>使用UML的sequence diagram描述角色与系统之间的交互场景实例；<br>在用例描述的基础上需进一步确定角色与系统之间的交互信息，并以可编程的方式将其命名；<br>系统顺序图中“一般”只需要三个UML的符号元素</p>\n<ul>\n<li>顺序图中的对象图标：&lt;类名：对象名&gt;</li>\n<li>角色，类的特殊标识；</li>\n<li>代表软件系统的对象，一般使用system或者系统命名；</li>\n<li>角色与system之间的交互信息，简称消息或操作；<ul>\n<li>同步消息：请求必须有应答才能发送后续消息；</li>\n<li>异步消息：无须等待消息应答就可发送其他消息；</li>\n<li>创建消息：创建一个对象实例的消息，SSD中不需要；</li>\n<li>删除消息：删除一个对象实例的消息。</li>\n</ul>\n</li>\n</ul>\n<p>注意：</p>\n<ol>\n<li>SSD是用于替代用例说明文本的一种方式；</li>\n<li>图中只有两个对象，表示角色对象与系统对象；</li>\n<li>图中的消息名称及参数要求以可编程的方式命名；</li>\n<li>消息名称和参数可以通过一个列表使用中文说明具体含义；</li>\n<li>用例图中的每个用例都应该对应一张SSD；</li>\n<li>角色发给系统的指令（系统事件）是操作契约关注的元素<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20231027114827.png\" alt=\"image.png\"></li>\n</ol>\n<h4 id=\"操作契约\"><a href=\"#操作契约\" class=\"headerlink\" title=\"操作契约\"></a>操作契约</h4>"},{"title":"数学","date":"2024-03-04T12:36:33.880Z","_content":"基础阶段（3-6月）：\n660A组\n1000题A组\n张宇真题大全解1987-2012\n\n强化阶段（7-9月）：\n880","source":"_posts/Notes/考研/数学.md","raw":"---\ntitle: 数学\ncategories:\n  - Notes\n  - 考研\ndate:\ntags:\n---\n基础阶段（3-6月）：\n660A组\n1000题A组\n张宇真题大全解1987-2012\n\n强化阶段（7-9月）：\n880","slug":"Notes/考研/数学","published":1,"updated":"2024-03-04T12:49:46.315Z","_id":"cltcxrc020000bw8ceaok5a23","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>基础阶段（3-6月）：<br>660A组<br>1000题A组<br>张宇真题大全解1987-2012</p>\n<p>强化阶段（7-9月）：<br>880</p>\n","site":{"data":{}},"excerpt":"","more":"<p>基础阶段（3-6月）：<br>660A组<br>1000题A组<br>张宇真题大全解1987-2012</p>\n<p>强化阶段（7-9月）：<br>880</p>\n"},{"title":"BUPT抢课脚本","date":"2024-03-04T14:36:58.293Z","_content":"\n浏览器f12打开控制台运行\n```\n// ----- 需要配置的参数 -----\n/* 需要抢的课程名称, 必修、选修、公选通用, 需要和教务系统上的课程名称完全一致 */\nlet COURSES = [\n  \"蛙泳[男]\",\n];\n/* 需要抢的课程分组名称, 必修、选修、公选通用, 可以用于体育专项的抢课, 需要完全一致 */\nlet COURSE_GROUPS = [\n];\n/* 抢课间隔, 单位毫秒. 推荐数值: 抢课 100ms, 捡漏 500ms */\nlet INTERVAL_MS = 1000;\n/* 是否开启公选课抢课, 默认关闭, 以防止抢到课程名一样的公选课 */\nlet ENABLE_GGXXK = false;\n// ------------------------\n\n// 以下不需要修改\n\nlet mainInterval;\nlet targetCourses = [];\n\nconst start = () => {\n  mainInterval = setInterval(handler, INTERVAL_MS);\n  console.log(\"--- start grabbing courses ---\");\n};\n\nconst stop = () => {\n  clearInterval(mainInterval);\n  console.log(\"--- stop grabbing courses ---\");\n};\n\nconst handler = () => {\n  if (targetCourses.length === 0) {\n    getCourses();\n  }\n\n  console.log(\n    `--- found ${targetCourses.length} courses ---`\n  );\n\n  let paths = [\n    \"/jsxsd/xsxkkc/xxxkOper\", // 选修\n    \"/jsxsd/xsxkkc/bxxkOper\", // 必修\n  ];\n  if (ENABLE_GGXXK) {\n    paths.push(\"/jsxsd/xsxkkc/ggxxkxkOper\"); // 公选\n  }\n  for (let course of targetCourses) {\n    for (let path of paths) {\n      $.get(path, course, console.log);\n    }\n  }\n};\n\nconst getCourses = () => {\n  let params = {\n    sEcho: 1,\n    iColumns: 11,\n    iDisplayStart: 0,\n    iDisplayLength: 999,\n  };\n  let paths = [\n    \"/jsxsd/xsxkkc/xsxkBxxk\", // 必修\n    \"/jsxsd/xsxkkc/xsxkXxxk\", // 选修\n  ];\n  if (ENABLE_GGXXK) {\n    paths.push(\"/jsxsd/xsxkkc/xsxkGgxxkxk\"); // 公选\n  }\n  for (let path of paths) {\n    $.post(path, params, (data) => {\n      let aaData = $.parseJSON(data).aaData;\n      for (let course of aaData) {\n        if (COURSES.includes(course.kcmc)) {\n          targetCourses.push(course);\n        } else if (COURSE_GROUPS.includes(course.fzmc)) {\n          targetCourses.push(course);\n        }\n      }\n    });\n  }\n};\n\nstart();\n```\n","source":"_posts/Notes/花里胡哨/BUPT抢课脚本.md","raw":"---\ntitle: BUPT抢课脚本\ncategories:\n  - Notes\n  - 花里胡哨\ndate:\ntags:\n---\n\n浏览器f12打开控制台运行\n```\n// ----- 需要配置的参数 -----\n/* 需要抢的课程名称, 必修、选修、公选通用, 需要和教务系统上的课程名称完全一致 */\nlet COURSES = [\n  \"蛙泳[男]\",\n];\n/* 需要抢的课程分组名称, 必修、选修、公选通用, 可以用于体育专项的抢课, 需要完全一致 */\nlet COURSE_GROUPS = [\n];\n/* 抢课间隔, 单位毫秒. 推荐数值: 抢课 100ms, 捡漏 500ms */\nlet INTERVAL_MS = 1000;\n/* 是否开启公选课抢课, 默认关闭, 以防止抢到课程名一样的公选课 */\nlet ENABLE_GGXXK = false;\n// ------------------------\n\n// 以下不需要修改\n\nlet mainInterval;\nlet targetCourses = [];\n\nconst start = () => {\n  mainInterval = setInterval(handler, INTERVAL_MS);\n  console.log(\"--- start grabbing courses ---\");\n};\n\nconst stop = () => {\n  clearInterval(mainInterval);\n  console.log(\"--- stop grabbing courses ---\");\n};\n\nconst handler = () => {\n  if (targetCourses.length === 0) {\n    getCourses();\n  }\n\n  console.log(\n    `--- found ${targetCourses.length} courses ---`\n  );\n\n  let paths = [\n    \"/jsxsd/xsxkkc/xxxkOper\", // 选修\n    \"/jsxsd/xsxkkc/bxxkOper\", // 必修\n  ];\n  if (ENABLE_GGXXK) {\n    paths.push(\"/jsxsd/xsxkkc/ggxxkxkOper\"); // 公选\n  }\n  for (let course of targetCourses) {\n    for (let path of paths) {\n      $.get(path, course, console.log);\n    }\n  }\n};\n\nconst getCourses = () => {\n  let params = {\n    sEcho: 1,\n    iColumns: 11,\n    iDisplayStart: 0,\n    iDisplayLength: 999,\n  };\n  let paths = [\n    \"/jsxsd/xsxkkc/xsxkBxxk\", // 必修\n    \"/jsxsd/xsxkkc/xsxkXxxk\", // 选修\n  ];\n  if (ENABLE_GGXXK) {\n    paths.push(\"/jsxsd/xsxkkc/xsxkGgxxkxk\"); // 公选\n  }\n  for (let path of paths) {\n    $.post(path, params, (data) => {\n      let aaData = $.parseJSON(data).aaData;\n      for (let course of aaData) {\n        if (COURSES.includes(course.kcmc)) {\n          targetCourses.push(course);\n        } else if (COURSE_GROUPS.includes(course.fzmc)) {\n          targetCourses.push(course);\n        }\n      }\n    });\n  }\n};\n\nstart();\n```\n","slug":"Notes/花里胡哨/BUPT抢课脚本","published":1,"updated":"2024-03-06T11:52:46.753Z","_id":"cltfqp2lc0000ss8cdw08gdiw","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>浏览器f12打开控制台运行</p>\n<figure class=\"highlight arcade\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs arcade\"><span class=\"hljs-comment\">// ----- 需要配置的参数 -----</span><br><span class=\"hljs-comment\">/* 需要抢的课程名称, 必修、选修、公选通用, 需要和教务系统上的课程名称完全一致 */</span><br>let COURSES = [<br>  <span class=\"hljs-string\">&quot;蛙泳[男]&quot;</span>,<br>];<br><span class=\"hljs-comment\">/* 需要抢的课程分组名称, 必修、选修、公选通用, 可以用于体育专项的抢课, 需要完全一致 */</span><br>let COURSE_GROUPS = [<br>];<br><span class=\"hljs-comment\">/* 抢课间隔, 单位毫秒. 推荐数值: 抢课 100ms, 捡漏 500ms */</span><br>let INTERVAL_MS = <span class=\"hljs-number\">1000</span>;<br><span class=\"hljs-comment\">/* 是否开启公选课抢课, 默认关闭, 以防止抢到课程名一样的公选课 */</span><br>let ENABLE_GGXXK = <span class=\"hljs-literal\">false</span>;<br><span class=\"hljs-comment\">// ------------------------</span><br><br><span class=\"hljs-comment\">// 以下不需要修改</span><br><br>let mainInterval;<br>let targetCourses = [];<br><br>const start = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  mainInterval = setInterval(handler, INTERVAL_MS);<br>  <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>(<span class=\"hljs-string\">&quot;--- start grabbing courses ---&quot;</span>);<br>&#125;;<br><br>const stop = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  clearInterval(mainInterval);<br>  <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>(<span class=\"hljs-string\">&quot;--- stop grabbing courses ---&quot;</span>);<br>&#125;;<br><br>const handler = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  <span class=\"hljs-keyword\">if</span> (targetCourses.<span class=\"hljs-built_in\">length</span> === <span class=\"hljs-number\">0</span>) &#123;<br>    getCourses();<br>  &#125;<br><br>  <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>(<br>    <span class=\"hljs-string\">`--- found <span class=\"hljs-subst\">$&#123;targetCourses.<span class=\"hljs-built_in\">length</span>&#125;</span> courses ---`</span><br>  );<br><br>  let paths = [<br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xxxkOper&quot;</span>, <span class=\"hljs-comment\">// 选修</span><br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/bxxkOper&quot;</span>, <span class=\"hljs-comment\">// 必修</span><br>  ];<br>  <span class=\"hljs-keyword\">if</span> (ENABLE_GGXXK) &#123;<br>    paths.<span class=\"hljs-built_in\">push</span>(<span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/ggxxkxkOper&quot;</span>); <span class=\"hljs-comment\">// 公选</span><br>  &#125;<br>  <span class=\"hljs-keyword\">for</span> (let course of targetCourses) &#123;<br>    <span class=\"hljs-keyword\">for</span> (let path of paths) &#123;<br>      $.get(path, course, <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>);<br>    &#125;<br>  &#125;<br>&#125;;<br><br>const getCourses = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  let params = &#123;<br>    <span class=\"hljs-attr\">sEcho</span>: <span class=\"hljs-number\">1</span>,<br>    <span class=\"hljs-attr\">iColumns</span>: <span class=\"hljs-number\">11</span>,<br>    <span class=\"hljs-attr\">iDisplayStart</span>: <span class=\"hljs-number\">0</span>,<br>    <span class=\"hljs-attr\">iDisplayLength</span>: <span class=\"hljs-number\">999</span>,<br>  &#125;;<br>  let paths = [<br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xsxkBxxk&quot;</span>, <span class=\"hljs-comment\">// 必修</span><br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xsxkXxxk&quot;</span>, <span class=\"hljs-comment\">// 选修</span><br>  ];<br>  <span class=\"hljs-keyword\">if</span> (ENABLE_GGXXK) &#123;<br>    paths.<span class=\"hljs-built_in\">push</span>(<span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xsxkGgxxkxk&quot;</span>); <span class=\"hljs-comment\">// 公选</span><br>  &#125;<br>  <span class=\"hljs-keyword\">for</span> (let path of paths) &#123;<br>    $.post(path, params, (data) =&gt; &#123;<br>      let aaData = $.parseJSON(data).aaData;<br>      <span class=\"hljs-keyword\">for</span> (let course of aaData) &#123;<br>        <span class=\"hljs-keyword\">if</span> (COURSES.<span class=\"hljs-built_in\">includes</span>(course.kcmc)) &#123;<br>          targetCourses.<span class=\"hljs-built_in\">push</span>(course);<br>        &#125; <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (COURSE_GROUPS.<span class=\"hljs-built_in\">includes</span>(course.fzmc)) &#123;<br>          targetCourses.<span class=\"hljs-built_in\">push</span>(course);<br>        &#125;<br>      &#125;<br>    &#125;);<br>  &#125;<br>&#125;;<br><br>start();<br></code></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>浏览器f12打开控制台运行</p>\n<figure class=\"highlight arcade\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs arcade\"><span class=\"hljs-comment\">// ----- 需要配置的参数 -----</span><br><span class=\"hljs-comment\">/* 需要抢的课程名称, 必修、选修、公选通用, 需要和教务系统上的课程名称完全一致 */</span><br>let COURSES = [<br>  <span class=\"hljs-string\">&quot;蛙泳[男]&quot;</span>,<br>];<br><span class=\"hljs-comment\">/* 需要抢的课程分组名称, 必修、选修、公选通用, 可以用于体育专项的抢课, 需要完全一致 */</span><br>let COURSE_GROUPS = [<br>];<br><span class=\"hljs-comment\">/* 抢课间隔, 单位毫秒. 推荐数值: 抢课 100ms, 捡漏 500ms */</span><br>let INTERVAL_MS = <span class=\"hljs-number\">1000</span>;<br><span class=\"hljs-comment\">/* 是否开启公选课抢课, 默认关闭, 以防止抢到课程名一样的公选课 */</span><br>let ENABLE_GGXXK = <span class=\"hljs-literal\">false</span>;<br><span class=\"hljs-comment\">// ------------------------</span><br><br><span class=\"hljs-comment\">// 以下不需要修改</span><br><br>let mainInterval;<br>let targetCourses = [];<br><br>const start = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  mainInterval = setInterval(handler, INTERVAL_MS);<br>  <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>(<span class=\"hljs-string\">&quot;--- start grabbing courses ---&quot;</span>);<br>&#125;;<br><br>const stop = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  clearInterval(mainInterval);<br>  <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>(<span class=\"hljs-string\">&quot;--- stop grabbing courses ---&quot;</span>);<br>&#125;;<br><br>const handler = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  <span class=\"hljs-keyword\">if</span> (targetCourses.<span class=\"hljs-built_in\">length</span> === <span class=\"hljs-number\">0</span>) &#123;<br>    getCourses();<br>  &#125;<br><br>  <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>(<br>    <span class=\"hljs-string\">`--- found <span class=\"hljs-subst\">$&#123;targetCourses.<span class=\"hljs-built_in\">length</span>&#125;</span> courses ---`</span><br>  );<br><br>  let paths = [<br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xxxkOper&quot;</span>, <span class=\"hljs-comment\">// 选修</span><br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/bxxkOper&quot;</span>, <span class=\"hljs-comment\">// 必修</span><br>  ];<br>  <span class=\"hljs-keyword\">if</span> (ENABLE_GGXXK) &#123;<br>    paths.<span class=\"hljs-built_in\">push</span>(<span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/ggxxkxkOper&quot;</span>); <span class=\"hljs-comment\">// 公选</span><br>  &#125;<br>  <span class=\"hljs-keyword\">for</span> (let course of targetCourses) &#123;<br>    <span class=\"hljs-keyword\">for</span> (let path of paths) &#123;<br>      $.get(path, course, <span class=\"hljs-built_in\">console</span>.<span class=\"hljs-built_in\">log</span>);<br>    &#125;<br>  &#125;<br>&#125;;<br><br>const getCourses = <span class=\"hljs-function\"><span class=\"hljs-params\">()</span> =&gt;</span> &#123;<br>  let params = &#123;<br>    <span class=\"hljs-attr\">sEcho</span>: <span class=\"hljs-number\">1</span>,<br>    <span class=\"hljs-attr\">iColumns</span>: <span class=\"hljs-number\">11</span>,<br>    <span class=\"hljs-attr\">iDisplayStart</span>: <span class=\"hljs-number\">0</span>,<br>    <span class=\"hljs-attr\">iDisplayLength</span>: <span class=\"hljs-number\">999</span>,<br>  &#125;;<br>  let paths = [<br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xsxkBxxk&quot;</span>, <span class=\"hljs-comment\">// 必修</span><br>    <span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xsxkXxxk&quot;</span>, <span class=\"hljs-comment\">// 选修</span><br>  ];<br>  <span class=\"hljs-keyword\">if</span> (ENABLE_GGXXK) &#123;<br>    paths.<span class=\"hljs-built_in\">push</span>(<span class=\"hljs-string\">&quot;/jsxsd/xsxkkc/xsxkGgxxkxk&quot;</span>); <span class=\"hljs-comment\">// 公选</span><br>  &#125;<br>  <span class=\"hljs-keyword\">for</span> (let path of paths) &#123;<br>    $.post(path, params, (data) =&gt; &#123;<br>      let aaData = $.parseJSON(data).aaData;<br>      <span class=\"hljs-keyword\">for</span> (let course of aaData) &#123;<br>        <span class=\"hljs-keyword\">if</span> (COURSES.<span class=\"hljs-built_in\">includes</span>(course.kcmc)) &#123;<br>          targetCourses.<span class=\"hljs-built_in\">push</span>(course);<br>        &#125; <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span> (COURSE_GROUPS.<span class=\"hljs-built_in\">includes</span>(course.fzmc)) &#123;<br>          targetCourses.<span class=\"hljs-built_in\">push</span>(course);<br>        &#125;<br>      &#125;<br>    &#125;);<br>  &#125;<br>&#125;;<br><br>start();<br></code></pre></td></tr></table></figure>\n"},{"title":"提高免疫力","date":"2024-03-06T11:52:11.098Z","_content":"\n多吃水果\n","source":"_posts/Notes/花里胡哨/提高免疫力.md","raw":"---\ntitle: 提高免疫力\ncategories:\n  - Notes\n  - 花里胡哨\ndate:\ntags:\n---\n\n多吃水果\n","slug":"Notes/花里胡哨/提高免疫力","published":1,"updated":"2024-03-06T13:17:44.561Z","_id":"cltfqp2lj0001ss8c0xtphcnz","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>多吃水果</p>\n","site":{"data":{}},"excerpt":"","more":"<p>多吃水果</p>\n"},{"title":"课程介绍","date":"2024-03-02T04:26:47.628Z","_content":"![af9fb09bb63d946586e6171cc9c2dba.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/af9fb09bb63d946586e6171cc9c2dba.jpg)\n","source":"_posts/Notes/课程/嵌入式系统设计与应用/课程介绍.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 嵌入式系统设计与应用\ntitle: 课程介绍\ndate: \ntags:\n---\n![af9fb09bb63d946586e6171cc9c2dba.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/af9fb09bb63d946586e6171cc9c2dba.jpg)\n","slug":"Notes/课程/嵌入式系统设计与应用/课程介绍","published":1,"updated":"2024-03-19T05:34:25.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltxy9nge00000k8cccts64n8","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/af9fb09bb63d946586e6171cc9c2dba.jpg\" alt=\"af9fb09bb63d946586e6171cc9c2dba.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/af9fb09bb63d946586e6171cc9c2dba.jpg\" alt=\"af9fb09bb63d946586e6171cc9c2dba.jpg\"></p>\n"},{"title":"课程介绍","date":"2024-03-02T04:22:53.372Z","_content":"![Uploading file...odxue]()\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240302130123.png)\n","source":"_posts/Notes/课程/智能信息网络实验/课程介绍.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 智能信息网络实验\ntitle: 课程介绍\ndate: \ntags:\n---\n![Uploading file...odxue]()\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240302130123.png)\n","slug":"Notes/课程/智能信息网络实验/课程介绍","published":1,"updated":"2024-03-19T05:34:36.011Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltxy9ngo00010k8c3ll8fkap","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"/\" alt=\"Uploading file...odxue\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240302130123.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/\" alt=\"Uploading file...odxue\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240302130123.png\" alt=\"image.png\"></p>\n"},{"title":"课程介绍","date":"2024-03-02T04:24:43.014Z","_content":"![a8c8015a481d6a60fb0adbb75b0d40c.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/a8c8015a481d6a60fb0adbb75b0d40c.jpg)\n","source":"_posts/Notes/课程/项目管理与经济决策/课程介绍.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 项目管理与经济决策\ntitle: 课程介绍\ndate: \ntags:\n---\n![a8c8015a481d6a60fb0adbb75b0d40c.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/a8c8015a481d6a60fb0adbb75b0d40c.jpg)\n","slug":"Notes/课程/项目管理与经济决策/课程介绍","published":1,"updated":"2024-03-19T05:34:29.100Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltxy9ngq00020k8c79fp7s05","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/a8c8015a481d6a60fb0adbb75b0d40c.jpg\" alt=\"a8c8015a481d6a60fb0adbb75b0d40c.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/a8c8015a481d6a60fb0adbb75b0d40c.jpg\" alt=\"a8c8015a481d6a60fb0adbb75b0d40c.jpg\"></p>\n"},{"title":"课程介绍","date":"2024-03-02T04:25:34.979Z","_content":"\n![3f64ec18d69c257aa7341a7253ff659.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3f64ec18d69c257aa7341a7253ff659.jpg)\n![3481a40ba72303cfc653daeb1c42a45.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3481a40ba72303cfc653daeb1c42a45.jpg)\n","source":"_posts/Notes/课程/脑与认知科学基础/课程介绍.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 脑与认知科学基础\ntitle: 课程介绍\ndate: \ntags:\n---\n\n![3f64ec18d69c257aa7341a7253ff659.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3f64ec18d69c257aa7341a7253ff659.jpg)\n![3481a40ba72303cfc653daeb1c42a45.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3481a40ba72303cfc653daeb1c42a45.jpg)\n","slug":"Notes/课程/脑与认知科学基础/课程介绍","published":1,"updated":"2024-03-19T05:34:22.542Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltxy9ngr00030k8c2k5w9x2p","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3f64ec18d69c257aa7341a7253ff659.jpg\" alt=\"3f64ec18d69c257aa7341a7253ff659.jpg\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3481a40ba72303cfc653daeb1c42a45.jpg\" alt=\"3481a40ba72303cfc653daeb1c42a45.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3f64ec18d69c257aa7341a7253ff659.jpg\" alt=\"3f64ec18d69c257aa7341a7253ff659.jpg\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/3481a40ba72303cfc653daeb1c42a45.jpg\" alt=\"3481a40ba72303cfc653daeb1c42a45.jpg\"></p>\n"},{"title":"课程介绍","date":"2024-03-02T04:25:11.330Z","_content":"![7e2290ae61e2a3415915799e05c65da.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/7e2290ae61e2a3415915799e05c65da.jpg)\n","source":"_posts/Notes/课程/计算机视觉/课程介绍.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 计算机视觉\ntitle: 课程介绍\ndate: \ntags:\n---\n![7e2290ae61e2a3415915799e05c65da.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/7e2290ae61e2a3415915799e05c65da.jpg)\n","slug":"Notes/课程/计算机视觉/课程介绍","published":1,"updated":"2024-03-19T05:34:18.960Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltxy9ngt00050k8c820376y0","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/7e2290ae61e2a3415915799e05c65da.jpg\" alt=\"7e2290ae61e2a3415915799e05c65da.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/7e2290ae61e2a3415915799e05c65da.jpg\" alt=\"7e2290ae61e2a3415915799e05c65da.jpg\"></p>\n"},{"title":"第一次实验","date":"2024-03-19T05:35:10.273Z","_content":"![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133528.png)\n\n### 预处理\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133545.png)\n\n分帧\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133800.png)\n\n语音信号具有短时平稳性\n### 短时分析技术\n- 时域分析\n- 频域分析\n- 倒谱域分析\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134104.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134207.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134307.png)\n\n### 语音特征提取\n#### 端点检测\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134633.png)\n\n#### 基音周期估计\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134817.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135114.png)\n\n#### 共振峰估计\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135328.png)\n\n#### MFCC特征提取\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135436.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135525.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135713.png)\n","source":"_posts/Notes/课程/语音信息处理/第一次实验.md","raw":"---\ntitle: 第一次实验\ncategories:\n  - Notes\n  - 课程\n  - 语音信息处理\ndate:\ntags:\n---\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133528.png)\n\n### 预处理\n\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133545.png)\n\n分帧\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133800.png)\n\n语音信号具有短时平稳性\n### 短时分析技术\n- 时域分析\n- 频域分析\n- 倒谱域分析\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134104.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134207.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134307.png)\n\n### 语音特征提取\n#### 端点检测\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134633.png)\n\n#### 基音周期估计\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134817.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135114.png)\n\n#### 共振峰估计\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135328.png)\n\n#### MFCC特征提取\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135436.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135525.png)\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135713.png)\n","slug":"Notes/课程/语音信息处理/第一次实验","published":1,"updated":"2024-03-19T05:57:17.655Z","_id":"cltxy9ngv00070k8c1gbu0d3l","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133528.png\" alt=\"image.png\"></p>\n<h3 id=\"预处理\"><a href=\"#预处理\" class=\"headerlink\" title=\"预处理\"></a>预处理</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133545.png\" alt=\"image.png\"></p>\n<p>分帧<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133800.png\" alt=\"image.png\"></p>\n<p>语音信号具有短时平稳性</p>\n<h3 id=\"短时分析技术\"><a href=\"#短时分析技术\" class=\"headerlink\" title=\"短时分析技术\"></a>短时分析技术</h3><ul>\n<li>时域分析</li>\n<li>频域分析</li>\n<li>倒谱域分析<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134104.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134207.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134307.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"语音特征提取\"><a href=\"#语音特征提取\" class=\"headerlink\" title=\"语音特征提取\"></a>语音特征提取</h3><h4 id=\"端点检测\"><a href=\"#端点检测\" class=\"headerlink\" title=\"端点检测\"></a>端点检测</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134633.png\" alt=\"image.png\"></p>\n<h4 id=\"基音周期估计\"><a href=\"#基音周期估计\" class=\"headerlink\" title=\"基音周期估计\"></a>基音周期估计</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134817.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135114.png\" alt=\"image.png\"></p>\n<h4 id=\"共振峰估计\"><a href=\"#共振峰估计\" class=\"headerlink\" title=\"共振峰估计\"></a>共振峰估计</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135328.png\" alt=\"image.png\"></p>\n<h4 id=\"MFCC特征提取\"><a href=\"#MFCC特征提取\" class=\"headerlink\" title=\"MFCC特征提取\"></a>MFCC特征提取</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135436.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135525.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135713.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133528.png\" alt=\"image.png\"></p>\n<h3 id=\"预处理\"><a href=\"#预处理\" class=\"headerlink\" title=\"预处理\"></a>预处理</h3><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133545.png\" alt=\"image.png\"></p>\n<p>分帧<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319133800.png\" alt=\"image.png\"></p>\n<p>语音信号具有短时平稳性</p>\n<h3 id=\"短时分析技术\"><a href=\"#短时分析技术\" class=\"headerlink\" title=\"短时分析技术\"></a>短时分析技术</h3><ul>\n<li>时域分析</li>\n<li>频域分析</li>\n<li>倒谱域分析<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134104.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134207.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134307.png\" alt=\"image.png\"></li>\n</ul>\n<h3 id=\"语音特征提取\"><a href=\"#语音特征提取\" class=\"headerlink\" title=\"语音特征提取\"></a>语音特征提取</h3><h4 id=\"端点检测\"><a href=\"#端点检测\" class=\"headerlink\" title=\"端点检测\"></a>端点检测</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134633.png\" alt=\"image.png\"></p>\n<h4 id=\"基音周期估计\"><a href=\"#基音周期估计\" class=\"headerlink\" title=\"基音周期估计\"></a>基音周期估计</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319134817.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135114.png\" alt=\"image.png\"></p>\n<h4 id=\"共振峰估计\"><a href=\"#共振峰估计\" class=\"headerlink\" title=\"共振峰估计\"></a>共振峰估计</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135328.png\" alt=\"image.png\"></p>\n<h4 id=\"MFCC特征提取\"><a href=\"#MFCC特征提取\" class=\"headerlink\" title=\"MFCC特征提取\"></a>MFCC特征提取</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135436.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135525.png\" alt=\"image.png\"><br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240319135713.png\" alt=\"image.png\"></p>\n"},{"title":"课程介绍","date":"2024-03-02T04:23:15.187Z","_content":"![f63de4a518d28cf3bd2bb2f21af8ac0.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f63de4a518d28cf3bd2bb2f21af8ac0.jpg)\n\n\n![6892a941f82cbfb0445a3420c32af33.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/6892a941f82cbfb0445a3420c32af33.jpg)\n","source":"_posts/Notes/课程/语音信息处理/课程介绍.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 语音信息处理\ntitle: 课程介绍\ndate: \ntags:\n---\n![f63de4a518d28cf3bd2bb2f21af8ac0.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f63de4a518d28cf3bd2bb2f21af8ac0.jpg)\n\n\n![6892a941f82cbfb0445a3420c32af33.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/6892a941f82cbfb0445a3420c32af33.jpg)\n","slug":"Notes/课程/语音信息处理/课程介绍","published":1,"updated":"2024-03-19T05:34:32.195Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cltxy9ngw00090k8c2zo2gz0b","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f63de4a518d28cf3bd2bb2f21af8ac0.jpg\" alt=\"f63de4a518d28cf3bd2bb2f21af8ac0.jpg\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/6892a941f82cbfb0445a3420c32af33.jpg\" alt=\"6892a941f82cbfb0445a3420c32af33.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/f63de4a518d28cf3bd2bb2f21af8ac0.jpg\" alt=\"f63de4a518d28cf3bd2bb2f21af8ac0.jpg\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/6892a941f82cbfb0445a3420c32af33.jpg\" alt=\"6892a941f82cbfb0445a3420c32af33.jpg\"></p>\n"},{"title":"钱币定位系统","date":"2024-04-02T02:53:29.046Z","_content":"### 算法整体流程\n为了实现钱币定位的功能，实验设计了Canny边缘检测和Hough圆变换两个主要的功能，最终实现了检测出输入图像中各个钱币的边缘并给出各个钱币的圆心坐标与半径的效果。\n\n**Canny边缘检测：**\n该算法以原始图像的灰度图为输入，先对图像进行高斯滤波，使图像更加平滑，然后计算梯度，得到图像的梯度图。接下来，进行非极大化抑制，去除所有非边缘的点。最后，用滞后阈值法将比高阈值大、比低阈值小的像素保留为强边缘，形成完整的边缘线。将处理好的图像保存在指定路径。\n\n**Hough圆变换：**\n将经过Canny边缘检测处理的图像作为霍夫变换的初始图象，对图中的像素点进行霍夫变换的投票，形成投票矩阵。之后，对投票矩阵中超过设定阈值的解码为圆，并将圆心和半径储存并打印。\n经过上述变换后，可以在指定路径下得到最终结果图，并会得到输出的圆心坐标与半径。\n\n### 函数功能说明\n在类`CannyEdgeDetector`中实现了下列函数：\n\n- `caculate_gradients()`\n根据灰度图计算图像的梯度图和梯度方向矩阵，对每个像素点，用Sobel算子计算水平和垂直方向上的梯度。根据计算出的水平和垂直梯度，使用 `cv2.cartToPolar()` 函数计算每个像素点的梯度幅值和梯度方向。\n\n- `non_maximum_suppression()`\n对图像梯度图进行非极大化抑制处理，对每个像素点，根据预设的阈值考虑是否将某些点的梯度置零，去除这些非边缘的点。\n\n- `hysteresis_thresholding()`\n实现了边缘检测中的双阈值滞后阈值化，把梯度大于高阈值的像素标记为边缘点，把与其相邻的梯度高于低阈值的点的梯度设为高阈值。\n\n\n在类`HoughCircleTransform()`中实现了下列函数：\n\n- `Hough_transform_algorithm()`\n该函数基于Canny边缘检测得到的梯度图，对梯度大于零的点进行投票，形成投票矩阵。\n\n- `Select_Circle()`\n对得到的投票矩阵进行遍历，找到投票数超过阈值的候选圆，选出合适的圆储存在列表中。\n\n### 函数输入参数说明\n- Guassian_kernal_size：高斯滤波器核大小\n- high_threshold：双阈值法的高阈值\n- low_threshold：双阈值法的低阈值\n- Hough_transform_step： 图像控件转参数空间的变化比例\n- Hough_transform_threshold： hough变换的门限值\n### 最终结果拟合图\n原始图像：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114130.png)\n经过Canny边缘检测得到的图像：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114026.png)\n经过Hough圆变换得到的图像：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114103.png)\n输出结果：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114249.png)\n\n### 参数对结果的影响\n在实验中调参时发现，预设参数对目标检测的效果有影响：\n- 减小`high_threshold`和`low_threshold`，可以使Canny处理后的图像边缘更加明显\n- 减小hough变换的门限值`Hough_transform_threshold`可增加检测出的圆数量","source":"_posts/Notes/课程/计算机视觉/钱币定位系统.md","raw":"---\ntitle: 钱币定位系统\ncategories:\n  - Notes\n  - 课程\n  - 计算机视觉\ndate:\ntags:\n---\n### 算法整体流程\n为了实现钱币定位的功能，实验设计了Canny边缘检测和Hough圆变换两个主要的功能，最终实现了检测出输入图像中各个钱币的边缘并给出各个钱币的圆心坐标与半径的效果。\n\n**Canny边缘检测：**\n该算法以原始图像的灰度图为输入，先对图像进行高斯滤波，使图像更加平滑，然后计算梯度，得到图像的梯度图。接下来，进行非极大化抑制，去除所有非边缘的点。最后，用滞后阈值法将比高阈值大、比低阈值小的像素保留为强边缘，形成完整的边缘线。将处理好的图像保存在指定路径。\n\n**Hough圆变换：**\n将经过Canny边缘检测处理的图像作为霍夫变换的初始图象，对图中的像素点进行霍夫变换的投票，形成投票矩阵。之后，对投票矩阵中超过设定阈值的解码为圆，并将圆心和半径储存并打印。\n经过上述变换后，可以在指定路径下得到最终结果图，并会得到输出的圆心坐标与半径。\n\n### 函数功能说明\n在类`CannyEdgeDetector`中实现了下列函数：\n\n- `caculate_gradients()`\n根据灰度图计算图像的梯度图和梯度方向矩阵，对每个像素点，用Sobel算子计算水平和垂直方向上的梯度。根据计算出的水平和垂直梯度，使用 `cv2.cartToPolar()` 函数计算每个像素点的梯度幅值和梯度方向。\n\n- `non_maximum_suppression()`\n对图像梯度图进行非极大化抑制处理，对每个像素点，根据预设的阈值考虑是否将某些点的梯度置零，去除这些非边缘的点。\n\n- `hysteresis_thresholding()`\n实现了边缘检测中的双阈值滞后阈值化，把梯度大于高阈值的像素标记为边缘点，把与其相邻的梯度高于低阈值的点的梯度设为高阈值。\n\n\n在类`HoughCircleTransform()`中实现了下列函数：\n\n- `Hough_transform_algorithm()`\n该函数基于Canny边缘检测得到的梯度图，对梯度大于零的点进行投票，形成投票矩阵。\n\n- `Select_Circle()`\n对得到的投票矩阵进行遍历，找到投票数超过阈值的候选圆，选出合适的圆储存在列表中。\n\n### 函数输入参数说明\n- Guassian_kernal_size：高斯滤波器核大小\n- high_threshold：双阈值法的高阈值\n- low_threshold：双阈值法的低阈值\n- Hough_transform_step： 图像控件转参数空间的变化比例\n- Hough_transform_threshold： hough变换的门限值\n### 最终结果拟合图\n原始图像：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114130.png)\n经过Canny边缘检测得到的图像：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114026.png)\n经过Hough圆变换得到的图像：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114103.png)\n输出结果：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114249.png)\n\n### 参数对结果的影响\n在实验中调参时发现，预设参数对目标检测的效果有影响：\n- 减小`high_threshold`和`low_threshold`，可以使Canny处理后的图像边缘更加明显\n- 减小hough变换的门限值`Hough_transform_threshold`可增加检测出的圆数量","slug":"Notes/课程/计算机视觉/钱币定位系统","published":1,"updated":"2024-04-02T03:50:23.386Z","_id":"cluhsoscm0000og8c7tdh1jvj","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"算法整体流程\"><a href=\"#算法整体流程\" class=\"headerlink\" title=\"算法整体流程\"></a>算法整体流程</h3><p>为了实现钱币定位的功能，实验设计了Canny边缘检测和Hough圆变换两个主要的功能，最终实现了检测出输入图像中各个钱币的边缘并给出各个钱币的圆心坐标与半径的效果。</p>\n<p><strong>Canny边缘检测：</strong><br>该算法以原始图像的灰度图为输入，先对图像进行高斯滤波，使图像更加平滑，然后计算梯度，得到图像的梯度图。接下来，进行非极大化抑制，去除所有非边缘的点。最后，用滞后阈值法将比高阈值大、比低阈值小的像素保留为强边缘，形成完整的边缘线。将处理好的图像保存在指定路径。</p>\n<p><strong>Hough圆变换：</strong><br>将经过Canny边缘检测处理的图像作为霍夫变换的初始图象，对图中的像素点进行霍夫变换的投票，形成投票矩阵。之后，对投票矩阵中超过设定阈值的解码为圆，并将圆心和半径储存并打印。<br>经过上述变换后，可以在指定路径下得到最终结果图，并会得到输出的圆心坐标与半径。</p>\n<h3 id=\"函数功能说明\"><a href=\"#函数功能说明\" class=\"headerlink\" title=\"函数功能说明\"></a>函数功能说明</h3><p>在类<code>CannyEdgeDetector</code>中实现了下列函数：</p>\n<ul>\n<li><p><code>caculate_gradients()</code><br>根据灰度图计算图像的梯度图和梯度方向矩阵，对每个像素点，用Sobel算子计算水平和垂直方向上的梯度。根据计算出的水平和垂直梯度，使用 <code>cv2.cartToPolar()</code> 函数计算每个像素点的梯度幅值和梯度方向。</p>\n</li>\n<li><p><code>non_maximum_suppression()</code><br>对图像梯度图进行非极大化抑制处理，对每个像素点，根据预设的阈值考虑是否将某些点的梯度置零，去除这些非边缘的点。</p>\n</li>\n<li><p><code>hysteresis_thresholding()</code><br>实现了边缘检测中的双阈值滞后阈值化，把梯度大于高阈值的像素标记为边缘点，把与其相邻的梯度高于低阈值的点的梯度设为高阈值。</p>\n</li>\n</ul>\n<p>在类<code>HoughCircleTransform()</code>中实现了下列函数：</p>\n<ul>\n<li><p><code>Hough_transform_algorithm()</code><br>该函数基于Canny边缘检测得到的梯度图，对梯度大于零的点进行投票，形成投票矩阵。</p>\n</li>\n<li><p><code>Select_Circle()</code><br>对得到的投票矩阵进行遍历，找到投票数超过阈值的候选圆，选出合适的圆储存在列表中。</p>\n</li>\n</ul>\n<h3 id=\"函数输入参数说明\"><a href=\"#函数输入参数说明\" class=\"headerlink\" title=\"函数输入参数说明\"></a>函数输入参数说明</h3><ul>\n<li>Guassian_kernal_size：高斯滤波器核大小</li>\n<li>high_threshold：双阈值法的高阈值</li>\n<li>low_threshold：双阈值法的低阈值</li>\n<li>Hough_transform_step： 图像控件转参数空间的变化比例</li>\n<li>Hough_transform_threshold： hough变换的门限值</li>\n</ul>\n<h3 id=\"最终结果拟合图\"><a href=\"#最终结果拟合图\" class=\"headerlink\" title=\"最终结果拟合图\"></a>最终结果拟合图</h3><p>原始图像：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114130.png\" alt=\"image.png\"><br>经过Canny边缘检测得到的图像：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114026.png\" alt=\"image.png\"><br>经过Hough圆变换得到的图像：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114103.png\" alt=\"image.png\"><br>输出结果：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114249.png\" alt=\"image.png\"></p>\n<h3 id=\"参数对结果的影响\"><a href=\"#参数对结果的影响\" class=\"headerlink\" title=\"参数对结果的影响\"></a>参数对结果的影响</h3><p>在实验中调参时发现，预设参数对目标检测的效果有影响：</p>\n<ul>\n<li>减小<code>high_threshold</code>和<code>low_threshold</code>，可以使Canny处理后的图像边缘更加明显</li>\n<li>减小hough变换的门限值<code>Hough_transform_threshold</code>可增加检测出的圆数量</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"算法整体流程\"><a href=\"#算法整体流程\" class=\"headerlink\" title=\"算法整体流程\"></a>算法整体流程</h3><p>为了实现钱币定位的功能，实验设计了Canny边缘检测和Hough圆变换两个主要的功能，最终实现了检测出输入图像中各个钱币的边缘并给出各个钱币的圆心坐标与半径的效果。</p>\n<p><strong>Canny边缘检测：</strong><br>该算法以原始图像的灰度图为输入，先对图像进行高斯滤波，使图像更加平滑，然后计算梯度，得到图像的梯度图。接下来，进行非极大化抑制，去除所有非边缘的点。最后，用滞后阈值法将比高阈值大、比低阈值小的像素保留为强边缘，形成完整的边缘线。将处理好的图像保存在指定路径。</p>\n<p><strong>Hough圆变换：</strong><br>将经过Canny边缘检测处理的图像作为霍夫变换的初始图象，对图中的像素点进行霍夫变换的投票，形成投票矩阵。之后，对投票矩阵中超过设定阈值的解码为圆，并将圆心和半径储存并打印。<br>经过上述变换后，可以在指定路径下得到最终结果图，并会得到输出的圆心坐标与半径。</p>\n<h3 id=\"函数功能说明\"><a href=\"#函数功能说明\" class=\"headerlink\" title=\"函数功能说明\"></a>函数功能说明</h3><p>在类<code>CannyEdgeDetector</code>中实现了下列函数：</p>\n<ul>\n<li><p><code>caculate_gradients()</code><br>根据灰度图计算图像的梯度图和梯度方向矩阵，对每个像素点，用Sobel算子计算水平和垂直方向上的梯度。根据计算出的水平和垂直梯度，使用 <code>cv2.cartToPolar()</code> 函数计算每个像素点的梯度幅值和梯度方向。</p>\n</li>\n<li><p><code>non_maximum_suppression()</code><br>对图像梯度图进行非极大化抑制处理，对每个像素点，根据预设的阈值考虑是否将某些点的梯度置零，去除这些非边缘的点。</p>\n</li>\n<li><p><code>hysteresis_thresholding()</code><br>实现了边缘检测中的双阈值滞后阈值化，把梯度大于高阈值的像素标记为边缘点，把与其相邻的梯度高于低阈值的点的梯度设为高阈值。</p>\n</li>\n</ul>\n<p>在类<code>HoughCircleTransform()</code>中实现了下列函数：</p>\n<ul>\n<li><p><code>Hough_transform_algorithm()</code><br>该函数基于Canny边缘检测得到的梯度图，对梯度大于零的点进行投票，形成投票矩阵。</p>\n</li>\n<li><p><code>Select_Circle()</code><br>对得到的投票矩阵进行遍历，找到投票数超过阈值的候选圆，选出合适的圆储存在列表中。</p>\n</li>\n</ul>\n<h3 id=\"函数输入参数说明\"><a href=\"#函数输入参数说明\" class=\"headerlink\" title=\"函数输入参数说明\"></a>函数输入参数说明</h3><ul>\n<li>Guassian_kernal_size：高斯滤波器核大小</li>\n<li>high_threshold：双阈值法的高阈值</li>\n<li>low_threshold：双阈值法的低阈值</li>\n<li>Hough_transform_step： 图像控件转参数空间的变化比例</li>\n<li>Hough_transform_threshold： hough变换的门限值</li>\n</ul>\n<h3 id=\"最终结果拟合图\"><a href=\"#最终结果拟合图\" class=\"headerlink\" title=\"最终结果拟合图\"></a>最终结果拟合图</h3><p>原始图像：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114130.png\" alt=\"image.png\"><br>经过Canny边缘检测得到的图像：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114026.png\" alt=\"image.png\"><br>经过Hough圆变换得到的图像：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114103.png\" alt=\"image.png\"><br>输出结果：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240402114249.png\" alt=\"image.png\"></p>\n<h3 id=\"参数对结果的影响\"><a href=\"#参数对结果的影响\" class=\"headerlink\" title=\"参数对结果的影响\"></a>参数对结果的影响</h3><p>在实验中调参时发现，预设参数对目标检测的效果有影响：</p>\n<ul>\n<li>减小<code>high_threshold</code>和<code>low_threshold</code>，可以使Canny处理后的图像边缘更加明显</li>\n<li>减小hough变换的门限值<code>Hough_transform_threshold</code>可增加检测出的圆数量</li>\n</ul>\n"},{"title":"构建汉语词向量","date":"2024-04-11T03:25:11.780Z","_content":"\n### 代码逻辑\n首先对训练数据和测试数据进行处理，按词进行分割然后储存起来，然后通过SVG和SGNS方法计算出训练数据的词向量，再在测试数据上进行测试。\n为此，在代码中定义了`load_data()`, `caculate_sim()`, `svd_embedding()`和`sgns_embedding()`四个方法，分别进行数据处理、计算测试集上向量相关性、通过svd和sgns得到训练集上向量的操作。\n最后将输出结果按要求写入文本文件中。\n### SVG\n```python\ndef svd_embedding(train_data, test_data, dim):\n    model = Word2Vec(train_data, vector_size=dim, window=2, sg=0)\n    vectors = model.wv.vectors\n    svd = TruncatedSVD(n_components=5)  # K=5\n    vec_svd = svd.fit_transform(vectors)  # 将词向量降维\n    word_index = model.wv.key_to_index\n    print('总奇异值个数：', len(word_index))\n\n    selected_singular_values_sum = np.sum(svd.explained_variance_ratio_)\n    all_singular_values_sum = np.sum(svd.explained_variance_)\n    ratio = selected_singular_values_sum / all_singular_values_sum\n    print(\"选取的奇异值之和:\", selected_singular_values_sum)\n    print(\"全部奇异值之和:\", all_singular_values_sum)\n    print(\"选取的奇异值之和与全部奇异值之和的比例:\", ratio)\n\n    svd_result = caculate_sim(test_data, vec_svd, word_index) \n\n    return vec_svd, svd_result\n```\n\nSVD分解算法的主要思想是将一个矩阵分解为三个矩阵的乘积，即$A = UΣV^T$，其中$A$是待分解的矩阵，$U$是左奇异矩阵，$Σ$是奇异值矩阵，$V^T$是右奇异矩阵。在代码中，保留的主成分数量为$K$，然后将原始矩阵$A$分解为$U_kΣ_kV_k^T$，其中$U_k$、$Σ_k$和$V_k^T$分别是前$K个$左奇异矩阵、奇异值矩阵和右奇异矩阵。\n在SVD中，非零奇异值的数量即为原始矩阵的秩，等于词汇表的大小，代码中`word_index`的长度即为非零奇异值的个数，在本实验数据中为24843。\n在代码中使用了TruncatedSVD方法，选取了K=5个奇异值进行模型分析。\n选取的奇异值之和: 0.47254738\n全部奇异值之和: 1.6644406\n选取的奇异值之和与全部奇异值之和的比例: 0.28390762\n\n### SGNS\n```python\ndef sgns_embedding(train_data, test_data, dim):\n    model = Word2Vec(train_data, vector_size=dim, window=2, sg=1, negative=8)  # 采用sgns算法\n    vec_sgns = model.wv.vectors\n    word_index = model.wv.key_to_index\n    sgns_result = caculate_sim(test_data, vec_sgns, word_index)\n  \n    return vec_sgns, sgns_result\n```\n初始词向量是由Word2Vec模型随机初始化的，是随机的小数值。\n在代码中，词向量的维数通过 `vector_size` 参数指定，在此选择了维数为128。\n在Word2Vec模型中，学习率、训练算法的学习率、训练批次大小和训练轮数等由模型的默认值控制，对使用者透明。","source":"_posts/Notes/课程/自然语言处理/构建汉语词向量.md","raw":"---\ntitle: 构建汉语词向量\ncategories:\n  - Notes\n  - 课程\n  - 自然语言处理\ndate:\ntags:\n---\n\n### 代码逻辑\n首先对训练数据和测试数据进行处理，按词进行分割然后储存起来，然后通过SVG和SGNS方法计算出训练数据的词向量，再在测试数据上进行测试。\n为此，在代码中定义了`load_data()`, `caculate_sim()`, `svd_embedding()`和`sgns_embedding()`四个方法，分别进行数据处理、计算测试集上向量相关性、通过svd和sgns得到训练集上向量的操作。\n最后将输出结果按要求写入文本文件中。\n### SVG\n```python\ndef svd_embedding(train_data, test_data, dim):\n    model = Word2Vec(train_data, vector_size=dim, window=2, sg=0)\n    vectors = model.wv.vectors\n    svd = TruncatedSVD(n_components=5)  # K=5\n    vec_svd = svd.fit_transform(vectors)  # 将词向量降维\n    word_index = model.wv.key_to_index\n    print('总奇异值个数：', len(word_index))\n\n    selected_singular_values_sum = np.sum(svd.explained_variance_ratio_)\n    all_singular_values_sum = np.sum(svd.explained_variance_)\n    ratio = selected_singular_values_sum / all_singular_values_sum\n    print(\"选取的奇异值之和:\", selected_singular_values_sum)\n    print(\"全部奇异值之和:\", all_singular_values_sum)\n    print(\"选取的奇异值之和与全部奇异值之和的比例:\", ratio)\n\n    svd_result = caculate_sim(test_data, vec_svd, word_index) \n\n    return vec_svd, svd_result\n```\n\nSVD分解算法的主要思想是将一个矩阵分解为三个矩阵的乘积，即$A = UΣV^T$，其中$A$是待分解的矩阵，$U$是左奇异矩阵，$Σ$是奇异值矩阵，$V^T$是右奇异矩阵。在代码中，保留的主成分数量为$K$，然后将原始矩阵$A$分解为$U_kΣ_kV_k^T$，其中$U_k$、$Σ_k$和$V_k^T$分别是前$K个$左奇异矩阵、奇异值矩阵和右奇异矩阵。\n在SVD中，非零奇异值的数量即为原始矩阵的秩，等于词汇表的大小，代码中`word_index`的长度即为非零奇异值的个数，在本实验数据中为24843。\n在代码中使用了TruncatedSVD方法，选取了K=5个奇异值进行模型分析。\n选取的奇异值之和: 0.47254738\n全部奇异值之和: 1.6644406\n选取的奇异值之和与全部奇异值之和的比例: 0.28390762\n\n### SGNS\n```python\ndef sgns_embedding(train_data, test_data, dim):\n    model = Word2Vec(train_data, vector_size=dim, window=2, sg=1, negative=8)  # 采用sgns算法\n    vec_sgns = model.wv.vectors\n    word_index = model.wv.key_to_index\n    sgns_result = caculate_sim(test_data, vec_sgns, word_index)\n  \n    return vec_sgns, sgns_result\n```\n初始词向量是由Word2Vec模型随机初始化的，是随机的小数值。\n在代码中，词向量的维数通过 `vector_size` 参数指定，在此选择了维数为128。\n在Word2Vec模型中，学习率、训练算法的学习率、训练批次大小和训练轮数等由模型的默认值控制，对使用者透明。","slug":"Notes/课程/自然语言处理/构建汉语词向量","published":1,"updated":"2024-04-11T06:09:22.384Z","_id":"cluuorj090000tw8cakpycf18","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"代码逻辑\"><a href=\"#代码逻辑\" class=\"headerlink\" title=\"代码逻辑\"></a>代码逻辑</h3><p>首先对训练数据和测试数据进行处理，按词进行分割然后储存起来，然后通过SVG和SGNS方法计算出训练数据的词向量，再在测试数据上进行测试。<br>为此，在代码中定义了<code>load_data()</code>, <code>caculate_sim()</code>, <code>svd_embedding()</code>和<code>sgns_embedding()</code>四个方法，分别进行数据处理、计算测试集上向量相关性、通过svd和sgns得到训练集上向量的操作。<br>最后将输出结果按要求写入文本文件中。</p>\n<h3 id=\"SVG\"><a href=\"#SVG\" class=\"headerlink\" title=\"SVG\"></a>SVG</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">svd_embedding</span>(<span class=\"hljs-params\">train_data, test_data, dim</span>):<br>    model = Word2Vec(train_data, vector_size=dim, window=<span class=\"hljs-number\">2</span>, sg=<span class=\"hljs-number\">0</span>)<br>    vectors = model.wv.vectors<br>    svd = TruncatedSVD(n_components=<span class=\"hljs-number\">5</span>)  <span class=\"hljs-comment\"># K=5</span><br>    vec_svd = svd.fit_transform(vectors)  <span class=\"hljs-comment\"># 将词向量降维</span><br>    word_index = model.wv.key_to_index<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;总奇异值个数：&#x27;</span>, <span class=\"hljs-built_in\">len</span>(word_index))<br><br>    selected_singular_values_sum = np.<span class=\"hljs-built_in\">sum</span>(svd.explained_variance_ratio_)<br>    all_singular_values_sum = np.<span class=\"hljs-built_in\">sum</span>(svd.explained_variance_)<br>    ratio = selected_singular_values_sum / all_singular_values_sum<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;选取的奇异值之和:&quot;</span>, selected_singular_values_sum)<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;全部奇异值之和:&quot;</span>, all_singular_values_sum)<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;选取的奇异值之和与全部奇异值之和的比例:&quot;</span>, ratio)<br><br>    svd_result = caculate_sim(test_data, vec_svd, word_index) <br><br>    <span class=\"hljs-keyword\">return</span> vec_svd, svd_result<br></code></pre></td></tr></table></figure>\n\n<p>SVD分解算法的主要思想是将一个矩阵分解为三个矩阵的乘积，即$A &#x3D; UΣV^T$，其中$A$是待分解的矩阵，$U$是左奇异矩阵，$Σ$是奇异值矩阵，$V^T$是右奇异矩阵。在代码中，保留的主成分数量为$K$，然后将原始矩阵$A$分解为$U_kΣ_kV_k^T$，其中$U_k$、$Σ_k$和$V_k^T$分别是前$K个$左奇异矩阵、奇异值矩阵和右奇异矩阵。<br>在SVD中，非零奇异值的数量即为原始矩阵的秩，等于词汇表的大小，代码中<code>word_index</code>的长度即为非零奇异值的个数，在本实验数据中为24843。<br>在代码中使用了TruncatedSVD方法，选取了K&#x3D;5个奇异值进行模型分析。<br>选取的奇异值之和: 0.47254738<br>全部奇异值之和: 1.6644406<br>选取的奇异值之和与全部奇异值之和的比例: 0.28390762</p>\n<h3 id=\"SGNS\"><a href=\"#SGNS\" class=\"headerlink\" title=\"SGNS\"></a>SGNS</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">sgns_embedding</span>(<span class=\"hljs-params\">train_data, test_data, dim</span>):<br>    model = Word2Vec(train_data, vector_size=dim, window=<span class=\"hljs-number\">2</span>, sg=<span class=\"hljs-number\">1</span>, negative=<span class=\"hljs-number\">8</span>)  <span class=\"hljs-comment\"># 采用sgns算法</span><br>    vec_sgns = model.wv.vectors<br>    word_index = model.wv.key_to_index<br>    sgns_result = caculate_sim(test_data, vec_sgns, word_index)<br>  <br>    <span class=\"hljs-keyword\">return</span> vec_sgns, sgns_result<br></code></pre></td></tr></table></figure>\n<p>初始词向量是由Word2Vec模型随机初始化的，是随机的小数值。<br>在代码中，词向量的维数通过 <code>vector_size</code> 参数指定，在此选择了维数为128。<br>在Word2Vec模型中，学习率、训练算法的学习率、训练批次大小和训练轮数等由模型的默认值控制，对使用者透明。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"代码逻辑\"><a href=\"#代码逻辑\" class=\"headerlink\" title=\"代码逻辑\"></a>代码逻辑</h3><p>首先对训练数据和测试数据进行处理，按词进行分割然后储存起来，然后通过SVG和SGNS方法计算出训练数据的词向量，再在测试数据上进行测试。<br>为此，在代码中定义了<code>load_data()</code>, <code>caculate_sim()</code>, <code>svd_embedding()</code>和<code>sgns_embedding()</code>四个方法，分别进行数据处理、计算测试集上向量相关性、通过svd和sgns得到训练集上向量的操作。<br>最后将输出结果按要求写入文本文件中。</p>\n<h3 id=\"SVG\"><a href=\"#SVG\" class=\"headerlink\" title=\"SVG\"></a>SVG</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">svd_embedding</span>(<span class=\"hljs-params\">train_data, test_data, dim</span>):<br>    model = Word2Vec(train_data, vector_size=dim, window=<span class=\"hljs-number\">2</span>, sg=<span class=\"hljs-number\">0</span>)<br>    vectors = model.wv.vectors<br>    svd = TruncatedSVD(n_components=<span class=\"hljs-number\">5</span>)  <span class=\"hljs-comment\"># K=5</span><br>    vec_svd = svd.fit_transform(vectors)  <span class=\"hljs-comment\"># 将词向量降维</span><br>    word_index = model.wv.key_to_index<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;总奇异值个数：&#x27;</span>, <span class=\"hljs-built_in\">len</span>(word_index))<br><br>    selected_singular_values_sum = np.<span class=\"hljs-built_in\">sum</span>(svd.explained_variance_ratio_)<br>    all_singular_values_sum = np.<span class=\"hljs-built_in\">sum</span>(svd.explained_variance_)<br>    ratio = selected_singular_values_sum / all_singular_values_sum<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;选取的奇异值之和:&quot;</span>, selected_singular_values_sum)<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;全部奇异值之和:&quot;</span>, all_singular_values_sum)<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;选取的奇异值之和与全部奇异值之和的比例:&quot;</span>, ratio)<br><br>    svd_result = caculate_sim(test_data, vec_svd, word_index) <br><br>    <span class=\"hljs-keyword\">return</span> vec_svd, svd_result<br></code></pre></td></tr></table></figure>\n\n<p>SVD分解算法的主要思想是将一个矩阵分解为三个矩阵的乘积，即$A &#x3D; UΣV^T$，其中$A$是待分解的矩阵，$U$是左奇异矩阵，$Σ$是奇异值矩阵，$V^T$是右奇异矩阵。在代码中，保留的主成分数量为$K$，然后将原始矩阵$A$分解为$U_kΣ_kV_k^T$，其中$U_k$、$Σ_k$和$V_k^T$分别是前$K个$左奇异矩阵、奇异值矩阵和右奇异矩阵。<br>在SVD中，非零奇异值的数量即为原始矩阵的秩，等于词汇表的大小，代码中<code>word_index</code>的长度即为非零奇异值的个数，在本实验数据中为24843。<br>在代码中使用了TruncatedSVD方法，选取了K&#x3D;5个奇异值进行模型分析。<br>选取的奇异值之和: 0.47254738<br>全部奇异值之和: 1.6644406<br>选取的奇异值之和与全部奇异值之和的比例: 0.28390762</p>\n<h3 id=\"SGNS\"><a href=\"#SGNS\" class=\"headerlink\" title=\"SGNS\"></a>SGNS</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">sgns_embedding</span>(<span class=\"hljs-params\">train_data, test_data, dim</span>):<br>    model = Word2Vec(train_data, vector_size=dim, window=<span class=\"hljs-number\">2</span>, sg=<span class=\"hljs-number\">1</span>, negative=<span class=\"hljs-number\">8</span>)  <span class=\"hljs-comment\"># 采用sgns算法</span><br>    vec_sgns = model.wv.vectors<br>    word_index = model.wv.key_to_index<br>    sgns_result = caculate_sim(test_data, vec_sgns, word_index)<br>  <br>    <span class=\"hljs-keyword\">return</span> vec_sgns, sgns_result<br></code></pre></td></tr></table></figure>\n<p>初始词向量是由Word2Vec模型随机初始化的，是随机的小数值。<br>在代码中，词向量的维数通过 <code>vector_size</code> 参数指定，在此选择了维数为128。<br>在Word2Vec模型中，学习率、训练算法的学习率、训练批次大小和训练轮数等由模型的默认值控制，对使用者透明。</p>\n"},{"title":"第二次实验","date":"2024-04-16T11:00:08.620Z","_content":"### 声纹识别实验\n模型：GMM（高斯混合模型）\n数据集：TIMIT\n\n#### GMM高斯混合模型（Gaussian mixture model）\n对于说话人识别，一组$N$个说话人集合，用一系列$GMM$ 示，即每个说话人$\\mathrm{s}_{\\mathrm{k}}$ 对应一个$\\mathrm{GMM}$  参数  $\\lambda_{\\mathrm{k}}, \\mathrm{k}=1,2, \\ldots, \\mathrm{N}$ 。说话人识别的目标是寻找一个说话人模型，使得给定说话人观测序列  $\\mathrm{X}=x_{1}, x_{2}, \\ldots, x_{t}, \\ldots, x_{T}, x_{t}$  是下标为 $\\mathrm{t}$ 的特征向量（时间维度下标，表示帧，frame），在某个模型参数下的后验概率最大（the maximum a posterior probability），该模型即为给定说话人观测序列，得到的说话人模型。假设帧间是相互独立的，预测模型（说话人模型）  $s_{\\text {predicted }}$ 表示为:\n$$\n\\mathrm{S}_{\\text {predicted }}=\\underset{\\mathrm{k} \\in \\mathcal{S}}{\\arg \\max } \\sum_{\\mathrm{t}=1}^{\\mathrm{T}} \\log \\left[\\mathrm{p}\\left(\\boldsymbol{x}_{\\boldsymbol{t}} \\mid \\lambda_{\\mathrm{k}}\\right)\\right]$$\n\n#### 代码实现\n训练GMM模型\n```python\ndef train_gmm(train_features, train_labels, num_components=16):\n    gmm_models = {}\n    unique_labels = set(train_labels)\n    print('training...')\n    for label in tqdm(unique_labels):\n        # 为每个说话人训练一个 GMM 模型\n        features = train_features[train_labels == label]  # 把这个说话人的所有数据作为训练数据 (n, 2000, 13)\n        gmm = GaussianMixture(n_components=num_components, covariance_type='diag')  # n_components:混合高斯模型数量, covariance_type:协方差类型\n        # TODO：修改fit时传入的数据，把帧数作为sample_num\n        gmm.fit(features.reshape(features.shape[1], features.shape[0]*features.shape[2]))  # 降到2维作为输入\n        gmm_models[label] = gmm  # N个人的GMM模型组成列表\n    # 保存模型\n    with open(\"gmm_models.pkl\", \"wb\") as f:\n        pickle.dump(gmm_models, f)\n    return gmm_models\n```\n\n对每个测试数据，找与之对应的最相似的说话人模型\n```python\ndef predict_speaker(gmm_models, test_features):\n    predictions = []\n    print('testing...')\n    for features in tqdm(test_features):  # (400, 13)\n        scores = {}\n        for label, gmm in gmm_models.items():  # 遍历每个说话人模型\n            # TODO：对一个语音段的每帧进行预测，把每帧的得分求和，得到该gmm的得分，再对所有gmm取最高分\n            score = gmm.score(features)\n            scores[label] = score\n        predicted_speaker = max(scores, key=scores.get)  # 选择得分最高的说话人\n        predictions.append(predicted_speaker)\n    return predictions\n```\n\n训练并预测\n```python\n# 读取数据\ntrain_features = np.load(\"mfcc/train_features.npy\")\ntest_features = np.load(\"mfcc/test_features.npy\")\ntrain_labels = np.load(\"mfcc/train_labels.npy\")\ntest_labels = np.load(\"mfcc/test_labels.npy\")\n\n# 训练并预测\nif load_model == True:\n    with open(\"gmm_models.pkl\", \"rb\") as f:\n        gmm_models = pickle.load(f)\nelse: \n    gmm_models = train_gmm(train_features, train_labels)\n\npredictions = predict_speaker(gmm_models, test_features)\n\naccuracy = accuracy_score(test_labels, predictions)\nprint(\"GMM预测准确率: {:.3f}%\".format(accuracy * 100))\n```\n#### 预测结果\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429142154.png)\n","source":"_posts/Notes/课程/语音信息处理/第二次实验.md","raw":"---\ntitle: 第二次实验\ncategories:\n  - Notes\n  - 课程\n  - 语音信息处理\ndate:\ntags:\n---\n### 声纹识别实验\n模型：GMM（高斯混合模型）\n数据集：TIMIT\n\n#### GMM高斯混合模型（Gaussian mixture model）\n对于说话人识别，一组$N$个说话人集合，用一系列$GMM$ 示，即每个说话人$\\mathrm{s}_{\\mathrm{k}}$ 对应一个$\\mathrm{GMM}$  参数  $\\lambda_{\\mathrm{k}}, \\mathrm{k}=1,2, \\ldots, \\mathrm{N}$ 。说话人识别的目标是寻找一个说话人模型，使得给定说话人观测序列  $\\mathrm{X}=x_{1}, x_{2}, \\ldots, x_{t}, \\ldots, x_{T}, x_{t}$  是下标为 $\\mathrm{t}$ 的特征向量（时间维度下标，表示帧，frame），在某个模型参数下的后验概率最大（the maximum a posterior probability），该模型即为给定说话人观测序列，得到的说话人模型。假设帧间是相互独立的，预测模型（说话人模型）  $s_{\\text {predicted }}$ 表示为:\n$$\n\\mathrm{S}_{\\text {predicted }}=\\underset{\\mathrm{k} \\in \\mathcal{S}}{\\arg \\max } \\sum_{\\mathrm{t}=1}^{\\mathrm{T}} \\log \\left[\\mathrm{p}\\left(\\boldsymbol{x}_{\\boldsymbol{t}} \\mid \\lambda_{\\mathrm{k}}\\right)\\right]$$\n\n#### 代码实现\n训练GMM模型\n```python\ndef train_gmm(train_features, train_labels, num_components=16):\n    gmm_models = {}\n    unique_labels = set(train_labels)\n    print('training...')\n    for label in tqdm(unique_labels):\n        # 为每个说话人训练一个 GMM 模型\n        features = train_features[train_labels == label]  # 把这个说话人的所有数据作为训练数据 (n, 2000, 13)\n        gmm = GaussianMixture(n_components=num_components, covariance_type='diag')  # n_components:混合高斯模型数量, covariance_type:协方差类型\n        # TODO：修改fit时传入的数据，把帧数作为sample_num\n        gmm.fit(features.reshape(features.shape[1], features.shape[0]*features.shape[2]))  # 降到2维作为输入\n        gmm_models[label] = gmm  # N个人的GMM模型组成列表\n    # 保存模型\n    with open(\"gmm_models.pkl\", \"wb\") as f:\n        pickle.dump(gmm_models, f)\n    return gmm_models\n```\n\n对每个测试数据，找与之对应的最相似的说话人模型\n```python\ndef predict_speaker(gmm_models, test_features):\n    predictions = []\n    print('testing...')\n    for features in tqdm(test_features):  # (400, 13)\n        scores = {}\n        for label, gmm in gmm_models.items():  # 遍历每个说话人模型\n            # TODO：对一个语音段的每帧进行预测，把每帧的得分求和，得到该gmm的得分，再对所有gmm取最高分\n            score = gmm.score(features)\n            scores[label] = score\n        predicted_speaker = max(scores, key=scores.get)  # 选择得分最高的说话人\n        predictions.append(predicted_speaker)\n    return predictions\n```\n\n训练并预测\n```python\n# 读取数据\ntrain_features = np.load(\"mfcc/train_features.npy\")\ntest_features = np.load(\"mfcc/test_features.npy\")\ntrain_labels = np.load(\"mfcc/train_labels.npy\")\ntest_labels = np.load(\"mfcc/test_labels.npy\")\n\n# 训练并预测\nif load_model == True:\n    with open(\"gmm_models.pkl\", \"rb\") as f:\n        gmm_models = pickle.load(f)\nelse: \n    gmm_models = train_gmm(train_features, train_labels)\n\npredictions = predict_speaker(gmm_models, test_features)\n\naccuracy = accuracy_score(test_labels, predictions)\nprint(\"GMM预测准确率: {:.3f}%\".format(accuracy * 100))\n```\n#### 预测结果\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429142154.png)\n","slug":"Notes/课程/语音信息处理/第二次实验","published":1,"updated":"2024-04-29T06:21:58.506Z","_id":"clv29w3np00006s8ccb3u248e","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"声纹识别实验\"><a href=\"#声纹识别实验\" class=\"headerlink\" title=\"声纹识别实验\"></a>声纹识别实验</h3><p>模型：GMM（高斯混合模型）<br>数据集：TIMIT</p>\n<h4 id=\"GMM高斯混合模型（Gaussian-mixture-model）\"><a href=\"#GMM高斯混合模型（Gaussian-mixture-model）\" class=\"headerlink\" title=\"GMM高斯混合模型（Gaussian mixture model）\"></a>GMM高斯混合模型（Gaussian mixture model）</h4><p>对于说话人识别，一组$N$个说话人集合，用一系列$GMM$ 示，即每个说话人$\\mathrm{s}<em>{\\mathrm{k}}$ 对应一个$\\mathrm{GMM}$  参数  $\\lambda</em>{\\mathrm{k}}, \\mathrm{k}&#x3D;1,2, \\ldots, \\mathrm{N}$ 。说话人识别的目标是寻找一个说话人模型，使得给定说话人观测序列  $\\mathrm{X}&#x3D;x_{1}, x_{2}, \\ldots, x_{t}, \\ldots, x_{T}, x_{t}$  是下标为 $\\mathrm{t}$ 的特征向量（时间维度下标，表示帧，frame），在某个模型参数下的后验概率最大（the maximum a posterior probability），该模型即为给定说话人观测序列，得到的说话人模型。假设帧间是相互独立的，预测模型（说话人模型）  $s_{\\text {predicted }}$ 表示为:<br>$$<br>\\mathrm{S}<em>{\\text {predicted }}&#x3D;\\underset{\\mathrm{k} \\in \\mathcal{S}}{\\arg \\max } \\sum</em>{\\mathrm{t}&#x3D;1}^{\\mathrm{T}} \\log \\left[\\mathrm{p}\\left(\\boldsymbol{x}<em>{\\boldsymbol{t}} \\mid \\lambda</em>{\\mathrm{k}}\\right)\\right]$$</p>\n<h4 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h4><p>训练GMM模型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">train_gmm</span>(<span class=\"hljs-params\">train_features, train_labels, num_components=<span class=\"hljs-number\">16</span></span>):<br>    gmm_models = &#123;&#125;<br>    unique_labels = <span class=\"hljs-built_in\">set</span>(train_labels)<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;training...&#x27;</span>)<br>    <span class=\"hljs-keyword\">for</span> label <span class=\"hljs-keyword\">in</span> tqdm(unique_labels):<br>        <span class=\"hljs-comment\"># 为每个说话人训练一个 GMM 模型</span><br>        features = train_features[train_labels == label]  <span class=\"hljs-comment\"># 把这个说话人的所有数据作为训练数据 (n, 2000, 13)</span><br>        gmm = GaussianMixture(n_components=num_components, covariance_type=<span class=\"hljs-string\">&#x27;diag&#x27;</span>)  <span class=\"hljs-comment\"># n_components:混合高斯模型数量, covariance_type:协方差类型</span><br>        <span class=\"hljs-comment\"># TODO：修改fit时传入的数据，把帧数作为sample_num</span><br>        gmm.fit(features.reshape(features.shape[<span class=\"hljs-number\">1</span>], features.shape[<span class=\"hljs-number\">0</span>]*features.shape[<span class=\"hljs-number\">2</span>]))  <span class=\"hljs-comment\"># 降到2维作为输入</span><br>        gmm_models[label] = gmm  <span class=\"hljs-comment\"># N个人的GMM模型组成列表</span><br>    <span class=\"hljs-comment\"># 保存模型</span><br>    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">&quot;gmm_models.pkl&quot;</span>, <span class=\"hljs-string\">&quot;wb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>        pickle.dump(gmm_models, f)<br>    <span class=\"hljs-keyword\">return</span> gmm_models<br></code></pre></td></tr></table></figure>\n\n<p>对每个测试数据，找与之对应的最相似的说话人模型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">predict_speaker</span>(<span class=\"hljs-params\">gmm_models, test_features</span>):<br>    predictions = []<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;testing...&#x27;</span>)<br>    <span class=\"hljs-keyword\">for</span> features <span class=\"hljs-keyword\">in</span> tqdm(test_features):  <span class=\"hljs-comment\"># (400, 13)</span><br>        scores = &#123;&#125;<br>        <span class=\"hljs-keyword\">for</span> label, gmm <span class=\"hljs-keyword\">in</span> gmm_models.items():  <span class=\"hljs-comment\"># 遍历每个说话人模型</span><br>            <span class=\"hljs-comment\"># TODO：对一个语音段的每帧进行预测，把每帧的得分求和，得到该gmm的得分，再对所有gmm取最高分</span><br>            score = gmm.score(features)<br>            scores[label] = score<br>        predicted_speaker = <span class=\"hljs-built_in\">max</span>(scores, key=scores.get)  <span class=\"hljs-comment\"># 选择得分最高的说话人</span><br>        predictions.append(predicted_speaker)<br>    <span class=\"hljs-keyword\">return</span> predictions<br></code></pre></td></tr></table></figure>\n\n<p>训练并预测</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 读取数据</span><br>train_features = np.load(<span class=\"hljs-string\">&quot;mfcc/train_features.npy&quot;</span>)<br>test_features = np.load(<span class=\"hljs-string\">&quot;mfcc/test_features.npy&quot;</span>)<br>train_labels = np.load(<span class=\"hljs-string\">&quot;mfcc/train_labels.npy&quot;</span>)<br>test_labels = np.load(<span class=\"hljs-string\">&quot;mfcc/test_labels.npy&quot;</span>)<br><br><span class=\"hljs-comment\"># 训练并预测</span><br><span class=\"hljs-keyword\">if</span> load_model == <span class=\"hljs-literal\">True</span>:<br>    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">&quot;gmm_models.pkl&quot;</span>, <span class=\"hljs-string\">&quot;rb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>        gmm_models = pickle.load(f)<br><span class=\"hljs-keyword\">else</span>: <br>    gmm_models = train_gmm(train_features, train_labels)<br><br>predictions = predict_speaker(gmm_models, test_features)<br><br>accuracy = accuracy_score(test_labels, predictions)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;GMM预测准确率: &#123;:.3f&#125;%&quot;</span>.<span class=\"hljs-built_in\">format</span>(accuracy * <span class=\"hljs-number\">100</span>))<br></code></pre></td></tr></table></figure>\n<h4 id=\"预测结果\"><a href=\"#预测结果\" class=\"headerlink\" title=\"预测结果\"></a>预测结果</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429142154.png\" alt=\"image.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"声纹识别实验\"><a href=\"#声纹识别实验\" class=\"headerlink\" title=\"声纹识别实验\"></a>声纹识别实验</h3><p>模型：GMM（高斯混合模型）<br>数据集：TIMIT</p>\n<h4 id=\"GMM高斯混合模型（Gaussian-mixture-model）\"><a href=\"#GMM高斯混合模型（Gaussian-mixture-model）\" class=\"headerlink\" title=\"GMM高斯混合模型（Gaussian mixture model）\"></a>GMM高斯混合模型（Gaussian mixture model）</h4><p>对于说话人识别，一组$N$个说话人集合，用一系列$GMM$ 示，即每个说话人$\\mathrm{s}<em>{\\mathrm{k}}$ 对应一个$\\mathrm{GMM}$  参数  $\\lambda</em>{\\mathrm{k}}, \\mathrm{k}&#x3D;1,2, \\ldots, \\mathrm{N}$ 。说话人识别的目标是寻找一个说话人模型，使得给定说话人观测序列  $\\mathrm{X}&#x3D;x_{1}, x_{2}, \\ldots, x_{t}, \\ldots, x_{T}, x_{t}$  是下标为 $\\mathrm{t}$ 的特征向量（时间维度下标，表示帧，frame），在某个模型参数下的后验概率最大（the maximum a posterior probability），该模型即为给定说话人观测序列，得到的说话人模型。假设帧间是相互独立的，预测模型（说话人模型）  $s_{\\text {predicted }}$ 表示为:<br>$$<br>\\mathrm{S}<em>{\\text {predicted }}&#x3D;\\underset{\\mathrm{k} \\in \\mathcal{S}}{\\arg \\max } \\sum</em>{\\mathrm{t}&#x3D;1}^{\\mathrm{T}} \\log \\left[\\mathrm{p}\\left(\\boldsymbol{x}<em>{\\boldsymbol{t}} \\mid \\lambda</em>{\\mathrm{k}}\\right)\\right]$$</p>\n<h4 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h4><p>训练GMM模型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">train_gmm</span>(<span class=\"hljs-params\">train_features, train_labels, num_components=<span class=\"hljs-number\">16</span></span>):<br>    gmm_models = &#123;&#125;<br>    unique_labels = <span class=\"hljs-built_in\">set</span>(train_labels)<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;training...&#x27;</span>)<br>    <span class=\"hljs-keyword\">for</span> label <span class=\"hljs-keyword\">in</span> tqdm(unique_labels):<br>        <span class=\"hljs-comment\"># 为每个说话人训练一个 GMM 模型</span><br>        features = train_features[train_labels == label]  <span class=\"hljs-comment\"># 把这个说话人的所有数据作为训练数据 (n, 2000, 13)</span><br>        gmm = GaussianMixture(n_components=num_components, covariance_type=<span class=\"hljs-string\">&#x27;diag&#x27;</span>)  <span class=\"hljs-comment\"># n_components:混合高斯模型数量, covariance_type:协方差类型</span><br>        <span class=\"hljs-comment\"># TODO：修改fit时传入的数据，把帧数作为sample_num</span><br>        gmm.fit(features.reshape(features.shape[<span class=\"hljs-number\">1</span>], features.shape[<span class=\"hljs-number\">0</span>]*features.shape[<span class=\"hljs-number\">2</span>]))  <span class=\"hljs-comment\"># 降到2维作为输入</span><br>        gmm_models[label] = gmm  <span class=\"hljs-comment\"># N个人的GMM模型组成列表</span><br>    <span class=\"hljs-comment\"># 保存模型</span><br>    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">&quot;gmm_models.pkl&quot;</span>, <span class=\"hljs-string\">&quot;wb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>        pickle.dump(gmm_models, f)<br>    <span class=\"hljs-keyword\">return</span> gmm_models<br></code></pre></td></tr></table></figure>\n\n<p>对每个测试数据，找与之对应的最相似的说话人模型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">predict_speaker</span>(<span class=\"hljs-params\">gmm_models, test_features</span>):<br>    predictions = []<br>    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&#x27;testing...&#x27;</span>)<br>    <span class=\"hljs-keyword\">for</span> features <span class=\"hljs-keyword\">in</span> tqdm(test_features):  <span class=\"hljs-comment\"># (400, 13)</span><br>        scores = &#123;&#125;<br>        <span class=\"hljs-keyword\">for</span> label, gmm <span class=\"hljs-keyword\">in</span> gmm_models.items():  <span class=\"hljs-comment\"># 遍历每个说话人模型</span><br>            <span class=\"hljs-comment\"># TODO：对一个语音段的每帧进行预测，把每帧的得分求和，得到该gmm的得分，再对所有gmm取最高分</span><br>            score = gmm.score(features)<br>            scores[label] = score<br>        predicted_speaker = <span class=\"hljs-built_in\">max</span>(scores, key=scores.get)  <span class=\"hljs-comment\"># 选择得分最高的说话人</span><br>        predictions.append(predicted_speaker)<br>    <span class=\"hljs-keyword\">return</span> predictions<br></code></pre></td></tr></table></figure>\n\n<p>训练并预测</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 读取数据</span><br>train_features = np.load(<span class=\"hljs-string\">&quot;mfcc/train_features.npy&quot;</span>)<br>test_features = np.load(<span class=\"hljs-string\">&quot;mfcc/test_features.npy&quot;</span>)<br>train_labels = np.load(<span class=\"hljs-string\">&quot;mfcc/train_labels.npy&quot;</span>)<br>test_labels = np.load(<span class=\"hljs-string\">&quot;mfcc/test_labels.npy&quot;</span>)<br><br><span class=\"hljs-comment\"># 训练并预测</span><br><span class=\"hljs-keyword\">if</span> load_model == <span class=\"hljs-literal\">True</span>:<br>    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">&quot;gmm_models.pkl&quot;</span>, <span class=\"hljs-string\">&quot;rb&quot;</span>) <span class=\"hljs-keyword\">as</span> f:<br>        gmm_models = pickle.load(f)<br><span class=\"hljs-keyword\">else</span>: <br>    gmm_models = train_gmm(train_features, train_labels)<br><br>predictions = predict_speaker(gmm_models, test_features)<br><br>accuracy = accuracy_score(test_labels, predictions)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;GMM预测准确率: &#123;:.3f&#125;%&quot;</span>.<span class=\"hljs-built_in\">format</span>(accuracy * <span class=\"hljs-number\">100</span>))<br></code></pre></td></tr></table></figure>\n<h4 id=\"预测结果\"><a href=\"#预测结果\" class=\"headerlink\" title=\"预测结果\"></a>预测结果</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429142154.png\" alt=\"image.png\"></p>\n"},{"title":"词袋模型+SVM图像分类","date":"2024-04-25T07:50:27.414Z","_content":"---\n### 算法流程描述\n首先，对数据集进行预处理。本实验使用scene_categories数据集，该数据集包括15个类别（文件夹名就是类别名），每个类中编号前150号的样本作为训练样本，15个类一共2250张训练样本；剩下的样本构成测试集合。对数据进行SIFT特征提取，得到每张图片的描述符，作为训练数据。\n然后，使用KMeans算法对SIFT特征进行聚类，得到视觉词汇，然后计算图像的词袋模型，将图像的词汇表示和标签作为SVM的训练数据，得到训练好的SVM分类器。\n对测试数据集中的每张图像，同样根据构建的视觉词汇，计算图像的词袋表示。使用训练好的SVM分类器对测试数据集进行分类预测，并计算分类准确率。\n使用分类预测结果和真实标签计算混淆矩阵，然后用matplotlib绘制其热力图。\n\n### 函数功能说明\n`extract_sift_features` 将输入的彩色图像转换为灰度图像，然后使用SIFT算法检测图像中的关键点，并计算这些关键点的SIFT特征描述符。\n```python\ndef extract_sift_features(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    sift = cv2.SIFT_create()\n    keypoints, descriptors = sift.detectAndCompute(gray, None)\n    return descriptors\n```\n`preprocess_data` 对输入的图像路径列表进行预处理，提取每张图像的SIFT特征描述符，并将所有特征描述符拼接成一个特征矩阵返回。\n```python\ndef preprocess_data(img_paths):\n    features = []\n    for img_path in img_paths:\n        image = cv2.imread(img_path)\n        if image is not None:\n            sift_features = extract_sift_features(image)\n            if sift_features is not None:\n                features.append(sift_features)\n    return np.concatenate(features, axis=0)\n```\n`build_vocabulary` 使用K均值聚类算法对给定的特征集进行聚类，并构建一个视觉词汇，返回聚类中心。\n```python\ndef build_vocabulary(features, vocab_size):\n    kmeans = KMeans(n_clusters=vocab_size)\n    kmeans.fit(features)\n    return kmeans.cluster_centers_\n```\n`compute_bovw` 根据给定的特征和视觉词汇计算图像的词袋表示，并返回词袋表示。\n```python\ndef compute_bovw(features, vocabulary):\n    kmeans = KMeans(n_clusters=vocabulary.shape[0], init=vocabulary, n_init=1, max_iter=1)\n    kmeans.fit(vocabulary)\n    bovw_representation = np.zeros((1, vocabulary.shape[0]))\n    if features is not None:\n        assignments = kmeans.predict(features)\n        for assignment in assignments:\n            bovw_representation[0, assignment] += 1\n    return bovw_representation\n```\n\n在主函数中，首先按每个类别前150个数据为训练数据的原创对原始数据进行训练集和测试集的划分，然后调用上面的函数进行SIFT特征提取、计算词汇表、计算词袋模型，然后训练svm分类器，对测试数据的词袋表示进行预测，计算预测结果与真实结果的准确率与混淆矩阵，并将混淆矩阵可视化。\n\n数据划分：\n```python\n# 加载和预处理训练数据\ntrain_labels = []\ntest_labels = []\ntrain_img_paths = []\ntest_img_paths  = []\nfor path, dirs, files in os.walk(img_path):\n    for i, file in enumerate(files):\n        if (i < 150):\n            train_img_paths.append(os.path.join(path, file))\n        else:\n            test_img_paths.append(os.path.join(path, file))\n    if (len(files) > 0):\n        train_labels.extend([path.split('/')[-1]] * 150)\n        test_labels.extend([path.split('/')[-1]] * (len(files) - 150))\n```\n\n训练和测试：\n```python\n# 训练SVM分类器\nprint(\"正在训练SVM分类器...\")\nclf = SVC(kernel='linear')\nclf.fit(train_bovw, train_labels)\n\n# 测试词袋\ntest_bovw = []\nfor img_path in tqdm(test_img_paths, desc=\"正在测试分类器\"):\n    image_features = preprocess_data([img_path])\n    bovw_representation = compute_bovw(image_features, vocabulary)\n    test_bovw.append(bovw_representation)\ntest_bovw = np.concatenate(test_bovw, axis=0)\n\n# 预测并评估分类器\npredictions = clf.predict(test_bovw)\naccuracy = accuracy_score(test_labels, predictions)\nprint(\"准确率:{:.3f}%\".format(accuracy*100))\n```\n\n计算并可视化混淆矩阵：\n```python\n# 计算混淆矩阵\nconf_mat = confusion_matrix(test_labels, predictions)\nnormalized_conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n\ncategories = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14']\n\n# 绘制混淆矩阵\nplt.figure(figsize=(10, 8))\nheatmap = sns.heatmap(normalized_conf_mat, annot=True, fmt='.2f', cmap='Blues', cbar=False)\n\nheatmap.set_xticks(np.arange(len(categories))+0.5)\n# 设置横坐标刻度标签为类别名称\nheatmap.set_xticklabels(categories, rotation=45, ha='right')\n\n# 设置纵坐标刻度在类别中心\nheatmap.set_yticks(np.arange(len(categories))+0.5)\n# 设置纵坐标刻度标签为类别名称\nheatmap.set_yticklabels(categories, rotation=0)\n\nplt.xlabel('预测类别')\nplt.ylabel('真实类别')\nplt.title('预测结果的混淆矩阵')\nplt.show()\n```\n\n### 函数参数说明\n`extract_sift_features`:\n    - `image`: 输入的图像，是一个三维的NumPy数组，表示为(height, width, channels)。\n`preprocess_data`:\n    - `img_paths`: 包含图像文件路径的列表，用于指定要预处理的图像数据集。\n`build_vocabulary`:\n    - `features`: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。\n    - `vocab_size`: 视觉词汇的大小，即要构建的聚类中心数量。\n`compute_bovw`:\n    - `features`: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。\n    - `vocabulary`: 视觉词汇，是一个二维NumPy数组，每行代表一个聚类中心。\n\n### 结果分析\n在实验中发现，适当增大`vocab_size`可以带来更高的准确率，但考虑设备性能，最终选择的值为3000。\n最终准确率如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429000700.png)\n\n预测结果的混淆矩阵如下，其中每行之和为1，每个元素为预测类别在帧数类别中所占比例：\n![Figure_1.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/Figure_1.png)\n","source":"_posts/Notes/课程/计算机视觉/词袋模型+SVM图像分类.md","raw":"---\ncategories:\n  - Notes\n  - 课程\n  - 计算机视觉\ntitle: 词袋模型+SVM图像分类\ndate: \ntags:\n---\n---\n### 算法流程描述\n首先，对数据集进行预处理。本实验使用scene_categories数据集，该数据集包括15个类别（文件夹名就是类别名），每个类中编号前150号的样本作为训练样本，15个类一共2250张训练样本；剩下的样本构成测试集合。对数据进行SIFT特征提取，得到每张图片的描述符，作为训练数据。\n然后，使用KMeans算法对SIFT特征进行聚类，得到视觉词汇，然后计算图像的词袋模型，将图像的词汇表示和标签作为SVM的训练数据，得到训练好的SVM分类器。\n对测试数据集中的每张图像，同样根据构建的视觉词汇，计算图像的词袋表示。使用训练好的SVM分类器对测试数据集进行分类预测，并计算分类准确率。\n使用分类预测结果和真实标签计算混淆矩阵，然后用matplotlib绘制其热力图。\n\n### 函数功能说明\n`extract_sift_features` 将输入的彩色图像转换为灰度图像，然后使用SIFT算法检测图像中的关键点，并计算这些关键点的SIFT特征描述符。\n```python\ndef extract_sift_features(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    sift = cv2.SIFT_create()\n    keypoints, descriptors = sift.detectAndCompute(gray, None)\n    return descriptors\n```\n`preprocess_data` 对输入的图像路径列表进行预处理，提取每张图像的SIFT特征描述符，并将所有特征描述符拼接成一个特征矩阵返回。\n```python\ndef preprocess_data(img_paths):\n    features = []\n    for img_path in img_paths:\n        image = cv2.imread(img_path)\n        if image is not None:\n            sift_features = extract_sift_features(image)\n            if sift_features is not None:\n                features.append(sift_features)\n    return np.concatenate(features, axis=0)\n```\n`build_vocabulary` 使用K均值聚类算法对给定的特征集进行聚类，并构建一个视觉词汇，返回聚类中心。\n```python\ndef build_vocabulary(features, vocab_size):\n    kmeans = KMeans(n_clusters=vocab_size)\n    kmeans.fit(features)\n    return kmeans.cluster_centers_\n```\n`compute_bovw` 根据给定的特征和视觉词汇计算图像的词袋表示，并返回词袋表示。\n```python\ndef compute_bovw(features, vocabulary):\n    kmeans = KMeans(n_clusters=vocabulary.shape[0], init=vocabulary, n_init=1, max_iter=1)\n    kmeans.fit(vocabulary)\n    bovw_representation = np.zeros((1, vocabulary.shape[0]))\n    if features is not None:\n        assignments = kmeans.predict(features)\n        for assignment in assignments:\n            bovw_representation[0, assignment] += 1\n    return bovw_representation\n```\n\n在主函数中，首先按每个类别前150个数据为训练数据的原创对原始数据进行训练集和测试集的划分，然后调用上面的函数进行SIFT特征提取、计算词汇表、计算词袋模型，然后训练svm分类器，对测试数据的词袋表示进行预测，计算预测结果与真实结果的准确率与混淆矩阵，并将混淆矩阵可视化。\n\n数据划分：\n```python\n# 加载和预处理训练数据\ntrain_labels = []\ntest_labels = []\ntrain_img_paths = []\ntest_img_paths  = []\nfor path, dirs, files in os.walk(img_path):\n    for i, file in enumerate(files):\n        if (i < 150):\n            train_img_paths.append(os.path.join(path, file))\n        else:\n            test_img_paths.append(os.path.join(path, file))\n    if (len(files) > 0):\n        train_labels.extend([path.split('/')[-1]] * 150)\n        test_labels.extend([path.split('/')[-1]] * (len(files) - 150))\n```\n\n训练和测试：\n```python\n# 训练SVM分类器\nprint(\"正在训练SVM分类器...\")\nclf = SVC(kernel='linear')\nclf.fit(train_bovw, train_labels)\n\n# 测试词袋\ntest_bovw = []\nfor img_path in tqdm(test_img_paths, desc=\"正在测试分类器\"):\n    image_features = preprocess_data([img_path])\n    bovw_representation = compute_bovw(image_features, vocabulary)\n    test_bovw.append(bovw_representation)\ntest_bovw = np.concatenate(test_bovw, axis=0)\n\n# 预测并评估分类器\npredictions = clf.predict(test_bovw)\naccuracy = accuracy_score(test_labels, predictions)\nprint(\"准确率:{:.3f}%\".format(accuracy*100))\n```\n\n计算并可视化混淆矩阵：\n```python\n# 计算混淆矩阵\nconf_mat = confusion_matrix(test_labels, predictions)\nnormalized_conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n\ncategories = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14']\n\n# 绘制混淆矩阵\nplt.figure(figsize=(10, 8))\nheatmap = sns.heatmap(normalized_conf_mat, annot=True, fmt='.2f', cmap='Blues', cbar=False)\n\nheatmap.set_xticks(np.arange(len(categories))+0.5)\n# 设置横坐标刻度标签为类别名称\nheatmap.set_xticklabels(categories, rotation=45, ha='right')\n\n# 设置纵坐标刻度在类别中心\nheatmap.set_yticks(np.arange(len(categories))+0.5)\n# 设置纵坐标刻度标签为类别名称\nheatmap.set_yticklabels(categories, rotation=0)\n\nplt.xlabel('预测类别')\nplt.ylabel('真实类别')\nplt.title('预测结果的混淆矩阵')\nplt.show()\n```\n\n### 函数参数说明\n`extract_sift_features`:\n    - `image`: 输入的图像，是一个三维的NumPy数组，表示为(height, width, channels)。\n`preprocess_data`:\n    - `img_paths`: 包含图像文件路径的列表，用于指定要预处理的图像数据集。\n`build_vocabulary`:\n    - `features`: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。\n    - `vocab_size`: 视觉词汇的大小，即要构建的聚类中心数量。\n`compute_bovw`:\n    - `features`: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。\n    - `vocabulary`: 视觉词汇，是一个二维NumPy数组，每行代表一个聚类中心。\n\n### 结果分析\n在实验中发现，适当增大`vocab_size`可以带来更高的准确率，但考虑设备性能，最终选择的值为3000。\n最终准确率如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429000700.png)\n\n预测结果的混淆矩阵如下，其中每行之和为1，每个元素为预测类别在帧数类别中所占比例：\n![Figure_1.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/Figure_1.png)\n","slug":"Notes/课程/计算机视觉/词袋模型+SVM图像分类","published":1,"updated":"2024-04-28T16:15:00.754Z","_id":"clveyesfe0000n88c82wj3w6i","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><hr>\n<h3 id=\"算法流程描述\"><a href=\"#算法流程描述\" class=\"headerlink\" title=\"算法流程描述\"></a>算法流程描述</h3><p>首先，对数据集进行预处理。本实验使用scene_categories数据集，该数据集包括15个类别（文件夹名就是类别名），每个类中编号前150号的样本作为训练样本，15个类一共2250张训练样本；剩下的样本构成测试集合。对数据进行SIFT特征提取，得到每张图片的描述符，作为训练数据。<br>然后，使用KMeans算法对SIFT特征进行聚类，得到视觉词汇，然后计算图像的词袋模型，将图像的词汇表示和标签作为SVM的训练数据，得到训练好的SVM分类器。<br>对测试数据集中的每张图像，同样根据构建的视觉词汇，计算图像的词袋表示。使用训练好的SVM分类器对测试数据集进行分类预测，并计算分类准确率。<br>使用分类预测结果和真实标签计算混淆矩阵，然后用matplotlib绘制其热力图。</p>\n<h3 id=\"函数功能说明\"><a href=\"#函数功能说明\" class=\"headerlink\" title=\"函数功能说明\"></a>函数功能说明</h3><p><code>extract_sift_features</code> 将输入的彩色图像转换为灰度图像，然后使用SIFT算法检测图像中的关键点，并计算这些关键点的SIFT特征描述符。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">extract_sift_features</span>(<span class=\"hljs-params\">image</span>):<br>    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br>    sift = cv2.SIFT_create()<br>    keypoints, descriptors = sift.detectAndCompute(gray, <span class=\"hljs-literal\">None</span>)<br>    <span class=\"hljs-keyword\">return</span> descriptors<br></code></pre></td></tr></table></figure>\n<p><code>preprocess_data</code> 对输入的图像路径列表进行预处理，提取每张图像的SIFT特征描述符，并将所有特征描述符拼接成一个特征矩阵返回。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">preprocess_data</span>(<span class=\"hljs-params\">img_paths</span>):<br>    features = []<br>    <span class=\"hljs-keyword\">for</span> img_path <span class=\"hljs-keyword\">in</span> img_paths:<br>        image = cv2.imread(img_path)<br>        <span class=\"hljs-keyword\">if</span> image <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:<br>            sift_features = extract_sift_features(image)<br>            <span class=\"hljs-keyword\">if</span> sift_features <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:<br>                features.append(sift_features)<br>    <span class=\"hljs-keyword\">return</span> np.concatenate(features, axis=<span class=\"hljs-number\">0</span>)<br></code></pre></td></tr></table></figure>\n<p><code>build_vocabulary</code> 使用K均值聚类算法对给定的特征集进行聚类，并构建一个视觉词汇，返回聚类中心。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">build_vocabulary</span>(<span class=\"hljs-params\">features, vocab_size</span>):<br>    kmeans = KMeans(n_clusters=vocab_size)<br>    kmeans.fit(features)<br>    <span class=\"hljs-keyword\">return</span> kmeans.cluster_centers_<br></code></pre></td></tr></table></figure>\n<p><code>compute_bovw</code> 根据给定的特征和视觉词汇计算图像的词袋表示，并返回词袋表示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">compute_bovw</span>(<span class=\"hljs-params\">features, vocabulary</span>):<br>    kmeans = KMeans(n_clusters=vocabulary.shape[<span class=\"hljs-number\">0</span>], init=vocabulary, n_init=<span class=\"hljs-number\">1</span>, max_iter=<span class=\"hljs-number\">1</span>)<br>    kmeans.fit(vocabulary)<br>    bovw_representation = np.zeros((<span class=\"hljs-number\">1</span>, vocabulary.shape[<span class=\"hljs-number\">0</span>]))<br>    <span class=\"hljs-keyword\">if</span> features <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:<br>        assignments = kmeans.predict(features)<br>        <span class=\"hljs-keyword\">for</span> assignment <span class=\"hljs-keyword\">in</span> assignments:<br>            bovw_representation[<span class=\"hljs-number\">0</span>, assignment] += <span class=\"hljs-number\">1</span><br>    <span class=\"hljs-keyword\">return</span> bovw_representation<br></code></pre></td></tr></table></figure>\n\n<p>在主函数中，首先按每个类别前150个数据为训练数据的原创对原始数据进行训练集和测试集的划分，然后调用上面的函数进行SIFT特征提取、计算词汇表、计算词袋模型，然后训练svm分类器，对测试数据的词袋表示进行预测，计算预测结果与真实结果的准确率与混淆矩阵，并将混淆矩阵可视化。</p>\n<p>数据划分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 加载和预处理训练数据</span><br>train_labels = []<br>test_labels = []<br>train_img_paths = []<br>test_img_paths  = []<br><span class=\"hljs-keyword\">for</span> path, dirs, files <span class=\"hljs-keyword\">in</span> os.walk(img_path):<br>    <span class=\"hljs-keyword\">for</span> i, file <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(files):<br>        <span class=\"hljs-keyword\">if</span> (i &lt; <span class=\"hljs-number\">150</span>):<br>            train_img_paths.append(os.path.join(path, file))<br>        <span class=\"hljs-keyword\">else</span>:<br>            test_img_paths.append(os.path.join(path, file))<br>    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">len</span>(files) &gt; <span class=\"hljs-number\">0</span>):<br>        train_labels.extend([path.split(<span class=\"hljs-string\">&#x27;/&#x27;</span>)[-<span class=\"hljs-number\">1</span>]] * <span class=\"hljs-number\">150</span>)<br>        test_labels.extend([path.split(<span class=\"hljs-string\">&#x27;/&#x27;</span>)[-<span class=\"hljs-number\">1</span>]] * (<span class=\"hljs-built_in\">len</span>(files) - <span class=\"hljs-number\">150</span>))<br></code></pre></td></tr></table></figure>\n\n<p>训练和测试：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 训练SVM分类器</span><br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;正在训练SVM分类器...&quot;</span>)<br>clf = SVC(kernel=<span class=\"hljs-string\">&#x27;linear&#x27;</span>)<br>clf.fit(train_bovw, train_labels)<br><br><span class=\"hljs-comment\"># 测试词袋</span><br>test_bovw = []<br><span class=\"hljs-keyword\">for</span> img_path <span class=\"hljs-keyword\">in</span> tqdm(test_img_paths, desc=<span class=\"hljs-string\">&quot;正在测试分类器&quot;</span>):<br>    image_features = preprocess_data([img_path])<br>    bovw_representation = compute_bovw(image_features, vocabulary)<br>    test_bovw.append(bovw_representation)<br>test_bovw = np.concatenate(test_bovw, axis=<span class=\"hljs-number\">0</span>)<br><br><span class=\"hljs-comment\"># 预测并评估分类器</span><br>predictions = clf.predict(test_bovw)<br>accuracy = accuracy_score(test_labels, predictions)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;准确率:&#123;:.3f&#125;%&quot;</span>.<span class=\"hljs-built_in\">format</span>(accuracy*<span class=\"hljs-number\">100</span>))<br></code></pre></td></tr></table></figure>\n\n<p>计算并可视化混淆矩阵：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 计算混淆矩阵</span><br>conf_mat = confusion_matrix(test_labels, predictions)<br>normalized_conf_mat = conf_mat.astype(<span class=\"hljs-string\">&#x27;float&#x27;</span>) / conf_mat.<span class=\"hljs-built_in\">sum</span>(axis=<span class=\"hljs-number\">1</span>)[:, np.newaxis]<br><br>categories = [<span class=\"hljs-string\">&#x27;00&#x27;</span>, <span class=\"hljs-string\">&#x27;01&#x27;</span>, <span class=\"hljs-string\">&#x27;02&#x27;</span>, <span class=\"hljs-string\">&#x27;03&#x27;</span>, <span class=\"hljs-string\">&#x27;04&#x27;</span>, <span class=\"hljs-string\">&#x27;05&#x27;</span>, <span class=\"hljs-string\">&#x27;06&#x27;</span>, <span class=\"hljs-string\">&#x27;07&#x27;</span>, <span class=\"hljs-string\">&#x27;08&#x27;</span>, <span class=\"hljs-string\">&#x27;09&#x27;</span>, <span class=\"hljs-string\">&#x27;10&#x27;</span>, <span class=\"hljs-string\">&#x27;11&#x27;</span>, <span class=\"hljs-string\">&#x27;12&#x27;</span>, <span class=\"hljs-string\">&#x27;13&#x27;</span>, <span class=\"hljs-string\">&#x27;14&#x27;</span>]<br><br><span class=\"hljs-comment\"># 绘制混淆矩阵</span><br>plt.figure(figsize=(<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">8</span>))<br>heatmap = sns.heatmap(normalized_conf_mat, annot=<span class=\"hljs-literal\">True</span>, fmt=<span class=\"hljs-string\">&#x27;.2f&#x27;</span>, cmap=<span class=\"hljs-string\">&#x27;Blues&#x27;</span>, cbar=<span class=\"hljs-literal\">False</span>)<br><br>heatmap.set_xticks(np.arange(<span class=\"hljs-built_in\">len</span>(categories))+<span class=\"hljs-number\">0.5</span>)<br><span class=\"hljs-comment\"># 设置横坐标刻度标签为类别名称</span><br>heatmap.set_xticklabels(categories, rotation=<span class=\"hljs-number\">45</span>, ha=<span class=\"hljs-string\">&#x27;right&#x27;</span>)<br><br><span class=\"hljs-comment\"># 设置纵坐标刻度在类别中心</span><br>heatmap.set_yticks(np.arange(<span class=\"hljs-built_in\">len</span>(categories))+<span class=\"hljs-number\">0.5</span>)<br><span class=\"hljs-comment\"># 设置纵坐标刻度标签为类别名称</span><br>heatmap.set_yticklabels(categories, rotation=<span class=\"hljs-number\">0</span>)<br><br>plt.xlabel(<span class=\"hljs-string\">&#x27;预测类别&#x27;</span>)<br>plt.ylabel(<span class=\"hljs-string\">&#x27;真实类别&#x27;</span>)<br>plt.title(<span class=\"hljs-string\">&#x27;预测结果的混淆矩阵&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"函数参数说明\"><a href=\"#函数参数说明\" class=\"headerlink\" title=\"函数参数说明\"></a>函数参数说明</h3><p><code>extract_sift_features</code>:<br>    - <code>image</code>: 输入的图像，是一个三维的NumPy数组，表示为(height, width, channels)。<br><code>preprocess_data</code>:<br>    - <code>img_paths</code>: 包含图像文件路径的列表，用于指定要预处理的图像数据集。<br><code>build_vocabulary</code>:<br>    - <code>features</code>: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。<br>    - <code>vocab_size</code>: 视觉词汇的大小，即要构建的聚类中心数量。<br><code>compute_bovw</code>:<br>    - <code>features</code>: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。<br>    - <code>vocabulary</code>: 视觉词汇，是一个二维NumPy数组，每行代表一个聚类中心。</p>\n<h3 id=\"结果分析\"><a href=\"#结果分析\" class=\"headerlink\" title=\"结果分析\"></a>结果分析</h3><p>在实验中发现，适当增大<code>vocab_size</code>可以带来更高的准确率，但考虑设备性能，最终选择的值为3000。<br>最终准确率如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429000700.png\" alt=\"image.png\"></p>\n<p>预测结果的混淆矩阵如下，其中每行之和为1，每个元素为预测类别在帧数类别中所占比例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/Figure_1.png\" alt=\"Figure_1.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h3 id=\"算法流程描述\"><a href=\"#算法流程描述\" class=\"headerlink\" title=\"算法流程描述\"></a>算法流程描述</h3><p>首先，对数据集进行预处理。本实验使用scene_categories数据集，该数据集包括15个类别（文件夹名就是类别名），每个类中编号前150号的样本作为训练样本，15个类一共2250张训练样本；剩下的样本构成测试集合。对数据进行SIFT特征提取，得到每张图片的描述符，作为训练数据。<br>然后，使用KMeans算法对SIFT特征进行聚类，得到视觉词汇，然后计算图像的词袋模型，将图像的词汇表示和标签作为SVM的训练数据，得到训练好的SVM分类器。<br>对测试数据集中的每张图像，同样根据构建的视觉词汇，计算图像的词袋表示。使用训练好的SVM分类器对测试数据集进行分类预测，并计算分类准确率。<br>使用分类预测结果和真实标签计算混淆矩阵，然后用matplotlib绘制其热力图。</p>\n<h3 id=\"函数功能说明\"><a href=\"#函数功能说明\" class=\"headerlink\" title=\"函数功能说明\"></a>函数功能说明</h3><p><code>extract_sift_features</code> 将输入的彩色图像转换为灰度图像，然后使用SIFT算法检测图像中的关键点，并计算这些关键点的SIFT特征描述符。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">extract_sift_features</span>(<span class=\"hljs-params\">image</span>):<br>    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br>    sift = cv2.SIFT_create()<br>    keypoints, descriptors = sift.detectAndCompute(gray, <span class=\"hljs-literal\">None</span>)<br>    <span class=\"hljs-keyword\">return</span> descriptors<br></code></pre></td></tr></table></figure>\n<p><code>preprocess_data</code> 对输入的图像路径列表进行预处理，提取每张图像的SIFT特征描述符，并将所有特征描述符拼接成一个特征矩阵返回。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">preprocess_data</span>(<span class=\"hljs-params\">img_paths</span>):<br>    features = []<br>    <span class=\"hljs-keyword\">for</span> img_path <span class=\"hljs-keyword\">in</span> img_paths:<br>        image = cv2.imread(img_path)<br>        <span class=\"hljs-keyword\">if</span> image <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:<br>            sift_features = extract_sift_features(image)<br>            <span class=\"hljs-keyword\">if</span> sift_features <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:<br>                features.append(sift_features)<br>    <span class=\"hljs-keyword\">return</span> np.concatenate(features, axis=<span class=\"hljs-number\">0</span>)<br></code></pre></td></tr></table></figure>\n<p><code>build_vocabulary</code> 使用K均值聚类算法对给定的特征集进行聚类，并构建一个视觉词汇，返回聚类中心。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">build_vocabulary</span>(<span class=\"hljs-params\">features, vocab_size</span>):<br>    kmeans = KMeans(n_clusters=vocab_size)<br>    kmeans.fit(features)<br>    <span class=\"hljs-keyword\">return</span> kmeans.cluster_centers_<br></code></pre></td></tr></table></figure>\n<p><code>compute_bovw</code> 根据给定的特征和视觉词汇计算图像的词袋表示，并返回词袋表示。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">compute_bovw</span>(<span class=\"hljs-params\">features, vocabulary</span>):<br>    kmeans = KMeans(n_clusters=vocabulary.shape[<span class=\"hljs-number\">0</span>], init=vocabulary, n_init=<span class=\"hljs-number\">1</span>, max_iter=<span class=\"hljs-number\">1</span>)<br>    kmeans.fit(vocabulary)<br>    bovw_representation = np.zeros((<span class=\"hljs-number\">1</span>, vocabulary.shape[<span class=\"hljs-number\">0</span>]))<br>    <span class=\"hljs-keyword\">if</span> features <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:<br>        assignments = kmeans.predict(features)<br>        <span class=\"hljs-keyword\">for</span> assignment <span class=\"hljs-keyword\">in</span> assignments:<br>            bovw_representation[<span class=\"hljs-number\">0</span>, assignment] += <span class=\"hljs-number\">1</span><br>    <span class=\"hljs-keyword\">return</span> bovw_representation<br></code></pre></td></tr></table></figure>\n\n<p>在主函数中，首先按每个类别前150个数据为训练数据的原创对原始数据进行训练集和测试集的划分，然后调用上面的函数进行SIFT特征提取、计算词汇表、计算词袋模型，然后训练svm分类器，对测试数据的词袋表示进行预测，计算预测结果与真实结果的准确率与混淆矩阵，并将混淆矩阵可视化。</p>\n<p>数据划分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 加载和预处理训练数据</span><br>train_labels = []<br>test_labels = []<br>train_img_paths = []<br>test_img_paths  = []<br><span class=\"hljs-keyword\">for</span> path, dirs, files <span class=\"hljs-keyword\">in</span> os.walk(img_path):<br>    <span class=\"hljs-keyword\">for</span> i, file <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(files):<br>        <span class=\"hljs-keyword\">if</span> (i &lt; <span class=\"hljs-number\">150</span>):<br>            train_img_paths.append(os.path.join(path, file))<br>        <span class=\"hljs-keyword\">else</span>:<br>            test_img_paths.append(os.path.join(path, file))<br>    <span class=\"hljs-keyword\">if</span> (<span class=\"hljs-built_in\">len</span>(files) &gt; <span class=\"hljs-number\">0</span>):<br>        train_labels.extend([path.split(<span class=\"hljs-string\">&#x27;/&#x27;</span>)[-<span class=\"hljs-number\">1</span>]] * <span class=\"hljs-number\">150</span>)<br>        test_labels.extend([path.split(<span class=\"hljs-string\">&#x27;/&#x27;</span>)[-<span class=\"hljs-number\">1</span>]] * (<span class=\"hljs-built_in\">len</span>(files) - <span class=\"hljs-number\">150</span>))<br></code></pre></td></tr></table></figure>\n\n<p>训练和测试：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 训练SVM分类器</span><br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;正在训练SVM分类器...&quot;</span>)<br>clf = SVC(kernel=<span class=\"hljs-string\">&#x27;linear&#x27;</span>)<br>clf.fit(train_bovw, train_labels)<br><br><span class=\"hljs-comment\"># 测试词袋</span><br>test_bovw = []<br><span class=\"hljs-keyword\">for</span> img_path <span class=\"hljs-keyword\">in</span> tqdm(test_img_paths, desc=<span class=\"hljs-string\">&quot;正在测试分类器&quot;</span>):<br>    image_features = preprocess_data([img_path])<br>    bovw_representation = compute_bovw(image_features, vocabulary)<br>    test_bovw.append(bovw_representation)<br>test_bovw = np.concatenate(test_bovw, axis=<span class=\"hljs-number\">0</span>)<br><br><span class=\"hljs-comment\"># 预测并评估分类器</span><br>predictions = clf.predict(test_bovw)<br>accuracy = accuracy_score(test_labels, predictions)<br><span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;准确率:&#123;:.3f&#125;%&quot;</span>.<span class=\"hljs-built_in\">format</span>(accuracy*<span class=\"hljs-number\">100</span>))<br></code></pre></td></tr></table></figure>\n\n<p>计算并可视化混淆矩阵：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs python\"><span class=\"hljs-comment\"># 计算混淆矩阵</span><br>conf_mat = confusion_matrix(test_labels, predictions)<br>normalized_conf_mat = conf_mat.astype(<span class=\"hljs-string\">&#x27;float&#x27;</span>) / conf_mat.<span class=\"hljs-built_in\">sum</span>(axis=<span class=\"hljs-number\">1</span>)[:, np.newaxis]<br><br>categories = [<span class=\"hljs-string\">&#x27;00&#x27;</span>, <span class=\"hljs-string\">&#x27;01&#x27;</span>, <span class=\"hljs-string\">&#x27;02&#x27;</span>, <span class=\"hljs-string\">&#x27;03&#x27;</span>, <span class=\"hljs-string\">&#x27;04&#x27;</span>, <span class=\"hljs-string\">&#x27;05&#x27;</span>, <span class=\"hljs-string\">&#x27;06&#x27;</span>, <span class=\"hljs-string\">&#x27;07&#x27;</span>, <span class=\"hljs-string\">&#x27;08&#x27;</span>, <span class=\"hljs-string\">&#x27;09&#x27;</span>, <span class=\"hljs-string\">&#x27;10&#x27;</span>, <span class=\"hljs-string\">&#x27;11&#x27;</span>, <span class=\"hljs-string\">&#x27;12&#x27;</span>, <span class=\"hljs-string\">&#x27;13&#x27;</span>, <span class=\"hljs-string\">&#x27;14&#x27;</span>]<br><br><span class=\"hljs-comment\"># 绘制混淆矩阵</span><br>plt.figure(figsize=(<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">8</span>))<br>heatmap = sns.heatmap(normalized_conf_mat, annot=<span class=\"hljs-literal\">True</span>, fmt=<span class=\"hljs-string\">&#x27;.2f&#x27;</span>, cmap=<span class=\"hljs-string\">&#x27;Blues&#x27;</span>, cbar=<span class=\"hljs-literal\">False</span>)<br><br>heatmap.set_xticks(np.arange(<span class=\"hljs-built_in\">len</span>(categories))+<span class=\"hljs-number\">0.5</span>)<br><span class=\"hljs-comment\"># 设置横坐标刻度标签为类别名称</span><br>heatmap.set_xticklabels(categories, rotation=<span class=\"hljs-number\">45</span>, ha=<span class=\"hljs-string\">&#x27;right&#x27;</span>)<br><br><span class=\"hljs-comment\"># 设置纵坐标刻度在类别中心</span><br>heatmap.set_yticks(np.arange(<span class=\"hljs-built_in\">len</span>(categories))+<span class=\"hljs-number\">0.5</span>)<br><span class=\"hljs-comment\"># 设置纵坐标刻度标签为类别名称</span><br>heatmap.set_yticklabels(categories, rotation=<span class=\"hljs-number\">0</span>)<br><br>plt.xlabel(<span class=\"hljs-string\">&#x27;预测类别&#x27;</span>)<br>plt.ylabel(<span class=\"hljs-string\">&#x27;真实类别&#x27;</span>)<br>plt.title(<span class=\"hljs-string\">&#x27;预测结果的混淆矩阵&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"函数参数说明\"><a href=\"#函数参数说明\" class=\"headerlink\" title=\"函数参数说明\"></a>函数参数说明</h3><p><code>extract_sift_features</code>:<br>    - <code>image</code>: 输入的图像，是一个三维的NumPy数组，表示为(height, width, channels)。<br><code>preprocess_data</code>:<br>    - <code>img_paths</code>: 包含图像文件路径的列表，用于指定要预处理的图像数据集。<br><code>build_vocabulary</code>:<br>    - <code>features</code>: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。<br>    - <code>vocab_size</code>: 视觉词汇的大小，即要构建的聚类中心数量。<br><code>compute_bovw</code>:<br>    - <code>features</code>: 特征矩阵，是一个二维NumPy数组，每行代表一个特征向量。<br>    - <code>vocabulary</code>: 视觉词汇，是一个二维NumPy数组，每行代表一个聚类中心。</p>\n<h3 id=\"结果分析\"><a href=\"#结果分析\" class=\"headerlink\" title=\"结果分析\"></a>结果分析</h3><p>在实验中发现，适当增大<code>vocab_size</code>可以带来更高的准确率，但考虑设备性能，最终选择的值为3000。<br>最终准确率如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240429000700.png\" alt=\"image.png\"></p>\n<p>预测结果的混淆矩阵如下，其中每行之和为1，每个元素为预测类别在帧数类别中所占比例：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/Figure_1.png\" alt=\"Figure_1.png\"></p>\n"},{"title":"目标跟踪","date":"2024-05-12T08:17:45.555Z","_content":"ppt\n1. 介绍算法： \n\thog算法\n\tkcf算法：岭回归->岭回归简化计算->核技巧（高斯核）\n2. 代码\n\thog  update  main\n3. 展示结果\n\n","source":"_posts/Notes/课程/计算机视觉/目标跟踪.md","raw":"---\ntitle: 目标跟踪\ncategories:\n  - Notes\n  - 课程\n  - 计算机视觉\ndate:\ntags:\n---\nppt\n1. 介绍算法： \n\thog算法\n\tkcf算法：岭回归->岭回归简化计算->核技巧（高斯核）\n2. 代码\n\thog  update  main\n3. 展示结果\n\n","slug":"Notes/课程/计算机视觉/目标跟踪","published":1,"updated":"2024-05-12T08:26:26.556Z","_id":"clw39tsza0000f08cbho81wex","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p>ppt</p>\n<ol>\n<li>介绍算法：<br> hog算法<br> kcf算法：岭回归-&gt;岭回归简化计算-&gt;核技巧（高斯核）</li>\n<li>代码<br> hog  update  main</li>\n<li>展示结果</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>ppt</p>\n<ol>\n<li>介绍算法：<br> hog算法<br> kcf算法：岭回归-&gt;岭回归简化计算-&gt;核技巧（高斯核）</li>\n<li>代码<br> hog  update  main</li>\n<li>展示结果</li>\n</ol>\n"},{"title":"基于 Transformer 的命名实体识别","date":"2024-05-18T16:28:15.070Z","_content":"\n### 数据介绍\n\n根据`train_TAG.txt`统计得到的标签集如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165851.png)\n标签集大小为9。\n### 模型框架\n#### 模型结构\n模型使用hugging face的`bert-base-chinese`作为预训练模型。模型接受预处理后的字符序列作为输入，输出字符对应的标签。\n#### 模型参数\nBERT模型包含12层Transformer encoder，每层Transformer encoder包含的多头自注意头数为12，隐藏层大小为768。\n在构建数据集时，设定单条文本最大长度为128，对数据集进行阶段或填充。使用库函数`BertTokenizer`对数据进行向量化操作。\n训练时的参数包括：\n- 优化器：AdamW\n- 学习率：5e-5\n- 批次大小：32\n- 训练轮数：5\n#### 训练算法\n训练过程中，模型的输入为编码后的字符序列，输出为每个字符对应的标签。损失函数采用交叉熵损失，优化器为AdamW。训练循环中每个epoch包括以下步骤：\n1. 前向传播计算损失。\n2. 反向传播更新模型参数。\n3. 每个epoch结束后输出损失值。\n\n### 实验结果\n#### 训练过程\n每个epoch结束后在扩展集上进行验证，记录扩展集上的损失和准确率。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165610.png)\nEpoch 1 - Training Loss: 0.0131668053176951, Validation Loss: 0.010239653369493392, Validation Accuracy: 0.9971221633572501\nEpoch 2 - Training Loss: 0.00557029220667432, Validation Loss: 0.009271262560837594, Validation Accuracy: 0.9975453132821626\nEpoch 3 - Training Loss: 0.003971861287672002, Validation Loss: 0.010250022615927534, Validation Accuracy: 0.9975541777907558\nEpoch 4 - Training Loss: 0.0030227819530087912, Validation Loss: 0.01101609307826279, Validation Accuracy: 0.9976459515267813\nEpoch 5 - Training Loss: 0.002555301935822686, Validation Loss: 0.01072908611020737, Validation Accuracy: 0.9976827131653596\n#### 训练损失和验证集性能曲线\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165639.png)\n\n#### 结果分析\n实验共进行了5个epoch，从折线图中可以看出，训练数据的损失值一直在下降，但在验证集上的损失值有所波动。验证集上准确率有所提升，提升幅度逐渐放缓。\n\n### 参考文献\n1. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.\n2. Hugging Face Transformers库：[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n3. PyTorch官方文档：https://pytorch.org/docs/stable/index.html\n4. ChatGPT，辅助完成少量代码和文档。","source":"_posts/Notes/课程/自然语言处理/基于 Transformer 的命名实体识别.md","raw":"---\ntitle: 基于 Transformer 的命名实体识别\ncategories:\n  - Notes\n  - 课程\n  - 自然语言处理\ndate:\ntags:\n---\n\n### 数据介绍\n\n根据`train_TAG.txt`统计得到的标签集如下：\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165851.png)\n标签集大小为9。\n### 模型框架\n#### 模型结构\n模型使用hugging face的`bert-base-chinese`作为预训练模型。模型接受预处理后的字符序列作为输入，输出字符对应的标签。\n#### 模型参数\nBERT模型包含12层Transformer encoder，每层Transformer encoder包含的多头自注意头数为12，隐藏层大小为768。\n在构建数据集时，设定单条文本最大长度为128，对数据集进行阶段或填充。使用库函数`BertTokenizer`对数据进行向量化操作。\n训练时的参数包括：\n- 优化器：AdamW\n- 学习率：5e-5\n- 批次大小：32\n- 训练轮数：5\n#### 训练算法\n训练过程中，模型的输入为编码后的字符序列，输出为每个字符对应的标签。损失函数采用交叉熵损失，优化器为AdamW。训练循环中每个epoch包括以下步骤：\n1. 前向传播计算损失。\n2. 反向传播更新模型参数。\n3. 每个epoch结束后输出损失值。\n\n### 实验结果\n#### 训练过程\n每个epoch结束后在扩展集上进行验证，记录扩展集上的损失和准确率。\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165610.png)\nEpoch 1 - Training Loss: 0.0131668053176951, Validation Loss: 0.010239653369493392, Validation Accuracy: 0.9971221633572501\nEpoch 2 - Training Loss: 0.00557029220667432, Validation Loss: 0.009271262560837594, Validation Accuracy: 0.9975453132821626\nEpoch 3 - Training Loss: 0.003971861287672002, Validation Loss: 0.010250022615927534, Validation Accuracy: 0.9975541777907558\nEpoch 4 - Training Loss: 0.0030227819530087912, Validation Loss: 0.01101609307826279, Validation Accuracy: 0.9976459515267813\nEpoch 5 - Training Loss: 0.002555301935822686, Validation Loss: 0.01072908611020737, Validation Accuracy: 0.9976827131653596\n#### 训练损失和验证集性能曲线\n![image.png](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165639.png)\n\n#### 结果分析\n实验共进行了5个epoch，从折线图中可以看出，训练数据的损失值一直在下降，但在验证集上的损失值有所波动。验证集上准确率有所提升，提升幅度逐渐放缓。\n\n### 参考文献\n1. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.\n2. Hugging Face Transformers库：[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n3. PyTorch官方文档：https://pytorch.org/docs/stable/index.html\n4. ChatGPT，辅助完成少量代码和文档。","slug":"Notes/课程/自然语言处理/基于 Transformer 的命名实体识别","published":1,"updated":"2024-05-19T09:11:34.591Z","_id":"clwcc2coe0000s88c2joqewbz","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"数据介绍\"><a href=\"#数据介绍\" class=\"headerlink\" title=\"数据介绍\"></a>数据介绍</h3><p>根据<code>train_TAG.txt</code>统计得到的标签集如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165851.png\" alt=\"image.png\"><br>标签集大小为9。</p>\n<h3 id=\"模型框架\"><a href=\"#模型框架\" class=\"headerlink\" title=\"模型框架\"></a>模型框架</h3><h4 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h4><p>模型使用hugging face的<code>bert-base-chinese</code>作为预训练模型。模型接受预处理后的字符序列作为输入，输出字符对应的标签。</p>\n<h4 id=\"模型参数\"><a href=\"#模型参数\" class=\"headerlink\" title=\"模型参数\"></a>模型参数</h4><p>BERT模型包含12层Transformer encoder，每层Transformer encoder包含的多头自注意头数为12，隐藏层大小为768。<br>在构建数据集时，设定单条文本最大长度为128，对数据集进行阶段或填充。使用库函数<code>BertTokenizer</code>对数据进行向量化操作。<br>训练时的参数包括：</p>\n<ul>\n<li>优化器：AdamW</li>\n<li>学习率：5e-5</li>\n<li>批次大小：32</li>\n<li>训练轮数：5</li>\n</ul>\n<h4 id=\"训练算法\"><a href=\"#训练算法\" class=\"headerlink\" title=\"训练算法\"></a>训练算法</h4><p>训练过程中，模型的输入为编码后的字符序列，输出为每个字符对应的标签。损失函数采用交叉熵损失，优化器为AdamW。训练循环中每个epoch包括以下步骤：</p>\n<ol>\n<li>前向传播计算损失。</li>\n<li>反向传播更新模型参数。</li>\n<li>每个epoch结束后输出损失值。</li>\n</ol>\n<h3 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h3><h4 id=\"训练过程\"><a href=\"#训练过程\" class=\"headerlink\" title=\"训练过程\"></a>训练过程</h4><p>每个epoch结束后在扩展集上进行验证，记录扩展集上的损失和准确率。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165610.png\" alt=\"image.png\"><br>Epoch 1 - Training Loss: 0.0131668053176951, Validation Loss: 0.010239653369493392, Validation Accuracy: 0.9971221633572501<br>Epoch 2 - Training Loss: 0.00557029220667432, Validation Loss: 0.009271262560837594, Validation Accuracy: 0.9975453132821626<br>Epoch 3 - Training Loss: 0.003971861287672002, Validation Loss: 0.010250022615927534, Validation Accuracy: 0.9975541777907558<br>Epoch 4 - Training Loss: 0.0030227819530087912, Validation Loss: 0.01101609307826279, Validation Accuracy: 0.9976459515267813<br>Epoch 5 - Training Loss: 0.002555301935822686, Validation Loss: 0.01072908611020737, Validation Accuracy: 0.9976827131653596</p>\n<h4 id=\"训练损失和验证集性能曲线\"><a href=\"#训练损失和验证集性能曲线\" class=\"headerlink\" title=\"训练损失和验证集性能曲线\"></a>训练损失和验证集性能曲线</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165639.png\" alt=\"image.png\"></p>\n<h4 id=\"结果分析\"><a href=\"#结果分析\" class=\"headerlink\" title=\"结果分析\"></a>结果分析</h4><p>实验共进行了5个epoch，从折线图中可以看出，训练数据的损失值一直在下降，但在验证集上的损失值有所波动。验证集上准确率有所提升，提升幅度逐渐放缓。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><ol>\n<li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.</li>\n<li>Hugging Face Transformers库：<a href=\"https://github.com/huggingface/transformers\">https://github.com/huggingface/transformers</a></li>\n<li>PyTorch官方文档：<a href=\"https://pytorch.org/docs/stable/index.html\">https://pytorch.org/docs/stable/index.html</a></li>\n<li>ChatGPT，辅助完成少量代码和文档。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"数据介绍\"><a href=\"#数据介绍\" class=\"headerlink\" title=\"数据介绍\"></a>数据介绍</h3><p>根据<code>train_TAG.txt</code>统计得到的标签集如下：<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165851.png\" alt=\"image.png\"><br>标签集大小为9。</p>\n<h3 id=\"模型框架\"><a href=\"#模型框架\" class=\"headerlink\" title=\"模型框架\"></a>模型框架</h3><h4 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h4><p>模型使用hugging face的<code>bert-base-chinese</code>作为预训练模型。模型接受预处理后的字符序列作为输入，输出字符对应的标签。</p>\n<h4 id=\"模型参数\"><a href=\"#模型参数\" class=\"headerlink\" title=\"模型参数\"></a>模型参数</h4><p>BERT模型包含12层Transformer encoder，每层Transformer encoder包含的多头自注意头数为12，隐藏层大小为768。<br>在构建数据集时，设定单条文本最大长度为128，对数据集进行阶段或填充。使用库函数<code>BertTokenizer</code>对数据进行向量化操作。<br>训练时的参数包括：</p>\n<ul>\n<li>优化器：AdamW</li>\n<li>学习率：5e-5</li>\n<li>批次大小：32</li>\n<li>训练轮数：5</li>\n</ul>\n<h4 id=\"训练算法\"><a href=\"#训练算法\" class=\"headerlink\" title=\"训练算法\"></a>训练算法</h4><p>训练过程中，模型的输入为编码后的字符序列，输出为每个字符对应的标签。损失函数采用交叉熵损失，优化器为AdamW。训练循环中每个epoch包括以下步骤：</p>\n<ol>\n<li>前向传播计算损失。</li>\n<li>反向传播更新模型参数。</li>\n<li>每个epoch结束后输出损失值。</li>\n</ol>\n<h3 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h3><h4 id=\"训练过程\"><a href=\"#训练过程\" class=\"headerlink\" title=\"训练过程\"></a>训练过程</h4><p>每个epoch结束后在扩展集上进行验证，记录扩展集上的损失和准确率。<br><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165610.png\" alt=\"image.png\"><br>Epoch 1 - Training Loss: 0.0131668053176951, Validation Loss: 0.010239653369493392, Validation Accuracy: 0.9971221633572501<br>Epoch 2 - Training Loss: 0.00557029220667432, Validation Loss: 0.009271262560837594, Validation Accuracy: 0.9975453132821626<br>Epoch 3 - Training Loss: 0.003971861287672002, Validation Loss: 0.010250022615927534, Validation Accuracy: 0.9975541777907558<br>Epoch 4 - Training Loss: 0.0030227819530087912, Validation Loss: 0.01101609307826279, Validation Accuracy: 0.9976459515267813<br>Epoch 5 - Training Loss: 0.002555301935822686, Validation Loss: 0.01072908611020737, Validation Accuracy: 0.9976827131653596</p>\n<h4 id=\"训练损失和验证集性能曲线\"><a href=\"#训练损失和验证集性能曲线\" class=\"headerlink\" title=\"训练损失和验证集性能曲线\"></a>训练损失和验证集性能曲线</h4><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/20240519165639.png\" alt=\"image.png\"></p>\n<h4 id=\"结果分析\"><a href=\"#结果分析\" class=\"headerlink\" title=\"结果分析\"></a>结果分析</h4><p>实验共进行了5个epoch，从折线图中可以看出，训练数据的损失值一直在下降，但在验证集上的损失值有所波动。验证集上准确率有所提升，提升幅度逐渐放缓。</p>\n<h3 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h3><ol>\n<li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.</li>\n<li>Hugging Face Transformers库：<a href=\"https://github.com/huggingface/transformers\">https://github.com/huggingface/transformers</a></li>\n<li>PyTorch官方文档：<a href=\"https://pytorch.org/docs/stable/index.html\">https://pytorch.org/docs/stable/index.html</a></li>\n<li>ChatGPT，辅助完成少量代码和文档。</li>\n</ol>\n"},{"title":"专业实习","date":"2024-08-26T02:03:53.660Z","_content":"![e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg)\n\n[实验文档](file:///C:/Users/wang/Documents/WeChat%20Files/wxid_3v4u4g0js7gz22/FileStorage/File/2024-08/9ceb9198bbb4b3df50474a6a8594956c_02ddf5d511aa3195f5837db881d7418d_8.pdf)\n\n大模型实验 三选一 \n- 实验1：基于 Stable_diffusion 实现图像生成图像的功能。主要 内容为完成各项基础运算配置、实现在潜在空间的反向过程特征 采样、补全推理阶段的关键步骤以及一系列基 本适配工作，实现由图像/文字生成图像。 \n- 实验2：基于 Llama2实现聊天机器人应用并在 DLP平台上进行 部署。主要内容为完成模型加载和适配、对话模板的应用、文本 生成的基本流程、模型推理的关键步骤，实现 DLP上的机器人聊 天功能。 \n- 实验3：基于 CodeLlama 实现代码生成并在 DLP平台上进行部 署。主要内容为补全推理模块代码，具体包括模型加载和设置、 文本生成和性能统计、结果处理和打印等步骤，实现 DLP上的代 码生成。 \n\n团队实验：基于pytorch自由选题 l 实验讲师讲解示例并提供文档作为参考 l 不限平台，不限主题，根据小组汇报和完成度来打分 l 在MLU平台上完成大模型移植，可额外获得10%加分","source":"_posts/Notes/课程/专业实习/专业实习.md","raw":"---\ntitle: 专业实习\ncategories:\n  - Notes\n  - 课程\n  - 专业实习\ndate:\ntags:\n---\n![e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg](https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg)\n\n[实验文档](file:///C:/Users/wang/Documents/WeChat%20Files/wxid_3v4u4g0js7gz22/FileStorage/File/2024-08/9ceb9198bbb4b3df50474a6a8594956c_02ddf5d511aa3195f5837db881d7418d_8.pdf)\n\n大模型实验 三选一 \n- 实验1：基于 Stable_diffusion 实现图像生成图像的功能。主要 内容为完成各项基础运算配置、实现在潜在空间的反向过程特征 采样、补全推理阶段的关键步骤以及一系列基 本适配工作，实现由图像/文字生成图像。 \n- 实验2：基于 Llama2实现聊天机器人应用并在 DLP平台上进行 部署。主要内容为完成模型加载和适配、对话模板的应用、文本 生成的基本流程、模型推理的关键步骤，实现 DLP上的机器人聊 天功能。 \n- 实验3：基于 CodeLlama 实现代码生成并在 DLP平台上进行部 署。主要内容为补全推理模块代码，具体包括模型加载和设置、 文本生成和性能统计、结果处理和打印等步骤，实现 DLP上的代 码生成。 \n\n团队实验：基于pytorch自由选题 l 实验讲师讲解示例并提供文档作为参考 l 不限平台，不限主题，根据小组汇报和完成度来打分 l 在MLU平台上完成大模型移植，可额外获得10%加分","slug":"Notes/课程/专业实习/专业实习","published":1,"updated":"2024-08-26T02:14:06.698Z","_id":"cm0ad79sp00008k8cdeer5jyv","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg\" alt=\"e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg\"></p>\n<p><a href=\"file:///C:/Users/wang/Documents/WeChat%20Files/wxid_3v4u4g0js7gz22/FileStorage/File/2024-08/9ceb9198bbb4b3df50474a6a8594956c_02ddf5d511aa3195f5837db881d7418d_8.pdf\">实验文档</a></p>\n<p>大模型实验 三选一 </p>\n<ul>\n<li>实验1：基于 Stable_diffusion 实现图像生成图像的功能。主要 内容为完成各项基础运算配置、实现在潜在空间的反向过程特征 采样、补全推理阶段的关键步骤以及一系列基 本适配工作，实现由图像&#x2F;文字生成图像。 </li>\n<li>实验2：基于 Llama2实现聊天机器人应用并在 DLP平台上进行 部署。主要内容为完成模型加载和适配、对话模板的应用、文本 生成的基本流程、模型推理的关键步骤，实现 DLP上的机器人聊 天功能。 </li>\n<li>实验3：基于 CodeLlama 实现代码生成并在 DLP平台上进行部 署。主要内容为补全推理模块代码，具体包括模型加载和设置、 文本生成和性能统计、结果处理和打印等步骤，实现 DLP上的代 码生成。</li>\n</ul>\n<p>团队实验：基于pytorch自由选题 l 实验讲师讲解示例并提供文档作为参考 l 不限平台，不限主题，根据小组汇报和完成度来打分 l 在MLU平台上完成大模型移植，可额外获得10%加分</p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://cdn.jsdelivr.net/gh/zhengyangWang1/image@main/img/e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg\" alt=\"e2ecdf84352b5a5f1fd2a63b3e19c840_.jpg\"></p>\n<p><a href=\"file:///C:/Users/wang/Documents/WeChat%20Files/wxid_3v4u4g0js7gz22/FileStorage/File/2024-08/9ceb9198bbb4b3df50474a6a8594956c_02ddf5d511aa3195f5837db881d7418d_8.pdf\">实验文档</a></p>\n<p>大模型实验 三选一 </p>\n<ul>\n<li>实验1：基于 Stable_diffusion 实现图像生成图像的功能。主要 内容为完成各项基础运算配置、实现在潜在空间的反向过程特征 采样、补全推理阶段的关键步骤以及一系列基 本适配工作，实现由图像&#x2F;文字生成图像。 </li>\n<li>实验2：基于 Llama2实现聊天机器人应用并在 DLP平台上进行 部署。主要内容为完成模型加载和适配、对话模板的应用、文本 生成的基本流程、模型推理的关键步骤，实现 DLP上的机器人聊 天功能。 </li>\n<li>实验3：基于 CodeLlama 实现代码生成并在 DLP平台上进行部 署。主要内容为补全推理模块代码，具体包括模型加载和设置、 文本生成和性能统计、结果处理和打印等步骤，实现 DLP上的代 码生成。</li>\n</ul>\n<p>团队实验：基于pytorch自由选题 l 实验讲师讲解示例并提供文档作为参考 l 不限平台，不限主题，根据小组汇报和完成度来打分 l 在MLU平台上完成大模型移植，可额外获得10%加分</p>\n"},{"_content":"### Emergent Correspondence from Image Diffusion\n使用预训练的扩散模型提取真实图像上的特征，找到两幅图共存的某个特征的对应。在输入图像中添加噪声，然后用U-Net生成特征图，用余弦距离找到不同图像的相似特征","source":"_posts/Notes/毕设/论文调研.md","raw":"---\ncategories:\n  - Notes\n  - 毕设\n---\n### Emergent Correspondence from Image Diffusion\n使用预训练的扩散模型提取真实图像上的特征，找到两幅图共存的某个特征的对应。在输入图像中添加噪声，然后用U-Net生成特征图，用余弦距离找到不同图像的相似特征","slug":"Notes/毕设/论文调研","published":1,"date":"2024-11-27T01:39:40.203Z","updated":"2024-11-27T01:49:13.711Z","_id":"cm3z89hyq0000uw8c2ye7an7z","title":"","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><h3 id=\"Emergent-Correspondence-from-Image-Diffusion\"><a href=\"#Emergent-Correspondence-from-Image-Diffusion\" class=\"headerlink\" title=\"Emergent Correspondence from Image Diffusion\"></a>Emergent Correspondence from Image Diffusion</h3><p>使用预训练的扩散模型提取真实图像上的特征，找到两幅图共存的某个特征的对应。在输入图像中添加噪声，然后用U-Net生成特征图，用余弦距离找到不同图像的相似特征</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Emergent-Correspondence-from-Image-Diffusion\"><a href=\"#Emergent-Correspondence-from-Image-Diffusion\" class=\"headerlink\" title=\"Emergent Correspondence from Image Diffusion\"></a>Emergent Correspondence from Image Diffusion</h3><p>使用预训练的扩散模型提取真实图像上的特征，找到两幅图共存的某个特征的对应。在输入图像中添加噪声，然后用U-Net生成特征图，用余弦距离找到不同图像的相似特征</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cln1vnr510006tou5048m8qov","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr58000stou5fd5ha4a7"},{"post_id":"cln1vnr510006tou5048m8qov","category_id":"cln1vnr56000jtou5f0jyf3fr","_id":"cln1vnr59000xtou5hr5aeyit"},{"post_id":"cln1vnr520007tou52vika6pq","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5a0011tou5329j3myb"},{"post_id":"cln1vnr520007tou52vika6pq","category_id":"cln1vnr57000ntou51p2o6tqt","_id":"cln1vnr5a0015tou5g7xx2m65"},{"post_id":"cln1vnr53000btou5fi1u1wxe","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5b0018tou59uyq6inp"},{"post_id":"cln1vnr53000btou5fi1u1wxe","category_id":"cln1vnr58000utou5glr24ut0","_id":"cln1vnr5b001btou5f0ey09ou"},{"post_id":"cln1vnr4x0002tou5bo0c25p2","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5c001ftou544tka3x8"},{"post_id":"cln1vnr4x0002tou5bo0c25p2","category_id":"cln1vnr57000ntou51p2o6tqt","_id":"cln1vnr5d001jtou564p3fn3f"},{"post_id":"cln1vnr54000ctou55wf55oh0","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5d001ntou5e00tby2a"},{"post_id":"cln1vnr54000ctou55wf55oh0","category_id":"cln1vnr58000utou5glr24ut0","_id":"cln1vnr5e001qtou50skhc2p0"},{"post_id":"cln1vnr55000gtou5bknz2q2h","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5e001utou52x42ad65"},{"post_id":"cln1vnr55000gtou5bknz2q2h","category_id":"cln1vnr58000utou5glr24ut0","_id":"cln1vnr5f001xtou5aei24eqi"},{"post_id":"cln1vnr500005tou5dhl327l4","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5g0021tou511schesy"},{"post_id":"cln1vnr500005tou5dhl327l4","category_id":"cln1vnr57000ntou51p2o6tqt","_id":"cln1vnr5g0024tou535wka6uv"},{"post_id":"cln1vnr56000itou57jeqfejp","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5h0028tou5b4hy0jjd"},{"post_id":"cln1vnr56000itou57jeqfejp","category_id":"cln1vnr58000utou5glr24ut0","_id":"cln1vnr5i002btou545xc1oy0"},{"post_id":"cln1vnr56000ltou5b2dpd2bt","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5i002etou50h2ffpfu"},{"post_id":"cln1vnr56000ltou5b2dpd2bt","category_id":"cln1vnr58000utou5glr24ut0","_id":"cln1vnr5j002itou52xvtd4al"},{"post_id":"cln1vnr57000mtou575vg8qzc","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5j002ltou5c7ygddrg"},{"post_id":"cln1vnr57000mtou575vg8qzc","category_id":"cln1vnr5h0027tou5caih55y1","_id":"cln1vnr5j002otou5gbfs28jd"},{"post_id":"cln1vnr58000ptou53www8psm","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5k002rtou567be84rr"},{"post_id":"cln1vnr58000ptou53www8psm","category_id":"cln1vnr5i002gtou5aw4qh5lm","_id":"cln1vnr5k002ttou5aqxa78ci"},{"post_id":"cln1vnr58000rtou5e46f86mt","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5k002wtou5h3yfh5t6"},{"post_id":"cln1vnr58000rtou5e46f86mt","category_id":"cln1vnr5i002gtou5aw4qh5lm","_id":"cln1vnr5k002ytou533vka38k"},{"post_id":"cln1vnr59000wtou56isihdri","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5k0030tou50hmfesn8"},{"post_id":"cln1vnr59000wtou56isihdri","category_id":"cln1vnr5i002gtou5aw4qh5lm","_id":"cln1vnr5l0033tou549yq1hvr"},{"post_id":"cln1vnr59000ztou58dyj1jgo","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5l0035tou5bkk72p9a"},{"post_id":"cln1vnr59000ztou58dyj1jgo","category_id":"cln1vnr5k002vtou5hpnodhuy","_id":"cln1vnr5l0038tou578gp25d7"},{"post_id":"cln1vnr5a0013tou54btlgbni","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5l003atou5f9826kur"},{"post_id":"cln1vnr5a0013tou54btlgbni","category_id":"cln1vnr5k0031tou5al0dbqcy","_id":"cln1vnr5m003dtou52oo5e3xi"},{"post_id":"cln1vnr5c001htou5cnq3208o","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5n003ttou5e9wietgu"},{"post_id":"cln1vnr5c001htou5cnq3208o","category_id":"cln1vnr5k002vtou5hpnodhuy","_id":"cln1vnr5n003vtou50s8n7uhm"},{"post_id":"cln1vnr5d001ktou54gp9gg5i","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5s0052tou52cl23tjf"},{"post_id":"cln1vnr5d001ktou54gp9gg5i","category_id":"cln1vnr5k0031tou5al0dbqcy","_id":"cln1vnr5s0053tou5dn7zgvnz"},{"post_id":"cln1vnr5d001ktou54gp9gg5i","category_id":"cln1vnr5r004rtou51a9m0u2j","_id":"cln1vnr5s0056tou58ejae4a6"},{"post_id":"cln1vnr5e001rtou51zes7wqs","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cln1vnr5t005htou5etykatl7"},{"post_id":"cln1vnr5e001rtou51zes7wqs","category_id":"cln1vnr5k0031tou5al0dbqcy","_id":"cln1vnr5t005jtou56l2t0sxw"},{"post_id":"cln1vnr5e001rtou51zes7wqs","category_id":"cln1vnr5r004rtou51a9m0u2j","_id":"cln1vnr5t005ltou5abbe06o2"},{"post_id":"clne436w9000028u5dbw0h1th","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clne4f7jm0000jou530wsf87s"},{"post_id":"clne436w9000028u5dbw0h1th","category_id":"cln1vnr5k0031tou5al0dbqcy","_id":"clne4f7jm0001jou57j36ex6y"},{"post_id":"clne436w9000028u5dbw0h1th","category_id":"cln1vnr5r004rtou51a9m0u2j","_id":"clne4f7jm0002jou5aocbgp3q"},{"post_id":"clnjr0wpf0000rou5hi9ibg4a","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clnjr0wpm0003rou50gln2f3e"},{"post_id":"clnjr0wpf0000rou5hi9ibg4a","category_id":"cln1vnr5k0031tou5al0dbqcy","_id":"clnjr0wpn0004rou5brzidygn"},{"post_id":"clnjr0wpf0000rou5hi9ibg4a","category_id":"cln1vnr5q004otou56wefgn2d","_id":"clnjrf1tb0000iwu5ca1dcvho"},{"post_id":"clo44jtk40000g0u5dfmkejl5","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clo44vj7i00012su5fy4wceqs"},{"post_id":"clo44jtk40000g0u5dfmkejl5","category_id":"clo44vj7e00002su5hwqvf7sc","_id":"clo44vj7j00022su5ho3f566x"},{"post_id":"clo4gan3f00009cu580hn4ios","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clo4gan3h00039cu5hzsl1bjb"},{"post_id":"clo4gan3f00009cu580hn4ios","category_id":"cln1vnr5k0031tou5al0dbqcy","_id":"clo4gan3h00049cu56qpq3vb7"},{"post_id":"clo4gan3f00009cu580hn4ios","category_id":"cln1vnr5q004otou56wefgn2d","_id":"clo4gan3i00059cu54ce1986c"},{"post_id":"clo5xc63m000008u57cfg4n7d","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clo5xc63r000208u5a7ix9uxq"},{"post_id":"clo5xc63m000008u57cfg4n7d","category_id":"cln1vnr5l003btou5eks44p4m","_id":"clo5xc63s000308u5727986n0"},{"post_id":"clo8stmx80000548cbgrv32y9","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clo9fqeqm0002v48c7mfoap1s"},{"post_id":"clo8stmx80000548cbgrv32y9","category_id":"cln1vnr5k0031tou5al0dbqcy","_id":"clo9fqeqn0003v48c8olt8q10"},{"post_id":"clo8stmx80000548cbgrv32y9","category_id":"cln1vnr5q004otou56wefgn2d","_id":"clo9fqeqn0004v48c7tdh7lzc"},{"post_id":"clof2n7700000rk8c4g6sf4ry","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clof2zhla0000gc8c1mwca0io"},{"post_id":"clof2n7700000rk8c4g6sf4ry","category_id":"cln1vnr5l003btou5eks44p4m","_id":"clof2zhla0001gc8c2xh5c4ml"},{"post_id":"cloww9wlj0000n48c0mfd6ilu","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clowwzsqw0000w48cek82bzuu"},{"post_id":"cloww9wlj0000n48c0mfd6ilu","category_id":"cln1vnr5l003btou5eks44p4m","_id":"clowwzsqw0001w48c23dbakev"},{"post_id":"clozmwiwz0000ug8ceroj0vt8","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clozn9dfw0000w48c9p9i3l8u"},{"post_id":"clozmwiwz0000ug8ceroj0vt8","category_id":"cln1vnr5l003btou5eks44p4m","_id":"clozn9dfw0001w48c82ebdznv"},{"post_id":"cls4q01yx0000ko8cesu726dg","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clswlytmg00015k8cg5p50e4f"},{"post_id":"cls4q01yx0000ko8cesu726dg","category_id":"clswlytm900005k8c4gmuc23c","_id":"clswlytmg00025k8cfzylb7xc"},{"post_id":"clt15zuhi00000w8capbb3ggw","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt16c09q0000188c3z159lrg"},{"post_id":"clt15zuhi00000w8capbb3ggw","category_id":"clswlytm900005k8c4gmuc23c","_id":"clt16c09r0001188cc2ujc0wl"},{"post_id":"clt9kr11r0000vw8c7idr7zk9","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr11y0006vw8chgqn37hy"},{"post_id":"clt9kr11r0000vw8c7idr7zk9","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr11z0009vw8ch30kckaq"},{"post_id":"clt9kr11v0001vw8c4uca0evn","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr120000fvw8c671v0vo6"},{"post_id":"clt9kr11v0001vw8c4uca0evn","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr121000ivw8c9n5z2t3g"},{"post_id":"clt9kr11w0003vw8cek8cdiee","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr123000nvw8c5xvk1v9c"},{"post_id":"clt9kr11w0003vw8cek8cdiee","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr124000qvw8cbvk0fpg9"},{"post_id":"clt9kr11x0005vw8c33u725wx","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr125000wvw8chmi3ggk9"},{"post_id":"clt9kr11x0005vw8c33u725wx","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr126000zvw8c21td1wqh"},{"post_id":"clt9kr11y0008vw8c39iw5btz","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr1270015vw8c74nz11l5"},{"post_id":"clt9kr11y0008vw8c39iw5btz","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr1280018vw8c7p8mgrud"},{"post_id":"clt9kr11z000bvw8c6ein5fn7","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr129001evw8ch2o9csn5"},{"post_id":"clt9kr11z000bvw8c6ein5fn7","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr129001hvw8capu33fif"},{"post_id":"clt9kr120000hvw8cdha62zj3","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12b001nvw8ca7uq4c9f"},{"post_id":"clt9kr120000hvw8cdha62zj3","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12b001qvw8cb5w18lps"},{"post_id":"clt9kr121000jvw8c614ueu3h","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12c001wvw8c006h4ujz"},{"post_id":"clt9kr121000jvw8c614ueu3h","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12d001zvw8c6kn6061p"},{"post_id":"clt9kr122000mvw8c8llb54rr","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12e0025vw8cgn5jcfql"},{"post_id":"clt9kr122000mvw8c8llb54rr","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12e0027vw8caaq77wjo"},{"post_id":"clt9kr123000pvw8c2ko81xt0","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12e002bvw8c352b5hk9"},{"post_id":"clt9kr123000pvw8c2ko81xt0","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12e002dvw8c2rsubldc"},{"post_id":"clt9kr124000svw8c7nfla28m","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12e002hvw8cdgfaf29x"},{"post_id":"clt9kr124000svw8c7nfla28m","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12e002ivw8cdll4ckqq"},{"post_id":"clt9kr124000vvw8c9kmt6st8","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12e002kvw8cbsrcaxrp"},{"post_id":"clt9kr124000vvw8c9kmt6st8","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12e002lvw8c0lzjhmqp"},{"post_id":"clt9kr125000yvw8cd8b0e20x","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12e002nvw8c8c3sbvt0"},{"post_id":"clt9kr125000yvw8cd8b0e20x","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12e002ovw8c2kdpcwxh"},{"post_id":"clt9kr1260011vw8c39ji8f3l","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12e002qvw8c1lz0h9id"},{"post_id":"clt9kr1260011vw8c39ji8f3l","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f002rvw8c1jl89dbu"},{"post_id":"clt9kr1260014vw8c6ildb8kk","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f002tvw8ce7qvgr7g"},{"post_id":"clt9kr1260014vw8c6ildb8kk","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f002uvw8c3zgh8g4l"},{"post_id":"clt9kr1270017vw8cdn33gn9p","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f002wvw8c5ax31edh"},{"post_id":"clt9kr1270017vw8cdn33gn9p","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f002xvw8c8ml768ca"},{"post_id":"clt9kr128001avw8c97lmc54b","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f002zvw8ca0kee4ue"},{"post_id":"clt9kr128001avw8c97lmc54b","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f0030vw8c2zo559ur"},{"post_id":"clt9kr128001dvw8c4xk51xbc","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f0032vw8c8jczevks"},{"post_id":"clt9kr128001dvw8c4xk51xbc","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f0033vw8c7awh0ub4"},{"post_id":"clt9kr129001gvw8cb0l3hj4h","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f0035vw8c64ry7bs8"},{"post_id":"clt9kr129001gvw8cb0l3hj4h","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f0036vw8ccuiy0gux"},{"post_id":"clt9kr12a001jvw8c07an5vfz","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f0038vw8c24gl30af"},{"post_id":"clt9kr12a001jvw8c07an5vfz","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f0039vw8c196o88if"},{"post_id":"clt9kr12a001mvw8c7fa5d1n5","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f003bvw8cgdnk8smi"},{"post_id":"clt9kr12a001mvw8c7fa5d1n5","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f003cvw8cdhuaazvc"},{"post_id":"clt9kr12b001pvw8cel9u1pa7","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f003evw8cb309bbot"},{"post_id":"clt9kr12b001pvw8cel9u1pa7","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f003fvw8cd2h19b8i"},{"post_id":"clt9kr12b001svw8c8l96hhyh","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f003hvw8cczxu7i7s"},{"post_id":"clt9kr12b001svw8c8l96hhyh","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f003ivw8c56rs4363"},{"post_id":"clt9kr12c001vvw8c68z32q8p","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f003kvw8ce1gc1efj"},{"post_id":"clt9kr12c001vvw8c68z32q8p","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f003lvw8c8v0u68p1"},{"post_id":"clt9kr12c001yvw8camv3dces","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f003nvw8c9jl19pak"},{"post_id":"clt9kr12c001yvw8camv3dces","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f003ovw8c77rhe1o3"},{"post_id":"clt9kr12d0021vw8cee0dfhuv","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f003qvw8caf7q1x1v"},{"post_id":"clt9kr12d0021vw8cee0dfhuv","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f003rvw8cg8r88lzd"},{"post_id":"clt9kr12d0024vw8cgpk19t43","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12f003tvw8c57ufc9sg"},{"post_id":"clt9kr12d0024vw8cgpk19t43","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12f003uvw8c8ixq7red"},{"post_id":"clt9kr12g003wvw8cfmtuftxy","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12i0042vw8c8guc736j"},{"post_id":"clt9kr12g003wvw8cfmtuftxy","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12i0045vw8c722men55"},{"post_id":"clt9kr12g003xvw8c4iui360g","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12j004bvw8c3y2cbmgx"},{"post_id":"clt9kr12g003xvw8c4iui360g","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12k004evw8c53lqhmqr"},{"post_id":"clt9kr12h003zvw8c8gkadmuy","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12k004kvw8cc3ckfpku"},{"post_id":"clt9kr12h003zvw8c8gkadmuy","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12l004nvw8c5oghfe9a"},{"post_id":"clt9kr12h0041vw8c96up9h3y","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12m004tvw8c671p0m3z"},{"post_id":"clt9kr12h0041vw8c96up9h3y","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12m004wvw8c5hci6jqn"},{"post_id":"clt9kr12i0044vw8c54tu48gt","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12n0051vw8cg59f500l"},{"post_id":"clt9kr12i0044vw8c54tu48gt","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12n0053vw8c6gwh7p6v"},{"post_id":"clt9kr12i0047vw8c2ayz424a","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12n0057vw8cg1ld2c4a"},{"post_id":"clt9kr12i0047vw8c2ayz424a","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12n0059vw8c54rzah16"},{"post_id":"clt9kr12j004avw8c9k8vg4hb","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12n005dvw8cbuldhv3x"},{"post_id":"clt9kr12j004avw8c9k8vg4hb","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12n005evw8cdpqycqo5"},{"post_id":"clt9kr12j004dvw8canyq6tw7","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12n005gvw8cheuy666u"},{"post_id":"clt9kr12j004dvw8canyq6tw7","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12n005hvw8ccssv1681"},{"post_id":"clt9kr12k004gvw8c56wn86f4","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12n005jvw8c0mv8gtxx"},{"post_id":"clt9kr12k004gvw8c56wn86f4","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12n005kvw8c0hlr11wk"},{"post_id":"clt9kr12k004jvw8cer857c0t","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12n005mvw8c79my1pxc"},{"post_id":"clt9kr12k004jvw8cer857c0t","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12n005nvw8ca7ug8w30"},{"post_id":"clt9kr12k004mvw8cbyc02qz0","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12n005pvw8c3vixf71h"},{"post_id":"clt9kr12k004mvw8cbyc02qz0","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12o005qvw8c0rrta05d"},{"post_id":"clt9kr12l004pvw8c16br7ue3","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12o005svw8c481s355p"},{"post_id":"clt9kr12l004pvw8c16br7ue3","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12o005tvw8c38lg0stx"},{"post_id":"clt9kr12l004svw8c1m6sgu9l","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12o005vvw8c59uha19n"},{"post_id":"clt9kr12l004svw8c1m6sgu9l","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12o005wvw8cfcf0fyva"},{"post_id":"clt9kr12m004vvw8c7ybt27l6","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12o005yvw8c07s2hu5s"},{"post_id":"clt9kr12m004vvw8c7ybt27l6","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12o005zvw8c2xel2iyy"},{"post_id":"clt9kr12m004yvw8c0j5r44bt","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9kr12o0061vw8c34ehdxk9"},{"post_id":"clt9kr12m004yvw8c0j5r44bt","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9kr12o0062vw8c4ued8iwl"},{"post_id":"clt9kr120000evw8c707pbyho","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clt9l3vys0008uo8c0p1ebj2l"},{"post_id":"clt9kr120000evw8c707pbyho","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clt9l3vyu000auo8cebq61ooh"},{"post_id":"clt9kr120000evw8c707pbyho","category_id":"clt9l3vyj0006uo8c852p2u0m","_id":"clt9l3vyu000buo8c1fuxfry5"},{"post_id":"clt9kr11y0008vw8c39iw5btz","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz0000huo8cef2u0dds"},{"post_id":"clt9kr11y0008vw8c39iw5btz","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vz0000iuo8c0hr770gj"},{"post_id":"clt9kr11r0000vw8c7idr7zk9","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz1000kuo8c9ca0d9ik"},{"post_id":"clt9kr11r0000vw8c7idr7zk9","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vz1000luo8c4let3i89"},{"post_id":"clt9kr11v0001vw8c4uca0evn","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz3000nuo8cea5octtw"},{"post_id":"clt9kr11v0001vw8c4uca0evn","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vz4000puo8c9hw3cir4"},{"post_id":"clt9kr123000pvw8c2ko81xt0","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz5000quo8chfyg0kr3"},{"post_id":"clt9kr123000pvw8c2ko81xt0","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vz6000suo8cgpau3phd"},{"post_id":"clt9kr124000svw8c7nfla28m","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz6000tuo8c15ok1o2j"},{"post_id":"clt9kr124000svw8c7nfla28m","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vz7000vuo8cfs1l2ik9"},{"post_id":"clt9kr11x0005vw8c33u725wx","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz8000wuo8c0nwbez0r"},{"post_id":"clt9kr11x0005vw8c33u725wx","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vz8000yuo8c78gj0t2v"},{"post_id":"clt9kr11z000bvw8c6ein5fn7","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vz90010uo8ccvqs0uy9"},{"post_id":"clt9kr11z000bvw8c6ein5fn7","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vza0011uo8cb4jcfqt1"},{"post_id":"clt9kr121000jvw8c614ueu3h","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzb0013uo8c80m0bqte"},{"post_id":"clt9kr121000jvw8c614ueu3h","category_id":"clt9l3vz4000ouo8ce26x1g5q","_id":"clt9l3vzb0014uo8cdhjd38kh"},{"post_id":"clt9kr120000hvw8cdha62zj3","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzc0016uo8cdaws3vw1"},{"post_id":"clt9kr120000hvw8cdha62zj3","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vzd0018uo8ceqnv0odj"},{"post_id":"clt9kr11w0003vw8cek8cdiee","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vze001auo8c4k70epf8"},{"post_id":"clt9kr11w0003vw8cek8cdiee","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vzf001buo8cgdbddy6s"},{"post_id":"clt9kr122000mvw8c8llb54rr","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzf001duo8c6f6q24ns"},{"post_id":"clt9kr122000mvw8c8llb54rr","category_id":"clt9l3vz4000ouo8ce26x1g5q","_id":"clt9l3vzf001fuo8cgkrwhmrf"},{"post_id":"clt9kr124000vvw8c9kmt6st8","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzg001huo8c1fngb9rd"},{"post_id":"clt9kr124000vvw8c9kmt6st8","category_id":"clt9l3vz4000ouo8ce26x1g5q","_id":"clt9l3vzg001iuo8c5mpn0sjt"},{"post_id":"clt9kr12b001svw8c8l96hhyh","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzg001kuo8cg2v9hw0c"},{"post_id":"clt9kr12b001svw8c8l96hhyh","category_id":"clt9l3vyx000euo8ccw6whuki","_id":"clt9l3vzg001muo8cfpj22c7p"},{"post_id":"clt9kr1260011vw8c39ji8f3l","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzg001nuo8c4ogib6wu"},{"post_id":"clt9kr1260011vw8c39ji8f3l","category_id":"clt9l3vz4000ouo8ce26x1g5q","_id":"clt9l3vzh001puo8c95er14yn"},{"post_id":"clt9kr129001gvw8cb0l3hj4h","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzh001ruo8ch996544z"},{"post_id":"clt9kr129001gvw8cb0l3hj4h","category_id":"clt9l3vz4000ouo8ce26x1g5q","_id":"clt9l3vzh001suo8c0a9ufvsv"},{"post_id":"clt9kr128001avw8c97lmc54b","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzh001uuo8ccoj0bm0c"},{"post_id":"clt9kr128001avw8c97lmc54b","category_id":"clt9l3vz4000ouo8ce26x1g5q","_id":"clt9l3vzi001wuo8cf4noemv7"},{"post_id":"clt9kr125000yvw8cd8b0e20x","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzi001xuo8c5xzm8jey"},{"post_id":"clt9kr125000yvw8cd8b0e20x","category_id":"clt9l3vz4000ouo8ce26x1g5q","_id":"clt9l3vzi001zuo8c9c1v59nn"},{"post_id":"clt9kr1260014vw8c6ildb8kk","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzi0021uo8c0qps286k"},{"post_id":"clt9kr1260014vw8c6ildb8kk","category_id":"clt9l3vzf001euo8cae9ugysl","_id":"clt9l3vzi0022uo8ch6l8fpoy"},{"post_id":"clt9kr1270017vw8cdn33gn9p","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzi0024uo8c33xf7l27"},{"post_id":"clt9kr1270017vw8cdn33gn9p","category_id":"clt9l3vzf001euo8cae9ugysl","_id":"clt9l3vzj0026uo8cef0mfhm4"},{"post_id":"clt9kr128001dvw8c4xk51xbc","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzj0027uo8c3cqha44l"},{"post_id":"clt9kr128001dvw8c4xk51xbc","category_id":"clt9l3vzf001euo8cae9ugysl","_id":"clt9l3vzj0029uo8chdb07k1k"},{"post_id":"clt9kr12b001pvw8cel9u1pa7","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzj002buo8caqmd3m4g"},{"post_id":"clt9kr12b001pvw8cel9u1pa7","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzj002duo8c51c7bxjt"},{"post_id":"clt9kr12d0024vw8cgpk19t43","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzj002euo8c5ony2rga"},{"post_id":"clt9kr12d0024vw8cgpk19t43","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzk002guo8cb0ssg9m5"},{"post_id":"clt9kr12c001yvw8camv3dces","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzk002iuo8cglo775ga"},{"post_id":"clt9kr12c001yvw8camv3dces","category_id":"clt9l3vzf001euo8cae9ugysl","_id":"clt9l3vzk002juo8c59idbar8"},{"post_id":"clt9kr12c001vvw8c68z32q8p","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzk002luo8c3iur3vfp"},{"post_id":"clt9kr12c001vvw8c68z32q8p","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzk002nuo8cfbsrdr8i"},{"post_id":"clt9kr12d0021vw8cee0dfhuv","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzl002puo8cg5s5a6e8"},{"post_id":"clt9kr12d0021vw8cee0dfhuv","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzl002quo8cdutl9yid"},{"post_id":"clt9kr12a001mvw8c7fa5d1n5","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzm002suo8c5i6wdxyb"},{"post_id":"clt9kr12a001mvw8c7fa5d1n5","category_id":"clt9l3vzf001euo8cae9ugysl","_id":"clt9l3vzm002tuo8c9x2yhe7x"},{"post_id":"clt9kr12a001jvw8c07an5vfz","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn002vuo8ce3dv4hha"},{"post_id":"clt9kr12a001jvw8c07an5vfz","category_id":"clt9l3vzf001euo8cae9ugysl","_id":"clt9l3vzn002xuo8c1n5fdde6"},{"post_id":"clt9kr12g003wvw8cfmtuftxy","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn002yuo8c1t2i9qrk"},{"post_id":"clt9kr12g003wvw8cfmtuftxy","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzn002zuo8cbqdvd0iv"},{"post_id":"clt9kr12i0047vw8c2ayz424a","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn0030uo8cd4d44tz2"},{"post_id":"clt9kr12i0047vw8c2ayz424a","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzn0031uo8c24tgfpsx"},{"post_id":"clt9kr12h003zvw8c8gkadmuy","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn0032uo8cb6whcmt3"},{"post_id":"clt9kr12h003zvw8c8gkadmuy","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzn0033uo8canu97sme"},{"post_id":"clt9kr12h0041vw8c96up9h3y","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn0034uo8c8ic5el22"},{"post_id":"clt9kr12h0041vw8c96up9h3y","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzn0035uo8c2a1p7c5x"},{"post_id":"clt9kr12g003xvw8c4iui360g","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn0036uo8cet3y8rj1"},{"post_id":"clt9kr12g003xvw8c4iui360g","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzn0037uo8cauz7aure"},{"post_id":"clt9kr12j004dvw8canyq6tw7","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn0038uo8cb4fn9ldz"},{"post_id":"clt9kr12j004dvw8canyq6tw7","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzn0039uo8cdi2s0p3r"},{"post_id":"clt9kr12k004gvw8c56wn86f4","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn003auo8c6wmab6z2"},{"post_id":"clt9kr12k004gvw8c56wn86f4","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzn003buo8c4ya56tql"},{"post_id":"clt9kr12l004svw8c1m6sgu9l","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzn003cuo8cclg98ld7"},{"post_id":"clt9kr12l004svw8c1m6sgu9l","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzo003duo8cf23r7c3z"},{"post_id":"clt9kr12j004avw8c9k8vg4hb","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzo003euo8cfvk6db71"},{"post_id":"clt9kr12j004avw8c9k8vg4hb","category_id":"clt9l3vzk002muo8c600wd1r4","_id":"clt9l3vzo003fuo8ccounhuaa"},{"post_id":"clt9kr12i0044vw8c54tu48gt","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzo003guo8c6dhsgr2s"},{"post_id":"clt9kr12i0044vw8c54tu48gt","category_id":"clt9l3vzk002muo8c600wd1r4","_id":"clt9l3vzo003huo8c8c672nn3"},{"post_id":"clt9kr12k004jvw8cer857c0t","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzo003iuo8c08bpan70"},{"post_id":"clt9kr12k004jvw8cer857c0t","category_id":"clt9l3vzk002muo8c600wd1r4","_id":"clt9l3vzo003juo8c9ytw384u"},{"post_id":"clt9kr12m004yvw8c0j5r44bt","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzo003kuo8ca147c27z"},{"post_id":"clt9kr12m004yvw8c0j5r44bt","category_id":"clt9l3vzk002muo8c600wd1r4","_id":"clt9l3vzo003luo8c9e4wdwy2"},{"post_id":"clt9kr12l004pvw8c16br7ue3","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzo003muo8c8go80vb0"},{"post_id":"clt9kr12l004pvw8c16br7ue3","category_id":"clt9l3vzg001luo8chay4btjb","_id":"clt9l3vzo003nuo8cc6l6fche"},{"post_id":"clt9kr12m004vvw8c7ybt27l6","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzp003ouo8cfsaybau3"},{"post_id":"clt9kr12m004vvw8c7ybt27l6","category_id":"clt9l3vzk002muo8c600wd1r4","_id":"clt9l3vzq003puo8c1mw82kyu"},{"post_id":"clt9kr12k004mvw8cbyc02qz0","category_id":"clt9l3vyr0007uo8cfcr960my","_id":"clt9l3vzq003quo8cen2jh57t"},{"post_id":"clt9kr12k004mvw8cbyc02qz0","category_id":"clt9l3vzk002muo8c600wd1r4","_id":"clt9l3vzq003ruo8c6rstam4p"},{"post_id":"cltcxrc020000bw8ceaok5a23","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltcy3bxj0000cg8cc1a63yty"},{"post_id":"cltcxrc020000bw8ceaok5a23","category_id":"clswlytm900005k8c4gmuc23c","_id":"cltcy3bxj0001cg8cdx4gas7x"},{"post_id":"cltfqp2lc0000ss8cdw08gdiw","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltfr0ztu0000848cflzfdrbo"},{"post_id":"cltfqp2lc0000ss8cdw08gdiw","category_id":"cln1vnr5k002vtou5hpnodhuy","_id":"cltfr0ztu0001848cf0th0mil"},{"post_id":"cltfqp2lj0001ss8c0xtphcnz","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltfr0ztu0002848c2r3mbksx"},{"post_id":"cltfqp2lj0001ss8c0xtphcnz","category_id":"cln1vnr5k002vtou5hpnodhuy","_id":"cltfr0ztu0003848c65iuhkko"},{"post_id":"cltxy9nge00000k8cccts64n8","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltxy9ngt00040k8c4ih04r39"},{"post_id":"cltxy9nge00000k8cccts64n8","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cltxy9ngu00060k8cg0m9anp9"},{"post_id":"cltxy9nge00000k8cccts64n8","category_id":"clt9lgqxf000dfg8c992n4ii9","_id":"cltxy9ngw00080k8c9jc8fmm1"},{"post_id":"cltxy9ngo00010k8c3ll8fkap","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltxy9ngx000a0k8c8mjd56jv"},{"post_id":"cltxy9ngo00010k8c3ll8fkap","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cltxy9ngy000b0k8c44t98wc2"},{"post_id":"cltxy9ngo00010k8c3ll8fkap","category_id":"clt9lgqxe0005fg8c49xh2nh1","_id":"cltxy9ngy000c0k8cavcpfqm1"},{"post_id":"cltxy9ngq00020k8c79fp7s05","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltxy9ngy000d0k8c87qg7sue"},{"post_id":"cltxy9ngq00020k8c79fp7s05","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cltxy9ngy000e0k8c9qqnf5l0"},{"post_id":"cltxy9ngq00020k8c79fp7s05","category_id":"clt9lgqxf000afg8c8jc22nzz","_id":"cltxy9ngy000f0k8c7av11gkf"},{"post_id":"cltxy9ngr00030k8c2k5w9x2p","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltxy9ngy000g0k8c7w8a0b85"},{"post_id":"cltxy9ngr00030k8c2k5w9x2p","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cltxy9ngy000h0k8c4qoyem0b"},{"post_id":"cltxy9ngr00030k8c2k5w9x2p","category_id":"clt9lgqx20000fg8c5t9xalcd","_id":"cltxy9ngy000i0k8cfxptaflg"},{"post_id":"cltxy9ngt00050k8c820376y0","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltxy9ngy000j0k8c6s1j1yf8"},{"post_id":"cltxy9ngt00050k8c820376y0","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cltxy9ngz000k0k8chcnr2jha"},{"post_id":"cltxy9ngt00050k8c820376y0","category_id":"clt9lgqxc0004fg8caiu0b7wa","_id":"cltxy9ngz000l0k8cf3bk6esq"},{"post_id":"cltxy9ngw00090k8c2zo2gz0b","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltxy9ngz000m0k8c39mheafb"},{"post_id":"cltxy9ngw00090k8c2zo2gz0b","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cltxy9ngz000n0k8c8lmw8rdr"},{"post_id":"cltxy9ngw00090k8c2zo2gz0b","category_id":"clt9lgqxe0007fg8cfh1e9arc","_id":"cltxy9ngz000o0k8c6ght6qep"},{"post_id":"cltxy9ngv00070k8c1gbu0d3l","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cltxylnzi0000ho8cga8000id"},{"post_id":"cltxy9ngv00070k8c1gbu0d3l","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cltxylnzi0001ho8cbmea020i"},{"post_id":"cltxy9ngv00070k8c1gbu0d3l","category_id":"clt9lgqxe0007fg8cfh1e9arc","_id":"cltxylnzi0002ho8cej2ldjpz"},{"post_id":"cluhsoscm0000og8c7tdh1jvj","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cluht0yji00009g8c61ic92qe"},{"post_id":"cluhsoscm0000og8c7tdh1jvj","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cluht0yjj00019g8cclrzb4nl"},{"post_id":"cluhsoscm0000og8c7tdh1jvj","category_id":"clt9lgqxc0004fg8caiu0b7wa","_id":"cluht0yjj00029g8ce0qv1ci4"},{"post_id":"cluuorj090000tw8cakpycf18","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cluup4enf0000ws8c4nro1618"},{"post_id":"cluuorj090000tw8cakpycf18","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cluup4enf0001ws8cb37g7172"},{"post_id":"cluuorj090000tw8cakpycf18","category_id":"clt9l3vyj0006uo8c852p2u0m","_id":"cluup4enf0002ws8cgrcu8rg9"},{"post_id":"clv29w3np00006s8ccb3u248e","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clv2a8y1n0000ug8c19qw2qh6"},{"post_id":"clv29w3np00006s8ccb3u248e","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clv2a8y1o0001ug8c58yedfcv"},{"post_id":"clv29w3np00006s8ccb3u248e","category_id":"clt9lgqxe0007fg8cfh1e9arc","_id":"clv2a8y1o0002ug8c7fyectcw"},{"post_id":"clveyesfe0000n88c82wj3w6i","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clveyesfj0001n88cb31dge55"},{"post_id":"clveyesfe0000n88c82wj3w6i","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clveyesfj0002n88ce5yogo6s"},{"post_id":"clveyesfe0000n88c82wj3w6i","category_id":"clt9lgqxc0004fg8caiu0b7wa","_id":"clveyesfj0003n88c75qt4ret"},{"post_id":"clw39tsza0000f08cbho81wex","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clw3a7yej0000ns8ca6lm207t"},{"post_id":"clw39tsza0000f08cbho81wex","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clw3a7yek0001ns8c26g32btk"},{"post_id":"clw39tsza0000f08cbho81wex","category_id":"clt9lgqxc0004fg8caiu0b7wa","_id":"clw3a7yek0002ns8ced9fca28"},{"post_id":"clwcc2coe0000s88c2joqewbz","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"clwccdzu10000448cedgc7g4e"},{"post_id":"clwcc2coe0000s88c2joqewbz","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"clwccdzu20001448cbc675xue"},{"post_id":"clwcc2coe0000s88c2joqewbz","category_id":"clt9l3vyj0006uo8c852p2u0m","_id":"clwccdzu20002448ca4db133v"},{"post_id":"cm0ad79sp00008k8cdeer5jyv","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cm0adj59o0001dg8cee1hcjbq"},{"post_id":"cm0ad79sp00008k8cdeer5jyv","category_id":"cln1vnr5l0036tou5h19r6mt8","_id":"cm0adj59o0002dg8ceypj5yd4"},{"post_id":"cm0ad79sp00008k8cdeer5jyv","category_id":"cm0adj59f0000dg8c7qn58xcp","_id":"cm0adj59o0003dg8cdoxnh0iq"},{"post_id":"cm3z89hyq0000uw8c2ye7an7z","category_id":"cln1vnr4z0003tou5gonj1cg6","_id":"cm3z8lhjx0001588c6ek1buqx"},{"post_id":"cm3z89hyq0000uw8c2ye7an7z","category_id":"cm3z8lhjp0000588c705rhuwx","_id":"cm3z8lhjx0002588chzzzhf3r"}],"PostTag":[{"post_id":"cln1vnr4x0002tou5bo0c25p2","tag_id":"cln1vnr520009tou5hac79x63","_id":"cln1vnr55000htou53xey9mlo"},{"post_id":"cln1vnr500005tou5dhl327l4","tag_id":"cln1vnr520009tou5hac79x63","_id":"cln1vnr58000qtou55ey70idr"},{"post_id":"cln1vnr500005tou5dhl327l4","tag_id":"cln1vnr56000ktou59tgw5gez","_id":"cln1vnr58000ttou5f5g36gic"},{"post_id":"cln1vnr57000mtou575vg8qzc","tag_id":"cln1vnr56000ktou59tgw5gez","_id":"cln1vnr59000ytou56a3m4r8b"},{"post_id":"cln1vnr510006tou5048m8qov","tag_id":"cln1vnr57000otou549xlegvw","_id":"cln1vnr5a0010tou5h5qodrb4"},{"post_id":"cln1vnr520007tou52vika6pq","tag_id":"cln1vnr59000vtou5dmt5256u","_id":"cln1vnr5b001ctou552b8bk0c"},{"post_id":"cln1vnr520007tou52vika6pq","tag_id":"cln1vnr520009tou5hac79x63","_id":"cln1vnr5c001gtou5crcucn4f"},{"post_id":"cln1vnr53000btou5fi1u1wxe","tag_id":"cln1vnr5b001atou58ovsgbth","_id":"cln1vnr5d001ltou5bc8i7d0s"},{"post_id":"cln1vnr54000ctou55wf55oh0","tag_id":"cln1vnr5b001atou58ovsgbth","_id":"cln1vnr5e001stou5gbyqdxst"},{"post_id":"cln1vnr55000gtou5bknz2q2h","tag_id":"cln1vnr5b001atou58ovsgbth","_id":"cln1vnr5f001ztou5ar9x6fdt"},{"post_id":"cln1vnr56000itou57jeqfejp","tag_id":"cln1vnr5b001atou58ovsgbth","_id":"cln1vnr5h0026tou5fzg3evnc"},{"post_id":"cln1vnr56000ltou5b2dpd2bt","tag_id":"cln1vnr5b001atou58ovsgbth","_id":"cln1vnr5i002dtou531qd4en8"},{"post_id":"cln1vnr58000ptou53www8psm","tag_id":"cln1vnr5h002atou53mvc6qgt","_id":"cln1vnr5j002ktou51f7v2g6u"},{"post_id":"cln1vnr58000rtou5e46f86mt","tag_id":"cln1vnr5h002atou53mvc6qgt","_id":"cln1vnr5k002ptou50vbc73ah"},{"post_id":"cln1vnr59000wtou56isihdri","tag_id":"cln1vnr5h002atou53mvc6qgt","_id":"cln1vnr5k002utou5ci8gd8q3"},{"post_id":"cln1vnr59000ztou58dyj1jgo","tag_id":"cln1vnr5k002stou5evedhk5u","_id":"cln1vnr5k002ztou512pjaq5q"},{"post_id":"cln1vnr5a0013tou54btlgbni","tag_id":"cln1vnr5k002xtou5fv3d9xo9","_id":"cln1vnr5l0034tou50svh16x4"},{"post_id":"cln1vnr5c001htou5cnq3208o","tag_id":"cln1vnr5n003ltou55cx8gk99","_id":"cln1vnr5n003rtou5h0oe5ocb"},{"post_id":"cln1vnr5c001htou5cnq3208o","tag_id":"cln1vnr59000vtou5dmt5256u","_id":"cln1vnr5n003utou5fxyy9daa"},{"post_id":"cln1vnr5c001htou5cnq3208o","tag_id":"cln1vnr56000ktou59tgw5gez","_id":"cln1vnr5n003xtou543u6dd99"},{"post_id":"cln1vnr5d001ktou54gp9gg5i","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"cln1vnr5o0041tou5ayjqep7d"},{"post_id":"cln1vnr5d001ktou54gp9gg5i","tag_id":"cln1vnr5n003wtou5f62rbowq","_id":"cln1vnr5o0042tou5f39i9hpz"},{"post_id":"cln1vnr5e001rtou51zes7wqs","tag_id":"cln1vnr5n003wtou5f62rbowq","_id":"cln1vnr5p004dtou5feb98gee"},{"post_id":"cln1vnr5e001rtou51zes7wqs","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"cln1vnr5p004etou5a5dd2v72"},{"post_id":"clne436w9000028u5dbw0h1th","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clne436wg000128u5fju4f736"},{"post_id":"clne436w9000028u5dbw0h1th","tag_id":"cln1vnr5n003wtou5f62rbowq","_id":"clne436wg000228u5blpv1f9q"},{"post_id":"clnjr0wpf0000rou5hi9ibg4a","tag_id":"cln1vnr5n003wtou5f62rbowq","_id":"clnjr0wpm0001rou5e8vv9dh2"},{"post_id":"clnjr0wpf0000rou5hi9ibg4a","tag_id":"cln1vnr5m003htou54t7k98rv","_id":"clnjr0wpm0002rou59vlq4lao"},{"post_id":"clo44jtk40000g0u5dfmkejl5","tag_id":"clo44jtkg0001g0u50jdw9y6o","_id":"clo44jtkq0003g0u56eed9qes"},{"post_id":"clo44jtk40000g0u5dfmkejl5","tag_id":"clo44jtkp0002g0u54hz32fqq","_id":"clo44jtkq0004g0u5ga69bd02"},{"post_id":"clo4gan3f00009cu580hn4ios","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clo4gan3h00019cu54zqefoe6"},{"post_id":"clo4gan3f00009cu580hn4ios","tag_id":"cln1vnr5m003htou54t7k98rv","_id":"clo4gan3h00029cu5e50f5ezs"},{"post_id":"clo5xc63m000008u57cfg4n7d","tag_id":"cln1vnr5l0037tou5904i8go8","_id":"clo5xc63r000108u52d6rdfzb"},{"post_id":"clo8stmx80000548cbgrv32y9","tag_id":"cln1vnr5m003htou54t7k98rv","_id":"clo9fqeqm0000v48c7r3g975n"},{"post_id":"clo8stmx80000548cbgrv32y9","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clo9fqeqm0001v48cfbur5ewz"},{"post_id":"clof2n7700000rk8c4g6sf4ry","tag_id":"cln1vnr5l0037tou5904i8go8","_id":"clof2n77e0001rk8c72ltehfr"},{"post_id":"clt9kr11r0000vw8c7idr7zk9","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr11w0002vw8c3xmh23bh"},{"post_id":"clt9kr11v0001vw8c4uca0evn","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr11x0004vw8cey6gdlv8"},{"post_id":"clt9kr11w0003vw8cek8cdiee","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr11y0007vw8c7d9cbloy"},{"post_id":"clt9kr11x0005vw8c33u725wx","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr11z000avw8c8n9nckyo"},{"post_id":"clt9kr11y0008vw8c39iw5btz","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr120000dvw8ccrmi8nnq"},{"post_id":"clt9kr11z000bvw8c6ein5fn7","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr120000gvw8cb9dc9kfq"},{"post_id":"clt9kr120000hvw8cdha62zj3","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr122000lvw8ch1is9c0p"},{"post_id":"clt9kr121000jvw8c614ueu3h","tag_id":"cln1vnr5o0043tou5fjtba555","_id":"clt9kr123000ovw8cea66ct3p"},{"post_id":"clt9kr121000jvw8c614ueu3h","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clt9kr124000rvw8c5q1jd4y6"},{"post_id":"clt9kr122000mvw8c8llb54rr","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clt9kr124000uvw8chclwdqpg"},{"post_id":"clt9kr122000mvw8c8llb54rr","tag_id":"cln1vnr5o0043tou5fjtba555","_id":"clt9kr125000xvw8cdngpbsks"},{"post_id":"clt9kr123000pvw8c2ko81xt0","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr1260010vw8c7nof5sjo"},{"post_id":"clt9kr124000svw8c7nfla28m","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr1260013vw8cevkx10q4"},{"post_id":"clt9kr124000vvw8c9kmt6st8","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clt9kr1270016vw8cdxbs0url"},{"post_id":"clt9kr125000yvw8cd8b0e20x","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clt9kr1280019vw8c62sr174m"},{"post_id":"clt9kr1260011vw8c39ji8f3l","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clt9kr128001cvw8c7xstaiah"},{"post_id":"clt9kr1260011vw8c39ji8f3l","tag_id":"cln1vnr5o0043tou5fjtba555","_id":"clt9kr129001fvw8c0r211jbn"},{"post_id":"clt9kr1260014vw8c6ildb8kk","tag_id":"cln1vnr5l0032tou583gfbdds","_id":"clt9kr129001ivw8cccwz3yhy"},{"post_id":"clt9kr1270017vw8cdn33gn9p","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12a001lvw8c47l93y89"},{"post_id":"clt9kr128001avw8c97lmc54b","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clt9kr12b001ovw8c4pqb8nrd"},{"post_id":"clt9kr128001dvw8c4xk51xbc","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12b001rvw8c0297f6ld"},{"post_id":"clt9kr129001gvw8cb0l3hj4h","tag_id":"cln1vnr5n003qtou565nr3zku","_id":"clt9kr12c001uvw8c6y4jabw1"},{"post_id":"clt9kr12a001mvw8c7fa5d1n5","tag_id":"cln1vnr5l0032tou583gfbdds","_id":"clt9kr12c001xvw8cbbm4dzte"},{"post_id":"clt9kr12a001mvw8c7fa5d1n5","tag_id":"cloqgf7hl00017c8cabmqfjkd","_id":"clt9kr12d0020vw8cbs0tgxuz"},{"post_id":"clt9kr12b001pvw8cel9u1pa7","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12d0023vw8cf0nda2v8"},{"post_id":"clt9kr12b001svw8c8l96hhyh","tag_id":"cln1vnr5s0054tou517v84gye","_id":"clt9kr12e0026vw8cfa7f28vd"},{"post_id":"clt9kr12c001vvw8c68z32q8p","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12e0028vw8c117s6nr9"},{"post_id":"clt9kr12c001vvw8c68z32q8p","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12e002avw8c8izy22zd"},{"post_id":"clt9kr12c001yvw8camv3dces","tag_id":"cln1vnr5l0032tou583gfbdds","_id":"clt9kr12e002cvw8c8ebi2b4s"},{"post_id":"clt9kr12d0021vw8cee0dfhuv","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12e002evw8c48a136e6"},{"post_id":"clt9kr12d0024vw8cgpk19t43","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12e002gvw8cemv77d6y"},{"post_id":"clt9kr12g003wvw8cfmtuftxy","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12g003yvw8cgu7pdbw2"},{"post_id":"clt9kr12g003xvw8c4iui360g","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12h0040vw8c0149fwr1"},{"post_id":"clt9kr12h003zvw8c8gkadmuy","tag_id":"cln1vnr5m003htou54t7k98rv","_id":"clt9kr12i0043vw8c53rz3fym"},{"post_id":"clt9kr12h003zvw8c8gkadmuy","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12i0046vw8cgr6j85ns"},{"post_id":"clt9kr12h0041vw8c96up9h3y","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12j0049vw8cec8e5ko0"},{"post_id":"clt9kr12h0041vw8c96up9h3y","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12j004cvw8cbglo7bvu"},{"post_id":"clt9kr12i0044vw8c54tu48gt","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clt9kr12k004fvw8cgs7s2ql3"},{"post_id":"clt9kr12i0047vw8c2ayz424a","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12k004ivw8ccw428pgu"},{"post_id":"clt9kr12j004avw8c9k8vg4hb","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clt9kr12k004lvw8cbh59ep6p"},{"post_id":"clt9kr12j004avw8c9k8vg4hb","tag_id":"cln1vnr5p004ftou59plr8d6y","_id":"clt9kr12l004ovw8ch0j6hye3"},{"post_id":"clt9kr12j004dvw8canyq6tw7","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12l004rvw8cfj0v4uub"},{"post_id":"clt9kr12k004gvw8c56wn86f4","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12m004uvw8cbsfodvi4"},{"post_id":"clt9kr12k004jvw8cer857c0t","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clt9kr12m004xvw8c1jxw7bka"},{"post_id":"clt9kr12k004jvw8cer857c0t","tag_id":"cln1vnr5p004ftou59plr8d6y","_id":"clt9kr12n0050vw8cado3cltj"},{"post_id":"clt9kr12k004mvw8cbyc02qz0","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clt9kr12n0052vw8cbmee2us8"},{"post_id":"clt9kr12l004pvw8c16br7ue3","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12n0054vw8c7uahe6wb"},{"post_id":"clt9kr12l004svw8c1m6sgu9l","tag_id":"cln1vnr5r004utou551b2d0ww","_id":"clt9kr12n0056vw8carwac31i"},{"post_id":"clt9kr12l004svw8c1m6sgu9l","tag_id":"cln1vnr5s0050tou50fnr2jaw","_id":"clt9kr12n0058vw8cdgxo7hjt"},{"post_id":"clt9kr12m004vvw8c7ybt27l6","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clt9kr12n005avw8c5b49h7w5"},{"post_id":"clt9kr12m004yvw8c0j5r44bt","tag_id":"cln1vnr5l003ctou5cih0bc5j","_id":"clt9kr12n005cvw8c47sd4og9"}],"Tag":[{"name":"TODO","_id":"cln1vnr500004tou51ahsavhx"},{"name":"Git","_id":"cln1vnr520009tou5hac79x63"},{"name":"Obsidian","_id":"cln1vnr56000ktou59tgw5gez"},{"name":"StableDiffusion","_id":"cln1vnr57000otou549xlegvw"},{"name":"Github","_id":"cln1vnr59000vtou5dmt5256u"},{"name":"Linux","_id":"cln1vnr5b001atou58ovsgbth"},{"name":"西安","_id":"cln1vnr5h002atou53mvc6qgt"},{"name":"powershell","_id":"cln1vnr5k002stou5evedhk5u"},{"name":"WSL","_id":"cln1vnr5k002xtou5fv3d9xo9"},{"name":"智能计算系统","_id":"cln1vnr5l0032tou583gfbdds"},{"name":"扩散模型","_id":"cln1vnr5l0037tou5904i8go8"},{"name":"软件工程","_id":"cln1vnr5l003ctou5cih0bc5j"},{"name":"Python","_id":"cln1vnr5m003htou54t7k98rv"},{"name":"hexo","_id":"cln1vnr5n003ltou55cx8gk99"},{"name":"数据库","_id":"cln1vnr5n003qtou565nr3zku"},{"name":"MySQL","_id":"cln1vnr5n003wtou5f62rbowq"},{"name":"SQL","_id":"cln1vnr5o0043tou5fjtba555"},{"name":"作业","_id":"cln1vnr5p004ftou59plr8d6y"},{"name":"神经网络","_id":"cln1vnr5r004utou551b2d0ww"},{"name":"深度学习","_id":"cln1vnr5s0050tou50fnr2jaw"},{"name":"操作系统","_id":"cln1vnr5s0054tou517v84gye"},{"name":"Vue","_id":"clno1l11c0001s4u55flx1en5"},{"name":"训练","_id":"clo44jtkg0001g0u50jdw9y6o"},{"name":"三分化","_id":"clo44jtkp0002g0u54hz32fqq"},{"name":"Pytorch","_id":"cloqgf7hl00017c8cabmqfjkd"}]}}